diff --git a/.clang-tidy b/.clang-tidy
index 75953817cd..3d63a5cafe 100644
--- a/.clang-tidy
+++ b/.clang-tidy
@@ -53,6 +53,7 @@ Checks: >
   -cppcoreguidelines-avoid-magic-numbers,
   -cppcoreguidelines-avoid-non-const-global-variables,
   -cppcoreguidelines-c-copy-assignment-signature,
+  -cppcoreguidelines-explicit-virtual-functions,
   -cppcoreguidelines-init-variables,
   -cppcoreguidelines-macro-usage,
   -cppcoreguidelines-misleading-capture-default-by-value,
@@ -117,6 +118,7 @@ Checks: >
   -hicpp-use-emplace,
   -hicpp-use-equals-default,
   -hicpp-use-nullptr,
+  -hicpp-use-override,
   -hicpp-vararg,
   -llvm-else-after-return,
   -llvm-header-guard,
@@ -154,6 +156,7 @@ Checks: >
   -modernize-use-equals-default,
   -modernize-use-nodiscard,
   -modernize-use-nullptr,
+  -modernize-use-override,
   -modernize-use-trailing-return-type,
   -modernize-use-transparent-functors,
   -modernize-use-using,
@@ -166,6 +169,7 @@ Checks: >
   -performance-no-int-to-ptr,
   -performance-noexcept-move-constructor,
   -performance-noexcept-swap,
+  -performance-type-promotion-in-math-fn,
   -performance-unnecessary-copy-initialization,
   -performance-unnecessary-value-param,
   -portability-simd-intrinsics,
diff --git a/.github/CODEOWNERS b/.github/CODEOWNERS
index 4861c84a2e..d54a008ed1 100644
--- a/.github/CODEOWNERS
+++ b/.github/CODEOWNERS
@@ -30,9 +30,9 @@ tt_stl/**/CMakeLists.txt @patrickroberts @ayerofieiev-tt @dmakoviichuk-tt @smina
 tt_metal/**/CMakeLists.txt @tenstorrent/metalium-developers-infra
 tt_metal/ @tenstorrent/metalium-developers-infra # nothing should be caught by this, if so, someone added a path somewhere
 tt_metal/api/ @tenstorrent/metalium-api-owners
-tt_metal/common/ @abhullar-tt @tt-aho @tt-asaigal @omilyutin-tt @cfjchu
+tt_metal/common/ @abhullar-tt @tt-aho @tt-asaigal @omilyutin-tt
 tt_metal/core_descriptors/ @abhullar-tt @aliuTT @ubcheema
-tt_metal/detail/ @abhullar-tt @tt-aho @tt-asaigal @omilyutin-tt @cfjchu # this should go away
+tt_metal/detail/ @abhullar-tt @tt-aho @tt-asaigal @omilyutin-tt # this should go away
 tt_metal/distributed/ @cfjchu @aliuTT @tt-asaigal @omilyutin-tt
 tt_metal/distributed/**/CMakeLists.txt @cfjchu @aliuTT @tt-asaigal @omilyutin-tt @tenstorrent/metalium-developers-infra
 tt_metal/fabric/ @ubcheema @aliuTT @aagarwalTT @tt-aho @SeanNijjar @yugaoTT @daminakaTT
@@ -48,11 +48,11 @@ tt_metal/hw/firmware/ @abhullar-tt @jbaumanTT @pgkeller @nathan-TT
 tt_metal/hw/toolchain/ @jbaumanTT @pgkeller @nathan-TT
 tt_metal/hw/**/CMakeLists.txt @tenstorrent/metalium-developers-infra @jbaumanTT @pgkeller @nathan-TT
 tt_metal/impl/allocator/ @abhullar-tt @tt-aho
-tt_metal/impl/buffers/ @abhullar-tt @jbaumanTT @cfjchu @omilyutin-tt @sminakov-tt @TT-BrianLiu
-tt_metal/impl/context/ @tt-dma @jbaumanTT @aliuTT @cfjchu @omilyutin-tt
+tt_metal/impl/buffers/ @abhullar-tt @jbaumanTT @omilyutin-tt @sminakov-tt @TT-BrianLiu
+tt_metal/impl/context/ @tt-dma @jbaumanTT @aliuTT
 tt_metal/impl/data_format/ @rtawfik01 @rdjogoTT @ttmtrajkovic @nvelickovicTT
 tt_metal/impl/debug/ @tt-dma @pgkeller
-tt_metal/impl/device/ @abhullar-tt @aliuTT @tt-asaigal @tt-aho @tt-dma @cfjchu @omilyutin-tt
+tt_metal/impl/device/ @abhullar-tt @aliuTT @tt-asaigal @tt-aho @tt-dma
 tt_metal/impl/dispatch/ @pgkeller @jbaumanTT @nhuang-tt @mpiseTT @tt-asaigal @tt-aho
 tt_metal/impl/dispatch/kernels/packet_* @ubcheema @aliuTT
 tt_metal/impl/event @jbaumanTT @nhuang-tt @mpiseTT @tt-asaigal @tt-aho
@@ -90,7 +90,7 @@ tt_metal/third_party/tt_llk @rtawfik01 @ttmtrajkovic @rdjogoTT @nvelickovicTT #d
 tests/tt_metal/distributed/ @cfjchu @tt-asaigal @omilyutin-tt
 tests/tt_metal/distributed/**/CMakeLists.txt @cfjchu @tt-asaigal @omilyutin-tt @tenstorrent/metalium-developers-infra
 tests/tt_metal/microbenchmarks/ethernet/ @ubcheema @aliuTT @aagarwalTT @tt-aho @SeanNijjar @yugaoTT @daminakaTT
-tests/tt_metal/tt_metal/ @abhullar-tt @aagarwalTT @jbaumanTT @tt-asaigal @nhuang-tt @tt-aho @cfjchu @omilyutin-tt
+tests/tt_metal/tt_metal/ @abhullar-tt @aagarwalTT @jbaumanTT @tt-asaigal @nhuang-tt @tt-aho
 tests/tt_metal/tt_metal/perf_microbenchmark/routing/ @ubcheema @aagarwalTT
 tests/tt_metal/tt_metal/sfpi/ @nathan-TT @pgkeller
 tests/tt_metal/tt_metal/test_kernels/sfpi/ @nathan-TT @pgkeller
@@ -116,8 +116,8 @@ ttnn/cpp/ttnn/deprecated/tt_lib/csrc/ @ayerofieiev-tt @razorback3 @dongjin-na
 
 ttnn/cpp/ttnn/operations/moreh*/ @razorback3 @dongjin-na @cfjchu @ayerofieiev-tt @dmakoviichuk-tt
 ttnn/cpp/ttnn/operations/moreh*/**/CMakeLists.txt @razorback3 @dongjin-na @cfjchu @ayerofieiev-tt @dmakoviichuk-tt @tenstorrent/metalium-developers-infra
-ttnn/cpp/ttnn/operations/ccl/ @SeanNijjar @cfjchu @omilyutin-tt @jvegaTT @tt-aho @sjameelTT @ntarafdar @nardoTT @llongTT @amorrisonTT
-ttnn/cpp/ttnn/operations/ccl/**/CMakeLists.txt @SeanNijjar @cfjchu @omilyutin-tt @jvegaTT @tt-aho @sjameelTT @ntarafdar @nardoTT @llongTT @amorrisonTT @tenstorrent/metalium-developers-infra
+ttnn/cpp/ttnn/operations/ccl/ @SeanNijjar @cfjchu @jvegaTT @tt-aho @sjameelTT @ntarafdar @nardoTT @llongTT @amorrisonTT
+ttnn/cpp/ttnn/operations/ccl/**/CMakeLists.txt @SeanNijjar @cfjchu @jvegaTT @tt-aho @sjameelTT @ntarafdar @nardoTT @llongTT @amorrisonTT @tenstorrent/metalium-developers-infra
 ttnn/cpp/ttnn/operations/pool/ @tenstorrent/metalium-developers-convolutions
 ttnn/cpp/ttnn/operations/pool/**/CMakeLists.txt @tenstorrent/metalium-developers-convolutions @tenstorrent/metalium-developers-infra
 ttnn/cpp/ttnn/operations/conv/ @tenstorrent/metalium-developers-convolutions
@@ -164,8 +164,8 @@ ttnn/cpp/ttnn/operations/experimental/transformer/rotary_embedding_llama_fused_q
 ttnn/tracy/ @mo-tenstorrent @sagarwalTT
 ttnn/api/tools/profiler/ @mo-tenstorrent
 tests/ttnn/ @tenstorrent/metalium-developers-ttnn-core @razorback3 @dongjin-na @bbradelTT
-tests/ttnn/unit_tests/gtests/ccl/ @SeanNijjar @jvegaTT @cfjchu @omilyutin-tt @tt-aho @sjameelTT @ntarafdar @nardoTT @llongTT @amorrisonTT
-tests/ttnn/unit_tests/operations/ccl/ @SeanNijjar @jvegaTT @cfjchu @omilyutin-tt @tt-aho @sjameelTT @ntarafdar @nardoTT @llongTT @amorrisonTT
+tests/ttnn/unit_tests/gtests/ccl/ @SeanNijjar @jvegaTT @cfjchu @tt-aho @sjameelTT @ntarafdar @nardoTT @llongTT @amorrisonTT
+tests/ttnn/unit_tests/operations/ccl/ @SeanNijjar @jvegaTT @tt-aho @sjameelTT @ntarafdar @nardoTT @llongTT @amorrisonTT
 tests/ttnn/unit_tests/operations/eltwise/ @patrickroberts @sjameelTT @ntarafdar @dchenTT
 tests/ttnn/unit_tests/operations/conv/ @tenstorrent/metalium-developers-convolutions
 tests/ttnn/unit_tests/operations/pool/ @tenstorrent/metalium-developers-convolutions
diff --git a/.github/workflows/all-post-commit-workflows.yaml b/.github/workflows/all-post-commit-workflows.yaml
index c3dda11564..00840cefbf 100644
--- a/.github/workflows/all-post-commit-workflows.yaml
+++ b/.github/workflows/all-post-commit-workflows.yaml
@@ -228,6 +228,11 @@ jobs:
             build-type: "Debug"
             publish-artifact: false
             skip-tt-train: false
+          - version: "24.04"
+            toolchain: "cmake/x86_64-linux-clang-17-libstdcpp-toolchain.cmake"
+            build-type: "Release"
+            publish-artifact: false
+            skip-tt-train: false
           - version: "22.04"
             toolchain: "cmake/x86_64-linux-clang-17-libcpp-toolchain.cmake"
             build-type: "Release"
@@ -238,16 +243,6 @@ jobs:
             build-type: "Release"
             publish-artifact: false
             skip-tt-train: true
-          #- version: "22.04"
-          #  toolchain: "cmake/x86_64-linux-gcc-12-toolchain.cmake"
-          #  build-type: "Debug"
-          #  publish-artifact: false
-          #  skip-tt-train: true
-          - version: "24.04"
-            toolchain: "cmake/x86_64-linux-clang-17-libstdcpp-toolchain.cmake"
-            build-type: "Release"
-            publish-artifact: false
-            skip-tt-train: false
     with:
       version: ${{ matrix.config.version }}
       toolchain: ${{ matrix.config.toolchain }}
diff --git a/.github/workflows/blackhole-llmbox-demo-tests-impl.yaml b/.github/workflows/blackhole-llmbox-demo-tests-impl.yaml
deleted file mode 100644
index 0718f1704d..0000000000
--- a/.github/workflows/blackhole-llmbox-demo-tests-impl.yaml
+++ /dev/null
@@ -1,90 +0,0 @@
-name: "[internal] Blackhole LLMBox Demo tests impl"
-
-on:
-  workflow_call:
-    inputs:
-      build-artifact-name:
-        required: true
-        type: string
-      wheel-artifact-name:
-        required: true
-        type: string
-      docker-image:
-        required: true
-        type: string
-      runner-label:
-        required: false
-        type: string
-        default: "BH-LLMBox"
-
-jobs:
-  single-card-demo-tests:
-    strategy:
-      fail-fast: false
-      matrix:
-        test-group: [
-          {
-            name: "llama3-8b quietbox data-parallel=4 performance",
-            arch: blackhole,
-            cmd:  LLAMA_DIR=/localdev/blackhole_demos/huggingface_data/meta-llama/Llama-3.1-8B-Instruct pytest models/tt_transformers/demo/simple_text_demo.py -k "performance and ci-32" --data_parallel 4,
-            owner_id: U05RWH3QUPM # Salar Hosseini
-          }
-        ]
-    name: ${{ matrix.test-group.name }}
-    runs-on: ["in-service", "${{ inputs.runner-label }}", "pipeline-perf"]
-    steps:
-      - name: â¬‡ï¸ Checkout
-        uses: actions/checkout@v4
-        with:
-          submodules: recursive
-      - name: â¬‡ï¸ Download Build
-        uses: actions/download-artifact@v4
-        timeout-minutes: 10
-        with:
-          name: ${{ inputs.build-artifact-name }}
-      - name: Extract files
-        run: tar -xvf ttm_any.tar
-      - name: â¬‡ï¸ Download Wheel
-        uses: actions/download-artifact@v4
-        timeout-minutes: 10
-        with:
-          name: ${{ inputs.wheel-artifact-name }}
-      - name: Enable Performance mode
-        if: ${{ contains(matrix.test-group.name, 'performance') }}
-        run: |
-          sudo cpupower frequency-set -g performance
-      - name: Run demo regression tests
-        uses: ./.github/actions/docker-run
-        timeout-minutes: 70
-        env:
-          LOGURU_LEVEL: INFO
-        with:
-          docker_image: ${{ inputs.docker-image }}
-          docker_password: ${{ secrets.GITHUB_TOKEN }}
-          docker_opts: |
-            -e TT_METAL_HOME=${{ github.workspace }}
-            -e ARCH_NAME=${{ matrix.test-group.arch }}
-            -e LD_LIBRARY_PATH=${{ github.workspace }}/build/lib
-            -e HF_TOKEN=${{ secrets.HUGGINGFACE_TOKEN }}
-            -v /localdev/blackhole_demos:/localdev/blackhole_demos:ro
-          install_wheel: true
-          run_args: |
-            if [[ "${{ matrix.test-group.name }}" == *"llama"* ]]; then
-              pip install -r ${{ github.workspace }}/models/tt_transformers/requirements.txt
-            fi
-            ${{ matrix.test-group.cmd }}
-      - uses: tenstorrent/tt-metal/.github/actions/upload-artifact-with-job-uuid@main
-        timeout-minutes: 10
-        if: ${{ !cancelled() }}
-        with:
-          path: generated/test_reports/
-          prefix: "test_reports_"
-      - name: Disable Performance mode
-        if: ${{ contains(matrix.test-group.name, 'performance') }}
-        run: |
-          sudo cpupower frequency-set -g ondemand
-      - uses: tenstorrent/tt-metal/.github/actions/slack-report@main
-        if: ${{ failure() }}
-        with:
-          slack_webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
-          owner: ${{ matrix.test-group.owner_id }}
diff --git a/.github/workflows/blackhole-llmbox-fabric-build-and-unit-tests.yaml b/.github/workflows/blackhole-llmbox-fabric-build-and-unit-tests.yaml
deleted file mode 100644
index 9c33901cd6..0000000000
--- a/.github/workflows/blackhole-llmbox-fabric-build-and-unit-tests.yaml
+++ /dev/null
@@ -1,86 +0,0 @@
-name: "[internal] Blackhole LLMBox Fabric unit tests impl"
-
-on:
-  workflow_call:
-    inputs:
-      arch:
-        required: true
-        type: string
-      runner-label:
-        required: true
-        type: string
-      timeout:
-        required: false
-        type: number
-        default: 10
-      build-artifact-name:
-        required: true
-        type: string
-      docker-image:
-        required: true
-        type: string
-      wheel-artifact-name:
-        required: true
-        type: string
-
-jobs:
-  fabric-tests:
-    strategy:
-      # Do not fail-fast because we need to ensure all tests go to completion
-      # so we try not to get hanging machines
-      fail-fast: false
-      matrix:
-        test-group: [
-          {name: fabric 1D unit tests, cmd: ./build/test/tt_metal/tt_fabric/fabric_unit_tests --gtest_filter="Fabric1D.*" },
-          {name: fabric 2D fixture unit tests, cmd: ./build/test/tt_metal/tt_fabric/fabric_unit_tests --gtest_filter="Fabric2D*Fixture.*" },
-          {name: fabric system health tests, cmd: ./build/test/tt_metal/tt_fabric/test_system_health },
-          # {name: t3000 fast fabric tests, cmd: "source tests/scripts/t3000/run_t3000_unit_tests.sh && run_t3000_ttfabric_tests" },
-        ]
-    name: ${{ inputs.arch }} ${{ inputs.runner-label }} ${{ matrix.test-group.name }}
-    runs-on: >-
-      ${{
-        ((inputs.runner-label == 'N150' || inputs.runner-label == 'N300') && format('tt-beta-ubuntu-2204-{0}-large-stable', inputs.runner-label))
-        || github.event.pull_request.head.repo.fork == true && format('tt-beta-ubuntu-2204-{0}-large-stable', inputs.runner-label)
-        || fromJSON(format('["{0}", "in-service", "cloud-virtual-machine"]', inputs.runner-label))
-      }}
-    container:
-      image: ${{ inputs.docker-image || 'docker-image-unresolved' }}
-      env:
-        ARCH_NAME: ${{ inputs.arch }}
-        LOGURU_LEVEL: INFO
-        LD_LIBRARY_PATH: /work/build/lib
-        PYTHONPATH: /work
-        TT_METAL_HOME: /work
-        GTEST_OUTPUT: xml:/work/generated/test_reports/
-      volumes:
-        - ${{ github.workspace }}/docker-job:/work # Subdir to workaround https://github.com/actions/runner/issues/691
-        - /dev/hugepages-1G:/dev/hugepages-1G
-      options: "--device /dev/tenstorrent"
-    defaults:
-      run:
-        shell: bash
-        working-directory: /work # https://github.com/actions/runner/issues/878
-    steps:
-      - name: â¬‡ï¸  Setup Job
-        uses: tenstorrent/tt-metal/.github/actions/setup-job@main
-        timeout-minutes: 10
-        with:
-          build-artifact-name: ${{ inputs.build-artifact-name }}
-          wheel-artifact-name: ${{ inputs.wheel-artifact-name }}
-      - name: ${{ matrix.test-group.name }} tests
-        timeout-minutes: ${{ inputs.timeout }}
-        run: |
-          ${{ matrix.test-group.cmd }}
-      - uses: tenstorrent/tt-metal/.github/actions/slack-report@main
-        if: ${{ failure() }}
-        with:
-          slack_webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
-          owner: U06CXU895AP # Michael Chiou
-      - uses: tenstorrent/tt-metal/.github/actions/upload-artifact-with-job-uuid@main
-        timeout-minutes: 10
-        if: ${{ !cancelled() }}
-        with:
-          prefix: "test_reports_"
-      - name: Generate gtest annotations on failure
-        uses: tenstorrent/tt-metal/.github/actions/generate-gtest-failure-message@main
-        if: ${{ failure() }}
diff --git a/.github/workflows/blackhole-nightly-tests.yaml b/.github/workflows/blackhole-nightly-tests.yaml
index dd0cced34c..59227c6f3d 100644
--- a/.github/workflows/blackhole-nightly-tests.yaml
+++ b/.github/workflows/blackhole-nightly-tests.yaml
@@ -82,22 +82,3 @@ jobs:
       docker-image: ${{ needs.build-artifact.outputs.dev-docker-image }}
       build-artifact-name: ${{ needs.build-artifact.outputs.build-artifact-name }}
       wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
-  blackhole-llmbox-demo-tests:
-    needs: build-artifact
-    secrets: inherit
-    uses: ./.github/workflows/blackhole-llmbox-demo-tests-impl.yaml
-    with:
-      runner-label: BH-LLMBox
-      docker-image: ${{ needs.build-artifact.outputs.dev-docker-image }}
-      build-artifact-name: ${{ needs.build-artifact.outputs.build-artifact-name }}
-      wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
-  blackhole-llmbox-fabric-unit-tests:
-    needs: build-artifact
-    secrets: inherit
-    uses: ./.github/workflows/blackhole-llmbox-fabric-build-and-unit-tests.yaml
-    with:
-      arch: blackhole
-      runner-label: BH-LLMBox
-      docker-image: ${{ needs.build-artifact.outputs.dev-docker-image }}
-      build-artifact-name: ${{ needs.build-artifact.outputs.build-artifact-name }}
-      wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
diff --git a/.github/workflows/blackhole-post-commit.yaml b/.github/workflows/blackhole-post-commit.yaml
index 3540840128..2da72c25ca 100644
--- a/.github/workflows/blackhole-post-commit.yaml
+++ b/.github/workflows/blackhole-post-commit.yaml
@@ -8,16 +8,12 @@ on:
           required: false
           type: string
           default: 'BH'
-      enable-ttnn-unit-tests:
-          description: 'Enable ttnn unit tests'
-          default: false
-          type: boolean
       enable-watcher:
           description: 'Enable watcher in BH Post commit'
           default: false
           type: boolean
-      enable-llmbox-tests:
-          description: 'Run tests on LLMBox instead of single card (must set runner-label to BH-LLMBox)'
+      enable-ttnn-unit-tests:
+          description: 'Enable ttnn unit tests'
           default: false
           type: boolean
   workflow_dispatch:
@@ -38,16 +34,12 @@ on:
           - RelWithDebInfo
           - ASan
           - TSan
-      enable-ttnn-unit-tests:
-        description: 'Enable ttnn unit tests'
-        default: false
-        type: boolean
       enable-watcher:
         description: 'Enable watcher in BH Post commit'
         default: false
         type: boolean
-      enable-llmbox-tests:
-        description: 'Run tests on LLMBox instead of single card (must set runner-label to BH-LLMBox)'
+      enable-ttnn-unit-tests:
+        description: 'Enable ttnn unit tests'
         default: false
         type: boolean
   schedule:
@@ -56,7 +48,7 @@ on:
   # push:
   #  branches: ["main"]
 
-run-name: ${{ inputs.enable-llmbox-tests == true && 'Blackhole LLMBox tests' || (inputs.enable-watcher == true && 'Blackhole post-commit tests (watcher enabled) ' || 'Blackhole post-commit tests') }}
+run-name: ${{ inputs.enable-watcher == true && 'Blackhole post-commit tests (watcher enabled) ' || 'Blackhole post-commit tests' }}
 
 permissions:
   actions: read
@@ -68,23 +60,6 @@ permissions:
   checks: write
 
 jobs:
-  generate-matrix:
-    runs-on: ubuntu-latest
-    outputs:
-      matrix: ${{ steps.set-matrix.outputs.matrix }}
-    steps:
-      - id: set-matrix
-        run: |
-          if [ "${{ inputs.enable-llmbox-tests }}" = "true" ]; then
-            if [ "${{ inputs.runner-label }}" != "BH-LLMBox" ]; then
-              echo "::warning::LLMBox tests are enabled but runner-label is not set to BH-LLMBox. Current value: ${{ inputs.runner-label }}"
-            fi
-            matrix='["BH-LLMBox"]'
-          else
-            matrix='["P100", "P150"]'
-          fi
-          echo "matrix=$matrix" >> $GITHUB_OUTPUT
-
   build-artifact:
     uses: ./.github/workflows/build-artifact.yaml
     permissions:
@@ -111,6 +86,7 @@ jobs:
     secrets: inherit
     with:
       arch: "blackhole"
+      timeout: 20
       runner-label: ${{ inputs.runner-label || 'BH' }}
       docker-image: ${{ needs.build-artifact-profiler.outputs.dev-docker-image }}
       build-artifact-name: ${{ needs.build-artifact-profiler.outputs.build-artifact-name }}
@@ -132,6 +108,7 @@ jobs:
     with:
       arch: blackhole
       runner-label: ${{ inputs.runner-label || 'BH' }}
+      timeout: 15
       docker-image: ${{ needs.build-artifact.outputs.dev-docker-image }}
       build-artifact-name: ${{ needs.build-artifact.outputs.build-artifact-name }}
       wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
@@ -161,16 +138,19 @@ jobs:
       wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
       enable-watcher: ${{ inputs.enable-watcher || false }}
   models-unit-tests:
-    needs: [build-artifact, generate-matrix]
+    needs: build-artifact
     secrets: inherit
     uses: ./.github/workflows/models-post-commit.yaml
     strategy:
       fail-fast: false
       matrix:
-        test-group: ${{ fromJson(needs.generate-matrix.outputs.matrix) }}
+        test-group: [
+          { runner-label: P100 },
+          { runner-label: P150 },
+        ]
     with:
       arch: blackhole
-      runner-label: ${{ matrix.test-group }}
+      runner-label: ${{ matrix.test-group.runner-label }}
       timeout: 20
       docker-image: ${{ needs.build-artifact.outputs.dev-docker-image }}
       wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
@@ -180,7 +160,7 @@ jobs:
     secrets: inherit
     uses: ./.github/workflows/blackhole-demo-tests-impl.yaml
     with:
-      runner-label: ${{ inputs.runner-label || 'BH' }}
+      runner-label: BH
       docker-image: ${{ needs.build-artifact.outputs.dev-docker-image }}
       build-artifact-name: ${{ needs.build-artifact.outputs.build-artifact-name }}
       wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
@@ -198,70 +178,37 @@ jobs:
       wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
 
   ttnn-stress-tests:
-    needs: [build-artifact, generate-matrix]
+    needs: build-artifact
     secrets: inherit
     uses: ./.github/workflows/ttnn-stress-tests-impl.yaml
     strategy:
       fail-fast: false
       matrix:
-        test-group: ${{ fromJson(needs.generate-matrix.outputs.matrix) }}
+        test-group: [
+          { runner-label: P100 },
+          { runner-label: P150 },
+        ]
     with:
       arch: blackhole
-      runner-label: ${{ matrix.test-group }}
+      runner-label: ${{ matrix.test-group.runner-label }}
       timeout: 45
       docker-image: ${{ needs.build-artifact.outputs.dev-docker-image }}
       build-artifact-name: ${{ needs.build-artifact.outputs.build-artifact-name }}
       wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
-  metalium-smoke-tests:
-    needs: [build-artifact, generate-matrix]
-    strategy:
-      fail-fast: false
-      matrix:
-        platform: ${{ fromJson(needs.generate-matrix.outputs.matrix) }}
-    uses: ./.github/workflows/smoke.yaml
-    with:
-      docker-image: ${{ needs.build-artifact.outputs.dev-docker-image }}
-      package-artifact-name: ${{ needs.build-artifact.outputs.packages-artifact-name }}
-      runner: ${{ matrix.platform }}
-      product: tt-metalium
-  ttnn-smoke-tests:
-    needs: [build-artifact, generate-matrix]
+  smoke-tests:
+    needs: build-artifact
     strategy:
       fail-fast: false
       matrix:
-        platform: ${{ fromJson(needs.generate-matrix.outputs.matrix) }}
+        platform: [
+          "P100",
+          "P150",
+        ]
     uses: ./.github/workflows/smoke.yaml
     with:
       docker-image: ${{ needs.build-artifact.outputs.dev-docker-image }}
       package-artifact-name: ${{ needs.build-artifact.outputs.packages-artifact-name }}
       runner: ${{ matrix.platform }}
-      product: tt-nn
-
-  # LLMBox-only demo tests
-  blackhole-llmbox-demo-tests:
-    needs: build-artifact
-    if: ${{ inputs.enable-llmbox-tests }}
-    secrets: inherit
-    uses: ./.github/workflows/blackhole-llmbox-demo-tests-impl.yaml
-    with:
-      runner-label: ${{ inputs.runner-label || 'BH' }}
-      docker-image: ${{ needs.build-artifact.outputs.dev-docker-image }}
-      build-artifact-name: ${{ needs.build-artifact.outputs.build-artifact-name }}
-      wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
-
-  # LLMBox-only fabric tests
-  blackhole-llmbox-fabric-unit-tests:
-    needs: build-artifact
-    if: ${{ inputs.enable-llmbox-tests }}
-    secrets: inherit
-    uses: ./.github/workflows/blackhole-llmbox-fabric-build-and-unit-tests.yaml
-    with:
-      arch: blackhole
-      runner-label: ${{ inputs.runner-label || 'BH' }}
-      docker-image: ${{ needs.build-artifact.outputs.dev-docker-image }}
-      build-artifact-name: ${{ needs.build-artifact.outputs.build-artifact-name }}
-      wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
-
 #   build-and-test-wheels:
 #     uses: Check all-post-commit yaml for directions
 #     secrets: inherit
diff --git a/.github/workflows/build-artifact.yaml b/.github/workflows/build-artifact.yaml
index 1201b2baca..4028fc2fa8 100644
--- a/.github/workflows/build-artifact.yaml
+++ b/.github/workflows/build-artifact.yaml
@@ -140,7 +140,7 @@ jobs:
   build-artifact:
     name: "ðŸ› ï¸ Build ${{ inputs.build-type }} ${{ inputs.distro }} ${{ inputs.version }}"
     needs: build-docker-image
-    timeout-minutes: 45
+    timeout-minutes: 30
     runs-on: tt-beta-ubuntu-2204-large
     environment: ${{ github.ref == 'refs/heads/main' && 'mainline' || '' }}
     outputs:
diff --git a/.github/workflows/build-docker-artifact.yaml b/.github/workflows/build-docker-artifact.yaml
index fb82e63c71..dd8a17488e 100644
--- a/.github/workflows/build-docker-artifact.yaml
+++ b/.github/workflows/build-docker-artifact.yaml
@@ -143,7 +143,7 @@ jobs:
     name: "ðŸ³ï¸ Build images"
     needs: check-docker-images
     if: needs.check-docker-images.outputs.dev-exists != 'true' || needs.check-docker-images.outputs.basic-dev-exists != 'true' || needs.check-docker-images.outputs.manylinux-exists != 'true'
-    timeout-minutes: 90
+    timeout-minutes: 60
     runs-on: tt-beta-ubuntu-2204-large
     steps:
       - name: â¬‡ï¸ Checkout
diff --git a/.github/workflows/build-wrapper.yaml b/.github/workflows/build-wrapper.yaml
index 59b176c1ef..ba2f238353 100644
--- a/.github/workflows/build-wrapper.yaml
+++ b/.github/workflows/build-wrapper.yaml
@@ -34,9 +34,6 @@ jobs:
           - version: "22.04"
             toolchain: "cmake/x86_64-linux-gcc-12-toolchain.cmake"
             build-type: "Release"
-          - version: "22.04"
-            toolchain: "cmake/x86_64-linux-gcc-12-toolchain.cmake"
-            build-type: "Debug"
           - version: "24.04"
             toolchain: "cmake/x86_64-linux-clang-17-libcpp-toolchain.cmake"
             build-type: "Release"
diff --git a/.github/workflows/docs-latest-public.yaml b/.github/workflows/docs-latest-public.yaml
index f688c10d24..1118f9f33c 100644
--- a/.github/workflows/docs-latest-public.yaml
+++ b/.github/workflows/docs-latest-public.yaml
@@ -16,6 +16,13 @@ on:
       build-artifact-name:
         required: true
         type: string
+concurrency:
+  # Note that people may spam the post-commit pipeline on their branch, and
+  # we have this docs pipeline in the post-commit pipeline, then people
+  # would have to wait until the previous one fully completes. That may be
+  # ok because each post-commit pipeline definitely takes more than 30 min
+  group: "pages-${{ github.ref }}"
+  cancel-in-progress: false
 
 jobs:
   build-docs:
@@ -78,15 +85,7 @@ jobs:
 
   deploy-docs:
     needs: build-docs
-    if: ${{ github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/heads/releases/v') }}
     runs-on: ubuntu-latest
-    concurrency:
-      # Note that people may spam the post-commit pipeline on their branch, and
-      # we have this docs pipeline in the post-commit pipeline, then people
-      # would have to wait until the previous one fully completes. That may be
-      # ok because each post-commit pipeline definitely takes more than 30 min
-      group: "pages-${{ github.ref }}"
-      cancel-in-progress: false # WARNING: This will only queue a single job; all else will still be cancelled!
     defaults:
       run:
         shell: bash
@@ -101,6 +100,7 @@ jobs:
           name: gh_pages_${{ inputs.version }}
           path: gh_pages/${{ inputs.version }}
       - name: Deploy to GitHub Pages
+        if: ${{ github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/heads/releases/v') }}
         uses: JamesIves/github-pages-deploy-action@v4
         id: deployment
         with:
diff --git a/.github/workflows/merge-gate.yaml b/.github/workflows/merge-gate.yaml
index 9a598ba50b..4de82792bb 100644
--- a/.github/workflows/merge-gate.yaml
+++ b/.github/workflows/merge-gate.yaml
@@ -54,7 +54,6 @@ jobs:
       docs-changed: ${{ steps.find-changes.outputs.docs-changed }}
       cmake-changed: ${{ steps.find-changes.outputs.cmake-changed }}
       tt-metalium-changed: ${{ steps.find-changes.outputs.tt-metalium-changed }}
-      tt-nn-changed: ${{ steps.find-changes.outputs.tt-nn-changed }}
       tt-metalium-or-tt-nn-tests-changed: ${{ steps.find-changes.outputs.tt-metalium-or-tt-nn-tests-changed }}
     steps:
       - id: find-changes
@@ -105,7 +104,7 @@ jobs:
       skip-tt-train: false
       publish-artifact: false
 
-  metalium-smoke-tests:
+  smoke-tests:
     needs: [find-changes, build-tsan]
     if: ${{ github.ref_name == 'main' ||
             needs.find-changes.outputs.cmake-changed == 'true' ||
@@ -125,29 +124,6 @@ jobs:
       package-artifact-name: ${{ needs.build-tsan.outputs.packages-artifact-name }}
       runner: ${{ matrix.platform }}
       per-test-timeout: 10 # TSan can be slow; relax the timeout somewhat
-      product: tt-metalium
-
-  ttnn-smoke-tests:
-    needs: [find-changes, build-tsan]
-    if: ${{ github.ref_name == 'main' ||
-            needs.find-changes.outputs.cmake-changed == 'true' ||
-            needs.find-changes.outputs.tt-nn-changed == 'true' ||
-            needs.find-changes.outputs.tt-metalium-or-tt-nn-tests-changed == 'true'
-        }}
-    strategy:
-      fail-fast: false
-      matrix:
-        platform: [
-          "N300",
-          "N300-llmbox",
-        ]
-    uses: ./.github/workflows/smoke.yaml
-    with:
-      docker-image: ${{ needs.build-tsan.outputs.dev-docker-image }}
-      package-artifact-name: ${{ needs.build-tsan.outputs.packages-artifact-name }}
-      runner: ${{ matrix.platform }}
-      per-test-timeout: 10 # TSan can be slow; relax the timeout somewhat
-      product: tt-nn
 
   build-asan:
     if: ${{ github.ref_name == 'main' ||
@@ -197,13 +173,13 @@ jobs:
         contains(join(needs.*.result, ','), 'success') ||
         contains(join(needs.*.result, ','), 'failure')
       }}
-    needs: [static-checks, code-analysis, find-changes, build, build-docs, build-tsan, metalium-smoke-tests, ttnn-smoke-tests]
+    needs: [static-checks, code-analysis, find-changes, build, build-docs, build-tsan, smoke-tests]
     runs-on: ubuntu-latest
     steps:
       - name: Check if all jobs passed
         uses: tenstorrent/tt-metal/.github/actions/workflow-status@main
         with:
           required-jobs: "static-checks, code-analysis, find-changes"
-          optional-jobs: "build, build-docs, build-tsan, metalium-smoke-tests, ttnn-smoke-tests"
+          optional-jobs: "build, build-docs, build-tsan, smoke-tests"
         env:
           NEEDS_CONTEXT: '${{ toJSON(needs) }}'
diff --git a/.github/workflows/pr-gate.yaml b/.github/workflows/pr-gate.yaml
index b31ed4ef0f..2e63b2902e 100644
--- a/.github/workflows/pr-gate.yaml
+++ b/.github/workflows/pr-gate.yaml
@@ -71,13 +71,12 @@ jobs:
     outputs:
       cmake-changed: ${{ steps.find-changes.outputs.cmake-changed }}
       tt-metalium-changed: ${{ steps.find-changes.outputs.tt-metalium-changed }}
-      tt-nn-changed: ${{ steps.find-changes.outputs.tt-nn-changed }}
       tt-metalium-or-tt-nn-tests-changed: ${{ steps.find-changes.outputs.tt-metalium-or-tt-nn-tests-changed }}
     steps:
       - id: find-changes
         uses: tenstorrent/tt-metal/.github/actions/find-changed-files@main
 
-  metalium-smoke-tests:
+  smoke-tests:
     needs: [ asan-build, find-changed-files ]
     if: ${{
         needs.find-changed-files.outputs.cmake-changed == 'true' ||
@@ -95,27 +94,6 @@ jobs:
       docker-image: ${{ needs.asan-build.outputs.dev-docker-image }}
       package-artifact-name: ${{ needs.asan-build.outputs.packages-artifact-name }}
       runner: ${{ matrix.platform }}
-      product: tt-metalium
-
-  ttnn-smoke-tests:
-    needs: [ asan-build, find-changed-files ]
-    if: ${{
-        needs.find-changed-files.outputs.cmake-changed == 'true' ||
-        needs.find-changed-files.outputs.tt-nn-changed == 'true' ||
-        needs.find-changed-files.outputs.tt-metalium-or-tt-nn-tests-changed == 'true'
-      }}
-    strategy:
-      fail-fast: false
-      matrix:
-        platform: [
-          "N300",
-        ]
-    uses: ./.github/workflows/smoke.yaml
-    with:
-      docker-image: ${{ needs.asan-build.outputs.dev-docker-image }}
-      package-artifact-name: ${{ needs.asan-build.outputs.packages-artifact-name }}
-      runner: ${{ matrix.platform }}
-      product: tt-nn
 
   build:
     if: github.event_name != 'pull_request' || !github.event.pull_request.draft
@@ -165,13 +143,13 @@ jobs:
         contains(join(needs.*.result, ','), 'success') ||
         contains(join(needs.*.result, ','), 'failure')
       }}
-    needs: [asan-build, metalium-smoke-tests, ttnn-smoke-tests, build, metalium-examples]
+    needs: [asan-build, smoke-tests, build, metalium-examples]
     runs-on: ubuntu-latest
     steps:
       - name: Check if all jobs passed
         uses: tenstorrent/tt-metal/.github/actions/workflow-status@main
         with:
           required-jobs: "asan-build, build"
-          optional-jobs: "metalium-smoke-tests, ttnn-smoke-tests, metalium-examples"
+          optional-jobs: "smoke-tests, metalium-examples"
         env:
           NEEDS_CONTEXT: '${{ toJSON(needs) }}'
diff --git a/.github/workflows/smoke.yaml b/.github/workflows/smoke.yaml
index 72414158f4..f25b33fde3 100644
--- a/.github/workflows/smoke.yaml
+++ b/.github/workflows/smoke.yaml
@@ -15,13 +15,10 @@ on:
       per-test-timeout:
         required: false
         type: string
-        default: 4.0 # TODO: Drop back to 3.5 when https://github.com/tenstorrent/github-ci-infra/issues/876 is resolved.
-      product:
-        required: true
-        type: string # tt-metalium or tt-nn
+        default: 3.5
 
 jobs:
-  smoke:
+  metalium-smoke:
     runs-on: >-
       ${{
         ((inputs.runner == 'N150' || inputs.runner == 'N300' || inputs.runner == 'N300-llmbox') && format('tt-beta-ubuntu-2204-{0}-large-stable', inputs.runner))
@@ -50,8 +47,7 @@ jobs:
 
       - name: Install packages
         run: |
-          # Ideally only ${{ inputs.product }}-validation, but APT doesn't resolve dependencies from files on disk without being told about them.
-          apt install -y ./pkgs/tt-metalium_*.deb ./pkgs/tt-metalium-jit_*.deb ./pkgs/${{ inputs.product }}_*.deb ./pkgs/${{ inputs.product }}-validation_*.deb
+          apt install ./pkgs/tt-metalium_*.deb ./pkgs/tt-metalium-jit_*.deb ./pkgs/tt-metalium-validation_*.deb
 
       - name: Run a test
         id: test
@@ -63,10 +59,9 @@ jobs:
           TT_METAL_WATCHER: 5
           TT_METAL_WATCHER_TEST_MODE: 1
         run: |
-          /usr/bin/${{ inputs.product }}-validation-smoke
+          /usr/bin/tt-metalium-validation-smoke
 
       - name: workaround
-        if: ${{ !cancelled() && (github.event_name != 'pull_request' || github.event.pull_request.head.repo.fork == false) }}
         run: |
           # The test-reporting action runs git ls-files with no way to opt-out.
           # Give it a dummy repo so it doesn't fail.
@@ -77,7 +72,7 @@ jobs:
         if: ${{ !cancelled() && (github.event_name != 'pull_request' || github.event.pull_request.head.repo.fork == false) }}
         uses: phoenix-actions/test-reporting@f957cd93fc2d848d556fa0d03c57bc79127b6b5e # v15
         with:
-          name: ${{ inputs.product }} ${{ inputs.runner }} smoke tests (${{ github.workflow }}, attempt ${{ github.run_attempt }})
+          name: Metalium ${{ inputs.runner }} smoke tests (${{ github.workflow }}, attempt ${{ github.run_attempt }})
           path: /work/test-reports/*.xml
           reporter: jest-junit
           working-directory: /work
diff --git a/.github/workflows/upstream-tests.yaml b/.github/workflows/upstream-tests.yaml
index 9d329b110d..da4f39321e 100644
--- a/.github/workflows/upstream-tests.yaml
+++ b/.github/workflows/upstream-tests.yaml
@@ -17,7 +17,6 @@ env:
   WH_6U_PROFILER_IMAGE_NAME: ghcr.io/tenstorrent/tt-metal/upstream-profiler-tests-wh-6u
   BLACKHOLE_IMAGE_NAME: ghcr.io/tenstorrent/tt-metal/upstream-tests-bh
   BLACKHOLE_PROFILER_IMAGE_NAME: ghcr.io/tenstorrent/tt-metal/upstream-profiler-tests-bh
-  BLACKHOLE_LLMBOX_IMAGE_NAME: ghcr.io/tenstorrent/tt-metal/upstream-tests-bh-llmbox
 
 jobs:
   build-artifact:
@@ -42,10 +41,13 @@ jobs:
     outputs:
       image-tag-suffix: ${{ steps.set-image-tag-suffix.outputs.image-tag-suffix }}
       wh-6u-image-tag: ${{ steps.set-image-tags.outputs.wh-6u-image-tag }}
+      wh-6u-base-image-name: ${{ steps.set-image-tags.outputs.wh-6u-base-image-name }}
       wh-6u-profiler-image-tag: ${{ steps.set-image-tags.outputs.wh-6u-profiler-image-tag }}
+      wh-6u-profiler-base-image-name: ${{ steps.set-image-tags.outputs.wh-6u-profiler-base-image-name }}
       bh-image-tag: ${{ steps.set-image-tags.outputs.bh-image-tag }}
+      bh-base-image-name: ${{ steps.set-image-tags.outputs.bh-base-image-name }}
       bh-profiler-image-tag: ${{ steps.set-image-tags.outputs.bh-profiler-image-tag }}
-      bh-llmbox-image-tag: ${{ steps.set-image-tags.outputs.bh-llmbox-image-tag }}
+      bh-profiler-base-image-name: ${{ steps.set-image-tags.outputs.bh-profiler-base-image-name }}
     steps:
       - uses: actions/checkout@v4
         with:
@@ -58,10 +60,13 @@ jobs:
         id: set-image-tags
         run: |
           echo "wh-6u-image-tag=${{ env.WH_6U_IMAGE_NAME }}:${{ steps.set-image-tag-suffix.outputs.image-tag-suffix }}" >> "$GITHUB_OUTPUT"
+          echo "wh-6u-base-image-name=${{ env.WH_6U_IMAGE_NAME }}" >> "$GITHUB_OUTPUT"
           echo "wh-6u-profiler-image-tag=${{ env.WH_6U_PROFILER_IMAGE_NAME }}:${{ steps.set-image-tag-suffix.outputs.image-tag-suffix }}" >> "$GITHUB_OUTPUT"
+          echo "wh-6u-profiler-base-image-name=${{ env.WH_6U_PROFILER_IMAGE_NAME }}" >> "$GITHUB_OUTPUT"
           echo "bh-image-tag=${{ env.BLACKHOLE_IMAGE_NAME }}:${{ steps.set-image-tag-suffix.outputs.image-tag-suffix }}" >> "$GITHUB_OUTPUT"
+          echo "bh-base-image-name=${{ env.BLACKHOLE_IMAGE_NAME }}" >> "$GITHUB_OUTPUT"
           echo "bh-profiler-image-tag=${{ env.BLACKHOLE_PROFILER_IMAGE_NAME }}:${{ steps.set-image-tag-suffix.outputs.image-tag-suffix }}" >> "$GITHUB_OUTPUT"
-          echo "bh-llmbox-image-tag=${{ env.BLACKHOLE_LLMBOX_IMAGE_NAME }}:${{ steps.set-image-tag-suffix.outputs.image-tag-suffix }}" >> "$GITHUB_OUTPUT"
+          echo "bh-profiler-base-image-name=${{ env.BLACKHOLE_PROFILER_IMAGE_NAME }}" >> "$GITHUB_OUTPUT"
   build-images:
     needs:
       - build-artifact
@@ -97,12 +102,6 @@ jobs:
             hw-topology: blackhole
             build-artifact-name: ${{ needs.build-artifact-profiler.outputs.build-artifact-name }}
             wheel-artifact-name: ${{ needs.build-artifact-profiler.outputs.wheel-artifact-name }}
-          - image-tag: ${{ needs.get-image-tags.outputs.bh-llmbox-image-tag }}
-            dockerfile: dockerfile/upstream_test_images/Dockerfile
-            test-command: dockerfile/upstream_test_images/run_upstream_tests_vanilla.sh
-            hw-topology: blackhole_llmbox
-            build-artifact-name: ${{ needs.build-artifact.outputs.build-artifact-name }}
-            wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
     runs-on: tt-beta-ubuntu-2204-large
     steps:
       - uses: actions/checkout@v4
@@ -176,20 +175,6 @@ jobs:
       - name: Run profiler image
         timeout-minutes: 10
         run: docker run -v /dev/hugepages-1G:/dev/hugepages-1G --device /dev/tenstorrent ${{ needs.get-image-tags.outputs.wh-6u-profiler-image-tag }}
-  test-bh-llmbox-image:
-    needs:
-      - get-image-tags
-      - build-images
-    runs-on:
-      - BH-LLMBox
-      - in-service
-      - cloud-virtual-machine
-    steps:
-      - name: Run image
-        timeout-minutes: 30
-        env:
-          LLAMA_DIR: /localdev/blackhole_demos/huggingface_data/meta-llama/Llama-3.1-8B-Instruct
-        run: docker run -v /dev/hugepages-1G:/dev/hugepages-1G --device /dev/tenstorrent -v $LLAMA_DIR:$LLAMA_DIR:ro -e LLAMA_DIR ${{ needs.get-image-tags.outputs.bh-llmbox-image-tag }}
   test-bh-image:
     needs:
       - get-image-tags
diff --git a/.github/workflows/vllm-nightly-tests.yaml b/.github/workflows/vllm-nightly-tests.yaml
index 36246b44e1..2006f394ca 100644
--- a/.github/workflows/vllm-nightly-tests.yaml
+++ b/.github/workflows/vllm-nightly-tests.yaml
@@ -27,4 +27,4 @@ jobs:
       docker-image: ${{ needs.build-artifact.outputs.dev-docker-image }}
       wheel-artifact-name: ${{ needs.build-artifact.outputs.wheel-artifact-name }}
       build-artifact-name: ${{ needs.build-artifact.outputs.build-artifact-name }}
-      vllm-commit: ${{ inputs.vllm-commit || 'dev' }}
+      vllm-commit: ${{ inputs.vllm-commit }}
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 8872414d91..8cb9f79a0f 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -165,7 +165,7 @@ add_compile_options(
     -Wno-deprecated-declarations
     # Vars that are used only in Asserts will appear as dead stores in builds
     # where the preprocessor has stripped away the assert.
-    "$<$<OR:$<CONFIG:Release>,$<CXX_COMPILER_ID:GNU>>:-Wno-unused-but-set-variable>"
+    "$<$<CONFIG:Release>:-Wno-unused-but-set-variable>"
     -Wno-unused-function
     -Wno-unused-variable
     "$<$<CXX_COMPILER_ID:Clang>:-Wno-c++11-narrowing>"
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index ba43d5250d..6689b7520d 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -170,8 +170,8 @@ You must run post-commit regressions before you commit something.
 These regressions will also run after every pushed commit to the GitHub repo.
 
 ```
-# Build directly with CMake for full control or run the provided script for building all tests.
-./build_metal.sh --build-tests
+cmake --build build --target install
+cmake --build build --target tests
 ./tests/scripts/run_tests.sh --tt-arch $ARCH_NAME --pipeline-type post_commit
 ```
 
@@ -219,10 +219,9 @@ We have a legacy suite of C++ integration tests that are built like standalone
 executables. This section goes over how to generally run such tests if there's
 a specific one you'd like to run.
 
-1. Build the API integration tests:
+1. Build the API integration tests using the make command,
 ```
-# Build directly with CMake for full control or run the provided script for building all tests.
-./build_metal.sh --build-tests
+cmake --build build --target tests
 ```
 2. Run the test binaries from the path **${TT_METAL_HOME}/build/test/tt_metal**
 
@@ -236,10 +235,9 @@ You can use `--gtest_filter` to filter out the specific test you'd like.
 For example, to build and run the `DispatchFixture.TensixDRAMLoopbackSingleCore` on
 fast dispatch, you can
 
-1. Build the tests:
+1. Build the unit tests:
    ```
-   # Build directly with CMake for full control or run the provided script for building all tests.
-   ./build_metal.sh --build-tests
+   cmake --build build --target tests
    ```
 2. Run the test:
    ```
diff --git a/README.md b/README.md
index 638e5ceb95..6ccb1d94c9 100644
--- a/README.md
+++ b/README.md
@@ -25,7 +25,7 @@
 
 | Release | Release Date |
 |---------|--------------|
-| 0.59.0 | ETA Jun 18, 2025 |
+| [0.59.0](https://github.com/tenstorrent/tt-metal/releases/tag/v0.59.0) | Jun 18, 2025 |
 | [0.58.0](https://github.com/tenstorrent/tt-metal/releases/tag/v0.58.0) | May 13, 2025 |
 | [0.57.0](https://github.com/tenstorrent/tt-metal/releases/tag/v0.57.0) | Apr 15, 2025 |
 | [0.56.0](https://github.com/tenstorrent/tt-metal/releases/tag/v0.56.0) | Mar 7, 2025 |
diff --git a/cmake/packaging.cmake b/cmake/packaging.cmake
index f3c20f6c7a..a5d1186292 100644
--- a/cmake/packaging.cmake
+++ b/cmake/packaging.cmake
@@ -10,8 +10,6 @@ set(CPACK_DEBIAN_METALIUM-DEV_PACKAGE_SECTION "libs")
 set(CPACK_DEBIAN_METALIUM-JIT_PACKAGE_SECTION "libs")
 set(CPACK_DEBIAN_METALIUM-EXAMPLES_PACKAGE_SECTION "doc")
 set(CPACK_DEBIAN_METALIUM-VALIDATION_PACKAGE_SECTION "utils")
-set(CPACK_DEBIAN_NN_PACKAGE_SECTION "libs")
-set(CPACK_DEBIAN_NN-VALIDATION_PACKAGE_SECTION "utils")
 
 set(CPACK_DEB_COMPONENT_INSTALL YES)
 set(CPACK_DEBIAN_PACKAGE_VERSION "${VERSION_DEB}")
@@ -112,19 +110,4 @@ cpack_add_component(
 )
 cpack_add_component(gtest GROUP metalium-validation)
 
-cpack_add_component_group(nn)
-cpack_add_component(nn DEPENDS metalium GROUP nn DESCRIPTION "TT-NN runtime library")
-cpack_add_component(ttnn-runtime GROUP nn)
-
-cpack_add_component_group(nn-validation)
-cpack_add_component(
-    nn-validation
-    DEPENDS
-        nn
-        metalium
-    GROUP nn-validation
-    DESCRIPTION "TT-NN validation tools"
-)
-cpack_add_component(ttnn-validation GROUP nn-validation)
-
 include(CPack)
diff --git a/conftest.py b/conftest.py
index d4198dc9fd..67a07a794b 100644
--- a/conftest.py
+++ b/conftest.py
@@ -364,7 +364,7 @@ def reset_fabric(fabric_config):
     import ttnn
 
     if fabric_config:
-        ttnn.set_fabric_config(ttnn.FabricConfig.DISABLED)
+        ttnn.initialize_fabric_config(ttnn.FabricConfig.DISABLED)
 
 
 # Set fabric config to passed in value
@@ -375,7 +375,7 @@ def set_fabric(fabric_config):
 
     # If fabric_config is not None, set it to fabric_config
     if fabric_config:
-        ttnn.set_fabric_config(fabric_config)
+        ttnn.initialize_fabric_config(fabric_config)
 
 
 @pytest.fixture(scope="function")
diff --git a/dockerfile/Dockerfile.manylinux b/dockerfile/Dockerfile.manylinux
index b0fe47be8e..8f34058658 100644
--- a/dockerfile/Dockerfile.manylinux
+++ b/dockerfile/Dockerfile.manylinux
@@ -88,14 +88,18 @@ RUN git clone --branch ${OMPI_TAG} --depth 1 https://github.com/open-mpi/ompi.gi
     rm -rf ompi-src
 
 COPY tt_metal/sfpi-version.sh /tmp/sfpi-version.sh
+
+# Clone SFPI repo recursively from the specified branch
 WORKDIR /tmp
-RUN set -x && sfpi_arch_os=$(uname -m)_$(uname -s) && source /tmp/sfpi-version.sh && \
-    sfpi_rpm_md5=$(eval echo "\$sfpi_${sfpi_arch_os}_rpm_md5") && \
-    test "${sfpi_rpm_md5}" && \
-    wget ${sfpi_url}/${sfpi_version}/sfpi-${sfpi_arch_os}.rpm && \
-    test "${sfpi_rpm_md5}" == "$(md5sum -b sfpi-${sfpi_arch_os}.rpm | cut -d' ' -f1)" && \
-    dnf -y install ./sfpi-${sfpi_arch_os}.rpm && \
-    rm -f sfpi-${sfpi_arch_os}.rpm sfpi-version.sh
+RUN source /tmp/sfpi-version.sh && \
+    git clone --recurse-submodules -b ${sfpi_version} https://github.com/tenstorrent/sfpi.git && \
+    cd sfpi && \
+    ./scripts/build.sh && \
+    tar cf - include | tar xf - -C build/sfpi && \
+    mkdir -p /opt/tenstorrent/sfpi && \
+    cp -a build/sfpi/* /opt/tenstorrent/sfpi/ && \
+    cd .. && \
+    rm -rf sfpi
 
 # ENV needed
 ENV PATH=${OMPI_PREFIX}/bin:$PATH
diff --git a/dockerfile/upstream_test_images/run_upstream_tests_vanilla.sh b/dockerfile/upstream_test_images/run_upstream_tests_vanilla.sh
index 717855fe40..a7a90d8974 100755
--- a/dockerfile/upstream_test_images/run_upstream_tests_vanilla.sh
+++ b/dockerfile/upstream_test_images/run_upstream_tests_vanilla.sh
@@ -26,7 +26,7 @@ test_suite_bh_single_pcie_small_ml_model_tests() {
     pytest models/demos/blackhole/resnet50/tests/upstream_pipeline
 }
 
-verify_llama_dir_() {
+test_suite_bh_single_pcie_llama_demo_tests() {
     if [ -z "${LLAMA_DIR}" ]; then
       echo "Error: LLAMA_DIR environment variable not detected. Please set this environment variable to tell the tests where to find the downloaded Llama weights." >&2
       exit 1
@@ -38,38 +38,13 @@ verify_llama_dir_() {
       echo "[upstream-tests] Error: Llama weights do not seem to exist in $LLAMA_DIR, exiting" >&2
       exit 1
     fi
-}
-
-test_suite_bh_single_pcie_llama_demo_tests() {
-    echo "[upstream-tests] Running BH upstream Llama demo model tests"
-
-    verify_llama_dir_
-
-    # TODO: remove me , just testing this out
-    pip3 install -r models/tt_transformers/requirements.txt
-    pytest models/tt_transformers/demo/simple_text_demo.py -k performance-batch-1
-}
 
-test_suite_bh_single_pcie_llama_demo_tests() {
     echo "[upstream-tests] Running BH upstream Llama demo model tests"
-
-    verify_llama_dir_
-
     # TODO: remove me , just testing this out
     pip3 install -r models/tt_transformers/requirements.txt
     pytest models/tt_transformers/demo/simple_text_demo.py -k performance-batch-1
 }
 
-test_suite_bh_llmbox_llama_demo_tests() {
-    echo "[upstream-tests] Running BH LLMBox upstream Llama demo model tests"
-
-    verify_llama_dir_
-
-    # TODO: remove me once upgraded
-    pip3 install -r models/tt_transformers/requirements.txt
-    pytest models/tt_transformers/demo/simple_text_demo.py -k "performance and ci-32" --data_parallel 4
-}
-
 test_suite_wh_6u_metal_unit_tests() {
     echo "[upstream-tests] running WH 6U upstream metalium unit tests. Note that skips should be treated as failures"
     ./build/test/tt_metal/tt_fabric/test_system_health
@@ -96,7 +71,17 @@ test_suite_wh_6u_model_unit_tests() {
 test_suite_wh_6u_llama_demo_tests() {
     echo "[upstream-tests] running WH 6U upstream Llama demo tests with weights"
 
-    verify_llama_dir_
+    if [ -z "${LLAMA_DIR}" ]; then
+        echo "[upstream-tests] Error: LLAMA_DIR environment variable not detected. Please set this environment variable to tell the tests where to find the downloaded Llama weights." >&2
+        exit 1
+    fi
+
+    if [ -d "$LLAMA_DIR" ] && [ "$(ls -A $LLAMA_DIR)" ]; then
+        echo "[upstream-tests] Llama weights exist, continuing"
+    else
+        echo "[upstream-tests] Error: Llama weights do not seem to exist in $LLAMA_DIR, exiting" >&2
+        exit 1
+    fi
 
     # TODO: to remove...
     pip install -r models/tt_transformers/requirements.txt
@@ -113,7 +98,17 @@ test_suite_wh_6u_llama_long_stress_tests() {
     echo "[upstream-tests] running WH 6U upstream Llama long stress tests. Note that on 6U systems built as of End of May 2025, this may take up to 4 hours to run."
     echo "[upstream-tests] Ensure that you have a TG directory populated with .bin files in LLAMA_DIR on the host."
 
-    verify_llama_dir_
+    if [ -z "${LLAMA_DIR}" ]; then
+        echo "[upstream-tests] Error: LLAMA_DIR environment variable not detected. Please set this environment variable to tell the tests where to find the downloaded Llama weights." >&2
+        exit 1
+    fi
+
+    if [ -d "$LLAMA_DIR" ] && [ "$(ls -A $LLAMA_DIR)" ]; then
+        echo "[upstream-tests] Llama weights exist, continuing"
+    else
+        echo "[upstream-tests] Error: Llama weights do not seem to exist in $LLAMA_DIR, exiting" >&2
+        exit 1
+    fi
 
     # TODO: to remove...
     pip install -r models/tt_transformers/requirements.txt
@@ -133,8 +128,6 @@ test_suite_bh_single_pcie_llama_demo_tests" # NOTE: This test MUST be last becau
 hw_topology_test_suites["blackhole_no_models"]="test_suite_bh_single_pcie_python_unit_tests
 test_suite_bh_single_pcie_metal_unit_tests"
 
-hw_topology_test_suites["blackhole_llmbox"]="test_suite_bh_llmbox_llama_demo_tests"
-
 hw_topology_test_suites["wh_6u"]="test_suite_wh_6u_model_unit_tests
 test_suite_wh_6u_llama_demo_tests
 test_suite_wh_6u_metal_unit_tests
diff --git a/docs/source/common/images/CQ_API_Test.png b/docs/source/common/images/CQ_API_Test.png
deleted file mode 100644
index 01a177e18d..0000000000
Binary files a/docs/source/common/images/CQ_API_Test.png and /dev/null differ
diff --git a/docs/source/common/images/Ethernet_Link_Latency_Test.png b/docs/source/common/images/Ethernet_Link_Latency_Test.png
deleted file mode 100644
index 6235724c31..0000000000
Binary files a/docs/source/common/images/Ethernet_Link_Latency_Test.png and /dev/null differ
diff --git a/docs/source/common/images/Ethernet_Link_Status_Test.png b/docs/source/common/images/Ethernet_Link_Status_Test.png
deleted file mode 100644
index 1eb3dbc198..0000000000
Binary files a/docs/source/common/images/Ethernet_Link_Status_Test.png and /dev/null differ
diff --git a/docs/source/common/images/Ethernet_Link_Test.png b/docs/source/common/images/Ethernet_Link_Test.png
deleted file mode 100644
index 1b6cb42ca3..0000000000
Binary files a/docs/source/common/images/Ethernet_Link_Test.png and /dev/null differ
diff --git a/docs/source/common/images/Fabric_API_Test.png b/docs/source/common/images/Fabric_API_Test.png
deleted file mode 100644
index 32250f2c14..0000000000
Binary files a/docs/source/common/images/Fabric_API_Test.png and /dev/null differ
diff --git a/docs/source/common/images/Memory_Buffer_Test.png b/docs/source/common/images/Memory_Buffer_Test.png
deleted file mode 100644
index 03b0e9b5da..0000000000
Binary files a/docs/source/common/images/Memory_Buffer_Test.png and /dev/null differ
diff --git a/docs/source/common/images/Program_API_Test.png b/docs/source/common/images/Program_API_Test.png
deleted file mode 100644
index f572197d9e..0000000000
Binary files a/docs/source/common/images/Program_API_Test.png and /dev/null differ
diff --git a/docs/source/common/images/TT_Flash_Expected_Output.png b/docs/source/common/images/TT_Flash_Expected_Output.png
deleted file mode 100644
index 0d124d256e..0000000000
Binary files a/docs/source/common/images/TT_Flash_Expected_Output.png and /dev/null differ
diff --git a/docs/source/common/images/TT_SMI_Expected_Output.png b/docs/source/common/images/TT_SMI_Expected_Output.png
deleted file mode 100644
index 0d9cac8cbc..0000000000
Binary files a/docs/source/common/images/TT_SMI_Expected_Output.png and /dev/null differ
diff --git a/docs/source/common/images/TT_SMI_Successful_Reset.png b/docs/source/common/images/TT_SMI_Successful_Reset.png
deleted file mode 100644
index c18c970c20..0000000000
Binary files a/docs/source/common/images/TT_SMI_Successful_Reset.png and /dev/null differ
diff --git a/infra/VERSION b/infra/VERSION
index 97819898da..9d8c1d0bbb 100644
--- a/infra/VERSION
+++ b/infra/VERSION
@@ -1,4 +1,4 @@
 # This VERSION file is not the official source of the software version
 # This is solely for helping with version management via our release system
 # as of this writing
-v0.60.0
+v0.59.0
diff --git a/install_dependencies.sh b/install_dependencies.sh
index 584b53265d..9960f8bb54 100755
--- a/install_dependencies.sh
+++ b/install_dependencies.sh
@@ -242,40 +242,23 @@ install_sfpi() {
 	    exit 1
 	fi
     fi
-    # determine packaging system
-    local pkg
-    if dpkg-query -f '${Version}' -W libc-bin >/dev/null 2>&1 ; then
-	pkg=deb
-    elif rpm -q --qf '%{VERSION}' glibc >/dev/null 2>&1 ; then
-	pkg=rpm
-    else
-	echo "Unknown packaging system" >&2
-	exit 1
-    fi
     local $(grep -v '^#' $version_file)
     local sfpi_arch_os=$(uname -m)_$(uname -s)
-    local sfpi_pkg_md5=$(eval echo "\$sfpi_${sfpi_arch_os}_${pkg}_md5")
-    if [ -z $(eval echo "$sfpi_${pkg}_md5") ] ; then
-	echo "SFPI $pkg package for ${sfpi_arch_os} is not available" >&2
+    local sfpi_deb_md5=$(eval echo "\$sfpi_${sfpi_arch_os}_deb_md5")
+    if [ -z "$sfpi_deb_md5" ] ; then
+	echo "SFPI debian package for ${sfpi_arch_os} is not available" >&2
 	exit 1
     fi
     local TEMP_DIR=$(mktemp -d)
-    wget -P $TEMP_DIR "$sfpi_url/$sfpi_version/sfpi-${sfpi_arch_os}.${pkg}"
-    if [ $(md5sum -b "${TEMP_DIR}/sfpi-${sfpi_arch_os}.${pkg}" | cut -d' ' -f1) \
-	     != "$sfpi_pkg_md5" ] ; then
-	echo "SFPI sfpi-${sfpi_arch_os}.${pkg} md5 mismatch" >&2
+    wget -P $TEMP_DIR "$sfpi_url/$sfpi_version/sfpi-${sfpi_arch_os}.deb"
+    if [ $(md5sum -b "${TEMP_DIR}/sfpi-${sfpi_arch_os}.deb" | cut -d' ' -f1) \
+	     != "$sfpi_deb_md5" ] ; then
+	echo "SFPI sfpi-${sfpi_arch_os}.deb md5 mismatch" >&2
 	rm -rf $TEMP_DIR
 	exit 1
     fi
     # we must select exactly this version
-    case "$pkg" in
-	deb)
-	    apt-get install -y --allow-downgrades $TEMP_DIR/sfpi-${sfpi_arch_os}.deb
-	    ;;
-	rpm)
-	    rpm --upgrade --force $TEMP_DIR/sfpi-${sfpi_arch_os}.rpm
-	    ;;
-    esac
+    apt-get install -y --allow-downgrades $TEMP_DIR/sfpi-${sfpi_arch_os}.deb
     rm -rf $TEMP_DIR
 }
 
diff --git a/models/demos/llama3_subdevices/demo/demo_decode.py b/models/demos/llama3_subdevices/demo/demo_decode.py
index 4715b8ed59..cb5ac6f2d8 100644
--- a/models/demos/llama3_subdevices/demo/demo_decode.py
+++ b/models/demos/llama3_subdevices/demo/demo_decode.py
@@ -35,7 +35,7 @@ TSU_PERF_DROP_LIMIT_PERCENT = 10
 
 # Constants for TSU thresholds based on the number of layers
 TSU_THRESHOLDS = {
-    "4U": {1: {"min": 390, "max": 448}, 10: {"min": 230, "max": 253}, 80: {"min": 52, "max": 56}},
+    "4U": {1: {"min": 390, "max": 448}, 10: {"min": 230, "max": 253}, 80: {"min": 49.5, "max": 54}},
     # TODO: Update thresholds for 6U 10L and 80L based on actual perf when 6U are available and added into CI
     "6U": {1: {"min": 480, "max": 550}, 10: {"min": 230, "max": 250}, 80: {"min": 49, "max": 53}},
 }
diff --git a/models/demos/llama3_subdevices/tests/decoder_perf_targets_4u.json b/models/demos/llama3_subdevices/tests/decoder_perf_targets_4u.json
index 89aba12ce8..9315697c29 100644
--- a/models/demos/llama3_subdevices/tests/decoder_perf_targets_4u.json
+++ b/models/demos/llama3_subdevices/tests/decoder_perf_targets_4u.json
@@ -35,9 +35,9 @@
         },
         "AllGatherAsync_0": {
             "op_name": "AllGatherAsync_Binary_Mult",
-            "kernel_duration": 13054.826388888889,
+            "kernel_duration": 11328.694444444445,
             "op_to_op": 959.5555,
-            "first_to_last_start": 96.77777777777777,
+            "first_to_last_start": 1565.888888888889,
             "non-overlapped-dispatch-time": 4351.1,
             "kernel_duration_relative_margin": 0.05,
             "op_to_op_duration_relative_margin": 0.3,
@@ -78,21 +78,21 @@
             "dispatch_duration_relative_margin": 0.2
         },
         "Matmul_3": {
-            "op_name": "FF2_MM",
-            "kernel_duration": 15891.0,
-            "op_to_op": 658.7777777777778,
-            "first_to_last_start": 2250.0,
-            "non-overlapped-dispatch-time": 6770.3,
+            "op_name": "FF3_MM",
+            "kernel_duration": 9435.333333333334,
+            "op_to_op": 688.7777777777778,
+            "first_to_last_start": 2132.3333333333335,
+            "non-overlapped-dispatch-time": 7093.7,
             "kernel_duration_relative_margin": 0.05,
             "op_to_op_duration_relative_margin": 0.2,
             "first_to_last_start_relative_margin": 0.2,
-            "dispatch_duration_relative_margin": 0.1
+            "dispatch_duration_relative_margin": 0.15
         },
-        "Matmul_RS_0": {
-            "op_name": "ReduceScatter_FF1_MM_FF3",
-            "kernel_duration": 11161.267361111111,
-            "op_to_op": 756.1111111111111,
-            "first_to_last_start": 2198.777777777778,
+        "Matmul_4": {
+            "op_name": "FF2_MM",
+            "kernel_duration": 15891.0,
+            "op_to_op": 658.7777777777778,
+            "first_to_last_start": 2250.0,
             "non-overlapped-dispatch-time": 6770.3,
             "kernel_duration_relative_margin": 0.05,
             "op_to_op_duration_relative_margin": 0.2,
@@ -133,10 +133,21 @@
             "dispatch_duration_relative_margin": 0.2
         },
         "LlamaReduceScatterDeviceOperation_0": {
+            "op_name": "ReduceScatter_FF1",
+            "kernel_duration": 9952.402777777777,
+            "op_to_op": 708.1111111111111,
+            "first_to_last_start": 1993.7777777777778,
+            "non-overlapped-dispatch-time": 8058.9,
+            "kernel_duration_relative_margin": 0.05,
+            "op_to_op_duration_relative_margin": 0.2,
+            "first_to_last_start_relative_margin": 0.2,
+            "dispatch_duration_relative_margin": 0.3
+        },
+        "LlamaReduceScatterDeviceOperation_1": {
             "op_name": "ReduceScatter_FF3",
             "kernel_duration": 9817.020833333334,
             "op_to_op": 812.1111111111111,
-            "first_to_last_start": 733.333333333334,
+            "first_to_last_start": 2018.2222222222222,
             "non-overlapped-dispatch-time": 7359.9,
             "kernel_duration_relative_margin": 0.05,
             "op_to_op_duration_relative_margin": 0.2,
@@ -146,7 +157,7 @@
         "RotaryEmbeddingLlamaFusedQK_0": {
             "op_name": "RotaryEmbeddingLlamaFusedQK",
             "kernel_duration": 2821,
-            "op_to_op": 765.3333333333334,
+            "op_to_op": 606.1111111111111,
             "first_to_last_start": 2560.8888888888887,
             "non-overlapped-dispatch-time": 2844.3,
             "kernel_duration_relative_margin": 0.1,
@@ -159,7 +170,7 @@
             "kernel_duration": 5963,
             "op_to_op": 860.2222222222222,
             "first_to_last_start": 145,
-            "non-overlapped-dispatch-time": 4669.555555555556,
+            "non-overlapped-dispatch-time": 5890.0,
             "kernel_duration_relative_margin": 0.2,
             "op_to_op_duration_relative_margin": 0.2,
             "first_to_last_start_relative_margin": 0.35,
@@ -190,8 +201,8 @@
         "Untilize_0": {
             "op_name": "Untilize",
             "kernel_duration": 1517.3333333333335,
-            "op_to_op": 1563.7777777777778,
-            "first_to_last_start": 1563.7777777777778,
+            "op_to_op": 800,
+            "first_to_last_start": 2043.3333333333333,
             "non-overlapped-dispatch-time": 3113.6,
             "kernel_duration_relative_margin": 0.2,
             "op_to_op_duration_relative_margin": 0.4,
@@ -301,7 +312,7 @@
         },
         "AllGatherAsync_0": {
             "op_name": "AllGatherAsync_0",
-            "kernel_duration": 9880.5 ,
+            "kernel_duration": 10161.59375,
             "op_to_op": 703.0,
             "non-overlapped-dispatch-time": 2910.0,
             "kernel_duration_relative_margin": 0.2,
diff --git a/models/demos/llama3_subdevices/tests/decoder_perf_targets_6u.json b/models/demos/llama3_subdevices/tests/decoder_perf_targets_6u.json
index 512055e1b7..2cd1aa267a 100644
--- a/models/demos/llama3_subdevices/tests/decoder_perf_targets_6u.json
+++ b/models/demos/llama3_subdevices/tests/decoder_perf_targets_6u.json
@@ -78,6 +78,17 @@
             "dispatch_duration_relative_margin": 0.1
         },
         "Matmul_3": {
+            "op_name": "FF3_MM",
+            "kernel_duration": 8510.037037037036,
+            "op_to_op": 581.1111111111111,
+            "first_to_last_start": 2132.3333333333335,
+            "non-overlapped-dispatch-time": 5732.1,
+            "kernel_duration_relative_margin": 0.05,
+            "op_to_op_duration_relative_margin": 0.2,
+            "first_to_last_start_relative_margin": 0.2,
+            "dispatch_duration_relative_margin": 0.15
+        },
+        "Matmul_4": {
             "op_name": "FF2_MM",
             "kernel_duration": 14030.0,
             "op_to_op": 623.1111111111112,
@@ -121,8 +132,8 @@
             "first_to_last_start_relative_margin": 0.2,
             "dispatch_duration_relative_margin": 0.2
         },
-        "Matmul_RS_0": {
-            "op_name": "Matmul_FF3_ReduceScatter_FF1",
+        "LlamaReduceScatterDeviceOperation_0": {
+            "op_name": "ReduceScatter_FF1",
             "kernel_duration": 9228.53125,
             "op_to_op": 673.8148148148148,
             "first_to_last_start": 1993.7777777777778,
@@ -132,7 +143,7 @@
             "first_to_last_start_relative_margin": 0.2,
             "dispatch_duration_relative_margin": 0.3
         },
-        "LlamaReduceScatterDeviceOperation_0": {
+        "LlamaReduceScatterDeviceOperation_1": {
             "op_name": "ReduceScatter_FF3",
             "kernel_duration": 9211.711805555557,
             "op_to_op": 756.3703703703704,
diff --git a/models/demos/llama3_subdevices/tests/test_decoder_device_perf.py b/models/demos/llama3_subdevices/tests/test_decoder_device_perf.py
index ab645514c4..1b15258a3a 100644
--- a/models/demos/llama3_subdevices/tests/test_decoder_device_perf.py
+++ b/models/demos/llama3_subdevices/tests/test_decoder_device_perf.py
@@ -186,7 +186,7 @@ def merge_device_rows(df):
         if not blocks:
             break
 
-        if "AllGather" in op_name or "ReduceScatter" in op_name or "AllReduce" or "Matmul_RS" in op_name:
+        if "AllGather" in op_name or "ReduceScatter" in op_name or "AllReduce" in op_name:
             # For collective ops, take the average duration over all rows within a block
             device_kernel_durations = [
                 d["DEVICE KERNEL DURATION [ns]"]
@@ -274,7 +274,7 @@ def print_dict(input_dict, dict_name):
 
 
 def is_collective_op(op_code):
-    return any(x in op_code for x in ("AllGather", "ReduceScatter", "AllReduce", "Matmul_RS"))
+    return any(x in op_code for x in ("AllGather", "ReduceScatter", "AllReduce"))
 
 
 def process_measurements(df, num_layers):
diff --git a/models/demos/llama3_subdevices/tt/generator.py b/models/demos/llama3_subdevices/tt/generator.py
index 1d1d486aaa..0736a30950 100644
--- a/models/demos/llama3_subdevices/tt/generator.py
+++ b/models/demos/llama3_subdevices/tt/generator.py
@@ -486,7 +486,7 @@ class Generator:
     def read_decode_output(self, tt_logits, unpadded_batch, is_tokens=True):
         logits = self.model.process_output_decode(tt_logits, B=unpadded_batch, S=1)
         if self.perm_table_tensor is not None:
-            logits = logits[self.perm_table_tensor]
+            logits = logits[self.perm_table_tensor, :]
         return logits
 
     def chat_completion(
diff --git a/models/demos/llama3_subdevices/tt/llama_ccl.py b/models/demos/llama3_subdevices/tt/llama_ccl.py
index 06e12ce734..43d8dd1ab8 100644
--- a/models/demos/llama3_subdevices/tt/llama_ccl.py
+++ b/models/demos/llama3_subdevices/tt/llama_ccl.py
@@ -550,55 +550,6 @@ class TT_CCL:
         self.gather_idx[cluster_axis] = (self.gather_idx[cluster_axis] + 1) % self.num_cbs
         return xqkv_reduced, q_heads_pre_rot_1BQD, k_heads_pre_rot_1BKD, v_heads_1BKD
 
-    def matmul_line_reduce_scatter(
-        self,
-        # Matmul
-        matmul_input,
-        matmul_weight,
-        # Reduce Scatter
-        input_tensor_mesh,
-        # Matmul
-        compute_kernel_config=None,
-        dtype=None,
-        program_config=None,
-        memory_config=None,
-        global_cb=None,
-        sub_device_id=None,
-        # Reduce Scatter
-        dim=3,
-        num_links=1,
-        math_op=ttnn.ReduceType.Sum,
-        buffer_key=None,
-        RS_memory_config=None,
-        cluster_axis=1,
-    ):
-        persistent_interim_buffer = self.reduce_scatter_buffers[cluster_axis][
-            self.reduce_scatter_buffer_idx[cluster_axis]
-        ]
-        w3_out, ttnn_tensor_out = ttnn.experimental.llama_rs_matmul(
-            matmul_input,
-            matmul_weight,
-            input_tensor_mesh,
-            persistent_interim_buffer,
-            dim,
-            self.gather_semaphore_handles[cluster_axis][self.gather_idx[cluster_axis]],
-            cluster_axis,
-            self.mesh_device,
-            num_links,
-            self.worker_sub_device_id,
-            memory_config_rs=RS_memory_config,
-            compute_kernel_config=compute_kernel_config,
-            dtype=dtype,
-            program_config=program_config,
-            memory_config_mm=memory_config,
-            global_cb=global_cb,
-            topology=ttnn.Topology.Ring if is_RING_6U else ttnn.Topology.Linear,
-        )
-        self.gather_idx[cluster_axis] = (self.gather_idx[cluster_axis] + 1) % self.num_cbs
-        self.reduce_scatter_buffer_idx[cluster_axis] = (self.reduce_scatter_buffer_idx[cluster_axis] + 1) % self.num_cbs
-        # ttnn.synchronize_device(self.mesh_device, sub_device_ids=[self.worker_sub_device_id])
-        return ttnn_tensor_out, w3_out
-
     def llama_rs_create_heads(
         self,
         input_tensor_mesh,
diff --git a/models/demos/llama3_subdevices/tt/llama_mlp.py b/models/demos/llama3_subdevices/tt/llama_mlp.py
index 23039e1224..1721a87b3a 100644
--- a/models/demos/llama3_subdevices/tt/llama_mlp.py
+++ b/models/demos/llama3_subdevices/tt/llama_mlp.py
@@ -136,13 +136,17 @@ class TtLlamaMLP(LightweightModule):
             global_cb=self.prefetcher_setup.global_circular_buffer if self.model_config["USE_PREFETCHER"] else None,
             sub_device_id=self.prefetcher_setup.worker_sub_device_id if mode == "decode" else None,
         )
-        w1_out_reduced, w3_out = self.tt_ccl.matmul_line_reduce_scatter(
-            x,
-            self.w3,
+
+        w1_out_reduced = self.tt_ccl.line_reduce_scatter(
             w1_out,
             cluster_axis=1,
             num_links=4 if is_RING_6U else 3,
-            RS_memory_config=self.model_config["REDUCE_SCATTER_OUT_MEMCFG"],
+            memory_config=self.model_config["REDUCE_SCATTER_OUT_MEMCFG"],
+        )
+
+        w3_out = ttnn.linear(
+            x,
+            self.w3,
             compute_kernel_config=self.args.compute_kernel_config_lofi
             if self.four_bit_mlp
             else self.args.compute_kernel_config_hifi2,
diff --git a/models/demos/llama3_subdevices/tt/model_config.py b/models/demos/llama3_subdevices/tt/model_config.py
index 9566ff3a3e..bf58638c41 100644
--- a/models/demos/llama3_subdevices/tt/model_config.py
+++ b/models/demos/llama3_subdevices/tt/model_config.py
@@ -1534,15 +1534,10 @@ class TtModelArgs:
                 )
             )
 
-            # Note PACKET_WORKER_CRS is 8 cores and it can NOT use any core in the following ranges:
-            # {1,6}-{2,7} (Reduce scatter ethernet worker cores),
-            # {1,0}-{2,0}, {1,4}-{2,5}, {1,9}-{2,9}, {5,0}-{6,2}, {5,4}-{6,7}, {5,9}-{6,9} (Matmul)
-            # {0,0}-{0,9}, {4,0}-{4,9} (Prefetcher)
-            # {3,6} (Matmul hop core)
             PACKET_WORKER_CRS = ttnn.CoreRangeSet(
                 [
-                    ttnn.CoreRange(ttnn.CoreCoord(1, 1), ttnn.CoreCoord(3, 2)),
-                    ttnn.CoreRange(ttnn.CoreCoord(1, 3), ttnn.CoreCoord(2, 3)),
+                    ttnn.CoreRange(ttnn.CoreCoord(1, 0), ttnn.CoreCoord(3, 1)),
+                    ttnn.CoreRange(ttnn.CoreCoord(1, 2), ttnn.CoreCoord(2, 2)),
                 ]
             )
             self.model_config["REDUCE_SCATTER_INTERIM_MEMCFG"] = ttnn.create_sharded_memory_config(
diff --git a/models/demos/segformer/README.md b/models/demos/segformer/README.md
index 4bb0f62714..0a902e9c2d 100644
--- a/models/demos/segformer/README.md
+++ b/models/demos/segformer/README.md
@@ -1,35 +1,6 @@
-# Segformer
+# Segformer Demo
 
-### Platforms:
-
-Wormhole N150, N300
-
-**Note:** On N300, make sure to use `WH_ARCH_YAML=wormhole_b0_80_arch_eth_dispatch.yaml` with the pytest.
-
-Or, make sure to set the following environment variable in the terminal:
-```
-export WH_ARCH_YAML=wormhole_b0_80_arch_eth_dispatch.yaml
-```
-
-To obtain the perf reports through profiler, please build with following command:
-```
-./build_metal.sh -p
-```
-
-### Introduction
-
-SegFormer's architecture is adept for both classification and segmentation tasks, utilizing a hierarchical design that extracts rich, multi-scale visual features. Its robust Transformer encoder generates powerful representations, suitable for discerning object categories, while a lightweight MLP decode head precisely maps these features for accurate pixel-level segmentation.
-
-Image classification: [source](https://huggingface.co/nvidia/mit-b0)
-Semantic segmentation: [source](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)
-
-### Details
-
-- Entry point for the model is models/demos/segformer/tt/ttnn_segformer_model.py
-- Batch Size: 1
-- Support Input Resolution: 512x512 (Height, Width)
-
-### How to run
+## How to run demo
 
 - Use the following command to run the Segformer Encoder model (Classification):
   ```python
@@ -37,7 +8,7 @@ Semantic segmentation: [source](https://huggingface.co/nvidia/segformer-b0-finet
   ```
 
 
-- Use the following command to run the Segformer Decoder model:
+- Use the following command to run the Segformer Decoder module model:
   ```python
   pytest tests/ttnn/integration_tests/segformer/test_segformer_decode_head.py
   ```
@@ -48,9 +19,9 @@ Semantic segmentation: [source](https://huggingface.co/nvidia/segformer-b0-finet
   pytest tests/ttnn/integration_tests/segformer/test_segformer_for_semantic_segmentation.py
   ```
 
-### Segformer Semantic Segmentation Demo
+## Segformer Semantic Segmentation Demo
 
-- Use the following command to run the demo script(Segmentation) which returns **mIoU** score for both reference, and ttnn models:
+- Use the following command to run the demo script(Segmentation) which returns **mIoU** score for both Reference,ttnn models:
   ```python
   pytest --disable-warnings models/demos/segformer/demo/demo_for_semantic_segmentation.py
   ```
@@ -66,12 +37,9 @@ Semantic segmentation: [source](https://huggingface.co/nvidia/segformer-b0-finet
   models/demos/segformer/demo/validation_data_ade20k/annotations/annotation.png
   ```
 
-### Segformer Image Classification Demo
+## Segformer Image Classification Demo
 
-- Use the following command to run the demo script(Classification) which returns **Accuracy** score for both reference, and ttnn models, validated with Imagenet Dataset samples:
+- Use the following command to run the demo script(Classification) which returns **Accuracy** score for both Reference,ttnn models and between them when validated with Imagenet Dataset samples:
   ```python
   pytest --disable-warnings models/demos/segformer/demo/demo_for_image_classification.py
   ```
-
-## ToDo:
-To add the performant model demo and the performance numbers using the trace+2cqs pipeline.
diff --git a/models/demos/ufld_v2/README.md b/models/demos/ufld_v2/README.md
index 560dcd3c02..d7df3bd9f5 100644
--- a/models/demos/ufld_v2/README.md
+++ b/models/demos/ufld_v2/README.md
@@ -1,20 +1,7 @@
 # Ultra-Fast-Lane-Detection-v2
 
 ### Platforms:
-
-Wormhole N150, N300
-
-**Note:** On N300, make sure to use `WH_ARCH_YAML=wormhole_b0_80_arch_eth_dispatch.yaml` with the pytest.
-
-Or, make sure to set the following environment variable in the terminal:
-```
-export WH_ARCH_YAML=wormhole_b0_80_arch_eth_dispatch.yaml
-```
-
-To obtain the perf reports through profiler, please build with following command:
-```
-./build_metal.sh -p
-```
+    WH N300,N150
 
 ### Introduction
 
@@ -22,7 +9,7 @@ The Ultra-Fast-Lane-Detection-v2 is a PyTorch-based implementation designed for
 
 Resource link - [source](https://github.com/cfzd/Ultra-Fast-Lane-Detection-v2)
 
-### Details
+### Model Details
 
 - The entry point to the UFLD_v2 is located at:`models/demos/ufld_v2/ttnn/ttnn_ufld_v2.py`
 - The model picks up trained weights from the **tusimple_res34.pth** file located at:`models/demos/ufld_v2/reference/tusimple_res34.pth`
@@ -30,6 +17,16 @@ Resource link - [source](https://github.com/cfzd/Ultra-Fast-Lane-Detection-v2)
 - Supported Input Resolution - (320,800) (Height,Width)
 
 ### How to Run:
+If running on Wormhole N300 (not required for N150 or Blackhole), the following environment variable needs to be set as the model requires at least 8x8 core grid size:
+```sh
+export WH_ARCH_YAML=wormhole_b0_80_arch_eth_dispatch.yaml
+```
+
+### Build Command to Use:
+To obtain the perf reports through profiler, please build with following command:
+```
+./build_metal.sh -p
+```
 
 Use the following command to run the model :
 
diff --git a/models/demos/yolov4/tests/perf/test_perf.py b/models/demos/yolov4/tests/perf/test_perf.py
index 7b9e25b7e1..330d6f3b3d 100644
--- a/models/demos/yolov4/tests/perf/test_perf.py
+++ b/models/demos/yolov4/tests/perf/test_perf.py
@@ -101,7 +101,7 @@ def test_yolov4(
 @pytest.mark.parametrize(
     "batch_size, model_name, expected_perf",
     [
-        (1, "yolov4", 93.5),
+        (1, "yolov4", 82),
     ],
 )
 @pytest.mark.models_device_performance_bare_metal
diff --git a/models/demos/yolov4/tt/common.py b/models/demos/yolov4/tt/common.py
index 9059501804..73ad62ab61 100644
--- a/models/demos/yolov4/tt/common.py
+++ b/models/demos/yolov4/tt/common.py
@@ -26,8 +26,8 @@ class Conv:
             shard_layout=conv_param.shard_layout,
             reshard_if_not_optimal=conv_param.reshard_if_not_optimal,
             deallocate_activation=conv_param.deallocate_activation,
-            enable_act_double_buffer=True,
-            enable_split_reader=True if conv_param.shard_layout == ttnn.TensorMemoryLayout.HEIGHT_SHARDED else False,
+            enable_act_double_buffer=conv_param.enable_act_double_buffer,
+            enable_split_reader=conv_param.enable_split_reader,
             output_layout=ttnn.TILE_LAYOUT,
         )
         config_override = None
diff --git a/models/demos/yolov4/tt/model_preprocessing.py b/models/demos/yolov4/tt/model_preprocessing.py
index 0abd7cd32c..2baefd4bf4 100644
--- a/models/demos/yolov4/tt/model_preprocessing.py
+++ b/models/demos/yolov4/tt/model_preprocessing.py
@@ -159,14 +159,14 @@ def _create_ds1_model_parameters(conv_args, resolution):
         conv_args.c1["enable_act_double_buffer"] = False
         conv_args.c1["deallocate_activation"] = True
         conv_args.c1["reshard_if_not_optimal"] = False
-        conv_args.c1["shard_layout"] = ttnn.TensorMemoryLayout.HEIGHT_SHARDED
+        conv_args.c1["shard_layout"] = None
 
-        conv_args.c2["act_block_h"] = 320
+        conv_args.c2["act_block_h"] = None
         conv_args.c2["enable_split_reader"] = False
         conv_args.c2["enable_act_double_buffer"] = False
         conv_args.c2["deallocate_activation"] = True
         conv_args.c2["reshard_if_not_optimal"] = False
-        conv_args.c2["shard_layout"] = ttnn.TensorMemoryLayout.HEIGHT_SHARDED
+        conv_args.c2["shard_layout"] = None
 
         conv_args.c3["act_block_h"] = None
         conv_args.c3["enable_split_reader"] = True
diff --git a/models/experimental/functional_unet/tests/test_unet_model.py b/models/experimental/functional_unet/tests/test_unet_model.py
index 2279b2983e..641d8dd2de 100644
--- a/models/experimental/functional_unet/tests/test_unet_model.py
+++ b/models/experimental/functional_unet/tests/test_unet_model.py
@@ -21,7 +21,16 @@ from models.experimental.functional_unet.tests.common import (
 )
 
 
-def run_unet_model(batch, groups, device, iterations=1):
+@pytest.mark.parametrize("batch", [1])
+@pytest.mark.parametrize("groups", [4])
+@pytest.mark.parametrize("device_params", [{"l1_small_size": UNET_L1_SMALL_REGION_SIZE}], indirect=True)
+def test_unet_model(batch, groups, device, use_program_cache, reset_seeds):
+    if (
+        not is_wormhole_b0(device)
+        and device.compute_with_storage_grid_size().x * device.compute_with_storage_grid_size().y != 110
+    ):
+        pytest.skip(f"Shallow UNet only support 110 cores on BH (was {device.compute_with_storage_grid_size()})")
+
     torch_input, ttnn_input = create_unet_input_tensors(
         batch,
         groups,
@@ -37,7 +46,7 @@ def run_unet_model(batch, groups, device, iterations=1):
     ttnn_model = unet_shallow_ttnn.UNet(parameters, device)
 
     torch_output_tensor = model(torch_input)
-    output_tensor = ttnn_model(ttnn_input, move_input_tensor_to_device=False, deallocate_input_activation=True).cpu()
+    output_tensor = ttnn_model(ttnn_input, move_input_tensor_to_device=False)
 
     B, C, H, W = torch_output_tensor.shape
     ttnn_output_tensor = ttnn.to_torch(output_tensor).reshape(B, C, H, W)
@@ -46,28 +55,3 @@ def run_unet_model(batch, groups, device, iterations=1):
         ttnn_output_tensor,
         UNET_FULL_MODEL_PCC if is_wormhole_b0(device) else UNET_FULL_MODEL_PCC_BH,
     )
-
-    for _ in range(iterations - 1):
-        _, ttnn_input = create_unet_input_tensors(
-            batch,
-            groups,
-            channel_order="first",
-            pad=False,
-            fold=True,
-            device=device,
-            memory_config=unet_shallow_ttnn.UNet.input_sharded_memory_config,
-        )
-        ttnn_model(ttnn_input, move_input_tensor_to_device=False, deallocate_input_activation=True).cpu()
-        ttnn.DumpDeviceProfiler(device)
-
-
-@pytest.mark.parametrize("batch", [1])
-@pytest.mark.parametrize("groups", [4])
-@pytest.mark.parametrize("device_params", [{"l1_small_size": UNET_L1_SMALL_REGION_SIZE}], indirect=True)
-def test_unet_model(batch, groups, device, use_program_cache, reset_seeds):
-    if (
-        not is_wormhole_b0(device)
-        and device.compute_with_storage_grid_size().x * device.compute_with_storage_grid_size().y != 110
-    ):
-        pytest.skip(f"Shallow UNet only support 110 cores on BH (was {device.compute_with_storage_grid_size()})")
-    run_unet_model(batch, groups, device)
diff --git a/models/experimental/functional_unet/tests/test_unet_perf.py b/models/experimental/functional_unet/tests/test_unet_perf.py
index c60c6c396c..64ce3f3491 100644
--- a/models/experimental/functional_unet/tests/test_unet_perf.py
+++ b/models/experimental/functional_unet/tests/test_unet_perf.py
@@ -15,44 +15,27 @@ from models.utility_functions import (
 )
 
 from models.experimental.functional_unet.tests.common import UNET_TRACE_REGION_SIZE, UNET_L1_SMALL_REGION_SIZE
-from models.experimental.functional_unet.tests.test_unet_model import run_unet_model
-
-UNET_DEVICE_TEST_TOTAL_ITERATIONS = 4
-
-
-@pytest.mark.parametrize("batch", [1])
-@pytest.mark.parametrize("groups", [4])
-@pytest.mark.parametrize("iterations", [UNET_DEVICE_TEST_TOTAL_ITERATIONS])
-@pytest.mark.parametrize("device_params", [{"l1_small_size": UNET_L1_SMALL_REGION_SIZE}], indirect=True)
-def test_unet_model(batch, groups, device, iterations, use_program_cache, reset_seeds):
-    if (
-        not is_wormhole_b0(device)
-        and device.compute_with_storage_grid_size().x * device.compute_with_storage_grid_size().y != 110
-    ):
-        pytest.skip(f"Shallow UNet only support 110 cores on BH (was {device.compute_with_storage_grid_size()})")
-    device.disable_and_clear_program_cache()  # Needed to give consistent device perf between iterations
-    run_unet_model(batch, groups, device, iterations)
 
 
 @skip_for_grayskull("UNet not currently supported on GS")
 @pytest.mark.models_device_performance_bare_metal
 @pytest.mark.parametrize(
     "batch, groups, expected_device_perf_fps",
-    ((1, 4, 1415.0),),
+    ((1, 4, 1420.0),),
 )
 def test_unet_perf_device(batch: int, groups: int, expected_device_perf_fps: float):
-    command = f"pytest models/experimental/functional_unet/tests/test_unet_perf.py::test_unet_model"
+    command = f"pytest models/experimental/functional_unet/tests/test_unet_model.py::test_unet_model[device_params0-{groups}-{batch}]"
     cols = ["DEVICE FW", "DEVICE KERNEL", "DEVICE BRISC KERNEL"]
 
-    total_batch = groups * batch * UNET_DEVICE_TEST_TOTAL_ITERATIONS
+    total_batch = groups * batch
 
     inference_time_key = "AVG DEVICE KERNEL SAMPLES/S"
     post_processed_results = run_device_perf(
-        command, subdir="unet_shallow", num_iterations=1, cols=cols, batch_size=total_batch
+        command, subdir="unet_shallow", num_iterations=3, cols=cols, batch_size=total_batch
     )
     expected_perf_cols = {inference_time_key: expected_device_perf_fps}
     expected_results = check_device_perf(
-        post_processed_results, margin=0.005, expected_perf_cols=expected_perf_cols, assert_on_fail=True
+        post_processed_results, margin=0.015, expected_perf_cols=expected_perf_cols, assert_on_fail=True
     )
     prep_device_perf_report(
         model_name=f"unet-shallow_batch-{batch}_groups-{groups}",
diff --git a/models/experimental/stable_diffusion_xl_base/demo/demo.py b/models/experimental/stable_diffusion_xl_base/demo/demo.py
index 4ee391075c..17f1b8c9d9 100644
--- a/models/experimental/stable_diffusion_xl_base/demo/demo.py
+++ b/models/experimental/stable_diffusion_xl_base/demo/demo.py
@@ -280,14 +280,14 @@ def run_demo_inference(
                     negative_pooled_prompt_embed,
                     dtype=ttnn.bfloat16,
                     device=ttnn_device,
-                    layout=ttnn.ROW_MAJOR_LAYOUT,
+                    layout=ttnn.TILE_LAYOUT,
                     memory_config=ttnn.DRAM_MEMORY_CONFIG,
                 ),
                 ttnn.from_torch(
                     add_text_embed,
                     dtype=ttnn.bfloat16,
                     device=ttnn_device,
-                    layout=ttnn.ROW_MAJOR_LAYOUT,
+                    layout=ttnn.TILE_LAYOUT,
                     memory_config=ttnn.DRAM_MEMORY_CONFIG,
                 ),
             ]
@@ -337,7 +337,7 @@ def run_demo_inference(
                 add_text_embeds,
                 dtype=ttnn.bfloat16,
                 device=ttnn_device,
-                layout=ttnn.ROW_MAJOR_LAYOUT,
+                layout=ttnn.TILE_LAYOUT,
                 memory_config=ttnn.DRAM_MEMORY_CONFIG,
             )
         ]
diff --git a/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_crossattnupblock.py b/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_crossattnupblock.py
index a252209d47..7c520efbc1 100644
--- a/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_crossattnupblock.py
+++ b/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_crossattnupblock.py
@@ -26,7 +26,7 @@ from models.experimental.stable_diffusion_xl_base.tests.test_common import SDXL_
             20,
             1280,
             0,
-            0.966,
+            0.965,
         ),
         (
             (1, 1280, 64, 64),
diff --git a/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_feedforward.py b/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_feedforward.py
index eb611c4178..55134cc56b 100644
--- a/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_feedforward.py
+++ b/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_feedforward.py
@@ -29,6 +29,7 @@ def test_feedforward(
     unet = UNet2DConditionModel.from_pretrained(
         "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float32, use_safetensors=True, subfolder="unet"
     )
+    # unet = pipe.unet
     unet.eval()
     state_dict = unet.state_dict()
 
diff --git a/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_geglu.py b/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_geglu.py
index 94ed192d96..7797d3f76a 100644
--- a/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_geglu.py
+++ b/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_geglu.py
@@ -8,7 +8,6 @@ import torch
 import pytest
 import ttnn
 from models.experimental.stable_diffusion_xl_base.tt.tt_geglu import TtGEGLU
-from models.experimental.stable_diffusion_xl_base.tt.model_configs import ModelOptimisations
 from diffusers import UNet2DConditionModel
 from tests.ttnn.utils_for_testing import assert_with_pcc
 from models.utility_functions import torch_random
@@ -27,6 +26,7 @@ def test_geglu(device, input_shape, module_path, use_program_cache, reset_seeds,
     unet = UNet2DConditionModel.from_pretrained(
         "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float32, use_safetensors=True, subfolder="unet"
     )
+    # unet = pipe.unet
     unet.eval()
     state_dict = unet.state_dict()
 
@@ -39,8 +39,7 @@ def test_geglu(device, input_shape, module_path, use_program_cache, reset_seeds,
 
     assert torch_geglu is not None, f"{module_path} is not a valid UNet module"
 
-    model_config = ModelOptimisations()
-    tt_geglu = TtGEGLU(device, state_dict, module_path, model_config, weights_dtype=transformer_weights_dtype)
+    tt_geglu = TtGEGLU(device, state_dict, module_path, weights_dtype=transformer_weights_dtype)
 
     torch_input_tensor = torch_random(input_shape, -0.1, 0.1, dtype=torch.float32)
     torch_output_tensor = torch_geglu(torch_input_tensor)
@@ -50,7 +49,7 @@ def test_geglu(device, input_shape, module_path, use_program_cache, reset_seeds,
         dtype=ttnn.bfloat16,
         device=device,
         layout=ttnn.TILE_LAYOUT,
-        memory_config=ttnn.DRAM_MEMORY_CONFIG,
+        memory_config=ttnn.L1_MEMORY_CONFIG,
     )
     ttnn_output_tensor = tt_geglu.forward(ttnn_input_tensor)
     output_tensor = ttnn.to_torch(ttnn_output_tensor).squeeze()
diff --git a/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_transformerblock.py b/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_transformerblock.py
index 4ad087eaee..93383e7da4 100644
--- a/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_transformerblock.py
+++ b/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_transformerblock.py
@@ -15,12 +15,12 @@ from models.experimental.stable_diffusion_xl_base.tests.test_common import SDXL_
 
 
 @pytest.mark.parametrize(
-    "input_shape, encoder_shape, down_block_id, block_id, query_dim, num_attn_heads, out_dim",
+    "input_shape, encoder_shape, down_block_id, block_id, query_dim, num_attn_heads, out_dim, pcc",
     [
-        ((1, 4096, 640), (1, 77, 2048), 1, 0, 640, 10, 640),
-        ((1, 4096, 640), (1, 77, 2048), 1, 1, 640, 10, 640),
-        ((1, 1024, 1280), (1, 77, 2048), 2, 0, 1280, 20, 1280),
-        ((1, 1024, 1280), (1, 77, 2048), 2, 1, 1280, 20, 1280),
+        ((1, 4096, 640), (1, 77, 2048), 1, 0, 640, 10, 640, 0.999),
+        ((1, 4096, 640), (1, 77, 2048), 1, 1, 640, 10, 640, 0.999),
+        ((1, 1024, 1280), (1, 77, 2048), 2, 0, 1280, 20, 1280, 0.999),
+        ((1, 1024, 1280), (1, 77, 2048), 2, 1, 1280, 20, 1280, 0.998),
     ],
 )
 @pytest.mark.parametrize("transformer_weights_dtype", [ttnn.bfloat16])
@@ -37,10 +37,12 @@ def test_transformerblock(
     use_program_cache,
     reset_seeds,
     transformer_weights_dtype,
+    pcc,
 ):
     unet = UNet2DConditionModel.from_pretrained(
         "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float32, use_safetensors=True, subfolder="unet"
     )
+    # unet = pipe.unet
     unet.eval()
     state_dict = unet.state_dict()
 
@@ -61,19 +63,19 @@ def test_transformerblock(
 
     torch_output_tensor = torch_transformerblock(torch_input_tensor, None, torch_encoder_tensor).unsqueeze(0)
 
-    ttnn_encoder_tensor = ttnn.from_torch(
-        torch_encoder_tensor,
+    ttnn_input_tensor = ttnn.from_torch(
+        torch_input_tensor,
         dtype=ttnn.bfloat16,
         device=device,
         layout=ttnn.TILE_LAYOUT,
         memory_config=ttnn.L1_MEMORY_CONFIG,
     )
-    ttnn_input_tensor = ttnn.from_torch(
-        torch_input_tensor,
+    ttnn_encoder_tensor = ttnn.from_torch(
+        torch_encoder_tensor,
         dtype=ttnn.bfloat16,
         device=device,
         layout=ttnn.TILE_LAYOUT,
-        memory_config=ttnn.DRAM_MEMORY_CONFIG,
+        memory_config=ttnn.L1_MEMORY_CONFIG,
     )
     ttnn_output_tensor = tt_transformerblock.forward(ttnn_input_tensor, None, ttnn_encoder_tensor)
     output_tensor = ttnn.to_torch(ttnn_output_tensor)
@@ -81,5 +83,5 @@ def test_transformerblock(
     del unet
     gc.collect()
 
-    _, pcc_message = assert_with_pcc(torch_output_tensor, output_tensor, 0.999)
+    _, pcc_message = assert_with_pcc(torch_output_tensor, output_tensor, pcc)
     logger.info(f"PCC is: {pcc_message}")
diff --git a/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_transformermodel.py b/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_transformermodel.py
index 8bc7030677..f10f645f07 100644
--- a/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_transformermodel.py
+++ b/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_transformermodel.py
@@ -18,7 +18,7 @@ from models.experimental.stable_diffusion_xl_base.tests.test_common import SDXL_
     "input_shape, encoder_shape, down_block_id, query_dim, num_attn_heads, out_dim, pcc",
     [
         ((1, 640, 64, 64), (1, 77, 2048), 1, 640, 10, 640, 0.999),
-        ((1, 1280, 32, 32), (1, 77, 2048), 2, 1280, 20, 1280, 0.997),
+        ((1, 1280, 32, 32), (1, 77, 2048), 2, 1280, 20, 1280, 0.996),
     ],
 )
 @pytest.mark.parametrize("transformer_weights_dtype", [ttnn.bfloat16])
@@ -39,6 +39,7 @@ def test_transformermodel(
     unet = UNet2DConditionModel.from_pretrained(
         "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float32, use_safetensors=True, subfolder="unet"
     )
+    # unet = pipe.unet
     unet.eval()
     state_dict = unet.state_dict()
 
@@ -59,26 +60,25 @@ def test_transformermodel(
 
     torch_output_tensor = torch_transformerblock(torch_input_tensor, encoder_hidden_states=torch_encoder_tensor).sample
 
-    ttnn_encoder_tensor = ttnn.from_torch(
-        torch_encoder_tensor,
-        dtype=ttnn.bfloat16,
-        device=device,
-        layout=ttnn.TILE_LAYOUT,
-        memory_config=ttnn.L1_MEMORY_CONFIG,
-    )
-
     ttnn_input_tensor = ttnn.from_torch(
         torch_input_tensor,
         dtype=ttnn.bfloat16,
         device=device,
         layout=ttnn.TILE_LAYOUT,
-        memory_config=ttnn.DRAM_MEMORY_CONFIG,
+        memory_config=ttnn.L1_MEMORY_CONFIG,
     )
     B, C, H, W = list(ttnn_input_tensor.shape)
 
     ttnn_input_tensor = ttnn.permute(ttnn_input_tensor, (0, 2, 3, 1))
     ttnn_input_tensor = ttnn.reshape(ttnn_input_tensor, (B, 1, H * W, C))
 
+    ttnn_encoder_tensor = ttnn.from_torch(
+        torch_encoder_tensor,
+        dtype=ttnn.bfloat16,
+        device=device,
+        layout=ttnn.TILE_LAYOUT,
+        memory_config=ttnn.L1_MEMORY_CONFIG,
+    )
     ttnn_output_tensor = tt_transformerblock.forward(ttnn_input_tensor, [B, C, H, W], None, ttnn_encoder_tensor)
     output_tensor = ttnn.to_torch(ttnn_output_tensor)
     output_tensor = output_tensor.reshape(B, H, W, C)
diff --git a/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_unet.py b/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_unet.py
index df96879a79..6ad1b2aa03 100644
--- a/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_unet.py
+++ b/models/experimental/stable_diffusion_xl_base/tests/pcc/test_module_tt_unet.py
@@ -18,6 +18,17 @@ def prepare_ttnn_tensors(
     device, torch_input_tensor, torch_timestep_tensor, torch_temb_tensor, torch_encoder_tensor, torch_time_ids
 ):
     torch.manual_seed(2025)
+    ttnn_input_tensor = ttnn.from_torch(
+        torch_input_tensor,
+        dtype=ttnn.bfloat16,
+        device=device,
+        layout=ttnn.TILE_LAYOUT,
+        memory_config=ttnn.L1_MEMORY_CONFIG,
+    )
+    B, C, H, W = list(ttnn_input_tensor.shape)
+
+    ttnn_input_tensor = ttnn.permute(ttnn_input_tensor, (0, 2, 3, 1))
+    ttnn_input_tensor = ttnn.reshape(ttnn_input_tensor, (B, 1, H * W, C))
 
     ttnn_timestep_tensor = ttnn.from_torch(
         torch_timestep_tensor,
@@ -38,7 +49,7 @@ def prepare_ttnn_tensors(
         torch_temb_tensor,
         dtype=ttnn.bfloat16,
         device=device,
-        layout=ttnn.ROW_MAJOR_LAYOUT,
+        layout=ttnn.TILE_LAYOUT,
         memory_config=ttnn.L1_MEMORY_CONFIG,
     )
     ttnn_time_ids = ttnn.from_torch(
@@ -53,31 +64,29 @@ def prepare_ttnn_tensors(
         "time_ids": ttnn_time_ids,
     }
 
-    ttnn_input_tensor = ttnn.from_torch(
-        torch_input_tensor,
-        dtype=ttnn.bfloat16,
-        device=device,
-        layout=ttnn.TILE_LAYOUT,
-        memory_config=ttnn.L1_MEMORY_CONFIG,
-    )
-    B, C, H, W = list(ttnn_input_tensor.shape)
-
-    ttnn_input_tensor = ttnn.permute(ttnn_input_tensor, (0, 2, 3, 1))
-    ttnn_input_tensor = ttnn.reshape(ttnn_input_tensor, (B, 1, H * W, C))
-
     return ttnn_input_tensor, [B, C, H, W], ttnn_timestep_tensor, ttnn_encoder_tensor, ttnn_added_cond_kwargs
 
 
-def run_unet_model(
+@pytest.mark.parametrize(
+    "input_shape, timestep_shape, encoder_shape, temb_shape, time_ids_shape",
+    [
+        ((1, 4, 128, 128), (1,), (1, 77, 2048), (1, 1280), (1, 6)),
+    ],
+)
+@pytest.mark.parametrize("conv_weights_dtype", [ttnn.bfloat16])
+@pytest.mark.parametrize("transformer_weights_dtype", [ttnn.bfloat16])
+@pytest.mark.parametrize("device_params", [{"l1_small_size": SDXL_L1_SMALL_SIZE}], indirect=True)
+def test_unet(
     device,
     input_shape,
     timestep_shape,
     encoder_shape,
     temb_shape,
     time_ids_shape,
+    use_program_cache,
+    reset_seeds,
     conv_weights_dtype,
     transformer_weights_dtype,
-    iterations=1,
 ):
     unet = UNet2DConditionModel.from_pretrained(
         "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float32, use_safetensors=True, subfolder="unet"
@@ -122,6 +131,7 @@ def run_unet_model(
     ) = prepare_ttnn_tensors(
         device, torch_input_tensor, torch_timestep_tensor, torch_temb_tensor, torch_encoder_tensor, torch_time_ids
     )
+
     ttnn_output_tensor, output_shape = tt_unet.forward(
         ttnn_input_tensor,
         [B, C, H, W],
@@ -130,76 +140,12 @@ def run_unet_model(
         added_cond_kwargs=ttnn_added_cond_kwargs,
     )
 
-    output_tensor = ttnn.to_torch(ttnn_output_tensor.cpu())
+    output_tensor = ttnn.to_torch(ttnn_output_tensor)
     output_tensor = output_tensor.reshape(B, output_shape[1], output_shape[2], output_shape[0])
     output_tensor = torch.permute(output_tensor, (0, 3, 1, 2))
 
-    ttnn.deallocate(ttnn_input_tensor)
-    ttnn.deallocate(ttnn_output_tensor)
-    ttnn.deallocate(ttnn_timestep_tensor)
-    ttnn.deallocate(ttnn_encoder_tensor)
-
-    ttnn.DumpDeviceProfiler(device)
-
-    _, pcc_message = assert_with_pcc(torch_output_tensor, output_tensor, 0.995)
-    logger.info(f"PCC of first iteration is: {pcc_message}")
-
-    for _ in range(iterations - 1):
-        (
-            ttnn_input_tensor,
-            [B, C, H, W],
-            ttnn_timestep_tensor,
-            ttnn_encoder_tensor,
-            ttnn_added_cond_kwargs,
-        ) = prepare_ttnn_tensors(
-            device, torch_input_tensor, torch_timestep_tensor, torch_temb_tensor, torch_encoder_tensor, torch_time_ids
-        )
-        ttnn_output_tensor, output_shape = tt_unet.forward(
-            ttnn_input_tensor,
-            [B, C, H, W],
-            timestep=ttnn_timestep_tensor,
-            encoder_hidden_states=ttnn_encoder_tensor,
-            added_cond_kwargs=ttnn_added_cond_kwargs,
-        )
-        ttnn.deallocate(ttnn_input_tensor)
-        ttnn.deallocate(ttnn_output_tensor)
-        ttnn.deallocate(ttnn_timestep_tensor)
-        ttnn.deallocate(ttnn_encoder_tensor)
-
-        ttnn.DumpDeviceProfiler(device)
-
     del unet
     gc.collect()
 
-
-@pytest.mark.parametrize(
-    "input_shape, timestep_shape, encoder_shape, temb_shape, time_ids_shape",
-    [
-        ((1, 4, 128, 128), (1,), (1, 77, 2048), (1, 1280), (1, 6)),
-    ],
-)
-@pytest.mark.parametrize("conv_weights_dtype", [ttnn.bfloat16])
-@pytest.mark.parametrize("transformer_weights_dtype", [ttnn.bfloat16])
-@pytest.mark.parametrize("device_params", [{"l1_small_size": SDXL_L1_SMALL_SIZE}], indirect=True)
-def test_unet(
-    device,
-    input_shape,
-    timestep_shape,
-    encoder_shape,
-    temb_shape,
-    time_ids_shape,
-    conv_weights_dtype,
-    transformer_weights_dtype,
-    use_program_cache,
-    reset_seeds,
-):
-    run_unet_model(
-        device,
-        input_shape,
-        timestep_shape,
-        encoder_shape,
-        temb_shape,
-        time_ids_shape,
-        conv_weights_dtype,
-        transformer_weights_dtype,
-    )
+    _, pcc_message = assert_with_pcc(torch_output_tensor, output_tensor, 0.995)
+    logger.info(f"PCC is: {pcc_message}")
diff --git a/models/experimental/stable_diffusion_xl_base/tests/test_sdxl_perf.py b/models/experimental/stable_diffusion_xl_base/tests/test_sdxl_perf.py
index f8c0a9823b..ed8c89f5c0 100644
--- a/models/experimental/stable_diffusion_xl_base/tests/test_sdxl_perf.py
+++ b/models/experimental/stable_diffusion_xl_base/tests/test_sdxl_perf.py
@@ -3,76 +3,24 @@
 # SPDX-License-Identifier: Apache-2.0
 
 import pytest
-import ttnn
-
 from models.perf.device_perf_utils import run_device_perf, check_device_perf, prep_device_perf_report
 
-from models.experimental.stable_diffusion_xl_base.tests.test_common import SDXL_L1_SMALL_SIZE
-from models.experimental.stable_diffusion_xl_base.tests.pcc.test_module_tt_unet import run_unet_model
-
-UNET_DEVICE_TEST_TOTAL_ITERATIONS = 3
-
-
-@pytest.mark.parametrize(
-    "input_shape, timestep_shape, encoder_shape, temb_shape, time_ids_shape",
-    [
-        ((1, 4, 128, 128), (1,), (1, 77, 2048), (1, 1280), (1, 6)),
-    ],
-)
-@pytest.mark.parametrize("conv_weights_dtype", [ttnn.bfloat16])
-@pytest.mark.parametrize("transformer_weights_dtype", [ttnn.bfloat16])
-@pytest.mark.parametrize("device_params", [{"l1_small_size": SDXL_L1_SMALL_SIZE}], indirect=True)
-@pytest.mark.parametrize("iterations", [UNET_DEVICE_TEST_TOTAL_ITERATIONS])
-def test_unet(
-    device,
-    input_shape,
-    timestep_shape,
-    encoder_shape,
-    temb_shape,
-    time_ids_shape,
-    conv_weights_dtype,
-    transformer_weights_dtype,
-    iterations,
-    use_program_cache,
-    reset_seeds,
-):
-    run_unet_model(
-        device,
-        input_shape,
-        timestep_shape,
-        encoder_shape,
-        temb_shape,
-        time_ids_shape,
-        conv_weights_dtype,
-        transformer_weights_dtype,
-        iterations=iterations,
-    )
-
 
 @pytest.mark.models_device_performance_bare_metal
 def test_sdxl_unet_perf_device():
-    expected_device_perf_cycles_per_iteration = 387473640
-
-    command = f"pytest models/experimental/stable_diffusion_xl_base/tests/test_sdxl_perf.py::test_unet"
+    command = f"pytest tests/nightly/single_card/stable_diffusion_xl_base/test_module_tt_unet.py::test_unet[device_params0-transformer_weights_dtype0-conv_weights_dtype0-input_shape0-timestep_shape0-encoder_shape0-temb_shape0-time_ids_shape0]"
     cols = ["DEVICE FW", "DEVICE KERNEL", "DEVICE BRISC KERNEL"]
 
-    batch_size = 1
-    total_batch_size = batch_size * UNET_DEVICE_TEST_TOTAL_ITERATIONS
-
     inference_time_key = "AVG DEVICE KERNEL DURATION [ns]"
-    post_processed_results = run_device_perf(
-        command, subdir="sdxl_unet", num_iterations=1, cols=cols, batch_size=total_batch_size
-    )
-    expected_perf_cols = {
-        inference_time_key: expected_device_perf_cycles_per_iteration * UNET_DEVICE_TEST_TOTAL_ITERATIONS
-    }
+    post_processed_results = run_device_perf(command, subdir="sdxl_unet", num_iterations=3, cols=cols, batch_size=1)
+    expected_perf_cols = {inference_time_key: 464623492}
     expected_results = check_device_perf(
         post_processed_results, margin=0.015, expected_perf_cols=expected_perf_cols, assert_on_fail=True
     )
     prep_device_perf_report(
-        model_name=f"sdxl_unet",
-        batch_size=total_batch_size,
+        model_name=f"sdxl_unet_single_iter",
+        batch_size=1,
         post_processed_results=post_processed_results,
         expected_results=expected_results,
-        comments=f"iterations={UNET_DEVICE_TEST_TOTAL_ITERATIONS}",
+        comments="",
     )
diff --git a/models/experimental/stable_diffusion_xl_base/tt/model_configs.py b/models/experimental/stable_diffusion_xl_base/tt/model_configs.py
index ca8251ca14..9b89b4beee 100644
--- a/models/experimental/stable_diffusion_xl_base/tt/model_configs.py
+++ b/models/experimental/stable_diffusion_xl_base/tt/model_configs.py
@@ -416,51 +416,6 @@ class ModelOptimisations:
             fused_activation=None,
         )
 
-        self.matmul_configs["2D_GEGLU_LINEAR_640"] = ttnn.MatmulMultiCoreReuseMultiCastProgramConfig(
-            compute_with_storage_grid_size=(8, 8),
-            in0_block_w=4,
-            per_core_M=16,
-            per_core_N=20,
-            out_subblock_h=1,
-            out_subblock_w=5,
-            transpose_mcast=False,
-            fused_activation=None,
-        )
-
-        self.matmul_configs["1D_GEGLU_LINEAR_1280"] = ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig(
-            compute_with_storage_grid_size=(8, 8),
-            in0_block_w=4,
-            out_subblock_h=1,
-            out_subblock_w=5,
-            per_core_M=32,
-            per_core_N=5,
-            fuse_batch=False,
-            fused_activation=None,
-            mcast_in0=True,
-        )
-
-        self.matmul_configs["2D_TM_LINEAR_640"] = ttnn.MatmulMultiCoreReuseMultiCastProgramConfig(
-            compute_with_storage_grid_size=(8, 8),
-            in0_block_w=2,
-            per_core_M=16,
-            per_core_N=3,
-            out_subblock_h=8,
-            out_subblock_w=1,
-            transpose_mcast=False,
-            fused_activation=None,
-        )
-
-        self.matmul_configs["2D_TM_LINEAR_1280"] = ttnn.MatmulMultiCoreReuseMultiCastProgramConfig(
-            compute_with_storage_grid_size=(8, 8),
-            in0_block_w=5,
-            per_core_M=4,
-            per_core_N=5,
-            out_subblock_h=1,
-            out_subblock_w=5,
-            transpose_mcast=False,
-            fused_activation=None,
-        )
-
         self.compute_configs["DEFAULT_MM_COMPUTE_CONFIG"] = ttnn.WormholeComputeKernelConfig(
             math_fidelity=ttnn.MathFidelity.HiFi2,
             math_approx_mode=False,
@@ -479,20 +434,6 @@ class ModelOptimisations:
             return None
 
         if not ("decoder" in matmul_path):
-            # # # GEGLU # # #
-            if "net.0.proj" in matmul_path:
-                if "down_blocks.1" in matmul_path or "up_blocks.1" in matmul_path:
-                    return self.matmul_configs["2D_GEGLU_LINEAR_640"]
-                else:
-                    return self.matmul_configs["1D_GEGLU_LINEAR_1280"]
-
-            # # # TM LINEAR # # #
-            if "proj_in" in matmul_path or "proj_out" in matmul_path:
-                if "down_blocks.1" in matmul_path or "up_blocks.1" in matmul_path:
-                    return self.matmul_configs["2D_TM_LINEAR_640"]
-                else:
-                    return self.matmul_configs["2D_TM_LINEAR_1280"]
-
             # # # Down block 1 # # #
             pattern_downn_block_1_dense_out = re.compile(
                 r"down_blocks\.1\.attentions\.[01]\.transformer_blocks\.[01]\.attn[12]\.dense_out"
diff --git a/models/experimental/stable_diffusion_xl_base/tt/tt_feedforward.py b/models/experimental/stable_diffusion_xl_base/tt/tt_feedforward.py
index faa0705272..f9823aa382 100644
--- a/models/experimental/stable_diffusion_xl_base/tt/tt_feedforward.py
+++ b/models/experimental/stable_diffusion_xl_base/tt/tt_feedforward.py
@@ -21,7 +21,7 @@ class TtFeedForward(nn.Module):
         super().__init__()
 
         self.device = device
-        self.tt_geglu = TtGEGLU(device, state_dict, f"{module_path}.net.0", model_config, weights_dtype=weights_dtype)
+        self.tt_geglu = TtGEGLU(device, state_dict, f"{module_path}.net.0", weights_dtype=weights_dtype)
 
         weights = state_dict[f"{module_path}.net.2.weight"].unsqueeze(0).unsqueeze(0)
         bias = state_dict[f"{module_path}.net.2.bias"]
diff --git a/models/experimental/stable_diffusion_xl_base/tt/tt_geglu.py b/models/experimental/stable_diffusion_xl_base/tt/tt_geglu.py
index b3118e8b47..0afb2ff7f8 100644
--- a/models/experimental/stable_diffusion_xl_base/tt/tt_geglu.py
+++ b/models/experimental/stable_diffusion_xl_base/tt/tt_geglu.py
@@ -9,7 +9,7 @@ from models.experimental.stable_diffusion_xl_base.tt.sdxl_utility import prepare
 
 
 class TtGEGLU(nn.Module):
-    def __init__(self, device, state_dict, module_path, model_config, weights_dtype=ttnn.bfloat16):
+    def __init__(self, device, state_dict, module_path, weights_dtype=ttnn.bfloat16):
         super().__init__()
 
         self.device = device
@@ -17,33 +17,17 @@ class TtGEGLU(nn.Module):
         bias = state_dict[f"{module_path}.proj.bias"]
 
         self.tt_weights, self.tt_bias = prepare_linear_params(device, weights, bias, weights_dtype)
-        self.program_config = model_config.get_matmul_config(matmul_path=f"{module_path}.proj")
-        self.compute_config = model_config.get_mm_compute_config(f"{module_path}.proj")
-
-    def forward(self, input_tensor):
-        output_memory_config = ttnn.create_sharded_memory_config(
-            (self.program_config.per_core_M * 32, self.program_config.per_core_N * 32),
-            core_grid=ttnn.CoreGrid(y=8, x=8),
-            strategy=ttnn.ShardStrategy.WIDTH if input_tensor.shape[-1] == 1280 else ttnn.ShardStrategy.BLOCK,
-            orientation=ttnn.ShardOrientation.ROW_MAJOR,
-            use_height_and_width_as_shard_shape=True,
-        )
+
+    def forward(self, hidden_states):
         hidden_states = ttnn.linear(
-            input_tensor,
+            hidden_states,
             self.tt_weights,
             bias=self.tt_bias,
-            memory_config=output_memory_config,
-            program_config=self.program_config,
-            compute_kernel_config=self.compute_config,
         )
-        ttnn.deallocate(input_tensor)
-
-        hidden_states = ttnn.sharded_to_interleaved(hidden_states, ttnn.L1_MEMORY_CONFIG)
         gate = hidden_states[:, :, :, hidden_states.shape[3] // 2 :]
         hidden_states = hidden_states[:, :, :, : hidden_states.shape[3] // 2]
-        gate = ttnn.gelu(gate)
 
         # ttnn.split not working properly
         # hidden_states, gate = ttnn.split(hidden_states, ceil(hidden_states.shape[3] / 2), 3)
 
-        return ttnn.multiply(hidden_states, gate)
+        return ttnn.multiply(hidden_states, ttnn.gelu(gate))
diff --git a/models/experimental/stable_diffusion_xl_base/tt/tt_resnetblock2d.py b/models/experimental/stable_diffusion_xl_base/tt/tt_resnetblock2d.py
index 596f3b97b5..26bf94551c 100644
--- a/models/experimental/stable_diffusion_xl_base/tt/tt_resnetblock2d.py
+++ b/models/experimental/stable_diffusion_xl_base/tt/tt_resnetblock2d.py
@@ -321,7 +321,6 @@ class TtResnetBlock2D(nn.Module):
         C = self.conv2_params["output_channels"]
 
         if self.tt_conv3_weights is not None:
-            input_tensor_pre_conv = input_tensor
             [input_tensor, [H, W], [self.tt_conv3_weights, self.tt_conv3_bias]] = ttnn.conv2d(
                 input_tensor=input_tensor,
                 weight_tensor=self.tt_conv3_weights,
@@ -343,7 +342,6 @@ class TtResnetBlock2D(nn.Module):
                 return_output_dim=True,
                 return_weights_and_bias=True,
             )
-            ttnn.deallocate(input_tensor_pre_conv)
             C = self.conv3_params["output_channels"]
             if input_tensor.is_sharded():
                 input_tensor = ttnn.sharded_to_interleaved(input_tensor, ttnn.L1_MEMORY_CONFIG)
diff --git a/models/experimental/stable_diffusion_xl_base/tt/tt_transformerblock.py b/models/experimental/stable_diffusion_xl_base/tt/tt_transformerblock.py
index e06e69dcdc..0bf92fcd18 100644
--- a/models/experimental/stable_diffusion_xl_base/tt/tt_transformerblock.py
+++ b/models/experimental/stable_diffusion_xl_base/tt/tt_transformerblock.py
@@ -74,11 +74,10 @@ class TtBasicTransformerBlock(nn.Module):
             else None
         )
 
-    def forward(self, input_tensor, attention_mask=None, encoder_hidden_states=None):
-        attn_hidden_states = ttnn.layer_norm(input_tensor, weight=self.tt_norm1_weights, bias=self.tt_norm1_bias)
+    def forward(self, hidden_states, attention_mask=None, encoder_hidden_states=None):
+        attn_hidden_states = ttnn.layer_norm(hidden_states, weight=self.tt_norm1_weights, bias=self.tt_norm1_bias)
         attn_hidden_states = self.attn1(attn_hidden_states, attention_mask, None)
-        hidden_states = ttnn.add(input_tensor, attn_hidden_states)
-        ttnn.deallocate(input_tensor)
+        hidden_states = ttnn.add(hidden_states, attn_hidden_states)
 
         attn_hidden_states = ttnn.layer_norm(hidden_states, weight=self.tt_norm2_weights, bias=self.tt_norm2_bias)
         attn_hidden_states = self.attn2(attn_hidden_states, attention_mask, encoder_hidden_states)
diff --git a/models/experimental/stable_diffusion_xl_base/tt/tt_transformermodel.py b/models/experimental/stable_diffusion_xl_base/tt/tt_transformermodel.py
index 900aade012..9fa8665984 100644
--- a/models/experimental/stable_diffusion_xl_base/tt/tt_transformermodel.py
+++ b/models/experimental/stable_diffusion_xl_base/tt/tt_transformermodel.py
@@ -66,11 +66,6 @@ class TtTransformer2DModel(nn.Module):
         bias = state_dict[f"{module_path}.proj_out.bias"]
         self.tt_weights_out, self.tt_bias_out = prepare_linear_params(device, weights, bias, weights_dtype)
 
-        self.program_config_in = model_config.get_matmul_config(matmul_path=f"{module_path}.proj_in")
-        self.compute_config_in = model_config.get_mm_compute_config(f"{module_path}.proj_in")
-        self.program_config_out = model_config.get_matmul_config(matmul_path=f"{module_path}.proj_out")
-        self.compute_config_out = model_config.get_mm_compute_config(f"{module_path}.proj_out")
-
     def forward(self, input_tensor, input_shape, attention_mask=None, encoder_hidden_states=None):
         B, C, H, W = input_shape
         hidden_states = ttnn.to_layout(input_tensor, ttnn.ROW_MAJOR_LAYOUT)
@@ -102,8 +97,6 @@ class TtTransformer2DModel(nn.Module):
             hidden_states,
             self.tt_weights_in,
             bias=self.tt_bias_in,
-            program_config=self.program_config_in,
-            compute_kernel_config=self.compute_config_in,
         )
 
         for i, transformer_block in enumerate(self.transformer_blocks):
@@ -113,8 +106,6 @@ class TtTransformer2DModel(nn.Module):
             hidden_states,
             self.tt_weights_out,
             bias=self.tt_bias_out,
-            program_config=self.program_config_out,
-            compute_kernel_config=self.compute_config_out,
         )
 
         hidden_states = ttnn.add(hidden_states, input_tensor)
diff --git a/models/experimental/stable_diffusion_xl_base/tt/tt_unet.py b/models/experimental/stable_diffusion_xl_base/tt/tt_unet.py
index 4fb3d55d37..a093433989 100644
--- a/models/experimental/stable_diffusion_xl_base/tt/tt_unet.py
+++ b/models/experimental/stable_diffusion_xl_base/tt/tt_unet.py
@@ -176,6 +176,7 @@ class TtUNet2DConditionModel(nn.Module):
         time_ids = added_cond_kwargs.get("time_ids")
         temb_add = self.add_time_proj.forward(time_ids)
         temb_add = ttnn.to_layout(temb_add, ttnn.ROW_MAJOR_LAYOUT)
+        text_embeds = ttnn.to_layout(text_embeds, ttnn.ROW_MAJOR_LAYOUT)
         temb_add = ttnn.reshape(temb_add, (text_embeds.shape[0], -1))
         temb_add = ttnn.concat([text_embeds, temb_add], -1)
         temb_add = ttnn.to_layout(temb_add, ttnn.TILE_LAYOUT)
diff --git a/models/experimental/yolov8s_world/tests/test_perf_yolov8s_world.py b/models/experimental/yolov8s_world/tests/test_perf_yolov8s_world.py
index c97a04aa28..70b54876e8 100644
--- a/models/experimental/yolov8s_world/tests/test_perf_yolov8s_world.py
+++ b/models/experimental/yolov8s_world/tests/test_perf_yolov8s_world.py
@@ -132,7 +132,7 @@ def test_perf(device, use_pretrained_weight, use_program_cache):
 @pytest.mark.parametrize(
     "batch_size, expected_perf",
     [
-        [1, 80.0],
+        [1, 79.2],
     ],
 )
 @pytest.mark.models_device_performance_bare_metal
diff --git a/scripts/debugging_scripts/parse_inspector_logs.py b/scripts/debugging_scripts/parse_inspector_logs.py
index aed8a22ae1..2b4581389e 100755
--- a/scripts/debugging_scripts/parse_inspector_logs.py
+++ b/scripts/debugging_scripts/parse_inspector_logs.py
@@ -17,7 +17,6 @@ Description:
 """
 
 from dataclasses import dataclass
-from functools import cache
 import os
 import sys
 import yaml
@@ -25,25 +24,6 @@ from datetime import datetime, timedelta, timezone
 from docopt import docopt
 
 
-# Note: This method is parsing enty by entry and should be used only for debugging large log files.
-def fast_parse_yaml_log_file(log_file: str):
-    log_entry = ""
-    with open(log_file, "r") as f:
-        while (line := f.readline()) != "":
-            if len(line) > 0 and line[0] != " " and line[0] != "\t" and line[0] != "\n":
-                if len(log_entry) > 0:
-                    parsed_entry = yaml.safe_load(log_entry)
-                    if log_entry[0] == "-":
-                        yield parsed_entry[0]
-                    else:
-                        yield parsed_entry
-                log_entry = line
-            else:
-                log_entry += line
-    if len(log_entry) > 0:
-        yield yaml.safe_load(log_entry)
-
-
 @dataclass
 class KernelData:
     watcher_kernel_id: int
@@ -84,7 +64,7 @@ def get_kernels(log_directory: str) -> list[KernelData]:
     return kernels
 
 
-def get_programs(log_directory: str, verbose: bool = False) -> dict[int, ProgramData]:
+def get_programs(log_directory: str, verbose: bool = False) -> list[ProgramData]:
     yaml_path = os.path.join(log_directory, "programs_log.yaml")
     with open(yaml_path, "r") as f:
         data = yaml.safe_load(f)
@@ -108,20 +88,25 @@ def get_programs(log_directory: str, verbose: bool = False) -> dict[int, Program
             f"  {convert_timestamp(timestamp_ns).strftime('%Y-%m-%d %H:%M:%S.%f')}: {message}"
         )
 
-    programs: dict[int, ProgramData] = {}
+    programs: list[ProgramData] = []
     for entry in data:
         if "program_created" in entry:
             info = entry["program_created"]
             program_id = int(info.get("id"))
-            programs[program_id] = ProgramData(
-                id=program_id, compiled=False, watcher_kernel_ids=[], binary_status_per_device={}
+            programs.append(
+                ProgramData(
+                    id=program_id,
+                    compiled=False,
+                    watcher_kernel_ids=[],
+                    binary_status_per_device={},
+                )
             )
             if verbose:
                 print_log(int(info.get("timestamp_ns")), f"Program {program_id} created")
         elif "program_destroyed" in entry:
             info = entry["program_destroyed"]
             program_id = int(info.get("id"))
-            del programs[program_id]
+            programs = [p for p in programs if p.id != program_id]
             if verbose:
                 print_log(int(info.get("timestamp_ns")), f"Program {program_id} destroyed")
         elif "program_compile_started" in entry:
@@ -168,9 +153,9 @@ def get_programs(log_directory: str, verbose: bool = False) -> dict[int, Program
     return programs
 
 
-def get_devices_in_use(programs: dict[int, ProgramData]) -> set[int]:
+def get_devices_in_use(programs: list[ProgramData]) -> set[int]:
     used_devices = set()
-    for program in programs.values():
+    for program in programs:
         # Only include devices with status "Committed"
         committed_devices = {
             device_id for device_id, status in program.binary_status_per_device.items() if status == "Committed"
@@ -179,30 +164,6 @@ def get_devices_in_use(programs: dict[int, ProgramData]) -> set[int]:
     return used_devices
 
 
-class InspectorData:
-    def __init__(self, log_directory: str):
-        self.log_directory = log_directory
-
-    @cache
-    def kernels(self) -> list[KernelData]:
-        return get_kernels(self.log_directory)
-
-    def programs(self) -> dict[int, ProgramData]:
-        return get_programs(self.log_directory)
-
-    def devices_in_use(self) -> set[int]:
-        return get_devices_in_use(self.programs())
-
-
-@cache
-def get_data() -> InspectorData:
-    log_directory = os.environ.get("TT_METAL_HOME", "")
-    if not log_directory:
-        raise ValueError("TT_METAL_HOME environment variable is not set")
-    log_directory = os.path.join(log_directory, "generated", "inspector")
-    return InspectorData(log_directory)
-
-
 def main():
     args = docopt(__doc__, argv=sys.argv[1:])
     log_directory = args["<log-directory>"]
@@ -215,7 +176,7 @@ def main():
 
     programs = get_programs(log_directory, verbose=True)
     print("Programs:")
-    for program in programs.values():
+    for program in programs:
         print(f"  Program ID {program.id}, compiled: {program.compiled}")
         print(f"    Binary status per device: {program.binary_status_per_device}")
         print(f"    Watcher Kernel IDs: {program.watcher_kernel_ids}")
diff --git a/tech_reports/prog_examples/matmul_single_core/matmul_single_core.md b/tech_reports/prog_examples/matmul_single_core/matmul_single_core.md
index df5d244e35..06adea15b9 100644
--- a/tech_reports/prog_examples/matmul_single_core/matmul_single_core.md
+++ b/tech_reports/prog_examples/matmul_single_core/matmul_single_core.md
@@ -172,19 +172,19 @@ We're using a special reader kernel to take in data from DRAM into L1, and a spe
 ``` cpp
 auto reader_id = tt_metal::CreateDataMovementKernel(
     program,
-    "tt_metal/programming_examples/matmul_single_core/kernels/dataflow/reader_single_core_mm.cpp",
+    "tt_metal/programming_examples/matmul_common/kernels/dataflow/reader_bmm_8bank.cpp",
     core,
     tt_metal::DataMovementConfig{.processor = DataMovementProcessor::RISCV_1, .noc = NOC::RISCV_1_default, .compile_args = reader_compile_time_args});
 
 auto writer_id = tt_metal::CreateDataMovementKernel(
     program,
-    "tt_metal/programming_examples/matmul_single_core/kernels/dataflow/writer_single_core_mm.cpp",
+    "tt_metal/programming_examples/matmul_common/kernels/dataflow/writer_bmm_8bank.cpp",
     core,
     tt_metal::DataMovementConfig{.processor = DataMovementProcessor::RISCV_0, .noc = NOC::RISCV_0_default, .compile_args = writer_compile_time_args});
 
 auto matmul_single_core_kernel_id = tt_metal::CreateComputeKernel(
     program,
-    "tt_metal/programming_examples/matmul_single_core/kernels/compute/mm.cpp",
+    "tt_metal/programming_examples/matmul_common/kernels/compute/bmm.cpp",
     core,
     tt_metal::ComputeConfig{.math_fidelity = math_fidelity, .compile_args = compute_args}
 );
diff --git a/tests/tests_common/sfpu_helper/sfpu_helper.hpp b/tests/tests_common/sfpu_helper/sfpu_helper.hpp
index 4bf9dcd407..1665ec7066 100644
--- a/tests/tests_common/sfpu_helper/sfpu_helper.hpp
+++ b/tests/tests_common/sfpu_helper/sfpu_helper.hpp
@@ -5,7 +5,7 @@
 // Sfpu golden functions
 #include <cmath>
 
-float exponential(float x) { return std::exp(x); }
+float exponential(float x) { return exp(x); }
 
 float reciprocal(float x) { return 1 / x; }
 
@@ -19,7 +19,7 @@ float relu(float x) { return fmaxf(x, 0.0f); }
 
 float ref_sqrt(float x) { return sqrtf(x); }
 
-float sigmoid(float x) { return 1.0f / (1.0f + std::exp(-x)); }
+float sigmoid(float x) { return 1.0f / (1.0f + exp(-x)); }
 
 float ref_log(float x) { return logf(x); }
 
@@ -27,7 +27,7 @@ float ref_log10(float x) { return ref_log(x) * 0.4342944819032518; }
 
 float ref_log2(float x) { return ref_log(x) * 1.4426950408889634f; }
 
-float ref_tanh(float x) { return std::tanh(x); }
+float ref_tanh(float x) { return tanh(x); }
 
 inline std::vector<std::uint32_t> create_random_vector_of_bfloat16_0_2_plus_1(uint32_t num_bytes, int seed) {
     return create_random_vector_of_bfloat16(num_bytes, 2.0f, seed, 1.0f);  // 0.0f..2.0f
diff --git a/tests/tt_eager/python_api_testing/unit_testing/misc/test_rs_matmul_1d_gather_in0.py b/tests/tt_eager/python_api_testing/unit_testing/misc/test_rs_matmul_1d_gather_in0.py
deleted file mode 100644
index ed2ec97860..0000000000
--- a/tests/tt_eager/python_api_testing/unit_testing/misc/test_rs_matmul_1d_gather_in0.py
+++ /dev/null
@@ -1,654 +0,0 @@
-# SPDX-FileCopyrightText: Â© 2024 Tenstorrent Inc.
-
-# SPDX-License-Identifier: Apache-2.0
-
-import pytest
-from loguru import logger
-import ttnn
-from models.utility_functions import is_wormhole_b0, is_grayskull, skip_for_wormhole_b0, is_blackhole
-from models.utility_functions import torch2tt_tensor, tt2torch_tensor, pad_by_zero, roundup32
-import torch
-import itertools
-from models.demos.llama3_subdevices.tt.model_config import LlamaOptimizations
-
-from tests.tt_eager.python_api_testing.sweep_tests.comparison_funcs import (
-    comp_equal,
-    comp_pcc,
-)
-from models.demos.llama3_subdevices.tt.model_config import TtModelArgs
-import random
-import math
-from models.utility_functions import is_wormhole_b0, is_grayskull, is_wormhole_b0, is_blackhole
-from tracy import signpost
-
-from models.demos.llama3_subdevices.tt.model_config import (
-    PREFETCHER_NOC1_GRID,
-)
-
-
-from tests.ttnn.unit_tests.operations.ccl.test_new_all_reduce import (
-    SUB_DEVICE_CRS,
-    QKV_CRS,
-    RING_CRS,
-    FF1_CRS,
-    FF1_CRS_RS_OUT,
-    NORM_CRS,
-)
-from tracy import signpost
-
-PACKET_WORKER_CRS = ttnn.CoreRangeSet(
-    [
-        ttnn.CoreRange(ttnn.CoreCoord(1, 1), ttnn.CoreCoord(3, 2)),
-        ttnn.CoreRange(ttnn.CoreCoord(1, 3), ttnn.CoreCoord(2, 3)),
-    ]
-)
-
-
-def gen_tensor(dim, shard_height, shard_width, num_devices_scatter, num_devices_fracture, num_cores, scheme="random"):
-    torch.manual_seed(2005)
-    factor = 0
-    torch_fracture_tensors = []
-    for _ in range(num_devices_fracture):
-        torch_scatter_tensors = []
-        for _ in range(num_devices_scatter):
-            torch_input_tensors = []
-            for _ in range(num_cores):
-                for _ in range(shard_width // 32):
-                    if scheme == "random":
-                        torch_input_tensors.append(torch.rand(1, 1, shard_height, 32))
-                    elif scheme == "sequential":
-                        torch_input_tensors.append(torch.ones(1, 1, shard_height, 32) * factor)
-                        factor += 1
-                    else:
-                        raise ValueError(f"Invalid scheme: {scheme}")
-            torch_scatter_tensors.append(torch.cat(torch_input_tensors, dim=dim))
-
-        torch_fracture_tensors.append(torch.cat(torch_scatter_tensors, dim=1))
-
-    return torch.cat(torch_fracture_tensors, dim=0)
-
-
-random.seed(10)
-
-
-def num_cores_to_rectangle_grid(num_cores, mesh_device):
-    """
-    Find a rectangular core grid size, given an number of cores.
-
-    Return None if rectangle grid is not possible.
-    """
-    x = mesh_device.compute_with_storage_grid_size().x
-    while x > 0 and num_cores % x != 0:
-        x -= 1
-
-    if x == 0:
-        return None
-
-    y = num_cores // x
-    return (x, y)
-
-
-def get_physical_to_logical_core_mapping(mesh_device):
-    """
-    Get a mapping from physical core coords to logical core coords
-
-    Returns a dictionary.
-    """
-    mapping = {}
-    grid = mesh_device.compute_with_storage_grid_size()
-    for x in range(grid.x):
-        for y in range(grid.y):
-            physical_core = mesh_device.worker_core_from_logical_core(ttnn.CoreCoord(x, y))
-            mapping[(physical_core.x, physical_core.y)] = (x, y)
-    return mapping
-
-
-def round_up(a, b):
-    """
-    Round up a to the nearest multiple of b
-    """
-    return b * math.ceil(a / b)
-
-
-# physical coords
-PREFETCHER_GRID = [
-    (8, 11),
-    (8, 9),
-    (8, 8),
-    (8, 7),
-    (8, 5),
-    (8, 3),
-    (8, 2),
-    (8, 1),
-    (7, 1),
-    (7, 2),
-    (7, 3),
-    (7, 5),
-    (7, 7),
-    (7, 8),
-    (7, 9),
-    (7, 11),
-    (3, 11),
-    (3, 7),
-    (3, 5),
-    (3, 1),
-    (2, 1),
-    (2, 5),
-    (2, 7),
-    (2, 11),
-]
-
-# dram sharded MM output logical coords
-PREFETCHER_NOC1_OUTPUT_GRID = [
-    (1, 9),
-    (2, 9),
-    (1, 0),
-    (2, 0),
-    (1, 4),
-    (2, 4),
-    (1, 5),
-    (2, 5),
-    (5, 0),
-    (6, 0),
-    (5, 9),
-    (6, 9),
-    (5, 1),
-    (6, 1),
-    (5, 7),
-    (6, 7),
-    (5, 6),
-    (6, 6),
-    (5, 2),
-    (6, 2),
-    (5, 4),
-    (6, 4),
-    (5, 5),
-    (6, 5),
-]
-
-LM_HEAD_32_GRID = list(
-    itertools.chain(
-        itertools.product([1, 2, 3], range(10)),  # Generates (1,0)-(1,9), (2,0)-(2,9), (3,0)-(3,9)
-        itertools.product([5], range(2)),  # Generates (5,0), (5,1)
-    )
-)
-
-
-def run_multi_core_matmul_1d(
-    mesh_device,
-    in0_dtype,
-    in1_dtype,
-    fidelity,
-    has_bias,
-    fp32_acc_mode,
-    packer_l1_acc,
-    B,
-    M,
-    K,
-    N,
-    activation,
-    grid,
-    use_arbitrary_cores,
-    num_iters,
-    shard_height,
-    shard_width,
-    input_grid,
-    output_grid,
-    dtype,
-    output_dtype=None,
-    max_dst_tiles=8,
-    pcc_threshold=0.98,
-    use_physical_to_logical_mapping=True,
-    hop_grid=None,
-    in1_is_dram_interleaved=False,
-    in1_is_in_dram=False,
-    untilize_out=False,
-    use_regular_grid=True,
-    scheme="random",
-    num_links=3,
-):
-    print("Making model args")
-    model_args = TtModelArgs(
-        mesh_device,
-        instruct=True,
-        max_batch_size=32,
-        optimizations=LlamaOptimizations.performance,
-        max_seq_len=128 * 1024,
-        dummy_weights="random",
-    )
-    print("Model args made")
-    model_args.n_layers = 1
-    model_configuration = model_args.get_model_config()
-
-    dim = 3
-    shard_height = 32
-    shard_width = 160
-    num_devices_scatter = 4
-    num_devices_fracture = 8
-    num_cores = 24
-    num_iters = 75
-    warmup_iters = 10
-    mesh_device.enable_program_cache()
-    num_pages_per_packet = 4
-    cyclic_buffer_size = 8
-    compute_grid = (mesh_device.compute_with_storage_grid_size().x, mesh_device.compute_with_storage_grid_size().y)
-    subdevice_shard_cores_grid = ttnn.CoreRangeSet(
-        [
-            ttnn.CoreRange(ttnn.CoreCoord(1, 1), ttnn.CoreCoord(3, 2)),
-            ttnn.CoreRange(ttnn.CoreCoord(1, 3), ttnn.CoreCoord(2, 3)),
-        ]
-    )
-    packet_workers_persistent_mem_config = ttnn.create_sharded_memory_config(
-        shape=(32, 512),
-        core_grid=PACKET_WORKER_CRS,
-        strategy=ttnn.ShardStrategy.WIDTH,
-        orientation=ttnn.ShardOrientation.ROW_MAJOR,
-        use_height_and_width_as_shard_shape=True,
-    )
-    output_mem_config = ttnn.MemoryConfig(
-        ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-        ttnn.BufferType.L1,
-        ttnn.ShardSpec(
-            FF1_CRS_RS_OUT,
-            [32, 32],
-            ttnn.ShardOrientation.ROW_MAJOR,
-        ),
-    )
-    assert not has_bias, "Bias not supported for gather_in0 mode."
-    if not isinstance(grid, tuple) and not use_arbitrary_cores:
-        pytest.skip("Grid is not a tuple and not using arbitrary cores")
-
-    if output_dtype is None:
-        output_dtype = in0_dtype
-
-    in0_shape = [1, B, M, K]
-    in1_shape = [1, 1, K, N]
-    num_cores = grid[0] * grid[1] if isinstance(grid, tuple) else len(grid)
-
-    storage_grid = num_cores_to_rectangle_grid(num_cores, mesh_device)
-    if storage_grid is None:
-        pytest.skip(f"Could not find a rectangle grid for num_cores: {num_cores}")
-
-    M *= B  # Fuse batch always enabled
-
-    K_per_shard = round_up(math.ceil(K / num_cores), ttnn.TILE_SIZE)
-    K_padded = K_per_shard * num_cores
-    N_per_shard = round_up(math.ceil(N / num_cores), ttnn.TILE_SIZE)
-    N_per_shard_in_dram = N_per_shard * 2
-    N_padded = N_per_shard * num_cores
-
-    logger.info(f"K_per_shard {K_per_shard}")
-    logger.info(f"K_padded {K_padded}")
-    logger.info(f"N_per_shard {N_per_shard}")
-    logger.info(f"N_padded {N_padded}")
-
-    in0_block_h = M // ttnn.TILE_SIZE
-    in0_block_w = K // num_cores // ttnn.TILE_SIZE
-    while (K / ttnn.TILE_SIZE) % in0_block_w != 0:
-        in0_block_w -= 1
-
-    out_block_h = M // ttnn.TILE_SIZE
-    out_block_w = N_padded // num_cores // ttnn.TILE_SIZE
-
-    num_blocks_y = (M // ttnn.TILE_SIZE - 1) // out_block_h + 1
-    num_blocks_x = (N_padded // ttnn.TILE_SIZE - 1) // out_block_w + 1
-    num_blocks_total = num_blocks_y * num_blocks_x
-
-    if num_blocks_total != num_cores:
-        pytest.skip(f"num_blocks_total {num_blocks_total} != num_cores {num_cores}")
-
-    out_subblock_h = 1
-    out_subblock_w = max_dst_tiles
-    while out_block_w % out_subblock_w != 0:
-        out_subblock_w -= 1
-
-    logger.debug("in0 block h w " + str(in0_block_h) + " " + str(in0_block_w))
-    logger.debug("in1 block h w " + str(in0_block_w) + " " + str(out_block_w))
-    logger.debug("out block h w " + str(out_block_h) + " " + str(out_block_w))
-    logger.debug("out subblock h w " + str(out_subblock_h) + " " + str(out_subblock_w))
-
-    if use_arbitrary_cores:
-        # x, y
-        if isinstance(grid, tuple):  # Generate random grid
-            CORE_RANGE = [(x, y) for y in range(storage_grid[1]) for x in range(storage_grid[0])]
-            random.shuffle(CORE_RANGE)
-        else:  # Use custom grid
-            if use_physical_to_logical_mapping:
-                mapping = get_physical_to_logical_core_mapping(mesh_device)
-                CORE_RANGE = [mapping[physical_coord] for physical_coord in grid]
-            else:
-                CORE_RANGE = grid
-
-        core_range_set = ttnn.CoreRangeSet(
-            [
-                ttnn.CoreRange(
-                    ttnn.CoreCoord(x, y),
-                    ttnn.CoreCoord(x, y),
-                )
-                for x, y in CORE_RANGE
-            ]
-        )
-    else:
-        core_range_set = ttnn.CoreRangeSet(
-            {
-                ttnn.CoreRange(
-                    ttnn.CoreCoord(0, 0),
-                    ttnn.CoreCoord(storage_grid[0] - 1, storage_grid[1] - 1),
-                ),
-            }
-        )
-
-    hop_core_range_set = ttnn.CoreRangeSet([])
-    if hop_grid is not None:
-        hop_core_range_set = ttnn.CoreRangeSet(
-            {
-                ttnn.CoreRange(
-                    ttnn.CoreCoord(x, y),
-                    ttnn.CoreCoord(x, y),
-                )
-                for x, y in hop_grid
-            }
-        )
-
-    in0_sharded_mem_config = ttnn.MemoryConfig(
-        ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-        ttnn.BufferType.L1,
-        ttnn.ShardSpec(
-            core_range_set,
-            [M, K_per_shard],
-            ttnn.ShardOrientation.ROW_MAJOR,
-        ),
-    )
-
-    if in1_is_in_dram:
-        if in1_is_dram_interleaved:
-            in1_sharded_mem_config = ttnn.DRAM_MEMORY_CONFIG
-        else:
-            in1_shard_grid = ttnn.CoreCoord(mesh_device.dram_grid_size().x - 1, mesh_device.dram_grid_size().y - 1)
-            in1_shard_grid = ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), in1_shard_grid)})
-            in1_sharded_mem_config = ttnn.MemoryConfig(
-                ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-                ttnn.BufferType.DRAM,
-                ttnn.ShardSpec(
-                    in1_shard_grid,
-                    [K, N_per_shard_in_dram],
-                    ttnn.ShardOrientation.ROW_MAJOR,
-                ),
-            )
-    else:
-        in1_sharded_mem_config = ttnn.MemoryConfig(
-            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-            ttnn.BufferType.L1,
-            ttnn.ShardSpec(
-                core_range_set,
-                [K, N_per_shard],
-                ttnn.ShardOrientation.ROW_MAJOR,
-            ),
-        )
-
-    dram_sharded_output_core_range_set = ttnn.CoreRangeSet(
-        [
-            ttnn.CoreRange(
-                ttnn.CoreCoord(x, y),
-                ttnn.CoreCoord(x, y),
-            )
-            for x, y in PREFETCHER_NOC1_OUTPUT_GRID
-        ]
-    )
-
-    output_sharded_mem_config = ttnn.MemoryConfig(
-        ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-        ttnn.BufferType.L1,
-        ttnn.ShardSpec(
-            dram_sharded_output_core_range_set if (in1_is_in_dram and not in1_is_dram_interleaved) else core_range_set,
-            [M, N_per_shard],
-            ttnn.ShardOrientation.ROW_MAJOR,
-        ),
-    )
-
-    in0 = torch.randn(in0_shape)
-    in1 = torch.randn(in1_shape)
-
-    in0_t = ttnn.from_torch(
-        in0,
-        device=mesh_device,
-        layout=ttnn.TILE_LAYOUT,
-        dtype=in0_dtype,
-        memory_config=in0_sharded_mem_config,
-    )
-    in1_t = ttnn.from_torch(
-        in1,
-        device=mesh_device,
-        layout=ttnn.TILE_LAYOUT,
-        dtype=in1_dtype,
-        memory_config=in1_sharded_mem_config,
-    )
-
-    program_config = ttnn.MatmulMultiCoreReuseMultiCast1DProgramConfig(
-        compute_with_storage_grid_size=storage_grid,
-        in0_block_w=in0_block_w,
-        out_subblock_h=out_subblock_h,
-        out_subblock_w=out_subblock_w,
-        per_core_M=out_block_h,
-        per_core_N=out_block_w,
-        fuse_batch=True,
-        fused_activation=activation,
-        mcast_in0=False,
-        gather_in0=True,
-        hop_cores=hop_core_range_set,
-        untilize_out=untilize_out,
-    )
-
-    if is_grayskull():
-        compute_kernel_config = ttnn.GrayskullComputeKernelConfig(
-            math_fidelity=fidelity,
-            math_approx_mode=True,
-        )
-    else:
-        compute_kernel_config = ttnn.WormholeComputeKernelConfig(
-            math_fidelity=fidelity,
-            math_approx_mode=True,
-            fp32_dest_acc_en=fp32_acc_mode,
-            packer_l1_acc=packer_l1_acc,
-            dst_full_sync_en=True,
-        )
-    rs_input_mem_config = model_configuration["SHARDED_FF12_OUT_RING_MEMCFG"]
-    input = gen_tensor(
-        dim,
-        shard_height,
-        shard_width,
-        num_devices_scatter,
-        num_devices_fracture,
-        rs_input_mem_config.shard_spec.num_cores(),
-        scheme=scheme,
-    )
-    intermediate_outputs = torch.chunk(input, chunks=num_devices_scatter, dim=dim)
-    output = torch.zeros(intermediate_outputs[0].shape)
-    for i in range(0, len(intermediate_outputs)):
-        output += intermediate_outputs[i]
-
-    scattered_output = torch.chunk(output, chunks=num_devices_scatter, dim=dim)
-    scattered_output = torch.cat(scattered_output, dim=dim)
-    cluster_shape = (8, 4)
-    tt_input = ttnn.from_torch(
-        input,
-        device=mesh_device,
-        layout=ttnn.TILE_LAYOUT,
-        dtype=dtype,
-        memory_config=rs_input_mem_config,
-        mesh_mapper=ttnn.ShardTensor2dMesh(mesh_device, dims=(0, 1), mesh_shape=cluster_shape),
-    )
-    buffer_mem_cfg = model_configuration["REDUCE_SCATTER_INTERIM_MEMCFG"]
-    tt_intermediate = ttnn.from_torch(
-        torch.zeros((*cluster_shape, 32, 512 * buffer_mem_cfg.shard_spec.num_cores())),
-        device=mesh_device,
-        layout=ttnn.TILE_LAYOUT,
-        dtype=ttnn.bfloat8_b,
-        memory_config=buffer_mem_cfg,
-        mesh_mapper=ttnn.ShardTensor2dMesh(mesh_device, dims=(0, 1), mesh_shape=cluster_shape),
-    )
-    ccl_sub_device_crs = ttnn.CoreRangeSet(
-        [
-            ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(6, 9)),
-        ]
-    )
-    worker_sub_device = ttnn.SubDevice(
-        [
-            ccl_sub_device_crs,
-        ]
-    )
-    worker_sub_device_id = ttnn.SubDeviceId(0)
-    sub_device_stall_group = [worker_sub_device_id]
-    sub_device_manager = mesh_device.create_sub_device_manager([worker_sub_device], 0)
-    mesh_device.load_sub_device_manager(sub_device_manager)
-    mesh_device.set_sub_device_stall_group(sub_device_stall_group)
-    ccl_semaphore_handle = ttnn.create_global_semaphore(mesh_device, ccl_sub_device_crs, 0)
-    worker_cores_range_set = ttnn.CoreRangeSet(
-        [
-            ttnn.CoreRange(ttnn.CoreCoord(1, 0), ttnn.CoreCoord(3, 9)),
-            ttnn.CoreRange(ttnn.CoreCoord(5, 0), ttnn.CoreCoord(6, 9)),
-        ]
-    )
-
-    worker_sub_device = ttnn.SubDevice([worker_cores_range_set])
-    worker_sub_device_id = ttnn.SubDeviceId(0)
-    signpost("start")
-    for _ in range(num_iters):
-        rs_out, matmul_out = ttnn.experimental.llama_rs_matmul(
-            in0_t,
-            in1_t,
-            tt_input,
-            tt_intermediate,
-            dim,
-            ccl_semaphore_handle,
-            1,
-            mesh_device,
-            num_links,
-            worker_sub_device_id,
-            program_config=program_config,
-            memory_config_mm=output_sharded_mem_config,
-            compute_kernel_config=compute_kernel_config,
-            dtype=output_dtype,
-            memory_config_rs=model_configuration["REDUCE_SCATTER_OUT_MEMCFG"],
-            topology=ttnn.Topology.Linear,
-        )
-
-
-@pytest.mark.skipif(is_grayskull(), reason="Test suite for WH only")
-@pytest.mark.skipif(is_blackhole(), reason="Test suite for WH only")
-@pytest.mark.parametrize("has_bias", [False], ids=["no_bias"])
-@pytest.mark.parametrize(
-    "B, M, K, N, in0_dtype, in1_dtype, output_dtype, fidelity, packer_l1_acc, fp32_acc_mode, grid, in1_is_dram_interleaved, untilize_out",
-    [
-        (
-            1,
-            32,
-            2048,
-            3584,
-            ttnn.bfloat16,
-            ttnn.bfloat4_b,
-            ttnn.bfloat8_b,
-            ttnn.MathFidelity.LoFi,
-            True,
-            False,
-            PREFETCHER_NOC1_GRID,
-            False,
-            False,
-        ),
-    ],
-    ids=[
-        "ff13",
-    ],
-)
-@pytest.mark.parametrize(
-    "num_iters",
-    [50],
-)
-@pytest.mark.parametrize(
-    "device_params",
-    [
-        {
-            "trace_region_size": 300000,
-            "dispatch_core_axis": ttnn.DispatchCoreAxis.COL,
-            "fabric_config": ttnn.FabricConfig.FABRIC_1D,
-        }
-    ],
-    indirect=True,
-)
-@pytest.mark.parametrize("shard_height", [32])
-@pytest.mark.parametrize("shard_width", [64])
-@pytest.mark.parametrize("input_grid", [(5, 5)])
-@pytest.mark.parametrize("output_grid", [(5, 1)])
-@pytest.mark.parametrize("dtype", [ttnn.bfloat8_b])
-@pytest.mark.parametrize(
-    "mesh_device",
-    [
-        (8, 4),  # TODO: Once fabric can be initialized on a SubMesh, revert to (1, 4)
-    ],
-    indirect=True,
-)
-def test_matmul_1d_ring_llama_with_rs_perf(
-    mesh_device,
-    in0_dtype,
-    in1_dtype,
-    output_dtype,
-    fidelity,
-    has_bias,
-    fp32_acc_mode,
-    packer_l1_acc,
-    B,
-    M,
-    K,
-    N,
-    grid,
-    in1_is_dram_interleaved,
-    untilize_out,
-    num_iters,
-    use_program_cache,
-    function_level_defaults,
-    shard_height,
-    shard_width,
-    input_grid,
-    output_grid,
-    dtype,
-):
-    # Only run these tests on unharvested TG
-    device_grid = (mesh_device.compute_with_storage_grid_size().x, mesh_device.compute_with_storage_grid_size().y)
-    if device_grid != (7, 10):
-        pytest.skip("Skipping test_run_prefetcher because it only works with a 7x10 grid")
-
-    if in1_is_dram_interleaved:
-        hop_grid = None
-    else:
-        hop_grid = [
-            (3, 6),
-        ]
-
-    run_multi_core_matmul_1d(
-        mesh_device,
-        in0_dtype,
-        in1_dtype,
-        fidelity,
-        has_bias,
-        fp32_acc_mode,
-        packer_l1_acc,
-        B,
-        M,
-        K,
-        N,
-        None,  # activation,
-        grid,
-        True,
-        num_iters,
-        shard_height,
-        shard_width,
-        input_grid,
-        output_grid,
-        dtype,
-        output_dtype=output_dtype,
-        use_physical_to_logical_mapping=False,
-        hop_grid=hop_grid,
-        in1_is_dram_interleaved=in1_is_dram_interleaved,
-        untilize_out=untilize_out,
-        use_regular_grid=True,
-    )
diff --git a/tests/tt_metal/microbenchmarks/ethernet/fabric_edm_bandwidth_golden.csv b/tests/tt_metal/microbenchmarks/ethernet/fabric_edm_bandwidth_golden.csv
index ce74b85ad3..13f52e8793 100644
--- a/tests/tt_metal/microbenchmarks/ethernet/fabric_edm_bandwidth_golden.csv
+++ b/tests/tt_metal/microbenchmarks/ethernet/fabric_edm_bandwidth_golden.csv
@@ -1,191 +1,185 @@
 Test Name,Noc Message Type,Packet Size,Line Size,Num Links,Disable Interior Workers,Unidirectional,Senders Are Unidirectional,Bandwidth (B/c),Packets/Second
-mcast_HalfRing,noc_unicast_write,4096,4,1,False,False,False,9.510263236470745,2321841.6104664905
-mcast_HalfRing,noc_unicast_write,4096,4,2,False,False,False,9.347262100270118,2282046.4111987595
-mcast_FullRing,noc_unicast_write,16,4,1,False,False,False,0.03799942149839042,2374963.8436494013
-mcast_FullRing,noc_unicast_write,2048,4,1,False,False,False,4.142215708600006,2022566.263964847
-mcast_FullRing,noc_unicast_write,4096,4,1,False,False,False,7.094924685062035,1732159.3469389735
-mcast_FullRing,noc_unicast_write,4096,4,2,False,False,False,7.005317798995507,1710282.6657703875
-mcast_HalfRing,noc_unicast_write,16,8,1,False,False,False,0.027357561185244367,1709847.574077773
-mcast_HalfRing,noc_unicast_write,2048,8,1,False,False,False,2.746863441653947,1341241.9148700912
-mcast_HalfRing,noc_unicast_write,4096,8,1,False,False,False,4.758234542862818,1161678.3551911176
-mcast_FullRing,noc_unicast_write,16,8,1,False,False,False,0.031197011171458896,1949813.198216181
-mcast_FullRing,noc_unicast_write,2048,8,1,False,False,False,3.109708445668686,1518412.326986663
-mcast_FullRing,noc_unicast_write,4096,8,1,False,False,False,5.411762521520979,1321231.0843557077
-unicast_FullRing,noc_unicast_write,4096,4,1,False,False,False,7.402834164336309,1807332.5596524193
-unicast_FullRing,noc_unicast_write,4096,4,2,False,False,False,7.306684565982506,1783858.5366168227
-mcast_SaturateChipToChipRing,noc_unicast_write,16,4,1,False,False,False,0.040206420264216745,2512901.2665135465
-mcast_SaturateChipToChipRing,noc_unicast_write,2048,4,1,False,False,False,4.465448964753358,2180395.002320976
-mcast_SaturateChipToChipRing,noc_unicast_write,4096,4,1,False,False,False,7.8798597895404106,1923793.8939307644
-mcast_SaturateChipToChipRing,noc_unicast_write,4096,4,2,False,False,False,7.71151641331598,1882694.4368447217
-unicast_Linear_0x4_mesh,noc_unicast_write,16,2,1,False,False,True,0.046239268534075166,2889954.2833796977
-unicast_Linear_0x4_mesh,noc_unicast_write,16,2,2,False,False,True,0.04624053337068027,2890033.335667517
-unicast_Linear_0x4_mesh,noc_unicast_write,2048,2,1,False,False,True,5.9283404038007435,2894697.4627933316
-unicast_Linear_0x4_mesh,noc_unicast_write,2048,2,2,False,False,True,5.928287720760512,2894671.7386525935
-unicast_Linear_0x4_mesh,noc_unicast_write,4096,2,1,False,False,True,10.996802859447959,2684766.3231074116
-unicast_Linear_0x4_mesh,noc_unicast_write,4096,2,2,False,False,True,10.998105162882716,2685084.268281913
-unicast_HalfRing_0x0_mesh,noc_unicast_write,16,8,1,False,False,True,0.028395589076223093,1774724.3172639434
-unicast_HalfRing_0x0_mesh,noc_unicast_write,2048,8,1,False,False,True,2.8746227581795876,1403624.3936423766
-unicast_HalfRing_0x0_mesh,noc_unicast_write,4096,8,1,False,False,True,4.962354277770261,1211512.2748462553
-unicast_FullRing_0x0_mesh,noc_unicast_write,16,8,1,False,False,True,0.033540935452437255,2096308.4657773285
-unicast_FullRing_0x0_mesh,noc_unicast_write,2048,8,1,False,False,True,3.4032041216657105,1661720.7625320852
-unicast_FullRing_0x0_mesh,noc_unicast_write,4096,8,1,False,False,True,5.914058456517849,1443861.927860803
-mcast_Linear_2x0_mesh,noc_unicast_write,16,4,1,False,False,True,0.04206191005788731,2628869.378617957
-mcast_Linear_2x0_mesh,noc_unicast_write,2048,4,1,False,False,True,5.383667363540084,2628743.8298535566
-mcast_Linear_2x0_mesh,noc_unicast_write,4096,4,1,False,False,True,10.58531651075173,2584305.7887577466
-unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,16,"[4, 2]","[1, 1]","[False, False]","[False, False]","[True, True]",0.046801058441037435,2925066.15256484
-unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,2048,"[4, 2]","[1, 1]","[False, False]","[False, False]","[True, True]",5.988955085980471,2924294.475576402
-unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 2]","[1, 1]","[False, False]","[False, False]","[True, True]",11.006387412906182,2687106.301979048
-mcast_Linear,noc_unicast_write,16,4,1,False,False,True,0.042207287891511916,2637955.493219495
-mcast_Linear,noc_unicast_write,2048,4,1,False,False,True,5.394562175964086,2634063.5624824637
-mcast_Linear,noc_unicast_write,4096,4,1,False,False,True,10.251785324261455,2502877.2764310194
-mcast_RingAsLinear,noc_unicast_write,16,4,1,False,False,True,0.03373273026520377,2108295.641575236
-mcast_RingAsLinear,noc_unicast_write,2048,4,1,False,False,True,3.646037352904958,1780291.676223124
-mcast_RingAsLinear,noc_unicast_write,4096,4,1,False,False,True,6.184980762516546,1510005.0689737662
-mcast_Linear,noc_unicast_write,16,4,1,True,False,True,0.0415823514105026,2598896.9631564124
-mcast_Linear,noc_unicast_write,2048,4,1,True,False,True,5.31993243737757,2597623.2604382667
-mcast_Linear,noc_unicast_write,4096,4,1,True,False,True,9.094657892098102,2220375.4619380133
-mcast_RingAsLinear,noc_unicast_write,16,4,1,True,False,True,0.03129389059837743,1955868.1623985893
-mcast_RingAsLinear,noc_unicast_write,2048,4,1,True,False,True,3.087967455114874,1507796.6089428097
-mcast_RingAsLinear,noc_unicast_write,4096,4,1,True,False,True,5.365757178684875,1309999.3112023622
-mcast_Linear,noc_unicast_write,16,4,1,True,True,True,0.05754268529882033,3596417.8311762707
-mcast_Linear,noc_unicast_write,2048,4,1,True,True,True,6.9226497827925675,3380200.089254183
-mcast_Linear,noc_unicast_write,4096,4,1,True,True,True,11.763832447527664,2872029.4061346836
-mcast_RingAsLinear,noc_unicast_write,16,4,1,True,True,True,0.03571699438590237,2232312.149118898
-mcast_RingAsLinear,noc_unicast_write,2048,4,1,True,True,True,3.4671630036913434,1692950.6853961637
-mcast_RingAsLinear,noc_unicast_write,4096,4,1,True,True,True,6.047968362510499,1476554.7760035398
-mcast_Linear,noc_unicast_write,16,4,2,False,False,True,0.0421366721659852,2633542.010374075
-mcast_Linear,noc_unicast_write,2048,4,2,False,False,True,5.390569352895774,2632113.9418436396
-mcast_Linear,noc_unicast_write,4096,4,2,False,False,True,10.256881737546317,2504121.517955644
-mcast_RingAsLinear,noc_unicast_write,16,4,2,False,False,True,0.032956617566764934,2059788.5979228085
-mcast_RingAsLinear,noc_unicast_write,2048,4,2,False,False,True,3.4891475670053054,1703685.3354518092
-mcast_RingAsLinear,noc_unicast_write,4096,4,2,False,False,True,6.113763891573114,1492618.1375910921
-unicast_Linear,noc_unicast_write,16,2,1,False,False,True,0.04672464542966312,2920290.339353945
-unicast_Linear,noc_unicast_write,2048,2,1,False,False,True,5.993295176605992,2926413.6604521447
-unicast_Linear,noc_unicast_write,4096,2,1,False,False,True,11.435063634900752,2791763.582739441
-unicast_RingAsLinear,noc_unicast_write,16,2,1,False,False,True,0.04097940627320179,2561212.892075112
-unicast_RingAsLinear,noc_unicast_write,2048,2,1,False,False,True,3.843988173767673,1876947.3504724966
-unicast_RingAsLinear,noc_unicast_write,4096,2,1,False,False,True,6.714744686710316,1639341.9645288857
-unicast_Linear,noc_unicast_write,16,2,2,False,False,True,0.04672596084016957,2920372.552510598
-unicast_Linear,noc_unicast_write,2048,2,2,False,False,True,5.994760906805939,2927129.3490263373
-unicast_Linear,noc_unicast_write,4096,2,2,False,False,True,11.071223592388252,2702935.447360413
-unicast_RingAsLinear,noc_unicast_write,16,2,2,False,False,True,0.035960117263245384,2247507.3289528363
-unicast_RingAsLinear,noc_unicast_write,2048,2,2,False,False,True,3.8280107302056634,1869145.864358234
-unicast_RingAsLinear,noc_unicast_write,4096,2,2,False,False,True,6.626118734854896,1617704.7692516835
-unicast_Linear,noc_unicast_write,16,4,1,False,False,True,0.04710509805375131,2944068.6283594565
-unicast_Linear,noc_unicast_write,2048,4,1,False,False,True,6.041902704612166,2950147.8049864094
-unicast_Linear,noc_unicast_write,4096,4,1,False,False,True,11.01408733634805,2688986.166100598
-unicast_RingAsLinear,noc_unicast_write,16,4,1,False,False,True,0.03480536043011855,2175335.0268824096
-unicast_RingAsLinear,noc_unicast_write,2048,4,1,False,False,True,3.6968881178013038,1805121.151270168
-unicast_RingAsLinear,noc_unicast_write,4096,4,1,False,False,True,6.221428099049631,1518903.3444945388
-unicast_Linear,noc_unicast_write,16,4,1,True,False,True,0.04642656741749839,2901660.4635936497
-unicast_Linear,noc_unicast_write,2048,4,1,True,False,True,5.865806249125804,2864163.207580959
-unicast_Linear,noc_unicast_write,4096,4,1,True,False,True,10.269174854125255,2507122.7671204237
-unicast_RingAsLinear,noc_unicast_write,16,4,1,True,False,True,0.03303925641849505,2064953.5261559405
-unicast_RingAsLinear,noc_unicast_write,2048,4,1,True,False,True,3.2155414403020877,1570088.5938975038
-unicast_RingAsLinear,noc_unicast_write,4096,4,1,True,False,True,5.710082812929546,1394063.1867503773
-unicast_Linear,noc_unicast_write,16,4,1,True,True,True,0.06636875392249705,4148047.120156066
-unicast_Linear,noc_unicast_write,2048,4,1,True,True,True,7.441623858253053,3633605.399537624
-unicast_Linear,noc_unicast_write,4096,4,1,True,True,True,11.778099851170653,2875512.65897721
-unicast_RingAsLinear,noc_unicast_write,16,4,1,True,True,True,0.036479336798361135,2279958.549897571
-unicast_RingAsLinear,noc_unicast_write,2048,4,1,True,True,True,3.5862564636012566,1751101.788867801
-unicast_RingAsLinear,noc_unicast_write,4096,4,1,True,True,True,6.550978246098086,1599359.9233637904
-unicast_Linear,noc_unicast_flush_atomic_inc,16,4,1,True,False,True,0.020683038204906704,1292689.8878066689
-unicast_Linear,noc_unicast_no_flush_atomic_inc,16,4,1,True,False,True,0.022734043532539015,1420877.7207836884
-unicast_RingAsLinear,noc_unicast_flush_atomic_inc,16,4,1,True,False,True,0.015793883346831676,987117.7091769797
-unicast_RingAsLinear,noc_unicast_no_flush_atomic_inc,16,4,1,True,False,True,0.016508893067266796,1031805.8167041747
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.02436523295366996,1522827.0596043724
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.028020986685753747,1751311.6678596092
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.024115953059412734,1507247.0662132958
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.025609048393850902,1600565.5246156815
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,2.7528735284938306,1344176.5275848783
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,3.19444016424211,1559785.2364463427
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.729960425904485,1332988.4892111744
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.973964501827438,1452131.1044079287
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,4.4696565739207745,1091224.7494923766
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,5.667038774772381,1383554.3883721633
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,4.341526546132103,1059943.004426783
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,5.348615434961558,1305814.3151761617
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.013169280803887141,823080.0502429464
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.015359031152314913,959939.447019682
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.028675222893179624,1792201.4308237266
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.030716977056280355,1919811.0660175222
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.468571069664788,717075.7176097598
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.7259647213653708,842756.2116041849
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,3.1728847008635523,1549260.1078435315
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,3.4503375130937615,1684735.1138153132
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,2.4313090853999006,593581.3196777101
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,3.0278111364512474,739211.7032351678
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,5.183054645656281,1265394.2005996779
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,6.0490525438854075,1476819.4687220233
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.03823333553772409,2389583.4711077553
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.047076653812043075,2942290.8632526924
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.03781656646635586,2363535.4041472413
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.041775984845343735,2610999.052833983
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,4.887706757021304,2386575.5649518087
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,5.9973341147047545,2928385.798195681
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,4.840672273889019,2363609.5087348726
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,5.4081561596640855,2640701.249835979
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,9.516495521249924,2323363.164367657
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,10.933250052717698,2669250.5011517815
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,False,True,8.203220200525875,2002739.3067690125
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,False,True,10.311459574893211,2517446.185276663
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,True,True,0.02170876397387201,1356797.7483670006
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,True,True,0.02599665741478125,1624791.0884238281
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,True,True,0.05019455724102605,3137159.8275641277
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,True,True,0.05739331851019357,3587082.4068870977
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,True,True,2.7803232774733173,1357579.7253287681
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,True,True,3.317659148801897,1619950.7562509263
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,True,True,6.41215323455716,3130934.1965611135
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,True,True,7.204050871574075,3517602.9646357787
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,True,True,5.422235903718935,1323788.0624313806
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,True,True,5.813444562770529,1419297.9889576489
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,True,True,11.728993529366832,2863523.8108805744
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,True,True,11.778035502179304,2875496.948774244
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.02132323685176148,1332702.3032350924
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.027431573579171895,1714473.3486982435
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.02114129092052114,1321330.6825325713
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.025059884902141383,1566242.8063838365
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,2.43413028616987,1188540.178793882
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,3.126361044698731,1526543.4788568022
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.4212935363474464,1182272.2345446516
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.9013269334896727,1416663.5417430042
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,4.0780807703622015,995625.1880767094
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,5.559845568230945,1357384.1719313832
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,3.9998374089529807,976522.8049201613
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,5.195518122315397,1268437.0415809073
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.011346113266752474,709132.0791720296
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.015059416905871778,941213.556616986
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.024493644044801387,1530852.7528000867
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.030155535182769165,1884720.9489230728
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.3031955905333619,636325.9719401181
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.6932853101744818,826799.4678586337
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,2.7797981912662135,1357323.3355792058
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,3.3928450166785336,1656662.6058000652
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,2.2008590238623893,537319.0976226536
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,2.996036658966209,731454.2624429221
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,4.717013724943744,1151614.6789413437
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,6.009019926209353,1467045.8804222054
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.0322443181002341,2015269.8812646312
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.03439884693345137,2149927.9333407106
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.029948345187808645,1871771.5742380403
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.03282037144045123,2051273.215028202
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,3.4997692339661355,1708871.696272527
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,3.5749852590772004,1745598.2710337893
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,2.9800571389578745,1455106.0248817746
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,3.2114568472129257,1568094.1636781865
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,6.124224977067704,1495172.11354192
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,6.172169945651414,1506877.4281375522
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,False,True,5.250346412626468,1281822.8546451337
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,False,True,5.656367212795642,1380949.026561436
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,True,True,0.01770127826185396,1106329.8913658725
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,True,True,0.019106379238497225,1194148.7024060765
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,True,True,0.03493628734781917,2183517.9592386982
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,True,True,0.03644329644011403,2277706.027507127
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,True,True,1.958553236042297,956324.8222862779
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,True,True,1.9535042243503453,953859.4845460671
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,True,True,3.3804456815569823,1650608.2429477454
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,True,True,3.5823198695916623,1749179.623824054
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,True,True,3.3824314257343597,825788.9222984277
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,True,True,3.3923307071895588,828205.7390599509
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,True,True,5.969964737199375,1457510.9221678162
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,True,True,6.512195771867861,1589891.5458661772
+mcast_HalfRing,noc_unicast_write,4096,4,1,False,False,False,9.484246073349043,2315489.764001231
+mcast_HalfRing,noc_unicast_write,4096,4,2,False,False,False,9.333515924795993,2278690.411327147
+mcast_FullRing,noc_unicast_write,16,4,1,False,False,False,0.03791728145983809,2369830.0912398803
+mcast_FullRing,noc_unicast_write,2048,4,1,False,False,False,4.134989143432557,2019037.6676916785
+mcast_FullRing,noc_unicast_write,4096,4,1,False,False,False,7.089412626298119,1730813.6294673143
+mcast_FullRing,noc_unicast_write,4096,4,2,False,False,False,6.995681456187986,1707930.043014645
+mcast_HalfRing,noc_unicast_write,16,8,1,False,False,False,0.027373459894297042,1710841.243393565
+mcast_HalfRing,noc_unicast_write,2048,8,1,False,False,False,2.748127904957714,1341859.3285926338
+mcast_HalfRing,noc_unicast_write,4096,8,1,False,False,False,4.752365665206931,1160245.5237321607
+mcast_FullRing,noc_unicast_write,16,8,1,False,False,False,0.031434188690024095,1964636.793126506
+mcast_FullRing,noc_unicast_write,2048,8,1,False,False,False,3.1006265154789268,1513977.7907611947
+mcast_FullRing,noc_unicast_write,4096,8,1,False,False,False,5.400723823383435,1318536.0896932215
+unicast_FullRing,noc_unicast_write,4096,4,1,False,False,False,7.396090392190272,1805686.1309058282
+unicast_FullRing,noc_unicast_write,4096,4,2,False,False,False,7.298637544002052,1781893.9316411258
+mcast_SaturateChipToChipRing,noc_unicast_write,16,4,1,False,False,False,0.04019469388589928,2512168.367868705
+mcast_SaturateChipToChipRing,noc_unicast_write,2048,4,1,False,False,False,4.4662776735569585,2180799.6452914835
+mcast_SaturateChipToChipRing,noc_unicast_write,4096,4,1,False,False,False,7.860566749165515,1919083.678995487
+mcast_SaturateChipToChipRing,noc_unicast_write,4096,4,2,False,False,False,7.688933507535316,1877181.0321131141
+unicast_Linear_0x4_mesh,noc_unicast_write,16,2,1,False,False,True,0.04599479813206501,2874674.883254063
+unicast_Linear_0x4_mesh,noc_unicast_write,16,2,2,False,False,True,0.045976048375308456,2873503.0234567784
+unicast_Linear_0x4_mesh,noc_unicast_write,2048,2,1,False,False,True,5.902735825825022,2882195.2274536244
+unicast_Linear_0x4_mesh,noc_unicast_write,2048,2,2,False,False,True,5.891408172263394,2876664.1466129855
+unicast_Linear_0x4_mesh,noc_unicast_write,4096,2,1,False,False,True,10.949952341909695,2673328.2084740466
+unicast_Linear_0x4_mesh,noc_unicast_write,4096,2,2,False,False,True,10.941612433315367,2671292.0979773845
+mcast_Linear_2x0_mesh,noc_unicast_write,16,4,1,False,False,True,0.04200142072430689,2625088.7952691806
+mcast_Linear_2x0_mesh,noc_unicast_write,2048,4,1,False,False,True,5.390742033600971,2632198.258594224
+mcast_Linear_2x0_mesh,noc_unicast_write,4096,4,1,False,False,True,10.541793223626707,2573679.986236989
+mcast_Linear,noc_unicast_write,16,4,1,False,False,True,0.04200415473595532,2625259.6709972075
+mcast_Linear,noc_unicast_write,2048,4,1,False,False,True,5.377968793705844,2625961.3250516816
+mcast_Linear,noc_unicast_write,4096,4,1,False,False,True,10.249990345277343,2502439.0491399765
+mcast_RingAsLinear,noc_unicast_write,16,4,1,False,False,True,0.0337157784322104,2107236.15201315
+mcast_RingAsLinear,noc_unicast_write,2048,4,1,False,False,True,3.615821012481349,1765537.6037506587
+mcast_RingAsLinear,noc_unicast_write,4096,4,1,False,False,True,6.15201164172078,1501955.9672169874
+mcast_Linear,noc_unicast_write,16,4,1,True,False,True,0.04161716499970413,2601072.812481508
+mcast_Linear,noc_unicast_write,2048,4,1,True,False,True,5.314600129084059,2595019.594279326
+mcast_Linear,noc_unicast_write,4096,4,1,True,False,True,9.213054344683329,2249280.8458699531
+mcast_RingAsLinear,noc_unicast_write,16,4,1,True,False,True,0.03135412693306766,1959632.9333167288
+mcast_RingAsLinear,noc_unicast_write,2048,4,1,True,False,True,3.093622479886299,1510557.851506982
+mcast_RingAsLinear,noc_unicast_write,4096,4,1,True,False,True,5.3754881846513785,1312375.045080903
+mcast_Linear,noc_unicast_write,16,4,1,True,True,True,0.05720941723501988,3575588.5771887423
+mcast_Linear,noc_unicast_write,2048,4,1,True,True,True,6.9119012375711915,3374951.7761578085
+mcast_Linear,noc_unicast_write,4096,4,1,True,True,True,11.76444873777891,2872179.867621804
+mcast_RingAsLinear,noc_unicast_write,16,4,1,True,True,True,0.03591170228157369,2244481.3925983557
+mcast_RingAsLinear,noc_unicast_write,2048,4,1,True,True,True,3.477461785700399,1697979.387549023
+mcast_RingAsLinear,noc_unicast_write,4096,4,1,True,True,True,6.077313592371061,1483719.138762466
+mcast_Linear,noc_unicast_write,16,4,2,False,False,True,0.04189251971357246,2618282.4820982786
+mcast_Linear,noc_unicast_write,2048,4,2,False,False,True,5.367956196481429,2621072.361563198
+mcast_Linear,noc_unicast_write,4096,4,2,False,False,True,10.247756591012216,2501893.698977592
+mcast_RingAsLinear,noc_unicast_write,16,4,2,False,False,True,0.0328288451181099,2051802.8198818688
+mcast_RingAsLinear,noc_unicast_write,2048,4,2,False,False,True,3.4715196183023496,1695077.9386241941
+mcast_RingAsLinear,noc_unicast_write,4096,4,2,False,False,True,6.081759666883492,1484804.6061727277
+unicast_Linear,noc_unicast_write,16,2,1,False,False,True,0.04670174746263568,2918859.21641473
+unicast_Linear,noc_unicast_write,2048,2,1,False,False,True,6.001236582928709,2930291.3002581587
+unicast_Linear,noc_unicast_write,4096,2,1,False,False,True,11.440524446377143,2793096.788666295
+unicast_RingAsLinear,noc_unicast_write,16,2,1,False,False,True,0.04126302775331905,2578939.2345824405
+unicast_RingAsLinear,noc_unicast_write,2048,2,1,False,False,True,3.842715911521391,1876326.1286725542
+unicast_RingAsLinear,noc_unicast_write,4096,2,1,False,False,True,6.703383072730316,1636568.1329907996
+unicast_Linear,noc_unicast_write,16,2,2,False,False,True,0.04660073555766023,2912545.9723537643
+unicast_Linear,noc_unicast_write,2048,2,2,False,False,True,5.9905702632392,2925083.136347266
+unicast_Linear,noc_unicast_write,4096,2,2,False,False,True,11.116776704707565,2714056.8126727454
+unicast_RingAsLinear,noc_unicast_write,16,2,2,False,False,True,0.03588268885987051,2242668.053741907
+unicast_RingAsLinear,noc_unicast_write,2048,2,2,False,False,True,3.8287684632567553,1869515.8511995876
+unicast_RingAsLinear,noc_unicast_write,4096,2,2,False,False,True,6.62853307407397,1618294.2075375903
+unicast_Linear,noc_unicast_write,16,4,1,False,False,True,0.046903831198739976,2931489.4499212485
+unicast_Linear,noc_unicast_write,2048,4,1,False,False,True,6.009069734241995,2934116.0811728495
+unicast_Linear,noc_unicast_write,4096,4,1,False,False,True,10.993558555722855,2683974.256768275
+unicast_RingAsLinear,noc_unicast_write,16,4,1,False,False,True,0.0347064644103347,2169154.025645919
+unicast_RingAsLinear,noc_unicast_write,2048,4,1,False,False,True,3.6654487640421767,1789769.904317469
+unicast_RingAsLinear,noc_unicast_write,4096,4,1,False,False,True,6.199068151111427,1513444.3728299383
+unicast_Linear,noc_unicast_write,16,4,1,True,False,True,0.04646663159307231,2904164.4745670194
+unicast_Linear,noc_unicast_write,2048,4,1,True,False,True,5.890538549677503,2876239.526209718
+unicast_Linear,noc_unicast_write,4096,4,1,True,False,True,10.276220208008912,2508842.8242209256
+unicast_RingAsLinear,noc_unicast_write,16,4,1,True,False,True,0.033092359596719134,2068272.474794946
+unicast_RingAsLinear,noc_unicast_write,2048,4,1,True,False,True,3.2150087156091987,1569828.474418554
+unicast_RingAsLinear,noc_unicast_write,4096,4,1,True,False,True,5.70989666934102,1394017.7415383349
+unicast_Linear,noc_unicast_write,16,4,1,True,True,True,0.06670924940420302,4169328.0877626888
+unicast_Linear,noc_unicast_write,2048,4,1,True,True,True,7.436400844445436,3631055.0998268733
+unicast_Linear,noc_unicast_write,4096,4,1,True,True,True,11.777839072765465,2875448.992374381
+unicast_RingAsLinear,noc_unicast_write,16,4,1,True,True,True,0.03666489766173969,2291556.1038587308
+unicast_RingAsLinear,noc_unicast_write,2048,4,1,True,True,True,3.5985807745646405,1757119.5188303909
+unicast_RingAsLinear,noc_unicast_write,4096,4,1,True,True,True,6.538654571092236,1596351.2136455653
+unicast_Linear,noc_unicast_flush_atomic_inc,16,4,1,True,False,True,0.020772471344667892,1298279.4590417433
+unicast_Linear,noc_unicast_no_flush_atomic_inc,16,4,1,True,False,True,0.02290331741741248,1431457.33858828
+unicast_RingAsLinear,noc_unicast_flush_atomic_inc,16,4,1,True,False,True,0.016091815069240567,1005738.4418275354
+unicast_RingAsLinear,noc_unicast_no_flush_atomic_inc,16,4,1,True,False,True,0.01653120386906214,1033200.2418163837
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.024266215152278975,1516638.447017436
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.02802616784023252,1751635.4900145326
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.0240273510545259,1501709.4409078688
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.02562697125966406,1601685.7037290037
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,2.7451846304886987,1340422.1828558098
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,3.1930976579100316,1559129.7157763827
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.713897062493986,1325145.0500458917
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.9749171847623668,1452596.2816222494
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,4.465725204717685,1090264.9425580287
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,5.667103186496525,1383570.1138907531
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,4.331867889007117,1057584.933839628
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,5.344867808604981,1304899.3673352005
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.013156630845532712,822289.4278457945
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.015352968708802617,959560.5443001635
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.02871079049213103,1794424.4057581895
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.030705476457022117,1919092.2785638822
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.4679851873556773,716789.6422635143
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.7272413424357547,843379.5617362084
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,3.1865660846583164,1555940.4710245684
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,3.4537529785984438,1686402.8215812715
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,2.4296324018078614,593171.9730976224
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,3.029394859554026,739598.3543833072
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,5.187324698645531,1266436.6940052565
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,6.051370432262421,1477385.3594390675
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.03798016204746496,2373760.12796656
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.04680488568703817,2925305.3554398855
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.03754500399367381,2346562.749604613
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.04195459977383324,2622162.485864578
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,4.860604946429749,2373342.258998901
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,5.976893208283769,2918404.8868573094
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,4.820960746386415,2353984.739446492
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,5.4313708501561955,2652036.54792783
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,9.532270610829809,2327214.5045971214
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,10.916553178555612,2665174.1158583034
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,False,True,8.377700957187106,2045337.147750758
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,False,True,10.319649765800511,2519445.7436036402
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,True,True,0.021839067014310036,1364941.6883943772
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,True,True,0.026135907798659225,1633494.2374162015
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,True,True,0.05032658493671212,3145411.5585445073
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,True,True,0.05774167856250503,3608854.910156565
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,True,True,2.7987187443723274,1366561.8869005505
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,True,True,3.331761483750521,1626836.6619875592
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,True,True,6.434237482458809,3141717.5207318403
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,True,True,7.2157488651623,3523314.875567529
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,True,True,5.432116225431444,1326200.2503494737
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,True,True,5.813988313940266,1419430.7407080727
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,True,True,11.731007874868638,2864015.594450351
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,True,True,11.777953373567007,2875476.8978435076
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.02077487829005093,1298429.8931281832
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.027394863809751917,1712178.9881094948
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.02062817046519277,1289260.6540745483
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.025111323992360258,1569457.749522516
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,2.3795176258251347,1161873.840734929
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,3.1196154193557306,1523249.7164822903
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.367300250458748,1155908.3254193107
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.9085650124425655,1420197.7599817214
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,4.001733543936936,976985.7285002286
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,5.552149076991984,1355505.1457499962
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,3.931086065025461,959737.8088441067
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,5.2035912707113985,1270408.025076025
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.011080801617461842,692550.1010913651
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.01497699122280918,936061.9514255737
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.023870081887584015,1491880.117974001
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.029994980996070515,1874686.3122544072
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.2720191609811302,621103.1059478174
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.6825578490890287,821561.4497505024
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,2.714426542203494,1325403.5850602998
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,3.3737110648745037,1647319.8558957537
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,2.1585829470002564,526997.7897949845
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,2.982398117128189,728124.5403144992
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,4.630587374294278,1130514.4956773138
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,5.9873658978407445,1461759.2524025254
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.032173761638826855,2010860.1024266784
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.03427252236345006,2142032.6477156286
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.029917223903009257,1869826.4939380786
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.03281784868438629,2051115.542774143
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,3.4820784854624818,1700233.6354797275
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,3.5596456099912266,1738108.2080035286
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,2.9820718003532636,1456089.746266242
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,3.210996785985065,1567869.52440677
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,6.100529941835855,1489387.1928310194
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,6.154203399990786,1502491.0644508756
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,False,True,5.25572455353255,1283135.8773272827
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,False,True,5.655563481076645,1380752.8029972278
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,True,True,0.017735955409591105,1108497.213099444
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,True,True,0.01889857364669027,1181160.8529181418
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,True,True,0.03497552074233094,2185970.046395684
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,True,True,0.03660145221980659,2287590.763737912
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,True,True,1.9466629003302036,950518.9943018572
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,True,True,1.9372936177484057,945944.1492912137
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,True,True,3.39722753057455,1658802.5051633546
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,True,True,3.5945978494949777,1755174.7311987197
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,True,True,3.3607680227944092,820500.0055650413
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,True,True,3.3461515609449837,816931.5334338339
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,True,True,5.975265086742001,1458804.9528178715
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,True,True,6.476256298854942,1581117.2604626324
+unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,16,"[4, 2]","[1, 1]","[False, False]","[False, False]","[True, True]",0.046324695304926264,2895293.4565578913
+unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,2048,"[4, 2]","[1, 1]","[False, False]","[False, False]","[True, True]",5.9333069357691,2887356.902231006
+unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 2]","[1, 1]","[False, False]","[False, False]","[True, True]",10.962518344186009,2676396.080123537
diff --git a/tests/tt_metal/microbenchmarks/ethernet/fabric_edm_bandwidth_golden_6u.csv b/tests/tt_metal/microbenchmarks/ethernet/fabric_edm_bandwidth_golden_6u.csv
index aa4b8d1014..db059cea93 100644
--- a/tests/tt_metal/microbenchmarks/ethernet/fabric_edm_bandwidth_golden_6u.csv
+++ b/tests/tt_metal/microbenchmarks/ethernet/fabric_edm_bandwidth_golden_6u.csv
@@ -1,260 +1,229 @@
 Test Name,Noc Message Type,Packet Size,Line Size,Num Links,Disable Interior Workers,Unidirectional,Senders Are Unidirectional,Bandwidth (B/c),Packets/Second
-mcast_HalfRing,noc_unicast_write,4096,4,1,False,False,False,11.315259945064772,2762514.635025579
-mcast_HalfRing,noc_unicast_write,4096,4,2,False,False,False,11.344635528033304,2769686.408211256
-mcast_HalfRing,noc_unicast_write,4096,4,3,False,False,False,10.332191041001545,2522507.578369518
-mcast_HalfRing,noc_unicast_write,4096,4,4,False,False,False,9.04124116021948,2207334.267631709
-mcast_FullRing,noc_unicast_write,16,4,1,False,False,False,0.03993087698987206,2495679.811867004
-mcast_FullRing,noc_unicast_write,2048,4,1,False,False,False,5.107273483131671,2493785.880435386
-mcast_FullRing,noc_unicast_write,4096,4,1,False,False,False,9.758622017132504,2382476.07840149
-mcast_FullRing,noc_unicast_write,4096,4,2,False,False,False,9.582090063061077,2339377.456802021
-mcast_FullRing,noc_unicast_write,4096,4,3,False,False,False,9.480224729930768,2314507.990705754
-mcast_FullRing,noc_unicast_write,4096,4,4,False,False,False,9.158696645318784,2236009.9231735314
-mcast_HalfRing,noc_unicast_write,16,8,1,False,False,False,0.034697845348108804,2168615.3342568004
-mcast_HalfRing,noc_unicast_write,16,8,2,False,False,False,0.027914425737095294,1744651.6085684558
-mcast_HalfRing,noc_unicast_write,16,8,3,False,False,False,0.027896882330300857,1743555.1456438035
-mcast_HalfRing,noc_unicast_write,16,8,4,False,False,False,0.02663773181894526,1664858.2386840787
-mcast_HalfRing,noc_unicast_write,2048,8,1,False,False,False,4.431328462290725,2163734.6007278934
-mcast_HalfRing,noc_unicast_write,2048,8,2,False,False,False,3.580073644850624,1748082.8343997188
-mcast_HalfRing,noc_unicast_write,2048,8,3,False,False,False,3.5700586165318176,1743192.6838534265
-mcast_HalfRing,noc_unicast_write,2048,8,4,False,False,False,3.4102302085643426,1665151.4690255579
-mcast_HalfRing,noc_unicast_write,4096,8,1,False,False,False,7.0573462928047235,1722984.9347667783
-mcast_HalfRing,noc_unicast_write,4096,8,2,False,False,False,7.058322821527327,1723223.345099445
-mcast_HalfRing,noc_unicast_write,4096,8,3,False,False,False,6.824439525045251,1666122.9309192507
-mcast_HalfRing,noc_unicast_write,4096,8,4,False,False,False,6.682158215646945,1631386.28311693
-mcast_FullRing,noc_unicast_write,16,8,1,False,False,False,0.033160192961827505,2072512.060114219
-mcast_FullRing,noc_unicast_write,2048,8,1,False,False,False,4.2706465215619085,2085276.6218564007
-mcast_FullRing,noc_unicast_write,4096,8,1,False,False,False,8.115598951504357,1981347.4002696183
-mcast_FullRing,noc_unicast_write,4096,8,2,False,False,False,7.982216261370578,1948783.2669361762
-mcast_FullRing,noc_unicast_write,4096,8,3,False,False,False,7.298803174600083,1781934.3687988485
-mcast_FullRing,noc_unicast_write,4096,8,4,False,False,False,6.896529221968592,1683722.9545821757
-unicast_FullRing,noc_unicast_write,4096,4,1,False,False,False,10.144678654961242,2476728.1872463967
-unicast_FullRing,noc_unicast_write,4096,4,2,False,False,False,10.11913386077452,2470491.665228154
-unicast_FullRing,noc_unicast_write,4096,4,3,False,False,False,10.058405422767292,2455665.386417796
-unicast_FullRing,noc_unicast_write,4096,4,4,False,False,False,9.740151689466684,2377966.721061202
-mcast_SaturateChipToChipRing,noc_unicast_write,16,4,1,False,False,False,0.040855547856554535,2553471.7410346586
-mcast_SaturateChipToChipRing,noc_unicast_write,2048,4,1,False,False,False,5.218206117307108,2547952.2057163613
-mcast_SaturateChipToChipRing,noc_unicast_write,4096,4,1,False,False,False,10.298973596396173,2514397.8506826595
-mcast_SaturateChipToChipRing,noc_unicast_write,4096,4,2,False,False,False,10.05844877159107,2455675.969626726
-mcast_SaturateChipToChipRing,noc_unicast_write,4096,4,3,False,False,False,9.619910998678401,2348611.083661719
-mcast_SaturateChipToChipRing,noc_unicast_write,4096,4,4,False,False,False,9.607580756513013,2345600.7706330596
-unicast_Linear_0x4_mesh,noc_unicast_write,16,2,1,False,False,True,0.046997340640793174,2937333.7900495734
-unicast_Linear_0x4_mesh,noc_unicast_write,16,2,2,False,False,True,0.04683862345091316,2927413.965682072
-unicast_Linear_0x4_mesh,noc_unicast_write,2048,2,1,False,False,True,6.020154495448221,2939528.5622305768
-unicast_Linear_0x4_mesh,noc_unicast_write,2048,2,2,False,False,True,6.020481366000187,2939688.166992279
-unicast_Linear_0x4_mesh,noc_unicast_write,4096,2,1,False,False,True,11.052203203822163,2698291.797808145
-unicast_Linear_0x4_mesh,noc_unicast_write,4096,2,2,False,False,True,11.04472218607086,2696465.377458706
-mcast_Linear_2x0_mesh,noc_unicast_write,16,4,1,False,False,True,0.04203373749932245,2627108.593707653
-mcast_Linear_2x0_mesh,noc_unicast_write,2048,4,1,False,False,True,5.380193982263539,2627047.842902119
-mcast_Linear_2x0_mesh,noc_unicast_write,4096,4,1,False,False,True,10.429891161564832,2546360.146866414
-unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,16,"[4, 2]","[1, 1]","[False, False]","[False, False]","[True, True]",0.04680522079766876,2925326.2998542977
-unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,2048,"[4, 2]","[1, 1]","[False, False]","[False, False]","[True, True]",5.983270361214385,2921518.7310617114
-unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 2]","[1, 1]","[False, False]","[False, False]","[True, True]",10.720351128141271,2617273.224643865
-mcast_HalfRing_0x4_mesh,noc_unicast_write,2048,8,1,False,False,False,4.04914982817965,1977123.9395408449
-mcast_HalfRing_0x4_mesh,noc_unicast_write,2048,8,2,False,False,False,4.033436241102118,1969451.2896006436
-mcast_HalfRing_0x4_mesh,noc_unicast_write,2048,8,3,False,False,False,3.40206704103261,1661165.547379204
-mcast_HalfRing_0x4_mesh,noc_unicast_write,2048,8,4,False,False,False,3.4888387153962124,1703534.529002057
-mcast_HalfRing_0x4_mesh,noc_unicast_write,4096,8,1,False,False,False,7.866040863198891,1920420.1326169167
-mcast_HalfRing_0x4_mesh,noc_unicast_write,4096,8,2,False,False,False,7.778103395129789,1898951.0242016087
-mcast_HalfRing_0x4_mesh,noc_unicast_write,4096,8,3,False,False,False,6.721409257662315,1640969.0570464637
-mcast_HalfRing_0x4_mesh,noc_unicast_write,4096,8,4,False,False,False,6.606567720666978,1612931.5724284614
-mcast_FullRing_0x4_mesh,noc_unicast_write,2048,8,1,False,False,False,4.198360810700896,2049980.8646000468
-mcast_FullRing_0x4_mesh,noc_unicast_write,2048,8,2,False,False,False,3.7731546523697617,1842360.6701024226
-mcast_FullRing_0x4_mesh,noc_unicast_write,2048,8,3,False,False,False,3.5181839432902122,1717863.253559674
-mcast_FullRing_0x4_mesh,noc_unicast_write,2048,8,4,False,False,False,3.4973956271181295,1707712.708553774
-mcast_FullRing_0x4_mesh,noc_unicast_write,4096,8,1,False,False,False,7.623551220992013,1861218.5598125032
-mcast_FullRing_0x4_mesh,noc_unicast_write,4096,8,2,False,False,False,7.197084669643491,1757100.7494246804
-mcast_FullRing_0x4_mesh,noc_unicast_write,4096,8,3,False,False,False,6.976400358564885,1703222.7437902552
-mcast_FullRing_0x4_mesh,noc_unicast_write,4096,8,4,False,False,False,6.93295729584237,1692616.5273052661
-mcast_Linear_0x4_mesh,noc_unicast_write,2048,8,1,False,False,True,4.970299879633437,2426904.2381022642
-mcast_Linear_0x4_mesh,noc_unicast_write,2048,8,2,False,False,True,4.946805655802712,2415432.449122418
-mcast_Linear_0x4_mesh,noc_unicast_write,2048,8,3,False,False,True,4.946346734842525,2415208.3666223264
-mcast_Linear_0x4_mesh,noc_unicast_write,2048,8,4,False,False,True,4.946286181974738,2415178.7997923526
-mcast_Linear_0x4_mesh,noc_unicast_write,4096,8,1,False,False,True,9.427634005863691,2301668.4584628153
-mcast_Linear_0x4_mesh,noc_unicast_write,4096,8,2,False,False,True,9.22905413945441,2253187.045765237
-mcast_Linear_0x4_mesh,noc_unicast_write,4096,8,3,False,False,True,9.231570214422094,2253801.321880394
-mcast_Linear_0x4_mesh,noc_unicast_write,4096,8,4,False,False,True,9.135040438625568,2230234.4820863204
-mcast_HalfRing_8x0_mesh,noc_unicast_write,2048,4,1,False,False,False,6.251838223215366,3052655.382429378
-mcast_HalfRing_8x0_mesh,noc_unicast_write,2048,4,2,False,False,False,6.231505590094864,3042727.3389135078
-mcast_HalfRing_8x0_mesh,noc_unicast_write,2048,4,3,False,False,False,5.6253642675721816,2746759.8962754793
-mcast_HalfRing_8x0_mesh,noc_unicast_write,2048,4,4,False,False,False,5.625011647248543,2746587.718383078
-mcast_HalfRing_8x0_mesh,noc_unicast_write,4096,4,1,False,False,False,11.384473983068695,2779412.5935226306
-mcast_HalfRing_8x0_mesh,noc_unicast_write,4096,4,2,False,False,False,11.353090561193369,2771750.62529135
-mcast_HalfRing_8x0_mesh,noc_unicast_write,4096,4,3,False,False,False,9.392647797159809,2293126.903603469
-mcast_HalfRing_8x0_mesh,noc_unicast_write,4096,4,4,False,False,False,9.441711876411496,2305105.438577025
-mcast_FullRing_8x0_mesh,noc_unicast_write,2048,4,1,False,False,False,4.3608974846266655,2129344.474915364
-mcast_FullRing_8x0_mesh,noc_unicast_write,2048,4,2,False,False,False,3.8400566168347443,1875027.64493884
-mcast_FullRing_8x0_mesh,noc_unicast_write,2048,4,3,False,False,False,3.8393854223765924,1874699.9132698206
-mcast_FullRing_8x0_mesh,noc_unicast_write,2048,4,4,False,False,False,3.841007040027013,1875491.71876319
-mcast_FullRing_8x0_mesh,noc_unicast_write,4096,4,1,False,False,False,9.4561332801354,2308626.289095557
-mcast_FullRing_8x0_mesh,noc_unicast_write,4096,4,2,False,False,False,9.410315606653862,2297440.333655728
-mcast_FullRing_8x0_mesh,noc_unicast_write,4096,4,3,False,False,False,9.44228731910572,2305245.927516045
-mcast_FullRing_8x0_mesh,noc_unicast_write,4096,4,4,False,False,False,9.374084669321897,2288594.8899711664
-mcast_Linear_8x0_mesh,noc_unicast_write,2048,4,1,False,False,True,5.380110098121203,2627006.8838482434
-mcast_Linear_8x0_mesh,noc_unicast_write,2048,4,2,False,False,True,5.378780067861531,2626357.4550105133
-mcast_Linear_8x0_mesh,noc_unicast_write,2048,4,3,False,False,True,5.0306133270096955,2456354.163578953
-mcast_Linear_8x0_mesh,noc_unicast_write,2048,4,4,False,False,True,5.029920752530177,2456015.992446375
-mcast_Linear_8x0_mesh,noc_unicast_write,4096,4,1,False,False,True,10.542487081962541,2573849.385244761
-mcast_Linear_8x0_mesh,noc_unicast_write,4096,4,2,False,False,True,10.332516578505192,2522587.055299119
-mcast_Linear_8x0_mesh,noc_unicast_write,4096,4,3,False,False,True,10.042668284248924,2451823.31158421
-mcast_Linear_8x0_mesh,noc_unicast_write,4096,4,4,False,False,True,10.012104539057624,2444361.4597308654
-unicast_FullRing_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[4, 4]","[False, False]","[False, False]","[False, False]",7.463274035267158,1822088.3875163961
-mcast_FullRing_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[4, 4]","[False, False]","[False, False]","[False, False]",6.889848629146124,1682091.950475128
-unicast_FullRing_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[1, 1]","[False, False]","[False, False]","[False, False]",8.51006427219215,2077652.4102031614
-mcast_FullRing_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[1, 1]","[False, False]","[False, False]","[False, False]",7.263528197769112,1773322.3139084748
-unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[4, 4]","[False, False]","[False, False]","[True, True]",9.514520847838698,2322881.0663668695
-mcast_Linear_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[4, 4]","[False, False]","[False, False]","[True, True]",9.220451979982885,2251086.909175509
-unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[1, 1]","[False, False]","[False, False]","[True, True]",9.662563456499937,2359024.281372055
-mcast_Linear_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[1, 1]","[False, False]","[False, False]","[True, True]",9.431080244099313,2302509.825219559
-mcast_Linear,noc_unicast_write,16,4,1,False,False,True,0.04214894082971652,2634308.8018572824
-mcast_Linear,noc_unicast_write,2048,4,1,False,False,True,5.3923651981436835,2632990.8194060954
-mcast_Linear,noc_unicast_write,4096,4,1,False,False,True,10.578682606385945,2582686.1831996934
-mcast_RingAsLinear,noc_unicast_write,16,4,1,False,False,True,0.036227806378813274,2264237.8986758296
-mcast_RingAsLinear,noc_unicast_write,2048,4,1,False,False,True,4.630138390326171,2260809.7609014506
-mcast_RingAsLinear,noc_unicast_write,4096,4,1,False,False,True,9.267801694068622,2262646.8979659723
-mcast_Linear,noc_unicast_write,16,4,1,True,False,True,0.041584495054680884,2599030.9409175552
-mcast_Linear,noc_unicast_write,2048,4,1,True,False,True,5.319266505355408,2597298.09831807
-mcast_Linear,noc_unicast_write,4096,4,1,True,False,True,9.988439040228547,2438583.750055798
-mcast_RingAsLinear,noc_unicast_write,16,4,1,True,False,True,0.03535375904864272,2209609.94054017
-mcast_RingAsLinear,noc_unicast_write,2048,4,1,True,False,True,4.523233018788269,2208609.8724552095
-mcast_RingAsLinear,noc_unicast_write,4096,4,1,True,False,True,8.992780384699772,2195503.0236083427
-mcast_Linear,noc_unicast_write,16,4,1,True,True,True,0.05751295402905112,3594559.626815695
-mcast_Linear,noc_unicast_write,2048,4,1,True,True,True,6.869100535223007,3354052.995714359
-mcast_Linear,noc_unicast_write,4096,4,1,True,True,True,11.748427615229817,2868268.460749467
-mcast_RingAsLinear,noc_unicast_write,16,4,1,True,True,True,0.046143360297061724,2883960.018566358
-mcast_RingAsLinear,noc_unicast_write,2048,4,1,True,True,True,5.888094916090047,2875046.345747093
-mcast_RingAsLinear,noc_unicast_write,4096,4,1,True,True,True,11.209896192693956,2736791.062669423
-mcast_Linear,noc_unicast_write,16,4,2,False,False,True,0.04209911364065682,2631194.6025410513
-mcast_Linear,noc_unicast_write,16,4,3,False,False,True,0.03925889607811971,2453681.004882482
-mcast_Linear,noc_unicast_write,16,4,4,False,False,True,0.03925438816678432,2453399.26042402
-mcast_Linear,noc_unicast_write,2048,4,2,False,False,True,5.3869196463734506,2630331.8585807863
-mcast_Linear,noc_unicast_write,2048,4,3,False,False,True,5.036415212447884,2459187.115453068
-mcast_Linear,noc_unicast_write,2048,4,4,False,False,True,5.035346045520917,2458665.06128951
-mcast_Linear,noc_unicast_write,4096,4,2,False,False,True,10.446504742910173,2550416.196999554
-mcast_Linear,noc_unicast_write,4096,4,3,False,False,True,10.052230121365218,2454157.74447393
-mcast_Linear,noc_unicast_write,4096,4,4,False,False,True,10.021136499879537,2446566.5282909027
-mcast_RingAsLinear,noc_unicast_write,16,4,2,False,False,True,0.035955048099989194,2247190.5062493244
-mcast_RingAsLinear,noc_unicast_write,16,4,3,False,False,True,0.035954075320282275,2247129.707517642
-mcast_RingAsLinear,noc_unicast_write,16,4,4,False,False,True,0.032747383379174044,2046711.4611983777
-mcast_RingAsLinear,noc_unicast_write,2048,4,2,False,False,True,4.598788101446414,2245502.002659382
-mcast_RingAsLinear,noc_unicast_write,2048,4,3,False,False,True,4.598488701523926,2245355.8112909794
-mcast_RingAsLinear,noc_unicast_write,2048,4,4,False,False,True,4.187838352813572,2044842.9457097522
-mcast_RingAsLinear,noc_unicast_write,4096,4,2,False,False,True,9.203361799774022,2246914.501897955
-mcast_RingAsLinear,noc_unicast_write,4096,4,3,False,False,True,9.198384639245603,2245699.374815821
-mcast_RingAsLinear,noc_unicast_write,4096,4,4,False,False,True,8.387645822952123,2047765.0934941708
-unicast_Linear,noc_unicast_write,16,2,1,False,False,True,0.046682979547588536,2917686.2217242834
-unicast_Linear,noc_unicast_write,2048,2,1,False,False,True,5.995630157562994,2927553.7878725557
-unicast_Linear,noc_unicast_write,4096,2,1,False,False,True,11.008994208505728,2687742.7266859687
-unicast_RingAsLinear,noc_unicast_write,16,2,1,False,False,True,0.043333592025849896,2708349.5016156184
-unicast_RingAsLinear,noc_unicast_write,2048,2,1,False,False,True,5.504248786884934,2687621.477971159
-unicast_RingAsLinear,noc_unicast_write,4096,2,1,False,False,True,10.944919146614286,2672099.4010288785
-unicast_Linear,noc_unicast_write,16,2,2,False,False,True,0.0466824531162288,2917653.3197643002
-unicast_Linear,noc_unicast_write,2048,2,2,False,False,True,5.995272809356443,2927379.301443576
-unicast_Linear,noc_unicast_write,4096,2,2,False,False,True,11.012699449235972,2688647.3264736263
-unicast_RingAsLinear,noc_unicast_write,16,2,2,False,False,True,0.04272266868004363,2670166.792502727
-unicast_RingAsLinear,noc_unicast_write,2048,2,2,False,False,True,5.486855625168994,2679128.723227048
-unicast_RingAsLinear,noc_unicast_write,4096,2,2,False,False,True,10.903517182888931,2661991.499728743
-unicast_Linear,noc_unicast_write,16,4,1,False,False,True,0.047083927918125934,2942745.494882871
-unicast_Linear,noc_unicast_write,2048,4,1,False,False,True,6.042032603492378,2950211.2321740123
-unicast_Linear,noc_unicast_write,4096,4,1,False,False,True,10.997981651535033,2685054.1141442955
-unicast_RingAsLinear,noc_unicast_write,16,4,1,False,False,True,0.039258613355105505,2453663.334694094
-unicast_RingAsLinear,noc_unicast_write,2048,4,1,False,False,True,5.025742574937736,2453975.866668816
-unicast_RingAsLinear,noc_unicast_write,4096,4,1,False,False,True,10.067750541370488,2457946.9095142796
-unicast_Linear,noc_unicast_write,16,4,1,True,False,True,0.0464334860192111,2902092.8762006937
-unicast_Linear,noc_unicast_write,2048,4,1,True,False,True,5.8458595550795645,2854423.6108786934
-unicast_Linear,noc_unicast_write,4096,4,1,True,False,True,10.116101279264484,2469751.2888829308
-unicast_RingAsLinear,noc_unicast_write,16,4,1,True,False,True,0.038204491114626295,2387780.6946641435
-unicast_RingAsLinear,noc_unicast_write,2048,4,1,True,False,True,4.893307966501293,2389310.5305182096
-unicast_RingAsLinear,noc_unicast_write,4096,4,1,True,False,True,9.529828041861027,2326618.1742824772
-unicast_Linear,noc_unicast_write,16,4,1,True,True,True,0.06693149227108593,4183218.2669428703
-unicast_Linear,noc_unicast_write,2048,4,1,True,True,True,7.358435406922825,3592986.0385365356
-unicast_Linear,noc_unicast_write,4096,4,1,True,True,True,11.777855836744196,2875453.085142626
-unicast_RingAsLinear,noc_unicast_write,16,4,1,True,True,True,0.05159792900747369,3224870.562967106
-unicast_RingAsLinear,noc_unicast_write,2048,4,1,True,True,True,6.319459086393208,3085673.382027934
-unicast_RingAsLinear,noc_unicast_write,4096,4,1,True,True,True,11.545422605664047,2818706.690835949
-unicast_Linear,noc_unicast_flush_atomic_inc,16,4,1,True,False,True,0.02072722263805493,1295451.414878433
-unicast_Linear,noc_unicast_no_flush_atomic_inc,16,4,1,True,False,True,0.022730875441877563,1420679.7151173477
-unicast_RingAsLinear,noc_unicast_flush_atomic_inc,16,4,1,True,False,True,0.018679548768414152,1167471.7980258844
-unicast_RingAsLinear,noc_unicast_no_flush_atomic_inc,16,4,1,True,False,True,0.01910328910223442,1193955.5688896514
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.02433196226908003,1520747.641817502
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.02798061452310975,1748788.4076943595
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.024138781822495528,1508673.8639059705
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.02560124544938862,1600077.840586789
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,2.764170831183132,1349692.7886636388
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,3.1895488079129466,1557396.8788637435
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.737040632150005,1336445.6211669948
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.97547813739834,1452870.1842765333
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,4.4749447301034895,1092515.8032479223
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,5.6500233396243065,1379400.2294004655
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,4.450601955595151,1086572.7430652224
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,5.310247619530144,1296447.1727368515
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.013188754143875641,824297.1339922276
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.015416345410503145,963521.5881564466
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.028731981085988255,1795748.8178742658
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.03083636717413594,1927272.9483834964
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.4875973153677,726365.8766443848
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.7439312066824584,851528.9095129191
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,3.210456381188459,1567605.6548771774
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,3.4863459889423423,1702317.377413253
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,2.4278962744977868,592748.1138910613
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,3.050631830278366,744783.1616890542
-mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,5.164203939633255,1260791.9774495251
-unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,6.090525868062571,1486944.7920074635
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.038154718803627315,2384669.925226707
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.04704135389250225,2940084.6182813905
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.03782333156792933,2363958.222995583
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.041779578698123,2611223.6686326875
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,4.888383588834619,2386906.049235654
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,6.024428424815306,2941615.4418043485
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,4.839593699094764,2363082.860886115
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,5.418191310117997,2645601.2256435533
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,9.803536623443465,2393441.558457877
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,11.015074994716844,2689227.293632042
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,False,True,9.594655214858495,2342445.1208150624
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,False,True,10.168244856797612,2482481.6544916043
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,True,True,0.021758963020043976,1359935.1887527485
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,True,True,0.026210538641640146,1638158.665102509
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,True,True,0.050434145000499454,3152134.062531216
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,True,True,0.05873715124816446,3671071.9530102788
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,True,True,2.787858638769077,1361259.1009614635
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,True,True,3.313705453394836,1618020.2409154472
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,True,True,6.422497708418009,3135985.209188481
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,True,True,7.196117118562969,3513729.061798325
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,True,True,5.422348347715739,1323815.514579038
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,True,True,5.812494696683954,1419066.088057606
-mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,True,True,11.691289947601414,2854318.8348636264
-unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,True,True,11.778001465233991,2875488.6389731425
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.021411125304345347,1338195.3315215842
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.027531344166472748,1720709.0104045467
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.02113991062059002,1321244.4137868762
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.025012410454593525,1563275.6534120953
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,2.4499653493792346,1196272.1432515795
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,3.153001628683915,1539551.5765058177
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.4460977049774937,1194383.6450085419
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.905617581710386,1418758.5848195243
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,4.041943681805661,986802.6566908351
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,5.623524592777567,1372930.8087835857
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,4.041583353823286,986714.6859920133
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,5.181809858674106,1265090.297527858
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.01136577346001042,710360.8412506512
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.015028839121180367,939302.4450737729
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.024540446398082116,1533777.8998801322
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.030055219609685824,1878451.225605364
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.309937085186523,639617.7173762319
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.708688800188176,834320.7032168828
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,2.776690045004885,1355805.6860375416
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,3.413970977762705,1666978.0164856957
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,2.169541190450345,529673.1421997913
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,3.0463180272945856,743729.9871324672
-mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,4.718268947296948,1151921.1297111688
-unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,6.0896235891560515,1486724.5090713017
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.03360960481002179,2100600.3006263617
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.0392569866932003,2453561.668325019
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.03289416471086872,2055885.294429295
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.03820379964962818,2387737.4781017615
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,4.299158587699954,2099198.529150368
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,5.0257080734644415,2453959.020246309
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,4.208665469580822,2055012.4363187607
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,4.8930187326164045,2389169.303035354
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,8.608087806026283,2101583.9370181356
-unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,10.048646883213207,2453282.9304719744
-mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,False,True,8.394556965984131,2049452.3842734697
+mcast_HalfRing,noc_unicast_write,4096,4,1,False,False,False,11.345792570298036,2769968.889232919
+mcast_HalfRing,noc_unicast_write,4096,4,2,False,False,False,11.330537619072304,2766244.535906324
+mcast_HalfRing,noc_unicast_write,4096,4,3,False,False,False,10.27558637642511,2515028.9642681913
+mcast_HalfRing,noc_unicast_write,4096,4,4,False,False,False,9.048954945670868,2209217.5160329267
+mcast_FullRing,noc_unicast_write,16,4,1,False,False,False,0.039852346888725826,2490771.680545364
+mcast_FullRing,noc_unicast_write,2048,4,1,False,False,False,5.096832200574682,2488687.5979368566
+mcast_FullRing,noc_unicast_write,4096,4,1,False,False,False,9.755098095596937,2381615.745995346
+mcast_FullRing,noc_unicast_write,4096,4,2,False,False,False,9.56834347200993,2336021.355471174
+mcast_FullRing,noc_unicast_write,4096,4,3,False,False,False,9.472529137451062,2312629.183948013
+mcast_FullRing,noc_unicast_write,4096,4,4,False,False,False,9.148423965647103,2233501.944738062
+mcast_HalfRing,noc_unicast_write,16,8,1,False,False,False,0.03420325116331804,2137703.1977073774
+mcast_HalfRing,noc_unicast_write,16,8,2,False,False,False,0.027875171072701502,1742198.192043844
+mcast_HalfRing,noc_unicast_write,16,8,3,False,False,False,0.027861608326664542,1741350.5204165338
+mcast_HalfRing,noc_unicast_write,16,8,4,False,False,False,0.026548973022773373,1659310.8139233359
+mcast_HalfRing,noc_unicast_write,2048,8,1,False,False,False,4.432033991839609,2164079.097577934
+mcast_HalfRing,noc_unicast_write,2048,8,2,False,False,False,3.5748720290072935,1745542.9829137176
+mcast_HalfRing,noc_unicast_write,2048,8,3,False,False,False,3.564763778254226,1740607.3136006964
+mcast_HalfRing,noc_unicast_write,2048,8,4,False,False,False,3.3985319619692658,1659439.4345553056
+mcast_HalfRing,noc_unicast_write,4096,8,1,False,False,False,7.067458237518352,1725453.6712691288
+mcast_HalfRing,noc_unicast_write,4096,8,2,False,False,False,6.850672264342723,1672527.4082867976
+mcast_HalfRing,noc_unicast_write,4096,8,3,False,False,False,6.831327752752033,1667804.6271367269
+mcast_HalfRing,noc_unicast_write,4096,8,4,False,False,False,6.687633224070424,1632722.9550953184
+mcast_FullRing,noc_unicast_write,16,8,1,False,False,False,0.032836544222804305,2052284.013925269
+mcast_FullRing,noc_unicast_write,2048,8,1,False,False,False,4.290401750140346,2094922.729560716
+mcast_FullRing,noc_unicast_write,4096,8,1,False,False,False,8.15279503722146,1990428.4758841456
+mcast_FullRing,noc_unicast_write,4096,8,2,False,False,False,8.000333417913216,1953206.400857719
+mcast_FullRing,noc_unicast_write,4096,8,3,False,False,False,7.237116039693315,1754667.0018782506
+mcast_FullRing,noc_unicast_write,4096,8,4,False,False,False,7.041485020659927,1681514.8976220526
+unicast_FullRing,noc_unicast_write,4096,4,1,False,False,False,10.147318619776387,2477372.7099063443
+unicast_FullRing,noc_unicast_write,4096,4,2,False,False,False,10.117036568003153,2469979.6308601447
+unicast_FullRing,noc_unicast_write,4096,4,3,False,False,False,10.04267716900118,2451825.480713179
+unicast_FullRing,noc_unicast_write,4096,4,4,False,False,False,9.744566931100785,2379044.6609132774
+mcast_SaturateChipToChipRing,noc_unicast_write,16,4,1,False,False,False,0.04083513725742413,2552196.078589008
+mcast_SaturateChipToChipRing,noc_unicast_write,2048,4,1,False,False,False,5.215351931676716,2546558.5603890214
+mcast_SaturateChipToChipRing,noc_unicast_write,4096,4,1,False,False,False,10.298628892779892,2514313.694526341
+mcast_SaturateChipToChipRing,noc_unicast_write,4096,4,2,False,False,False,10.05479676034252,2454784.365317998
+mcast_SaturateChipToChipRing,noc_unicast_write,4096,4,3,False,False,False,9.610898646437342,2346410.8023528666
+mcast_SaturateChipToChipRing,noc_unicast_write,4096,4,4,False,False,False,9.602947354607602,2344469.568995997
+unicast_Linear_0x4_mesh,noc_unicast_write,16,2,1,False,False,True,0.04600862192948342,2875538.870592714
+unicast_Linear_0x4_mesh,noc_unicast_write,16,2,2,False,False,True,0.045976324491703,2873520.2807314373
+unicast_Linear_0x4_mesh,noc_unicast_write,2048,2,1,False,False,True,5.975592894847231,2888473.0931871245
+unicast_Linear_0x4_mesh,noc_unicast_write,2048,2,2,False,False,True,5.971529828437907,2886489.174041947
+unicast_Linear_0x4_mesh,noc_unicast_write,4096,2,1,False,False,True,11.176111824966155,2728542.9260171275
+unicast_Linear_0x4_mesh,noc_unicast_write,4096,2,2,False,False,True,11.125537947929223,2728402.819318658
+mcast_Linear_2x0_mesh,noc_unicast_write,16,4,1,False,False,True,0.041978790176908004,2623674.38605675
+mcast_Linear_2x0_mesh,noc_unicast_write,2048,4,1,False,False,True,5.385862498838361,2629815.6732609184
+mcast_Linear_2x0_mesh,noc_unicast_write,4096,4,1,False,False,True,10.405971935467146,2540520.4920574087
+mcast_Linear_0x4_mesh,noc_unicast_write,2048,8,4,False,False,True,4.93858040018085,2411416.2110258057
+mcast_Linear_0x4_mesh,noc_unicast_write,4096,8,4,False,False,True,9.119016055496026,2226322.2791738342
+mcast_Linear_8x0_mesh,noc_unicast_write,2048,4,4,False,False,True,5.03613030042878,2459047.99825624
+mcast_Linear_8x0_mesh,noc_unicast_write,4096,4,4,False,False,True,10.029921906552985,2448711.4029670376
+unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,16,"[4, 2]","[1, 1]","[False, False]","[False, False]","[True, True]",0.047289818995557745,2955613.6872223592
+unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,2048,"[4, 2]","[1, 1]","[False, False]","[False, False]","[True, True]",6.0373207387590515,2947910.516972193
+unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 2]","[1, 1]","[False, False]","[False, False]","[True, True]",10.735075094025603,2620867.942877345
+unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[4, 4]","[False, False]","[False, False]","[True, True]",9.511498736678604,2322143.246259425
+mcast_Linear_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[4, 4]","[False, False]","[False, False]","[True, True]",9.220338835015085,2251059.285892355
+unicast_Linear_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[1, 1]","[False, False]","[False, False]","[True, True]",9.66159129528129,2358786.9373245337
+mcast_Linear_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[1, 1]","[False, False]","[False, False]","[True, True]",9.42300348337968,2300537.9598094923
+unicast_FullRing_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[4, 4]","[False, False]","[False, False]","[False, False]",7.307342329194562,1771812.092088516
+mcast_FullRing_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[4, 4]","[False, False]","[False, False]","[False, False]",6.79217569534689,1646038.9881217992
+unicast_FullRing_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[1, 1]","[False, False]","[False, False]","[False, False]",8.376766518602965,1867307.7084234813
+mcast_FullRing_all_rows_all_cols_mesh,noc_unicast_write,4096,"[4, 8]","[1, 1]","[False, False]","[False, False]","[False, False]",7.144104148854267,1731959.0207163738
+mcast_Linear,noc_unicast_write,16,4,1,False,False,True,0.04201191048165132,2625744.4051032076
+mcast_Linear,noc_unicast_write,2048,4,1,False,False,True,5.373852813293946,2623951.5689911847
+mcast_Linear,noc_unicast_write,4096,4,1,False,False,True,10.575302183418431,2581860.8846236407
+mcast_RingAsLinear,noc_unicast_write,16,4,1,False,False,True,0.036197154339219564,2262322.1462012227
+mcast_RingAsLinear,noc_unicast_write,2048,4,1,False,False,True,4.627383005514522,2259464.358161388
+mcast_RingAsLinear,noc_unicast_write,4096,4,1,False,False,True,9.259742531796384,2260679.3290518518
+mcast_Linear,noc_unicast_write,16,4,1,True,False,True,0.0415381914203956,2596136.9637747253
+mcast_Linear,noc_unicast_write,2048,4,1,True,False,True,5.312664233277945,2594074.332655247
+mcast_Linear,noc_unicast_write,4096,4,1,True,False,True,9.995446288440785,2440294.5040138634
+mcast_RingAsLinear,noc_unicast_write,16,4,1,True,False,True,0.03538045532346272,2211278.45771642
+mcast_RingAsLinear,noc_unicast_write,2048,4,1,True,False,True,4.525056989369773,2209500.4830907094
+mcast_RingAsLinear,noc_unicast_write,4096,4,1,True,False,True,9.003577922070713,2198139.141130545
+mcast_Linear,noc_unicast_write,16,4,1,True,True,True,0.057801898911595664,3612618.681974729
+mcast_Linear,noc_unicast_write,2048,4,1,True,True,True,6.868244155012715,3353634.8413148024
+mcast_Linear,noc_unicast_write,4096,4,1,True,True,True,11.74145701505905,2866566.654067151
+mcast_RingAsLinear,noc_unicast_write,16,4,1,True,True,True,0.04568052519585025,2855032.824740641
+mcast_RingAsLinear,noc_unicast_write,2048,4,1,True,True,True,5.842195232243806,2852634.390744046
+mcast_RingAsLinear,noc_unicast_write,4096,4,1,True,True,True,11.189378176806107,2731781.7814468034
+mcast_Linear,noc_unicast_write,16,4,2,False,False,True,0.041897392552434715,2618587.0345271695
+mcast_Linear,noc_unicast_write,16,4,3,False,False,True,0.039382046463430694,2461377.9039644185
+mcast_Linear,noc_unicast_write,16,4,4,False,False,True,0.03937936981022208,2461210.61313888
+mcast_Linear,noc_unicast_write,2048,4,2,False,False,True,5.367305582196983,2620754.678807121
+mcast_Linear,noc_unicast_write,2048,4,3,False,False,True,5.04594162928392,2463838.686173789
+mcast_Linear,noc_unicast_write,2048,4,4,False,False,True,5.045878473552529,2463807.8484143205
+mcast_Linear,noc_unicast_write,4096,4,2,False,False,True,10.428810086899919,2546096.2126220507
+mcast_Linear,noc_unicast_write,4096,4,3,False,False,True,10.083477766863925,2461786.564175763
+mcast_Linear,noc_unicast_write,4096,4,4,False,False,True,10.048038750889445,2453134.4606663682
+mcast_RingAsLinear,noc_unicast_write,16,4,2,False,False,True,0.035925297116801806,2245331.069800113
+mcast_RingAsLinear,noc_unicast_write,16,4,3,False,False,True,0.03592277149705226,2245173.2185657662
+mcast_RingAsLinear,noc_unicast_write,16,4,4,False,False,True,0.03272374032965538,2045233.7706034612
+mcast_RingAsLinear,noc_unicast_write,2048,4,2,False,False,True,4.594409257873743,2243363.8954461636
+mcast_RingAsLinear,noc_unicast_write,2048,4,3,False,False,True,4.593084915543249,2242717.243917602
+mcast_RingAsLinear,noc_unicast_write,2048,4,4,False,False,True,4.184783188385494,2043351.1662038546
+mcast_RingAsLinear,noc_unicast_write,4096,4,2,False,False,True,9.194407961526744,2244728.5062321154
+mcast_RingAsLinear,noc_unicast_write,4096,4,3,False,False,True,9.187957040087301,2243153.574240064
+mcast_RingAsLinear,noc_unicast_write,4096,4,4,False,False,True,8.38554822213045,2047252.9839185667
+unicast_Linear,noc_unicast_write,16,2,1,False,False,True,0.04661634597218528,2913521.6232615802
+unicast_Linear,noc_unicast_write,2048,2,1,False,False,True,5.999666707968084,2929524.7597500407
+unicast_Linear,noc_unicast_write,4096,2,1,False,False,True,11.065915674978496,2713846.6003365465
+unicast_RingAsLinear,noc_unicast_write,16,2,1,False,False,True,0.04284636030278291,2677897.5189239318
+unicast_RingAsLinear,noc_unicast_write,2048,2,1,False,False,True,5.5201597622644005,2695390.5089181643
+unicast_RingAsLinear,noc_unicast_write,4096,2,1,False,False,True,10.976066988737593,2679703.854672264
+unicast_Linear,noc_unicast_write,16,2,2,False,False,True,0.04652257017777059,2907660.636110662
+unicast_Linear,noc_unicast_write,2048,2,2,False,False,True,5.987767558079481,2923714.6279684966
+unicast_Linear,noc_unicast_write,4096,2,2,False,False,True,11.062490297247494,2710568.920226439
+unicast_RingAsLinear,noc_unicast_write,16,2,2,False,False,True,0.04265565525144902,2665978.4532155637
+unicast_RingAsLinear,noc_unicast_write,2048,2,2,False,False,True,5.485258788844949,2678349.017990698
+unicast_RingAsLinear,noc_unicast_write,4096,2,2,False,False,True,10.908847338857802,2663292.8073383304
+unicast_Linear,noc_unicast_write,16,4,1,False,False,True,0.04690578994077484,2931611.8712984277
+unicast_Linear,noc_unicast_write,2048,4,1,False,False,True,6.009612563095063,2934381.1343237613
+unicast_Linear,noc_unicast_write,4096,4,1,False,False,True,11.005971599434224,2687004.785018121
+unicast_RingAsLinear,noc_unicast_write,16,4,1,False,False,True,0.039312324978381136,2457020.311148821
+unicast_RingAsLinear,noc_unicast_write,2048,4,1,False,False,True,5.0336210802741945,2457822.793102634
+unicast_RingAsLinear,noc_unicast_write,4096,4,1,False,False,True,10.083781179634252,2461860.6395591437
+unicast_Linear,noc_unicast_write,16,4,1,True,False,True,0.046475010597391675,2904688.1623369795
+unicast_Linear,noc_unicast_write,2048,4,1,True,False,True,5.857022613726942,2859874.3231088584
+unicast_Linear,noc_unicast_write,4096,4,1,True,False,True,10.104887673232307,2467013.5920977313
+unicast_RingAsLinear,noc_unicast_write,16,4,1,True,False,True,0.03838017525779304,2398760.953612065
+unicast_RingAsLinear,noc_unicast_write,2048,4,1,True,False,True,4.914636271561107,2399724.7419731966
+unicast_RingAsLinear,noc_unicast_write,4096,4,1,True,False,True,9.556538914180962,2333139.3833449613
+unicast_Linear,noc_unicast_write,16,4,1,True,True,True,0.06812029703855524,4257518.564909702
+unicast_Linear,noc_unicast_write,2048,4,1,True,True,True,7.357558537901513,3592557.879834723
+unicast_Linear,noc_unicast_write,4096,4,1,True,True,True,11.77752598459801,2875372.5548334983
+unicast_RingAsLinear,noc_unicast_write,16,4,1,True,True,True,0.051546303738592196,3221643.983662012
+unicast_RingAsLinear,noc_unicast_write,2048,4,1,True,True,True,6.329059549342529,3090361.1080774064
+unicast_RingAsLinear,noc_unicast_write,4096,4,1,True,True,True,11.55555273437569,2821179.86679094
+unicast_Linear,noc_unicast_flush_atomic_inc,16,4,1,True,False,True,0.02077203338590259,1298252.086618912
+unicast_Linear,noc_unicast_no_flush_atomic_inc,16,4,1,True,False,True,0.022888395817211468,1430524.7385757167
+unicast_RingAsLinear,noc_unicast_flush_atomic_inc,16,4,1,True,False,True,0.01874435633789917,1171522.2711186982
+unicast_RingAsLinear,noc_unicast_no_flush_atomic_inc,16,4,1,True,False,True,0.01919068514847989,1199417.8217799931
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.024261569193714815,1516348.074607176
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.02798938665199533,1749336.6657497082
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.024094242823736834,1505890.1764835522
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.02563360606769244,1602100.3792307775
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,2.7606335462577545,1347965.598758669
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,3.192968235812553,1559066.521392848
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.765768173743828,1350472.7410858537
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.979529179687316,1454848.2322691972
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,4.4632230340272105,1089654.0610417994
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,5.651815861617689,1379837.8568402561
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,4.449998637622487,1086425.4486383025
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,5.314354065790626,1297449.7230934147
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.013151261893580006,821953.8683487504
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.015374666423189605,960916.6514493503
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.02870939181010589,1794336.9881316181
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.03075239678895005,1922024.799309378
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.483643685017534,724435.3930749678
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.743744917500814,851437.9479984443
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,3.2331160499379403,1578669.9462587598
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,3.4844507406533523,1701391.9632096447
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,2.4188982996591806,590551.3426902296
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,3.045580672652851,743549.9689093875
+mcast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,5.153999387761244,1258300.6317776474
+unicast_Linear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,6.091231886587686,1487117.159811447
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.037965982681211916,2372873.917575745
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.04680356137951771,2925222.586219857
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.03754600705841466,2346625.4411509163
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.04195715770360819,2622322.3564755116
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,4.867292977711274,2376607.899273083
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,5.991642682420396,2925606.778525584
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,4.821474376023702,2354235.5351678235
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,5.4368781827755,2654725.6751833493
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,9.752589065665479,2381003.1898597362
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,10.993225512004749,2683892.9472667845
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,False,True,9.572333786624782,2336995.553375191
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,False,True,10.153200542393096,2478808.726170189
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,True,True,0.021626279600775906,1351642.475048494
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,True,True,0.025859149222353967,1616196.8263971228
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,True,True,0.05031679137368929,3144799.4608555804
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,True,True,0.05748638241914087,3592898.9011963042
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,True,True,2.784244325705306,1359494.299660794
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,True,True,3.270870748675563,1597104.8577517397
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,True,True,6.412534802208762,3131120.5088909967
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,True,True,7.121473473198075,3477281.9693349977
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,True,True,5.399000469176568,1318115.3489200606
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,True,True,5.812313610589491,1419021.877585325
+mcast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,True,True,11.670942949362113,2849351.3059966094
+unicast_Linear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,True,True,11.77757525805175,2875384.5844852906
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.020893318172227584,1305832.385764224
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,False,True,0.027584206684874744,1724012.9178046715
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.020660972265969857,1291310.766623116
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,False,True,0.02512058706058343,1570036.6912864645
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,2.399762221997492,1171758.8974597128
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,False,True,3.159246972813176,1542601.0609439334
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.3673633394286195,1155939.1305803806
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,False,True,2.90963985563299,1420722.5857582958
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,3.968014941746264,968753.6478872715
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,False,True,5.633693733049726,1375413.5090453431
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,3.99540442652183,975440.5338188062
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,False,True,5.197951600979246,1269031.1525828238
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.011356871523737043,709804.4702335652
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,False,True,True,0.015039798644167792,939987.415260487
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.024502669320795808,1531416.832549738
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,16,4,1,True,True,True,0.030073865172095815,1879616.5732559885
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.3050286385603516,637221.0149220467
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,False,True,True,1.712484855368843,836174.2457855679
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,2.7745439333248916,1354757.7799437947
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,2048,4,1,True,True,True,3.4248566917799135,1672293.3065331609
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,2.1721899479454776,530319.8115101263
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,False,True,True,3.05370128669411,745532.5406968042
+mcast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,4.709589585598084,1149802.1449214073
+unicast_RingAsLinear,noc_fused_unicast_write_flush_atomic_inc,4096,4,1,True,True,True,6.108927182900445,1491437.300512804
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.0336302935546812,2101893.3471675753
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,False,True,0.03931284319689805,2457052.6998061277
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.032932541558191676,2058283.8473869797
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,False,True,0.038382440187144144,2398902.511696509
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,4.300677680125422,2099940.2734987414
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,False,True,5.033508438023745,2457767.7920037815
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,4.211410374293313,2056352.7218229065
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,False,True,4.9146961846647175,2399753.996418319
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,8.604185349045894,2100631.188731908
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,False,True,10.068244124464563,2458067.4131993563
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,False,True,8.404106969714059,2051783.9281528464
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,False,True,9.548405415130096,2331153.665803246
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,True,True,0.018500967652642204,1156310.4782901378
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,False,True,True,0.022034792744939823,1377174.546558739
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,True,True,0.042029294102769926,2626830.8814231204
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,16,4,1,True,True,True,0.05153925778282655,3221203.6114266594
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,True,True,2.3815783011284464,1162880.0298478743
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,False,True,True,2.832280253135324,1382949.3423512324
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,True,True,5.369875264588608,2622009.4065374066
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,2048,4,1,True,True,True,6.328020252754662,3089853.6390403626
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,True,True,4.767728761862104,1163996.2797514903
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,False,True,True,5.3902877595118905,1315988.2225370826
+mcast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,True,True,10.691903875555427,2610328.0946180243
+unicast_RingAsLinear,noc_fused_unicast_write_no_flush_atomic_inc,4096,4,1,True,True,True,11.548417678580554,2819437.909809706
diff --git a/tests/tt_metal/microbenchmarks/ethernet/test_fabric_edm_bandwidth.py b/tests/tt_metal/microbenchmarks/ethernet/test_fabric_edm_bandwidth.py
index 5d19af18a0..5d9d7a0e08 100644
--- a/tests/tt_metal/microbenchmarks/ethernet/test_fabric_edm_bandwidth.py
+++ b/tests/tt_metal/microbenchmarks/ethernet/test_fabric_edm_bandwidth.py
@@ -17,8 +17,6 @@ from tabulate import tabulate
 import pandas as pd
 from models.utility_functions import enable_persistent_kernel_cache, disable_persistent_kernel_cache
 
-from conftest import is_6u
-
 from tt_metal.tools.profiler.common import PROFILER_LOGS_DIR, PROFILER_DEVICE_SIDE_LOG
 
 profiler_log_path = PROFILER_LOGS_DIR / PROFILER_DEVICE_SIDE_LOG
@@ -650,14 +648,6 @@ def initialize_daemon_mode(request):
         stop_fabric_edm_daemon()
 
 
-# restart daemon since we want to re-program the device init side fabric for different topology
-@pytest.fixture(scope="module")
-def restart_fabric_edm_daemon():
-    stop_fabric_edm_daemon()
-    start_fabric_edm_daemon()
-    yield
-
-
 @pytest.mark.ubench_quick_tests
 @pytest.mark.parametrize("num_messages", [200000])
 @pytest.mark.parametrize("num_op_invocations", [1])
@@ -957,44 +947,6 @@ def test_fabric_t3k_4chip_cols_mcast_bw(
     )
 
 
-@pytest.mark.ubench_quick_tests
-@pytest.mark.parametrize("num_messages", [200000])
-@pytest.mark.parametrize("num_op_invocations", [1])
-@pytest.mark.parametrize("line_sync", [True])
-@pytest.mark.parametrize("line_size", [8])
-@pytest.mark.parametrize("num_links", [1])
-@pytest.mark.parametrize("packet_size", [16, 2048, 4096])
-@pytest.mark.parametrize("fabric_test_mode", [FabricTestMode.HalfRing, FabricTestMode.FullRing])
-def test_fabric_t3k_8chip_ring_mcast_bw(
-    num_messages,
-    num_links,
-    num_op_invocations,
-    line_sync,
-    line_size,
-    packet_size,
-    fabric_test_mode,
-):
-    if is_6u():
-        pytest.skip("Skip test for 6U as this is already covered by later tests")
-    run_fabric_edm(
-        is_unicast=True,
-        num_messages=num_messages,
-        num_links=num_links,
-        noc_message_type="noc_unicast_write",
-        num_op_invocations=num_op_invocations,
-        line_sync=line_sync,
-        line_size=line_size,
-        packet_size=packet_size,
-        fabric_mode=fabric_test_mode,
-        disable_sends_for_interior_workers=False,
-        unidirectional=False,
-        senders_are_unidirectional=True,
-        test_mode="1D_fabric_on_mesh",
-        num_cluster_rows=0,
-        num_cluster_cols=0,
-    )
-
-
 # expected_Mpps = expected millions of packets per second
 @pytest.mark.ubench_quick_tests
 @pytest.mark.parametrize("num_messages", [200000])
@@ -1081,13 +1033,11 @@ def test_fabric_t3k_all_rows_and_cols_mcast_bw(
 @pytest.mark.parametrize("num_op_invocations", [1])
 @pytest.mark.parametrize("line_sync", [True])
 @pytest.mark.parametrize("line_size", [8])
-@pytest.mark.parametrize("num_links", [1, 2, 3, 4])
+@pytest.mark.parametrize("num_links", [4])
 @pytest.mark.parametrize("packet_size", [2048, 4096])
-@pytest.mark.parametrize("fabric_test_mode", [FabricTestMode.HalfRing, FabricTestMode.FullRing, FabricTestMode.Linear])
+@pytest.mark.parametrize("fabric_test_mode", [FabricTestMode.FullRing, FabricTestMode.Linear])
 @pytest.mark.parametrize("num_cluster_cols", [4])
 def test_fabric_6u_4chip_cols_mcast_bw(
-    # restart since previous test is using linear topology
-    restart_fabric_edm_daemon,
     is_unicast,
     num_messages,
     num_links,
@@ -1098,9 +1048,9 @@ def test_fabric_6u_4chip_cols_mcast_bw(
     fabric_test_mode,
     num_cluster_cols,
 ):
-    if not is_6u():
-        pytest.skip("Skip test for T3K since the mesh shape is not supported")
-    is_ring = fabric_test_mode == FabricTestMode.FullRing or fabric_test_mode == FabricTestMode.HalfRing
+    is_ring = fabric_test_mode == FabricTestMode.FullRing
+    if is_ring:
+        pytest.skip("Baseline numbers not yet available for full-6u ring fabric test mode")
     update_machine_type_suffix("6u")
     run_fabric_edm(
         is_unicast=is_unicast,
@@ -1127,9 +1077,9 @@ def test_fabric_6u_4chip_cols_mcast_bw(
 @pytest.mark.parametrize("num_op_invocations", [1])
 @pytest.mark.parametrize("line_sync", [True])
 @pytest.mark.parametrize("line_size", [4])
-@pytest.mark.parametrize("num_links", [1, 2, 3, 4])
+@pytest.mark.parametrize("num_links", [4])
 @pytest.mark.parametrize("packet_size", [2048, 4096])
-@pytest.mark.parametrize("fabric_test_mode", [FabricTestMode.HalfRing, FabricTestMode.FullRing, FabricTestMode.Linear])
+@pytest.mark.parametrize("fabric_test_mode", [FabricTestMode.FullRing, FabricTestMode.Linear])
 @pytest.mark.parametrize("num_cluster_rows", [8])
 def test_fabric_6u_4chip_rows_mcast_bw(
     is_unicast,
@@ -1142,9 +1092,9 @@ def test_fabric_6u_4chip_rows_mcast_bw(
     fabric_test_mode,
     num_cluster_rows,
 ):
-    if not is_6u():
-        pytest.skip("Skip test for T3K since the mesh shape is not supported")
-    is_ring = fabric_test_mode == FabricTestMode.FullRing or fabric_test_mode == FabricTestMode.HalfRing
+    is_ring = fabric_test_mode == FabricTestMode.FullRing
+    if is_ring:
+        pytest.skip("Baseline numbers not yet available for full-6u ring fabric test mode")
     update_machine_type_suffix("6u")
     run_fabric_edm(
         is_unicast=is_unicast,
@@ -1187,8 +1137,6 @@ def test_fabric_6u_all_rows_and_cols_mcast_bw(
     num_cluster_rows,
     num_cluster_cols,
 ):
-    if not is_6u():
-        pytest.skip("Skip test for T3K since the mesh shape is not supported")
     is_ring = fabric_test_mode == FabricTestMode.FullRing
     update_machine_type_suffix("6u")
     run_fabric_edm(
diff --git a/tests/tt_metal/tt_fabric/common/fabric_fixture.hpp b/tests/tt_metal/tt_fabric/common/fabric_fixture.hpp
index 4c4b62baf2..22e2b30c21 100644
--- a/tests/tt_metal/tt_fabric/common/fabric_fixture.hpp
+++ b/tests/tt_metal/tt_fabric/common/fabric_fixture.hpp
@@ -26,10 +26,8 @@ class ControlPlaneFixture : public ::testing::Test {
                    "Control plane test suite can only be run with slow dispatch or TT_METAL_SLOW_DISPATCH_MODE set");
                GTEST_SKIP();
            }
-           // reserve max available planes
-           uint8_t num_routing_planes = std::numeric_limits<uint8_t>::max();
            tt::tt_metal::MetalContext::instance().get_cluster().configure_ethernet_cores_for_fabric_routers(
-               tt::tt_metal::FabricConfig::FABRIC_2D, num_routing_planes);
+               tt::tt_metal::FabricConfig::FABRIC_2D);
        }
 
        void TearDown() override {
@@ -46,7 +44,7 @@ public:
     bool slow_dispatch_;
 
     const std::vector<tt::tt_metal::IDevice*>& get_devices() const { return devices_; }
-    void SetUpDevices(tt_metal::FabricConfig fabric_config, std::optional<uint8_t> num_routing_planes = std::nullopt) {
+    void SetUpDevices(tt_metal::FabricConfig fabric_config) {
         slow_dispatch_ = getenv("TT_METAL_SLOW_DISPATCH_MODE");
         if (slow_dispatch_) {
             log_info(tt::LogTest, "Running fabric api tests with slow dispatch");
@@ -60,7 +58,7 @@ public:
         for (unsigned int id = 0; id < num_devices; id++) {
             ids.push_back(id);
         }
-        tt::tt_metal::detail::SetFabricConfig(fabric_config, num_routing_planes);
+        tt::tt_metal::detail::InitializeFabricConfig(fabric_config);
         devices_map_ = tt::tt_metal::detail::CreateDevices(ids);
         for (auto& [id, device] : devices_map_) {
             devices_.push_back(device);
@@ -89,7 +87,7 @@ public:
 
     void TearDown() override {
         tt::tt_metal::detail::CloseDevices(devices_map_);
-        tt::tt_metal::detail::SetFabricConfig(tt::tt_metal::FabricConfig::DISABLED);
+        tt::tt_metal::detail::InitializeFabricConfig(tt::tt_metal::FabricConfig::DISABLED);
     }
 };
 
diff --git a/tests/tt_metal/tt_fabric/fabric_data_movement/kernels/fabric_mux_receiver_client.cpp b/tests/tt_metal/tt_fabric/fabric_data_movement/kernels/fabric_mux_receiver_client.cpp
index b112bd7d4a..8cb4252cb6 100644
--- a/tests/tt_metal/tt_fabric/fabric_data_movement/kernels/fabric_mux_receiver_client.cpp
+++ b/tests/tt_metal/tt_fabric/fabric_data_movement/kernels/fabric_mux_receiver_client.cpp
@@ -104,9 +104,7 @@ void kernel_main() {
             poll_ptr = base_poll_ptr + ptr_offset;
 
             // poll on the last word in the payload -> this ensures that the entire payload is written
-            while (*poll_ptr != expected_val) {
-                invalidate_l1_cache();
-            }
+            while (*poll_ptr != expected_val);
 
             // check for data correctness
             match = check_packet_data(
diff --git a/tests/tt_metal/tt_fabric/fabric_data_movement/kernels/fabric_mux_sender_client.cpp b/tests/tt_metal/tt_fabric/fabric_data_movement/kernels/fabric_mux_sender_client.cpp
index e6b7a73081..e5035e8627 100644
--- a/tests/tt_metal/tt_fabric/fabric_data_movement/kernels/fabric_mux_sender_client.cpp
+++ b/tests/tt_metal/tt_fabric/fabric_data_movement/kernels/fabric_mux_sender_client.cpp
@@ -87,9 +87,7 @@ void kernel_main() {
         uint32_t dest_payload_slot_id = 0;
         for (uint32_t packet_id = 0; packet_id < num_packets; packet_id++) {
             // wait until we have atleast 1 credit
-            while (credit_handshake_ptr[0] == 0) {
-                invalidate_l1_cache();
-            }
+            while (credit_handshake_ptr[0] == 0);
 
             seed = prng_next(seed);
             fill_packet_data(payload_start_ptr, packet_payload_size_bytes / 16, seed);
@@ -111,9 +109,7 @@ void kernel_main() {
         }
         noc_async_write_barrier();
         // wait for all credits to be returned before disconnecting
-        while (credit_handshake_ptr[0] != num_credits) {
-            invalidate_l1_cache();
-        }
+        while (credit_handshake_ptr[0] != num_credits);
         tt::tt_fabric::fabric_client_disconnect(mux_connection_handle);
     }
 
diff --git a/tests/tt_metal/tt_metal/api/test_host_buffer.cpp b/tests/tt_metal/tt_metal/api/test_host_buffer.cpp
index c460c8d72d..e29f10939e 100644
--- a/tests/tt_metal/tt_metal/api/test_host_buffer.cpp
+++ b/tests/tt_metal/tt_metal/api/test_host_buffer.cpp
@@ -22,12 +22,15 @@ using ::testing::Pointwise;
 
 TEST(HostBufferTest, Empty) {
     HostBuffer buffer;
+
+    EXPECT_FALSE(buffer.is_borrowed());
     EXPECT_TRUE(buffer.view_bytes().empty());
 }
 
 TEST(HostBufferTest, BasicOwned) {
     HostBuffer buffer(std::vector<int>{1, 2, 3});
 
+    EXPECT_FALSE(buffer.is_borrowed());
     EXPECT_THAT(buffer.view_as<int>(), Pointwise(Eq(), {1, 2, 3}));
 
     auto writable_view = buffer.view_as<int>();
@@ -46,6 +49,7 @@ TEST(HostBufferTest, BasicBorrowed) {
 
     EXPECT_EQ(num_increments, 1);
     EXPECT_EQ(num_decrements, 0);
+    EXPECT_TRUE(buffer.is_borrowed());
     EXPECT_THAT(buffer.view_as<int>(), Pointwise(Eq(), {1, 2, 3}));
 
     auto writable_view = buffer.view_as<int>();
@@ -58,18 +62,39 @@ TEST(HostBufferTest, IncorrectCast) {
     EXPECT_ANY_THROW({ buffer.view_as<float>(); });
 }
 
-TEST(HostBufferTest, ShallowCopy) {
+TEST(HostBufferTest, MakePinFromOwned) {
     HostBuffer buffer(std::vector<int>{1, 2, 3});
-    HostBuffer copy = buffer;
+    HostBuffer borrowed(buffer.view_as<int>(), buffer.pin());
 
+    EXPECT_FALSE(buffer.is_borrowed());
+    EXPECT_TRUE(borrowed.is_borrowed());
     EXPECT_THAT(buffer.view_as<int>(), Pointwise(Eq(), {1, 2, 3}));
-    EXPECT_THAT(copy.view_as<int>(), Pointwise(Eq(), {1, 2, 3}));
+    EXPECT_THAT(borrowed.view_as<int>(), Pointwise(Eq(), {1, 2, 3}));
 
     auto writable_view = buffer.view_as<int>();
     writable_view[1] = 5;
     EXPECT_THAT(buffer.view_as<int>(), Pointwise(Eq(), {1, 5, 3}));
-    EXPECT_THAT(copy.view_as<int>(), Pointwise(Eq(), {1, 5, 3}));
+    EXPECT_THAT(borrowed.view_as<int>(), Pointwise(Eq(), {1, 5, 3}));
 }
 
+TEST(HostBufferTest, MakePinFromBorrowed) {
+    int num_increments = 0;
+    int num_decrements = 0;
+    std::vector<int> vec = {1, 2, 3};
+    HostBuffer buffer(
+        tt::stl::Span<int>(vec.data(), vec.size()),
+        MemoryPin([&]() { num_increments++; }, [&]() { num_decrements++; }));
+
+    EXPECT_EQ(num_increments, 1);
+    EXPECT_EQ(num_decrements, 0);
+    {
+        HostBuffer borrowed(buffer.view_bytes(), buffer.pin());
+        EXPECT_EQ(num_increments, 2);
+        EXPECT_EQ(num_decrements, 0);
+    }
+
+    EXPECT_EQ(num_increments, 2);
+    EXPECT_EQ(num_decrements, 1);
+}
 }  // namespace
 }  // namespace tt::tt_metal
diff --git a/tests/tt_metal/tt_metal/common/multi_device_fixture.hpp b/tests/tt_metal/tt_metal/common/multi_device_fixture.hpp
index 7318b1cf06..c45a15cd27 100644
--- a/tests/tt_metal/tt_metal/common/multi_device_fixture.hpp
+++ b/tests/tt_metal/tt_metal/common/multi_device_fixture.hpp
@@ -159,7 +159,7 @@ protected:
             (config_.num_cqs >= 2 and is_n300_or_t3k_cluster) ? DispatchCoreType::ETH : DispatchCoreType::WORKER;
 
         if (config_.fabric_config != FabricConfig::DISABLED) {
-            tt::tt_metal::detail::SetFabricConfig(config_.fabric_config);
+            tt::tt_metal::detail::InitializeFabricConfig(config_.fabric_config);
         }
         mesh_device_ = MeshDevice::create(
             MeshDeviceConfig(get_mesh_shape(*mesh_device_type)),
@@ -178,7 +178,7 @@ protected:
         mesh_device_->close();
         mesh_device_.reset();
         if (config_.fabric_config != FabricConfig::DISABLED) {
-            tt::tt_metal::detail::SetFabricConfig(tt::tt_metal::FabricConfig::DISABLED);
+            tt::tt_metal::detail::InitializeFabricConfig(tt::tt_metal::FabricConfig::DISABLED);
         }
     }
 
diff --git a/tests/tt_metal/tt_metal/data_movement/CMakeLists.txt b/tests/tt_metal/tt_metal/data_movement/CMakeLists.txt
index 36a18cc804..af92846a9d 100644
--- a/tests/tt_metal/tt_metal/data_movement/CMakeLists.txt
+++ b/tests/tt_metal/tt_metal/data_movement/CMakeLists.txt
@@ -8,8 +8,6 @@ set(UNIT_TESTS_DATA_MOVEMENT_SRC
     ${CMAKE_CURRENT_SOURCE_DIR}/one_to_one/test_one_to_one.cpp
     ${CMAKE_CURRENT_SOURCE_DIR}/one_from_one/test_one_from_one.cpp
     ${CMAKE_CURRENT_SOURCE_DIR}/one_from_all/test_one_from_all.cpp
-    ${CMAKE_CURRENT_SOURCE_DIR}/all_to_all/test_all_to_all.cpp
-    ${CMAKE_CURRENT_SOURCE_DIR}/all_from_all/test_all_from_all.cpp
 )
 
 add_executable(unit_tests_data_movement ${UNIT_TESTS_DATA_MOVEMENT_SRC})
diff --git a/tests/tt_metal/tt_metal/data_movement/all_from_all/README.md b/tests/tt_metal/tt_metal/data_movement/all_from_all/README.md
deleted file mode 100644
index a8f5938ae6..0000000000
--- a/tests/tt_metal/tt_metal/data_movement/all_from_all/README.md
+++ /dev/null
@@ -1,48 +0,0 @@
-# All from All Core Data Movement Tests
-
-This test suite implements tests that measure the functionality and performance (i.e. bandwidth) of data movement transactions between Tensix cores.
-
-## Test Flow
-
-L1 space is allocated on all Tensix cores involved in the data movement test. Based on a given number of reservable pages, subsets of these pages are designated as either requestor pages (masters) or responder pages (subordinates).
-
-The subsets are determined as follows:
-- num_responder_pages = num_reservable_pages / (1 + num_subordinate_cores)
-- num_requestor_pages = num_responder_pages * num_subordinate_cores
-
-The starting address of the requestor space in L1 immediately follows the final address allocated for the responder space.
-
-Each subordinate core contains a portion of the entire data that is to be received buy each master core.
-Each master core issues a NOC asynchronous read to each subordinate (responder) core, requesting and receiving data from all the subordinate cores and storing it its requestor pages.
-By the end of the test, every master core should have received the same data from every subordinate core.
-
-Test attributes such as pages per transaction and number of transactions per subordinate core, and latency measures such as kernel and pre-determined scope cycles are recorded by the profiler.
-
-A pcc check is performed by cross-checking the data of all the subordinate cores pieced-together against the data received by each master core. This ensures that the data integrity is maintained throughout the data movement process.
-
-Test expectations are that pcc checks pass and sufficient test attribute data is captured by the profiler for higher level bandwidth/regression checks.
-
-## Test Parameters
-| Parameter                             | Data Type             | Description                                                               |
-| ------------------------------------- | --------------------- | ------------------------------------------------------------------------- |
-| test_id                               | uint32_t              | Test ID for identifying different test cases.                             |
-| mst_logical_start_coord               | CoreCoord             | Logical starting coordinates for the master core range.                   |
-| sub_logical_start_coord               | CoreCoord             | Logical starting coordinates for the subordinate core range.              |
-| mst_grid_size                         | CoreCoord             | Grid size of the master core range.                                       |
-| sub_grid_size                         | CoreCoord             | Grid size of the subordinate core range.                                  |
-| num_of_transactions_per_subordinate   | uint32_t              | Number of transactions performed per subordinate core.                    |
-| pages_reservable_per_transaction      | uint32_t              | Number of reservable pages per transaction in L1.                         |
-| bytes_per_page                        | uint32_t              | Size of each page in bytes.                                               |
-| l1_data_format                        | DataFormat            | Data format used for L1 data movement.                                    |
-| noc_id                                | NOC                   | Specifies which NOC to use for the test.                                  |
-
-## Test Cases
-Each test case uses bfloat16 as L1 data format and flit size (32B for WH, 64B for BH) as page size. Each test case has multiple runs, and each run has a unique runtime host id, assigned by a global counter.
-
-1. **All from All Packet Sizes:** Tests different number of transactions and transaction sizes by varying the num_of_transactions and transaction_size_pages parameters.
-a. **2x2 Packet Sizes**
-2. **All from All Directed Ideal:** Tests the most optimal data movement setup between two ranges of cores.
-a. **2x2 from 1x1**
-b. **4x4 from 1x1**
-c. **1x1 from 2x2**
-d. **1x1 from 4x4**
diff --git a/tests/tt_metal/tt_metal/data_movement/all_from_all/kernels/requestor.cpp b/tests/tt_metal/tt_metal/data_movement/all_from_all/kernels/requestor.cpp
deleted file mode 100644
index 828c424380..0000000000
--- a/tests/tt_metal/tt_metal/data_movement/all_from_all/kernels/requestor.cpp
+++ /dev/null
@@ -1,99 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include "dataflow_api.h"
-#include "debug/dprint.h"
-#include "ckernel.h"
-
-void kernel_main() {
-    // Compile-time arguments
-    const uint32_t test_id = get_compile_time_arg_val(0);
-    const uint32_t mst_l1_base_address = get_compile_time_arg_val(1);
-    const uint32_t sub_l1_base_address = get_compile_time_arg_val(2);
-    const uint32_t total_size_bytes_per_subordinate = get_compile_time_arg_val(3);
-    const uint32_t num_of_transactions = get_compile_time_arg_val(4);
-    const uint32_t bytes_per_transaction_per_subordinate = get_compile_time_arg_val(5);
-    const uint32_t num_subordinates = get_compile_time_arg_val(6);
-
-    // Runtime arguments
-    const uint32_t master_index = get_arg_val<uint32_t>(0);
-
-    // Derivative values
-    uint32_t master_l1_local_address = mst_l1_base_address;
-    uint32_t subordinate_l1_local_address = sub_l1_base_address;
-
-    uint32_t subordinate_x_coord;
-    uint32_t subordinate_y_coord;
-
-    uint64_t subordinate_l1_noc_base_address;
-    uint64_t subordinate_l1_noc_address;
-
-    {
-        DeviceZoneScopedN("RISCV0");
-
-        // Loop Order 1: Number of transactions -> Number of subordinates
-
-        for (uint32_t i = 0; i < num_of_transactions; i++) {
-            master_l1_local_address = mst_l1_base_address + (i * bytes_per_transaction_per_subordinate);
-
-            for (uint32_t j = 0; j < num_subordinates; j++) {
-                // Subordinate coordiantes are stored in the runtime arguments starting at index 1
-                // Each x coordinate is stores in an odd index
-                // Each y coordinate is stored in the next even index
-                // The first subordinate's coordinates are at indices 1 and 2, the second at indices 3 and 4, etc.
-                subordinate_x_coord = get_arg_val<uint32_t>(1 + (j * 2));
-                subordinate_y_coord = get_arg_val<uint32_t>(1 + (j * 2) + 1);
-
-                subordinate_l1_noc_address =
-                    get_noc_addr(subordinate_x_coord, subordinate_y_coord, subordinate_l1_local_address);
-
-                noc_async_read(
-                    subordinate_l1_noc_address, master_l1_local_address, bytes_per_transaction_per_subordinate);
-
-                master_l1_local_address += total_size_bytes_per_subordinate;
-            }
-
-            subordinate_l1_local_address += bytes_per_transaction_per_subordinate;
-        }
-
-        /*
-
-        // Loop Order 2: Number of subordinates -> Number of transactions
-
-        for (uint32_t i = 0; i < num_subordinates; i++) {
-
-            subordinate_x_coord = get_arg_val<uint32_t>(1 + (i * 2));
-            subordinate_y_coord = get_arg_val<uint32_t>(1 + (i * 2) + 1);
-
-            subordinate_l1_noc_base_address =
-                get_noc_addr(subordinate_x_coord, subordinate_y_coord, 0);
-
-            subordinate_l1_local_address = sub_l1_base_address;
-
-            for (uint32_t j = 0; j < num_of_transactions; j++) {
-
-                subordinate_l1_noc_address =
-                    subordinate_l1_noc_base_address | subordinate_l1_local_address;
-
-                noc_async_read(subordinate_l1_noc_address, master_l1_local_address,
-                                bytes_per_transaction_per_subordinate);
-
-                master_l1_local_address += bytes_per_transaction_per_subordinate;
-                subordinate_l1_local_address += bytes_per_transaction_per_subordinate;
-
-                //noc_async_read_barrier(); // 27.7 B/cycle
-            }
-            //noc_async_read_barrier(); // 27.7 B/cycle
-        }
-
-        */
-
-        noc_async_read_barrier();  // 30.63 B/cycle
-    }
-
-    DeviceTimestampedData("Test id", test_id);
-    DeviceTimestampedData("Number of transactions", num_of_transactions);
-    DeviceTimestampedData("Transaction size in bytes", bytes_per_transaction_per_subordinate * num_subordinates);
-    DeviceTimestampedData("Total bytes", num_of_transactions * bytes_per_transaction_per_subordinate * num_subordinates)
-}
diff --git a/tests/tt_metal/tt_metal/data_movement/all_from_all/test_all_from_all.cpp b/tests/tt_metal/tt_metal/data_movement/all_from_all/test_all_from_all.cpp
deleted file mode 100644
index 6b2c4a7295..0000000000
--- a/tests/tt_metal/tt_metal/data_movement/all_from_all/test_all_from_all.cpp
+++ /dev/null
@@ -1,476 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include "device_fixture.hpp"
-#include "tt_metal/test_utils/comparison.hpp"
-#include "tt_metal/test_utils/stimulus.hpp"
-#include "tt_metal/test_utils/print_helpers.hpp"
-#include "dm_common.hpp"
-
-namespace tt::tt_metal {
-
-using namespace std;
-using namespace tt;
-using namespace tt::test_utils;
-
-namespace unit_tests::dm::all_from_all {
-
-constexpr uint32_t START_ID = 70;
-
-// Test Config (i.e. test parameters)
-struct AllFromAllConfig {
-    /* Test ID */
-    uint32_t test_id = START_ID;
-
-    /* Grid configurations */
-    CoreCoord mst_logical_start_coord = CoreCoord();
-    CoreCoord sub_logical_start_coord = CoreCoord();
-    CoreCoord mst_grid_size = CoreCoord();
-    CoreCoord sub_grid_size = CoreCoord();
-
-    /* Transaction size configurations */
-    uint32_t num_of_transactions_per_subordinate = 1;
-    uint32_t pages_reservable_per_transaction = 1;
-    uint32_t bytes_per_page = 32;
-
-    /* Write configurations */
-    DataFormat l1_data_format = DataFormat::Invalid;
-    NOC noc_id = NOC::NOC_0;
-
-    // TODO: Add the following parameters
-    //  1. Virtual Channel (only useful for unicast)
-    //  2. Posted flag (posted multicast has much better performance at larger grid sizes, than non-posted due to
-    //  response packets) (60, 45, 23, vs 60, 60, 60 at posted)
-};
-
-/// @brief Performs communication from L1 Sender Cores to L1 Receiver Cores.
-/// @param device The device on which the test is executed.
-/// @param test_config Configuration of the test, defined by a specific struct.
-/// @return Status of the test execution (e.g., success or failure).
-bool run_dm(IDevice* device, const AllFromAllConfig& test_config) {
-    /* ================ SETUP ================ */
-
-    // Program
-    Program program = CreateProgram();
-
-    // Initialize core sets //
-    /*
-        - CoreRange: Represents a single rectangular range of cores in a 2D grid. It is defined by a starting coordinate
-        (`start_coord`) and an ending coordinate (`end_coord`). For example, a CoreRange from (0, 0) to (3, 3) would
-        include all cores in that rectangular area.
-
-        - CoreRangeSet: Represents a collection of CoreRange objects. It is used to manage multiple ranges of cores
-        and provides functionality for operations like merging, subtracting, and finding intersections between ranges.
-    */
-
-    // Logical Cores
-
-    // Master
-
-    CoreCoord mst_logical_start_coord = test_config.mst_logical_start_coord;
-    CoreCoord mst_logical_end_coord = CoreCoord(
-        mst_logical_start_coord.x + test_config.mst_grid_size.x - 1,
-        mst_logical_start_coord.y + test_config.mst_grid_size.y - 1);
-
-    CoreRangeSet mst_logical_core_set({CoreRange(mst_logical_start_coord, mst_logical_end_coord)});
-    uint32_t num_masters = mst_logical_core_set.num_cores();
-
-    // Subordinate
-
-    CoreCoord sub_logical_start_coord = test_config.sub_logical_start_coord;
-    CoreCoord sub_logical_end_coord = CoreCoord(
-        sub_logical_start_coord.x + test_config.sub_grid_size.x - 1,
-        sub_logical_start_coord.y + test_config.sub_grid_size.y - 1);
-
-    CoreRangeSet sub_logical_core_set({CoreRange(sub_logical_start_coord, sub_logical_end_coord)});
-    uint32_t num_subordinates = sub_logical_core_set.num_cores();
-
-    // Subordinate Worker Coordinates
-
-    std::vector<uint32_t> sub_worker_coordinates = {};
-    for (auto& sub_logical_core : corerange_to_cores(sub_logical_core_set)) {
-        CoreCoord sub_worker_core = device->worker_core_from_logical_core(sub_logical_core);
-        sub_worker_coordinates.push_back(sub_worker_core.x);
-        sub_worker_coordinates.push_back(sub_worker_core.y);
-    }
-
-    // Transaction Configurations
-
-    // Determine pages per transaction for the master and subordinate cores
-
-    // Subordinate Send Data Block
-    const size_t pages_requested_per_transaction_per_subordinate =
-        test_config.pages_reservable_per_transaction / (num_subordinates + 1);
-    if (pages_requested_per_transaction_per_subordinate == 0) {
-        log_warning(
-            tt::LogTest,
-            "Pages requested per transaction per subordinate is 0. Skipping the current set of configurations.");
-        return 1;
-    }
-    const size_t bytes_requested_per_transaction_per_subordinate =
-        pages_requested_per_transaction_per_subordinate * test_config.bytes_per_page;
-    const size_t total_size_bytes_per_subordinate =
-        bytes_requested_per_transaction_per_subordinate * test_config.num_of_transactions_per_subordinate;
-
-    // Master Receive Data Block
-    const size_t pages_received_per_transaction = pages_requested_per_transaction_per_subordinate * num_subordinates;
-    const size_t bytes_received_per_transaction = pages_received_per_transaction * test_config.bytes_per_page;
-    const size_t total_size_bytes_received =
-        bytes_received_per_transaction * test_config.num_of_transactions_per_subordinate;
-
-    // Obtain L1 Address for Storing Data
-
-    L1AddressInfo core_l1_info = tt::tt_metal::unit_tests::dm::get_l1_address_and_size(device, {0, 0});
-    uint32_t sub_l1_base_address = core_l1_info.base_address;
-    uint32_t mst_l1_base_address = sub_l1_base_address + total_size_bytes_per_subordinate;
-
-    // Possible To-Do: Implement checks to see that the needed space is available in all master and subordinate cores
-
-    // Kernels
-
-    // Compile-time arguments for kernels
-
-    std::vector<uint32_t> requestor_compile_args = {
-        //     0: Test ID
-        (uint32_t)test_config.test_id,  // test_id
-        // 1 - 3: L1 Addresses
-        (uint32_t)mst_l1_base_address,
-        (uint32_t)sub_l1_base_address,
-        (uint32_t)total_size_bytes_per_subordinate,  // subordinate L1 address offset
-        // 4 - 5: Transaction parameters
-        (uint32_t)test_config.num_of_transactions_per_subordinate,  // num_of_transactions
-        (uint32_t)bytes_requested_per_transaction_per_subordinate,  // transaction_size_bytes
-        //     6: Subordinate count
-        (uint32_t)num_subordinates,  // num_subordinates
-    };
-
-    // NOTE: BASE ADDRESS PER MASTER (THIS IS THE SUBORDINATE ADDRESS)
-
-    // Create kernels
-    auto requestor_kernel = CreateKernel(
-        program,
-        "tests/tt_metal/tt_metal/data_movement/all_from_all/kernels/requestor.cpp",
-        mst_logical_core_set,
-        DataMovementConfig{
-            .processor = DataMovementProcessor::RISCV_0,
-            .noc = test_config.noc_id,
-            .compile_args = requestor_compile_args});
-
-    // Run-time Arguments for kernels
-
-    // Pre-fill requestor_runtime_args with sub_worker_coordinates
-    std::vector<uint32_t> requestor_runtime_args = sub_worker_coordinates;
-
-    // Reserve space for the first element (master index)
-    requestor_runtime_args.insert(requestor_runtime_args.begin(), 0);  // Placeholder for the first element
-
-    uint32_t i = 0;  // Initialize the counter
-    for (auto& mst_logical_core : corerange_to_cores(mst_logical_core_set)) {
-        // Update the first element (subordinate address offset)
-        requestor_runtime_args[0] = i;
-
-        // Assign runtime arguments to the kernels
-        SetRuntimeArgs(program, requestor_kernel, mst_logical_core, requestor_runtime_args);
-
-        ++i;  // Increment the counter
-    }
-
-    // Assign unique id
-    log_info(tt::LogTest, "Running Test ID: {}, Run ID: {}", test_config.test_id, unit_tests::dm::runtime_host_id);
-    program.set_runtime_id(unit_tests::dm::runtime_host_id++);
-
-    /* ================ RUNNING THE PROGRAM ================ */
-
-    // Setting up Inputs and Golden Output
-
-    std::vector<uint32_t> packed_input;
-    packed_input.reserve(total_size_bytes_per_subordinate / sizeof(uint32_t));
-
-    std::vector<uint32_t> packed_golden;
-    packed_golden.reserve(total_size_bytes_received / sizeof(uint32_t));
-
-    // Generate random input data for each master core
-    for (auto& sub_logical_core : corerange_to_cores(sub_logical_core_set)) {
-        packed_input = generate_packed_uniform_random_vector<uint32_t, bfloat16>(
-            -100.0f,
-            100.0f,
-            total_size_bytes_per_subordinate / bfloat16::SIZEOF,
-            chrono::system_clock::now().time_since_epoch().count());
-
-        /*packed_input = generate_increment_vector<uint32_t>(
-            1,  // Start at 1
-            total_size_bytes_per_subordinate / sizeof(uint32_t),  // Number of elements
-            2.0f,  // Increment by 1
-            1.0f,  // Start value
-            1,     // Count (not relevant here since slide is false)
-            true  // Slide is false to ensure consistent increments
-        );*/
-
-        tt_metal::detail::WriteToDeviceL1(device, sub_logical_core, sub_l1_base_address, packed_input);
-        MetalContext::instance().get_cluster().l1_barrier(device->id());
-
-        packed_golden.insert(packed_golden.end(), packed_input.begin(), packed_input.end());
-    }
-
-    // LAUNCH PROGRAM
-    detail::LaunchProgram(device, program);
-
-    std::vector<uint32_t> packed_output;
-    // packed_output.reserve(total_size_bytes_received / sizeof(uint32_t));
-
-    bool pcc = false;
-
-    for (auto& mst_logical_core : corerange_to_cores(mst_logical_core_set)) {
-        tt_metal::detail::ReadFromDeviceL1(
-            device, mst_logical_core, mst_l1_base_address, total_size_bytes_received, packed_output);
-
-        // Results comparison
-        pcc = is_close_packed_vectors<bfloat16, uint32_t>(
-            packed_output, packed_golden, [&](const bfloat16& a, const bfloat16& b) { return is_close(a, b); });
-        if (!pcc) {
-            log_error(tt::LogTest, "PCC Check failed");  // TO-DO: Print the failed core's coordinates here
-            log_info(tt::LogTest, "Golden vector");
-            print_vector<uint32_t>(packed_golden);
-            log_info(tt::LogTest, "Output vector");
-            print_vector<uint32_t>(packed_output);
-            return pcc;
-        }
-    }
-
-    return pcc;
-}
-
-void packet_sizes_test(
-    tt::ARCH arch_,
-    std::vector<IDevice*>& devices_,
-    uint32_t num_devices_,
-    uint32_t test_case_id,
-    CoreCoord mst_start_coord,
-    CoreCoord sub_start_coord,
-    CoreCoord mst_grid_size,
-    CoreCoord sub_grid_size) {
-    NOC noc_id = NOC::NOC_0;
-
-    auto [bytes_per_page, max_reservable_bytes, max_reservable_pages] =
-        tt::tt_metal::unit_tests::dm::compute_physical_constraints(arch_, devices_.at(0));
-
-    /* Running the Test */
-
-    uint32_t max_transactions_per_subordinate = 256;
-    uint32_t max_reservable_pages_per_transaction = 4096;
-
-    for (uint32_t num_of_transactions_per_subordinate = 1;
-         num_of_transactions_per_subordinate <= max_transactions_per_subordinate;
-         num_of_transactions_per_subordinate *= 4) {
-        for (uint32_t pages_reservable_per_transaction = 1;
-             pages_reservable_per_transaction <= max_reservable_pages_per_transaction;
-             pages_reservable_per_transaction *= 2) {
-            // Check if the total data size is within the limits
-            if (num_of_transactions_per_subordinate * pages_reservable_per_transaction > max_reservable_pages) {
-                continue;
-            }
-
-            // Test config
-            unit_tests::dm::all_from_all::AllFromAllConfig test_config = {
-
-                .test_id = unit_tests::dm::all_from_all::START_ID + test_case_id,
-
-                .mst_logical_start_coord = mst_start_coord,
-                .sub_logical_start_coord = sub_start_coord,
-                .mst_grid_size = mst_grid_size,
-                .sub_grid_size = sub_grid_size,
-
-                .num_of_transactions_per_subordinate = num_of_transactions_per_subordinate,
-                .pages_reservable_per_transaction = pages_reservable_per_transaction,
-                .bytes_per_page = bytes_per_page,
-
-                .l1_data_format = DataFormat::Float16_b,
-                .noc_id = noc_id,
-            };
-
-            // Run
-            for (unsigned int id = 0; id < num_devices_; id++) {
-                EXPECT_TRUE(run_dm(devices_.at(id), test_config));
-            }
-        }
-    }
-}
-
-void directed_ideal_test(
-    tt::ARCH arch_,
-    std::vector<IDevice*>& devices_,
-    uint32_t num_devices_,
-    uint32_t test_case_id,
-    CoreCoord mst_start_coord,
-    CoreCoord sub_start_coord,
-    CoreCoord mst_grid_size,
-    CoreCoord sub_grid_size) {
-    NOC noc_id = NOC::NOC_0;
-
-    // Physical Constraints
-    auto [bytes_per_page, max_reservable_bytes, max_reservable_pages] =
-        tt::tt_metal::unit_tests::dm::compute_physical_constraints(arch_, devices_.at(0));
-    /* Running the Test */
-
-    uint32_t num_of_transactions_per_subordinate = 1;
-    uint32_t pages_reservable_per_transaction = max_reservable_pages / num_of_transactions_per_subordinate;
-
-    // Test config
-    unit_tests::dm::all_from_all::AllFromAllConfig test_config = {
-
-        .test_id = unit_tests::dm::all_from_all::START_ID + test_case_id,
-
-        .mst_logical_start_coord = mst_start_coord,
-        .sub_logical_start_coord = sub_start_coord,
-        .mst_grid_size = mst_grid_size,
-        .sub_grid_size = sub_grid_size,
-
-        .num_of_transactions_per_subordinate = num_of_transactions_per_subordinate,
-        .pages_reservable_per_transaction = pages_reservable_per_transaction,
-        .bytes_per_page = bytes_per_page,
-
-        .l1_data_format = DataFormat::Float16_b,
-        .noc_id = noc_id,
-    };
-
-    // Run
-    for (unsigned int id = 0; id < num_devices_; id++) {
-        EXPECT_TRUE(run_dm(devices_.at(id), test_config));
-    }
-}
-
-}  // namespace unit_tests::dm::all_from_all
-
-/* =============================================================  /
-/  ========== TEST CASES FOR ALL-TO-ALL DATA MOVEMENT ==========  /
-/  ============================================================= */
-
-/*
-IDEAS:
-    - Implement a for loop that shuffles through several coordinates to test grids of
-        different locations
-    - Implement a for loop that shuffles through several grid sizes to test grids of
-        different sizes
-*/
-
-/* ======== PACKET SIZES ======== */
-
-TEST_F(DeviceFixture, TensixDataMovementAllFromAllPacketSizes) {
-    uint32_t test_case_id = 0;
-
-    /* Parameters */
-
-    CoreCoord mst_start_coord = {0, 0};
-    CoreCoord sub_start_coord = {0, 0};
-
-    CoreCoord mst_grid_size = {
-        devices_.at(0)->compute_with_storage_grid_size().x, devices_.at(0)->compute_with_storage_grid_size().y};
-    CoreCoord sub_grid_size = {
-        devices_.at(0)->compute_with_storage_grid_size().x, devices_.at(0)->compute_with_storage_grid_size().y};
-
-    tt::tt_metal::unit_tests::dm::all_from_all::packet_sizes_test(
-        arch_, devices_, num_devices_, test_case_id, mst_start_coord, sub_start_coord, mst_grid_size, sub_grid_size);
-}
-
-/* ======== DIRECTED IDEAL ======== */
-
-/* ======== All from All ======== */
-TEST_F(DeviceFixture, TensixDataMovementAllFromAllDirectedIdeal) {
-    uint32_t test_case_id = 1;
-
-    /* Parameters */
-
-    CoreCoord mst_start_coord = {0, 0};
-    CoreCoord sub_start_coord = {0, 0};
-
-    CoreCoord mst_grid_size = {
-        devices_.at(0)->compute_with_storage_grid_size().x, devices_.at(0)->compute_with_storage_grid_size().y};
-    CoreCoord sub_grid_size = {
-        devices_.at(0)->compute_with_storage_grid_size().x, devices_.at(0)->compute_with_storage_grid_size().y};
-
-    unit_tests::dm::all_from_all::directed_ideal_test(
-        arch_, devices_, num_devices_, test_case_id, mst_start_coord, sub_start_coord, mst_grid_size, sub_grid_size);
-}
-
-/* ======== 2x2 to 1x1 ======== */
-TEST_F(DeviceFixture, TensixDataMovementAllFromAll2x2From1x1DirectedIdeal) {
-    uint32_t test_case_id = 2;
-
-    /* Parameters */
-
-    CoreCoord mst_start_coord = {0, 0};
-    CoreCoord sub_start_coord = {4, 4};
-
-    CoreCoord mst_grid_size = {2, 2};
-    CoreCoord sub_grid_size = {1, 1};
-
-    unit_tests::dm::all_from_all::directed_ideal_test(
-        arch_, devices_, num_devices_, test_case_id, mst_start_coord, sub_start_coord, mst_grid_size, sub_grid_size);
-}
-
-/* ======== 4x4 to 1x1 ======== */
-TEST_F(DeviceFixture, TensixDataMovementAllFromAll4x4From1x1DirectedIdeal) {
-    uint32_t test_case_id = 3;
-
-    /* Parameters */
-
-    CoreCoord mst_start_coord = {0, 0};
-    CoreCoord sub_start_coord = {0, 0};
-
-    CoreCoord mst_grid_size = {4, 4};
-    CoreCoord sub_grid_size = {1, 1};
-
-    unit_tests::dm::all_from_all::directed_ideal_test(
-        arch_, devices_, num_devices_, test_case_id, mst_start_coord, sub_start_coord, mst_grid_size, sub_grid_size);
-}
-
-/* ======== 1x1 to 2x2 ======== */
-TEST_F(DeviceFixture, TensixDataMovementAllFromAll1x1From2x2DirectedIdeal) {
-    uint32_t test_case_id = 4;
-
-    /* Parameters */
-
-    CoreCoord mst_start_coord = {0, 0};
-    CoreCoord sub_start_coord = {4, 4};
-
-    CoreCoord mst_grid_size = {1, 1};
-    CoreCoord sub_grid_size = {2, 2};
-
-    unit_tests::dm::all_from_all::directed_ideal_test(
-        arch_, devices_, num_devices_, test_case_id, mst_start_coord, sub_start_coord, mst_grid_size, sub_grid_size);
-}
-
-/* ======== 1x1 to 4x4 ======== */
-TEST_F(DeviceFixture, TensixDataMovementAllFromAll1x1From4x4DirectedIdeal) {
-    uint32_t test_case_id = 5;
-
-    /* Parameters */
-
-    CoreCoord mst_start_coord = {0, 0};
-    CoreCoord sub_start_coord = {0, 0};
-
-    CoreCoord mst_grid_size = {1, 1};
-    CoreCoord sub_grid_size = {4, 4};
-
-    unit_tests::dm::all_from_all::directed_ideal_test(
-        arch_, devices_, num_devices_, test_case_id, mst_start_coord, sub_start_coord, mst_grid_size, sub_grid_size);
-}
-
-/* ======== 2x2 to 2x2 ======== */
-TEST_F(DeviceFixture, TensixDataMovementAllFromAll2x2From2x2DirectedIdeal) {
-    uint32_t test_case_id = 6;
-
-    /* Parameters */
-
-    CoreCoord mst_start_coord = {0, 0};
-    CoreCoord sub_start_coord = {0, 0};
-
-    CoreCoord mst_grid_size = {2, 2};
-    CoreCoord sub_grid_size = {2, 2};
-
-    unit_tests::dm::all_from_all::directed_ideal_test(
-        arch_, devices_, num_devices_, test_case_id, mst_start_coord, sub_start_coord, mst_grid_size, sub_grid_size);
-}
-
-}  // namespace tt::tt_metal
diff --git a/tests/tt_metal/tt_metal/data_movement/all_to_all/README.md b/tests/tt_metal/tt_metal/data_movement/all_to_all/README.md
deleted file mode 100644
index 6ecbedc3b5..0000000000
--- a/tests/tt_metal/tt_metal/data_movement/all_to_all/README.md
+++ /dev/null
@@ -1,48 +0,0 @@
-# All to All Core Data Movement Tests
-
-This test suite implements tests that measure the functionality and performance (i.e. bandwidth) of data movement transactions between Tensix cores.
-
-## Test Flow
-
-L1 space is allocated on all Tensix cores involved in the data movement test. Based on a given number of reservable pages, subsets of these pages are designated as either sender pages or receiver pages.
-
-The subsets are determined as follows:
-- num_sender_pages = num_reservable_pages / (1 + num_master_cores)
-- num_receiver_pages = num_sender_pages * num_master_cores
-
-The starting address of the receiver space in L1 immediately follows the final address allocated for the sender space.
-
-Each master core contains a portion of the entire data that is to be received by each subordinate core.
-Each master core issues a NOC asynchronous (unicast) write to each receiver core, writing its portion of data to its corresponding portion of receiver pages on each subordinate core.
-By the end of the test, every subordinate core should have received the same data from every master core.
-
-Test attributes such as pages per transaction and number of transactions per master core, and latency measures such as kernel and pre-determined scope cycles are recorded by the profiler.
-
-A pcc check is performed by cross-checking the data of all the master cores pieced-together against the data received by each subordinate core. This ensures that the data integrity is maintained throughout the data movement process.
-
-Test expectations are that pcc checks pass and sufficient test attribute data is captured by the profiler for higher level bandwidth/regression checks.
-
-## Test Parameters
-| Parameter                         | Data Type             | Description                                                               |
-| --------------------------------- | --------------------- | ------------------------------------------------------------------------- |
-| test_id                           | uint32_t              | Test ID for identifying different test cases.                             |
-| mst_logical_start_coord           | CoreCoord             | Logical starting coordinates for the master core range.                   |
-| sub_logical_start_coord           | CoreCoord             | Logical starting coordinates for the subordinate core range.              |
-| mst_grid_size                     | CoreCoord             | Grid size of the master core range.                                       |
-| sub_grid_size                     | CoreCoord             | Grid size of the subordinate core range.                                  |
-| num_of_transactions_per_master    | uint32_t              | Number of transactions issued per master core.                            |
-| pages_reservable_per_transaction  | uint32_t              | Number of reservable pages per transaction in L1.                         |
-| bytes_per_page                    | uint32_t              | Size of each page in bytes.                                               |
-| l1_data_format                    | DataFormat            | Data format used for L1 data movement.                                    |
-| noc_id                            | NOC                   | Specifies which NOC to use for the test.                                  |
-
-## Test Cases
-Each test case uses bfloat16 as L1 data format and flit size (32B for WH, 64B for BH) as page size. Each test case has multiple runs, and each run has a unique runtime host id, assigned by a global counter.
-
-1. **All to All Packet Sizes:** Tests different number of transactions and transaction sizes by varying the num_of_transactions and transaction_size_pages parameters.
-a. **2x2 Packet Sizes**
-2. **All to All Directed Ideal:** Tests the most optimal data movement setup between two ranges of cores.
-a. **2x2 to 1x1**
-b. **4x4 to 1x1**
-c. **1x1 to 2x2**
-d. **1x1 to 4x4**
diff --git a/tests/tt_metal/tt_metal/data_movement/all_to_all/kernels/sender.cpp b/tests/tt_metal/tt_metal/data_movement/all_to_all/kernels/sender.cpp
deleted file mode 100644
index ba5a3732fa..0000000000
--- a/tests/tt_metal/tt_metal/data_movement/all_to_all/kernels/sender.cpp
+++ /dev/null
@@ -1,57 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include "dataflow_api.h"
-#include "debug/dprint.h"
-#include "ckernel.h"
-
-void kernel_main() {
-    // Compile-time arguments
-    const uint32_t test_id = get_compile_time_arg_val(0);
-    const uint32_t mst_l1_base_address = get_compile_time_arg_val(1);
-    const uint32_t sub_l1_base_address = get_compile_time_arg_val(2);
-    const uint32_t total_size_bytes_per_master = get_compile_time_arg_val(3);
-    const uint32_t num_of_transactions = get_compile_time_arg_val(4);
-    const uint32_t bytes_per_transaction_per_master = get_compile_time_arg_val(5);
-    const uint32_t num_subordinates = get_compile_time_arg_val(6);
-
-    // Runtime arguments
-    const uint32_t master_index = get_arg_val<uint32_t>(0);
-
-    // Derivative values
-    uint32_t master_l1_local_address = mst_l1_base_address;
-    uint32_t subordinate_l1_local_address = sub_l1_base_address + (master_index * total_size_bytes_per_master);
-
-    uint32_t subordinate_x_coord;
-    uint32_t subordinate_y_coord;
-    uint64_t subordinate_l1_noc_address;
-
-    {
-        DeviceZoneScopedN("RISCV0");
-
-        for (uint32_t i = 0; i < num_of_transactions; i++) {
-            for (uint32_t j = 0; j < num_subordinates; j++) {
-                // Subordinate coordiantes are stored in the runtime arguments starting at index 1
-                // Each x coordinate is stores in an odd index
-                // Each y coordinate is stored in the next even index
-                // The first subordinate's coordinates are at indices 1 and 2, the second at indices 3 and 4, etc.
-                subordinate_x_coord = get_arg_val<uint32_t>(1 + (j * 2));
-                subordinate_y_coord = get_arg_val<uint32_t>(1 + (j * 2) + 1);
-
-                subordinate_l1_noc_address =
-                    get_noc_addr(subordinate_x_coord, subordinate_y_coord, subordinate_l1_local_address);
-
-                noc_async_write(master_l1_local_address, subordinate_l1_noc_address, bytes_per_transaction_per_master);
-            }
-            master_l1_local_address += bytes_per_transaction_per_master;
-            subordinate_l1_local_address += bytes_per_transaction_per_master;
-        }
-        noc_async_write_barrier();
-    }
-
-    DeviceTimestampedData("Test id", test_id);
-    DeviceTimestampedData("Number of transactions", num_of_transactions);
-    DeviceTimestampedData("Transaction size in bytes", bytes_per_transaction_per_master * num_subordinates);
-    DeviceTimestampedData("Total bytes", num_of_transactions * bytes_per_transaction_per_master * num_subordinates)
-}
diff --git a/tests/tt_metal/tt_metal/data_movement/all_to_all/test_all_to_all.cpp b/tests/tt_metal/tt_metal/data_movement/all_to_all/test_all_to_all.cpp
deleted file mode 100644
index 8168cff523..0000000000
--- a/tests/tt_metal/tt_metal/data_movement/all_to_all/test_all_to_all.cpp
+++ /dev/null
@@ -1,471 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include "device_fixture.hpp"
-#include "tt_metal/test_utils/comparison.hpp"
-#include "tt_metal/test_utils/stimulus.hpp"
-#include "tt_metal/test_utils/print_helpers.hpp"
-#include "dm_common.hpp"
-namespace tt::tt_metal {
-
-using namespace std;
-using namespace tt;
-using namespace tt::test_utils;
-
-namespace unit_tests::dm::all_to_all {
-
-constexpr uint32_t START_ID = 60;
-
-// Test Config (i.e. test parameters)
-struct AllToAllConfig {
-    /* Test ID */
-    uint32_t test_id = START_ID;
-
-    /* Grid configurations */
-    CoreCoord mst_logical_start_coord = CoreCoord();
-    CoreCoord sub_logical_start_coord = CoreCoord();
-    CoreCoord mst_grid_size = CoreCoord();
-    CoreCoord sub_grid_size = CoreCoord();
-
-    /* Transaction size configurations */
-    uint32_t num_of_transactions_per_master = 1;
-    uint32_t pages_reservable_per_transaction = 1;
-    uint32_t bytes_per_page = 32;
-
-    /* Write configurations */
-    DataFormat l1_data_format = DataFormat::Invalid;
-    NOC noc_id = NOC::NOC_0;
-
-    // TODO: Add the following parameters
-    //  1. Virtual Channel (only useful for unicast)
-    //  2. Posted flag (posted multicast has much better performance at larger grid sizes, than non-posted due to
-    //  response packets) (60, 45, 23, vs 60, 60, 60 at posted)
-};
-
-/// @brief Performs communication from L1 Sender Cores to L1 Receiver Cores.
-/// @param device The device on which the test is executed.
-/// @param test_config Configuration of the test, defined by a specific struct.
-/// @return Status of the test execution (e.g., success or failure).
-bool run_dm(IDevice* device, const AllToAllConfig& test_config) {
-    /* ================ SETUP ================ */
-
-    // Program
-    Program program = CreateProgram();
-
-    // Initialize core sets //
-    /*
-        - CoreRange: Represents a single rectangular range of cores in a 2D grid. It is defined by a starting coordinate
-        (`start_coord`) and an ending coordinate (`end_coord`). For example, a CoreRange from (0, 0) to (3, 3) would
-        include all cores in that rectangular area.
-
-        - CoreRangeSet: Represents a collection of CoreRange objects. It is used to manage multiple ranges of cores
-        and provides functionality for operations like merging, subtracting, and finding intersections between ranges.
-    */
-
-    // Logical Cores
-
-    // Master
-
-    CoreCoord mst_logical_start_coord = test_config.mst_logical_start_coord;
-    CoreCoord mst_logical_end_coord = CoreCoord(
-        mst_logical_start_coord.x + test_config.mst_grid_size.x - 1,
-        mst_logical_start_coord.y + test_config.mst_grid_size.y - 1);
-
-    CoreRangeSet mst_logical_core_set({CoreRange(mst_logical_start_coord, mst_logical_end_coord)});
-    uint32_t num_masters = mst_logical_core_set.num_cores();
-
-    // Subordinate
-
-    CoreCoord sub_logical_start_coord = test_config.sub_logical_start_coord;
-    CoreCoord sub_logical_end_coord = CoreCoord(
-        sub_logical_start_coord.x + test_config.sub_grid_size.x - 1,
-        sub_logical_start_coord.y + test_config.sub_grid_size.y - 1);
-
-    CoreRangeSet sub_logical_core_set({CoreRange(sub_logical_start_coord, sub_logical_end_coord)});
-    uint32_t num_subordinates = sub_logical_core_set.num_cores();
-
-    // Subordinate Worker Coordinates
-
-    std::vector<uint32_t> sub_worker_coordinates = {};
-    for (auto& sub_logical_core : corerange_to_cores(sub_logical_core_set)) {
-        CoreCoord sub_worker_core = device->worker_core_from_logical_core(sub_logical_core);
-        sub_worker_coordinates.push_back(sub_worker_core.x);
-        sub_worker_coordinates.push_back(sub_worker_core.y);
-    }
-
-    // Transaction Configurations
-
-    // Determine pages per transaction for the master and subordinate cores
-
-    const size_t pages_sent_per_transaction_per_master =
-        test_config.pages_reservable_per_transaction / (num_masters + 1);
-    if (pages_sent_per_transaction_per_master == 0) {
-        log_warning(
-            tt::LogTest, "Pages sent per transaction per master is 0. Skipping the current set of configurations.");
-        return 1;
-    }
-    const size_t bytes_sent_per_transaction_per_master =
-        pages_sent_per_transaction_per_master * test_config.bytes_per_page;
-    const size_t total_size_bytes_per_master =
-        bytes_sent_per_transaction_per_master * test_config.num_of_transactions_per_master;
-
-    const size_t pages_received_per_transaction = pages_sent_per_transaction_per_master * num_masters;
-    const size_t bytes_received_per_transaction = pages_received_per_transaction * test_config.bytes_per_page;
-    const size_t total_size_bytes_received =
-        bytes_received_per_transaction * test_config.num_of_transactions_per_master;
-
-    // Obtain L1 Address for Storing Data
-
-    L1AddressInfo core_l1_info = tt::tt_metal::unit_tests::dm::get_l1_address_and_size(device, {0, 0});
-    uint32_t mst_l1_base_address = core_l1_info.base_address;
-    uint32_t sub_l1_base_address = mst_l1_base_address + total_size_bytes_per_master;
-
-    // Possible To-Do: Implement checks to see that the needed space is available in all master and subordinate cores
-
-    // Kernels
-
-    // Compile-time arguments for kernels
-
-    std::vector<uint32_t> sender_compile_args = {
-        //     0: Test ID
-        (uint32_t)test_config.test_id,  // test_id
-        // 1 - 2: L1 Addresses
-        (uint32_t)mst_l1_base_address,
-        (uint32_t)sub_l1_base_address,
-        (uint32_t)total_size_bytes_per_master,  // subordinate L1 address offset
-        // 3 - 4: Transaction parameters
-        (uint32_t)test_config.num_of_transactions_per_master,  // num_of_transactions
-        (uint32_t)bytes_sent_per_transaction_per_master,       // transaction_size_bytes
-        //     5: Subordinate count
-        (uint32_t)num_subordinates,  // num_subordinates
-    };
-
-    // NOTE: BASE ADDRESS PER MASTER (THIS IS THE SUBORDINATE ADDRESS)
-
-    // Create kernels
-    auto sender_kernel = CreateKernel(
-        program,
-        "tests/tt_metal/tt_metal/data_movement/all_to_all/kernels/sender.cpp",
-        mst_logical_core_set,
-        DataMovementConfig{
-            .processor = DataMovementProcessor::RISCV_0,
-            .noc = test_config.noc_id,
-            .compile_args = sender_compile_args});
-
-    // Run-time Arguments for kernels
-
-    // Pre-fill sender_runtime_args with sub_worker_coordinates
-    std::vector<uint32_t> sender_runtime_args = sub_worker_coordinates;
-
-    // Reserve space for the first element (master index)
-    sender_runtime_args.insert(sender_runtime_args.begin(), 0);  // Placeholder for the first element
-
-    uint32_t i = 0;  // Initialize the counter
-    for (auto& mst_logical_core : corerange_to_cores(mst_logical_core_set)) {
-        // Update the first element (subordinate address offset)
-        sender_runtime_args[0] = i;
-
-        // Assign runtime arguments to the kernels
-        SetRuntimeArgs(program, sender_kernel, mst_logical_core, sender_runtime_args);
-
-        ++i;  // Increment the counter
-    }
-
-    // Assign unique id
-    log_info(tt::LogTest, "Running Test ID: {}, Run ID: {}", test_config.test_id, unit_tests::dm::runtime_host_id);
-    program.set_runtime_id(unit_tests::dm::runtime_host_id++);
-
-    /* ================ RUNNING THE PROGRAM ================ */
-
-    // Setting up Inputs and Golden Output
-
-    std::vector<uint32_t> packed_input;
-    packed_input.reserve(total_size_bytes_per_master / sizeof(uint32_t));
-
-    std::vector<uint32_t> packed_golden;
-    packed_golden.reserve(total_size_bytes_received / sizeof(uint32_t));
-
-    // Generate random input data for each master core
-    for (auto& mst_logical_core : corerange_to_cores(mst_logical_core_set)) {
-        packed_input = generate_packed_uniform_random_vector<uint32_t, bfloat16>(
-            -100.0f,
-            100.0f,
-            total_size_bytes_per_master / bfloat16::SIZEOF,
-            chrono::system_clock::now().time_since_epoch().count());
-
-        /*packed_input = generate_increment_vector<uint32_t>(
-            1,  // Start at 1
-            total_size_bytes_per_master / sizeof(uint32_t),  // Number of elements
-            2.0f,  // Increment by 1
-            1.0f,  // Start value
-            1,     // Count (not relevant here since slide is false)
-            true  // Slide is false to ensure consistent increments
-        );*/
-
-        tt_metal::detail::WriteToDeviceL1(device, mst_logical_core, mst_l1_base_address, packed_input);
-        MetalContext::instance().get_cluster().l1_barrier(device->id());
-
-        packed_golden.insert(packed_golden.end(), packed_input.begin(), packed_input.end());
-    }
-
-    // LAUNCH PROGRAM
-    detail::LaunchProgram(device, program);
-
-    std::vector<uint32_t> packed_output;
-    packed_output.reserve(total_size_bytes_received / sizeof(uint32_t));
-
-    bool pcc = false;
-
-    for (auto& sub_logical_core : corerange_to_cores(sub_logical_core_set)) {
-        tt_metal::detail::ReadFromDeviceL1(
-            device, sub_logical_core, sub_l1_base_address, total_size_bytes_received, packed_output);
-
-        // Results comparison
-        pcc = is_close_packed_vectors<bfloat16, uint32_t>(
-            packed_output, packed_golden, [&](const bfloat16& a, const bfloat16& b) { return is_close(a, b); });
-        if (!pcc) {
-            log_error(tt::LogTest, "PCC Check failed");  // TO-DO: Print the failed core's coordinates here
-            log_info(tt::LogTest, "Golden vector");
-            print_vector<uint32_t>(packed_golden);
-            log_info(tt::LogTest, "Output vector");
-            print_vector<uint32_t>(packed_output);
-            return pcc;
-        }
-    }
-
-    return pcc;
-}
-
-void packet_sizes_test(
-    tt::ARCH arch_,
-    std::vector<IDevice*>& devices_,
-    uint32_t num_devices_,
-    uint32_t test_case_id,
-    CoreCoord mst_start_coord,
-    CoreCoord sub_start_coord,
-    CoreCoord mst_grid_size,
-    CoreCoord sub_grid_size) {
-    NOC noc_id = NOC::NOC_0;
-
-    auto [bytes_per_page, max_reservable_bytes, max_reservable_pages] =
-        tt::tt_metal::unit_tests::dm::compute_physical_constraints(arch_, devices_.at(0));
-
-    /* Running the Test */
-
-    uint32_t max_transactions_per_master = 256;
-    uint32_t max_reservable_pages_per_transaction = 4096;
-
-    for (uint32_t num_of_transactions_per_master = 1; num_of_transactions_per_master <= max_transactions_per_master;
-         num_of_transactions_per_master *= 4) {
-        for (uint32_t pages_reservable_per_transaction = 1;
-             pages_reservable_per_transaction <= max_reservable_pages_per_transaction;
-             pages_reservable_per_transaction *= 2) {
-            // Check if the total data size is within the limits
-            if (num_of_transactions_per_master * pages_reservable_per_transaction > max_reservable_pages) {
-                continue;
-            }
-
-            // Test config
-            unit_tests::dm::all_to_all::AllToAllConfig test_config = {
-
-                .test_id = unit_tests::dm::all_to_all::START_ID + test_case_id,
-
-                .mst_logical_start_coord = mst_start_coord,
-                .sub_logical_start_coord = sub_start_coord,
-                .mst_grid_size = mst_grid_size,
-                .sub_grid_size = sub_grid_size,
-
-                .num_of_transactions_per_master = num_of_transactions_per_master,
-                .pages_reservable_per_transaction = pages_reservable_per_transaction,
-                .bytes_per_page = bytes_per_page,
-
-                .l1_data_format = DataFormat::Float16_b,
-                .noc_id = noc_id,
-            };
-
-            // Run
-            for (unsigned int id = 0; id < num_devices_; id++) {
-                EXPECT_TRUE(run_dm(devices_.at(id), test_config));
-            }
-        }
-    }
-}
-
-void directed_ideal_test(
-    tt::ARCH arch_,
-    std::vector<IDevice*>& devices_,
-    uint32_t num_devices_,
-    uint32_t test_case_id,
-    CoreCoord mst_start_coord,
-    CoreCoord sub_start_coord,
-    CoreCoord mst_grid_size,
-    CoreCoord sub_grid_size) {
-    NOC noc_id = NOC::NOC_0;
-
-    // Physical Constraints
-    auto [bytes_per_page, max_reservable_bytes, max_reservable_pages] =
-        tt::tt_metal::unit_tests::dm::compute_physical_constraints(arch_, devices_.at(0));
-    /* Running the Test */
-
-    uint32_t num_of_transactions_per_master = 1;
-    uint32_t pages_reservable_per_transaction = max_reservable_pages / num_of_transactions_per_master;
-
-    // Test config
-    unit_tests::dm::all_to_all::AllToAllConfig test_config = {
-
-        .test_id = unit_tests::dm::all_to_all::START_ID + test_case_id,
-
-        .mst_logical_start_coord = mst_start_coord,
-        .sub_logical_start_coord = sub_start_coord,
-        .mst_grid_size = mst_grid_size,
-        .sub_grid_size = sub_grid_size,
-
-        .num_of_transactions_per_master = num_of_transactions_per_master,
-        .pages_reservable_per_transaction = pages_reservable_per_transaction,
-        .bytes_per_page = bytes_per_page,
-
-        .l1_data_format = DataFormat::Float16_b,
-        .noc_id = noc_id,
-    };
-
-    // Run
-    for (unsigned int id = 0; id < num_devices_; id++) {
-        EXPECT_TRUE(run_dm(devices_.at(id), test_config));
-    }
-}
-
-}  // namespace unit_tests::dm::all_to_all
-
-/* =============================================================  /
-/  ========== TEST CASES FOR ALL-TO-ALL DATA MOVEMENT ==========  /
-/  ============================================================= */
-
-/*
-IDEAS:
-    - Implement a for loop that shuffles through several coordinates to test grids of
-        different locations
-    - Implement a for loop that shuffles through several grid sizes to test grids of
-        different sizes
-*/
-
-/* ======== PACKET SIZES ======== */
-
-TEST_F(DeviceFixture, TensixDataMovementAllToAllPacketSizes) {
-    uint32_t test_case_id = 0;
-
-    /* Parameters */
-
-    CoreCoord mst_start_coord = {0, 0};
-    CoreCoord sub_start_coord = {0, 0};
-
-    CoreCoord mst_grid_size = {
-        devices_.at(0)->compute_with_storage_grid_size().x, devices_.at(0)->compute_with_storage_grid_size().y};
-    CoreCoord sub_grid_size = {
-        devices_.at(0)->compute_with_storage_grid_size().x, devices_.at(0)->compute_with_storage_grid_size().y};
-
-    tt::tt_metal::unit_tests::dm::all_to_all::packet_sizes_test(
-        arch_, devices_, num_devices_, test_case_id, mst_start_coord, sub_start_coord, mst_grid_size, sub_grid_size);
-}
-
-/* ======== DIRECTED IDEAL ======== */
-
-/* ======== All to All ======== */
-TEST_F(DeviceFixture, TensixDataMovementAllToAllDirectedIdeal) {
-    uint32_t test_case_id = 1;
-
-    /* Parameters */
-
-    CoreCoord mst_start_coord = {0, 0};
-    CoreCoord sub_start_coord = {0, 0};
-
-    CoreCoord mst_grid_size = {
-        devices_.at(0)->compute_with_storage_grid_size().x, devices_.at(0)->compute_with_storage_grid_size().y};
-    CoreCoord sub_grid_size = {
-        devices_.at(0)->compute_with_storage_grid_size().x, devices_.at(0)->compute_with_storage_grid_size().y};
-
-    unit_tests::dm::all_to_all::directed_ideal_test(
-        arch_, devices_, num_devices_, test_case_id, mst_start_coord, sub_start_coord, mst_grid_size, sub_grid_size);
-}
-
-/* ======== 2x2 to 1x1 ======== */
-TEST_F(DeviceFixture, TensixDataMovementAllToAll2x2To1x1DirectedIdeal) {
-    uint32_t test_case_id = 2;
-
-    /* Parameters */
-
-    CoreCoord mst_start_coord = {0, 0};
-    CoreCoord sub_start_coord = {4, 4};
-
-    CoreCoord mst_grid_size = {2, 2};
-    CoreCoord sub_grid_size = {1, 1};
-
-    unit_tests::dm::all_to_all::directed_ideal_test(
-        arch_, devices_, num_devices_, test_case_id, mst_start_coord, sub_start_coord, mst_grid_size, sub_grid_size);
-}
-
-/* ======== 4x4 to 1x1 ======== */
-TEST_F(DeviceFixture, TensixDataMovementAllToAll4x4To1x1DirectedIdeal) {
-    uint32_t test_case_id = 3;
-
-    /* Parameters */
-
-    CoreCoord mst_start_coord = {0, 0};
-    CoreCoord sub_start_coord = {0, 0};
-
-    CoreCoord mst_grid_size = {4, 4};
-    CoreCoord sub_grid_size = {1, 1};
-
-    unit_tests::dm::all_to_all::directed_ideal_test(
-        arch_, devices_, num_devices_, test_case_id, mst_start_coord, sub_start_coord, mst_grid_size, sub_grid_size);
-}
-
-/* ======== 1x1 to 2x2 ======== */
-TEST_F(DeviceFixture, TensixDataMovementAllToAll1x1To2x2DirectedIdeal) {
-    uint32_t test_case_id = 4;
-
-    /* Parameters */
-
-    CoreCoord mst_start_coord = {0, 0};
-    CoreCoord sub_start_coord = {4, 4};
-
-    CoreCoord mst_grid_size = {1, 1};
-    CoreCoord sub_grid_size = {2, 2};
-
-    unit_tests::dm::all_to_all::directed_ideal_test(
-        arch_, devices_, num_devices_, test_case_id, mst_start_coord, sub_start_coord, mst_grid_size, sub_grid_size);
-}
-
-/* ======== 1x1 to 4x4 ======== */
-TEST_F(DeviceFixture, TensixDataMovementAllToAll1x1To4x4DirectedIdeal) {
-    uint32_t test_case_id = 5;
-
-    /* Parameters */
-
-    CoreCoord mst_start_coord = {0, 0};
-    CoreCoord sub_start_coord = {0, 0};
-
-    CoreCoord mst_grid_size = {1, 1};
-    CoreCoord sub_grid_size = {4, 4};
-
-    unit_tests::dm::all_to_all::directed_ideal_test(
-        arch_, devices_, num_devices_, test_case_id, mst_start_coord, sub_start_coord, mst_grid_size, sub_grid_size);
-}
-
-/* ======== 2x2 to 2x2 ======== */
-TEST_F(DeviceFixture, TensixDataMovementAllToAll2x2To2x2DirectedIdeal) {
-    uint32_t test_case_id = 6;
-
-    /* Parameters */
-
-    CoreCoord mst_start_coord = {0, 0};
-    CoreCoord sub_start_coord = {0, 0};
-
-    CoreCoord mst_grid_size = {2, 2};
-    CoreCoord sub_grid_size = {2, 2};
-
-    unit_tests::dm::all_to_all::directed_ideal_test(
-        arch_, devices_, num_devices_, test_case_id, mst_start_coord, sub_start_coord, mst_grid_size, sub_grid_size);
-}
-
-}  // namespace tt::tt_metal
diff --git a/tests/tt_metal/tt_metal/data_movement/dm_common.hpp b/tests/tt_metal/tt_metal/data_movement/dm_common.hpp
index 02f894fa53..ecff143ae9 100644
--- a/tests/tt_metal/tt_metal/data_movement/dm_common.hpp
+++ b/tests/tt_metal/tt_metal/data_movement/dm_common.hpp
@@ -8,7 +8,6 @@
 #include <cstdint>
 #include "device_fixture.hpp"
 #include <tuple>
-
 namespace tt::tt_metal::unit_tests::dm {
 
 // Unique id for each test run
diff --git a/tests/tt_metal/tt_metal/data_movement/documentation/ideal_performance.md b/tests/tt_metal/tt_metal/data_movement/documentation/ideal_performance.md
index 8efbc67522..6c22d44934 100644
--- a/tests/tt_metal/tt_metal/data_movement/documentation/ideal_performance.md
+++ b/tests/tt_metal/tt_metal/data_movement/documentation/ideal_performance.md
@@ -10,4 +10,3 @@ Units are **Bytes/cycle**.
 | One To One                        | 29            | 60        |
 | One From One                      | 28            | 60        |
 | One To All (Multicast + Linked)   | 19            | 34        |
-| One From All                      | 30            | 60        |
diff --git a/tests/tt_metal/tt_metal/data_movement/one_from_all/README.md b/tests/tt_metal/tt_metal/data_movement/one_from_all/README.md
index 88a0631b9e..69e0409411 100644
--- a/tests/tt_metal/tt_metal/data_movement/one_from_all/README.md
+++ b/tests/tt_metal/tt_metal/data_movement/one_from_all/README.md
@@ -3,7 +3,7 @@
 This test suite implements tests that measure the functionality and performance (i.e. bandwidth) of data movement transactions between a gatherer Tensix core and a grid of sender Tensix cores.
 
 ## Test Flow
-Data is written directly into the L1 memory of the sender cores. The gatherer kernel issues read NOC transactions to request a transfer of this data from each sender core to its L1 memory. A read barrier is placed after these transactions in order to ensure data validity.
+Sharded L1 buffers are created on both the gatherer core and on all the sender cores -making sure that the sender core buffers have a total size equal to the size of the gatherer core buffer. Data is written into the L1 buffer of the sender cores. The gatherer kernel issues read NOC transactions to request a transfer of this data to its L1 buffer. A read barrier is placed after these transactions in order to ensure data validity.
 
 Test attributes such as transaction sizes and number of transactions as well as latency measures like kernel and pre-determined scope cycles are recorded by the profiler. Resulting data is cross-checked with original data and validated through a pcc check.
 
diff --git a/tests/tt_metal/tt_metal/data_movement/one_from_all/kernels/gatherer.cpp b/tests/tt_metal/tt_metal/data_movement/one_from_all/kernels/gatherer.cpp
index 8ae400e32e..2807c50dee 100644
--- a/tests/tt_metal/tt_metal/data_movement/one_from_all/kernels/gatherer.cpp
+++ b/tests/tt_metal/tt_metal/data_movement/one_from_all/kernels/gatherer.cpp
@@ -8,22 +8,24 @@
 
 // L1 to L1 request
 void kernel_main() {
-    uint32_t l1_local_addr = get_compile_time_arg_val(0);
+    uint32_t dst_addr = get_compile_time_arg_val(0);
     constexpr uint32_t num_of_transactions = get_compile_time_arg_val(1);
-    constexpr uint32_t transaction_size_bytes = get_compile_time_arg_val(2);
-    constexpr uint32_t test_id = get_compile_time_arg_val(3);
-    constexpr uint32_t total_subordinate_cores = get_compile_time_arg_val(4);
+    constexpr uint32_t transaction_num_pages = get_compile_time_arg_val(2);
+    constexpr uint32_t page_size_bytes = get_compile_time_arg_val(3);
+    constexpr uint32_t test_id = get_compile_time_arg_val(4);
+    constexpr uint32_t total_subordinate_cores = get_compile_time_arg_val(5);
 
     std::array<uint32_t, total_subordinate_cores> subordinate_l1_byte_addresses;
     std::array<std::array<uint32_t, 2>, total_subordinate_cores> responder_coords;
     uint32_t rt_args_idx = 0;
     for (uint32_t i = 0; i < total_subordinate_cores; i++) {
-        subordinate_l1_byte_addresses[i] = l1_local_addr;
+        subordinate_l1_byte_addresses[i] = get_arg_val<uint32_t>(rt_args_idx++);
         responder_coords[i][0] = get_arg_val<uint32_t>(rt_args_idx++);
         responder_coords[i][1] = get_arg_val<uint32_t>(rt_args_idx++);
     }
 
-    constexpr uint32_t subordinate_size_bytes = num_of_transactions * transaction_size_bytes;
+    constexpr uint32_t transaction_size_bytes = transaction_num_pages * page_size_bytes;
+    constexpr uint32_t subordinate_size_bytes = num_of_transactions * transaction_num_pages * page_size_bytes;
 
     DeviceTimestampedData("Number of transactions", num_of_transactions);
     DeviceTimestampedData("Transaction size in bytes", transaction_size_bytes * total_subordinate_cores);
@@ -40,10 +42,10 @@ void kernel_main() {
                  * Then, we OR it with the local address in each iteration to get the full NOC address. */
                 uint64_t src_noc_addr = src_base_noc_addr | subordinate_l1_byte_addresses[sub_core];
 
-                noc_async_read(src_noc_addr, l1_local_addr, transaction_size_bytes);
+                noc_async_read(src_noc_addr, dst_addr, transaction_size_bytes);
 
                 subordinate_l1_byte_addresses[sub_core] += transaction_size_bytes;
-                l1_local_addr += transaction_size_bytes;
+                dst_addr += transaction_size_bytes;
             }
         }
         noc_async_read_barrier();
diff --git a/tests/tt_metal/tt_metal/data_movement/one_from_all/test_one_from_all.cpp b/tests/tt_metal/tt_metal/data_movement/one_from_all/test_one_from_all.cpp
index 700c5437da..3a09fe26af 100644
--- a/tests/tt_metal/tt_metal/data_movement/one_from_all/test_one_from_all.cpp
+++ b/tests/tt_metal/tt_metal/data_movement/one_from_all/test_one_from_all.cpp
@@ -42,40 +42,37 @@ bool run_dm(IDevice* device, const OneFromAllConfig& test_config) {
     CoreRangeSet master_core_set({CoreRange(test_config.master_core_coord)});
     size_t total_subordinate_cores = test_config.subordinate_core_set.num_cores();
 
-    const size_t transaction_size_bytes = test_config.transaction_size_pages * test_config.page_size_bytes;
-    const size_t total_size_bytes = test_config.num_of_transactions * transaction_size_bytes * total_subordinate_cores;
-
-    // Obtain L1 Address for Storing Data
-    // NOTE: We don't know if the whole block of memory is actually available.
-    //       This is something that could probably be checked
-    L1AddressInfo master_l1_info =
-        tt::tt_metal::unit_tests::dm::get_l1_address_and_size(device, test_config.master_core_coord);
-
-    auto sub_core_list = corerange_to_cores(test_config.subordinate_core_set);
-    for (auto& core : sub_core_list) {
-        L1AddressInfo subordinate_l1_info = tt::tt_metal::unit_tests::dm::get_l1_address_and_size(device, core);
-        // Checks that both master and subordinate cores have the same L1 base address and size
-        if (master_l1_info.base_address != subordinate_l1_info.base_address ||
-            master_l1_info.size != subordinate_l1_info.size) {
-            log_error(tt::LogTest, "Mismatch in L1 address or size between master and subordinate cores");
-            return false;
-        }
-    }
-    // Check if the L1 size is sufficient for the test configuration
-    if (master_l1_info.size < total_size_bytes) {
-        log_error(tt::LogTest, "Insufficient L1 size for the test configuration");
-        return false;
-    }
-    // Assigns a "safe" L1 local address for the master and subordinate cores
-    uint32_t l1_base_address = master_l1_info.base_address;
+    const size_t total_size_bytes = test_config.num_of_transactions * test_config.transaction_size_pages *
+                                    test_config.page_size_bytes * total_subordinate_cores;
+    const size_t total_size_pages =
+        test_config.num_of_transactions * test_config.transaction_size_pages * total_subordinate_cores;
 
-    const size_t subordinate_size_bytes = test_config.num_of_transactions * transaction_size_bytes;
+    auto master_shard_parameters = ShardSpecBuffer(
+        master_core_set,
+        {1, total_size_bytes / 2},
+        ShardOrientation::ROW_MAJOR,
+        {1, test_config.page_size_bytes / 2},
+        {1, total_size_pages});
+    auto master_l1_buffer = CreateBuffer(ShardedBufferConfig{
+        .device = device,
+        .size = total_size_bytes,
+        .page_size = test_config.page_size_bytes,
+        .buffer_type = BufferType::L1,
+        .buffer_layout = TensorMemoryLayout::WIDTH_SHARDED,
+        .shard_parameters = std::move(master_shard_parameters),
+    });
+    uint32_t master_l1_byte_address = master_l1_buffer->address();
+
+    const size_t subordinate_size_bytes =
+        test_config.num_of_transactions * test_config.transaction_size_pages * test_config.page_size_bytes;
+    const size_t subordinate_size_pages = test_config.num_of_transactions * test_config.transaction_size_pages;
 
     // Compile-time arguments for kernels
     vector<uint32_t> gatherer_compile_args = {
-        (uint32_t)l1_base_address,
+        (uint32_t)master_l1_byte_address,
         (uint32_t)test_config.num_of_transactions,
-        (uint32_t)transaction_size_bytes,
+        (uint32_t)test_config.transaction_size_pages,
+        (uint32_t)test_config.page_size_bytes,
         (uint32_t)test_config.test_id,
         (uint32_t)total_subordinate_cores,
     };
@@ -93,7 +90,27 @@ bool run_dm(IDevice* device, const OneFromAllConfig& test_config) {
     // Runtime Arguments
     vector<uint32_t> master_runtime_args;
 
-    for (auto& core : sub_core_list) {
+    // Create buffers for each subordinate core and add to runtime args
+    vector<shared_ptr<Buffer>> subordinate_l1_buffers;
+    for (auto& core : corerange_to_cores(test_config.subordinate_core_set)) {
+        auto subordinate_shard_parameters = ShardSpecBuffer(
+            CoreRangeSet({CoreRange(core)}),
+            {1, subordinate_size_bytes / 2},
+            ShardOrientation::ROW_MAJOR,
+            {1, test_config.page_size_bytes / 2},
+            {1, subordinate_size_pages});
+        ShardedBufferConfig subordinate_buffer_config{
+            .device = device,
+            .size = subordinate_size_bytes,
+            .page_size = test_config.page_size_bytes,
+            .buffer_type = BufferType::L1,
+            .buffer_layout = TensorMemoryLayout::WIDTH_SHARDED,
+            .shard_parameters = std::move(subordinate_shard_parameters),
+        };
+        auto subordinate_l1_buffer = CreateBuffer(subordinate_buffer_config);
+        subordinate_l1_buffers.push_back(subordinate_l1_buffer);
+        master_runtime_args.push_back((uint32_t)subordinate_l1_buffer->address());
+
         CoreCoord physical_core = device->worker_core_from_logical_core(core);
         master_runtime_args.push_back(physical_core.x);
         master_runtime_args.push_back(physical_core.y);
@@ -110,20 +127,18 @@ bool run_dm(IDevice* device, const OneFromAllConfig& test_config) {
 
     // Golden output
     vector<uint32_t> packed_golden = packed_input;
+    vector<uint32_t> packed_output;
 
     // Launch program and record outputs
     for (size_t i = 0; i < total_subordinate_cores; i++) {
         auto begin = packed_input.data() + i * subordinate_size_bytes / sizeof(uint32_t);
         auto end = packed_input.data() + (i + 1) * subordinate_size_bytes / sizeof(uint32_t);
-        vector<uint32_t> partial_input(begin, end);
-        detail::WriteToDeviceL1(device, sub_core_list[i], l1_base_address, partial_input);
+        detail::WriteToBuffer(subordinate_l1_buffers[i], vector<uint32_t>(begin, end));
     }
     MetalContext::instance().get_cluster().l1_barrier(device->id());
 
     detail::LaunchProgram(device, program);
-
-    vector<uint32_t> packed_output;
-    detail::ReadFromDeviceL1(device, test_config.master_core_coord, l1_base_address, total_size_bytes, packed_output);
+    detail::ReadFromBuffer(master_l1_buffer, packed_output);
 
     // Results comparison
     bool pcc = is_close_packed_vectors<bfloat16, uint32_t>(
@@ -143,23 +158,23 @@ bool run_dm(IDevice* device, const OneFromAllConfig& test_config) {
 
 /* ========== Test case for one from all data movement; Test id = 15 ========== */
 TEST_F(DeviceFixture, TensixDataMovementOneFromAllPacketSizes) {
-    // Physical Constraints
-    auto [page_size_bytes, max_transmittable_bytes, max_transmittable_pages] =
-        tt::tt_metal::unit_tests::dm::compute_physical_constraints(arch_, devices_.at(0));
-
     // Parameters
     uint32_t max_transactions = 256;
     uint32_t max_transaction_size_pages = 64;
+    uint32_t page_size_bytes = arch_ == tt::ARCH::BLACKHOLE ? 64 : 32;  // =Flit size: 32 bytes for WH, 64 for BH
 
     // Cores
     CoreCoord master_core_coord = {0, 0};
     CoreRangeSet subordinate_core_set = {CoreRange(CoreCoord(1, 1), CoreCoord(4, 4))};
     size_t total_subordinate_cores = subordinate_core_set.num_cores();
 
+    uint32_t l1_size = 1024 * 1024;  // 1MB
+
     for (uint32_t num_of_transactions = 1; num_of_transactions <= max_transactions; num_of_transactions *= 4) {
         for (uint32_t transaction_size_pages = 1; transaction_size_pages <= max_transaction_size_pages;
              transaction_size_pages *= 2) {
-            if (num_of_transactions * transaction_size_pages * total_subordinate_cores > max_transmittable_pages) {
+            if (num_of_transactions * transaction_size_pages * page_size_bytes * total_subordinate_cores >=
+                1024 * 1024) {
                 continue;
             }
 
@@ -184,23 +199,22 @@ TEST_F(DeviceFixture, TensixDataMovementOneFromAllPacketSizes) {
 
 /* ========== Test case for one from all data movement; Test id = 30 ========== */
 TEST_F(DeviceFixture, TensixDataMovementOneFromAllDirectedIdeal) {
-    // Physical Constraints
-    auto [page_size_bytes, max_transmittable_bytes, max_transmittable_pages] =
-        tt::tt_metal::unit_tests::dm::compute_physical_constraints(arch_, devices_.at(0));
+    // Parameters
+    uint32_t num_of_transactions, page_size_bytes;
+    uint32_t transaction_size_pages = 128;
+    if (arch_ == tt::ARCH::BLACKHOLE) {
+        page_size_bytes = 64;  // (=flit size): 64 bytes for BH
+        num_of_transactions = 5;
+    } else {
+        page_size_bytes = 32;  // (=flit size): 32 bytes for WH
+        num_of_transactions = 10;
+    }
 
     // Cores
     CoreCoord master_core_coord = {0, 0};
     CoreRangeSet subordinate_core_set = {CoreRange(CoreCoord(1, 1), CoreCoord(4, 4))};
     size_t total_subordinate_cores = subordinate_core_set.num_cores();
 
-    // Parameters
-    // Ideal: Less transactions, more data per transaction
-    uint32_t num_of_transactions = 1;
-    if (max_transmittable_pages % (num_of_transactions * total_subordinate_cores) != 0) {
-        log_error(tt::LogTest, "Max transmittable pages not evenly divisible by transactions and subordinate cores");
-    }
-    uint32_t transaction_size_pages = max_transmittable_pages / num_of_transactions / total_subordinate_cores;
-
     // Test config
     unit_tests::dm::core_to_core::OneFromAllConfig test_config = {
         .test_id = 30,
diff --git a/tests/tt_metal/tt_metal/data_movement/one_to_one/kernels/sender.cpp b/tests/tt_metal/tt_metal/data_movement/one_to_one/kernels/sender.cpp
index 9b103aa0ea..c5dd1aae7b 100644
--- a/tests/tt_metal/tt_metal/data_movement/one_to_one/kernels/sender.cpp
+++ b/tests/tt_metal/tt_metal/data_movement/one_to_one/kernels/sender.cpp
@@ -24,7 +24,7 @@ void kernel_main() {
 
     DeviceTimestampedData("Number of transactions", num_of_transactions);
     DeviceTimestampedData("Transaction size in bytes", bytes_per_transaction);
-    DeviceTimestampedData("Total bytes", num_of_transactions * bytes_per_transaction);
+    DeviceTimestampedData("Total bytes transferred", num_of_transactions * bytes_per_transaction);
     DeviceTimestampedData("Test id", test_id);
 
     {
diff --git a/tests/tt_metal/tt_metal/data_movement/test_data_movement.py b/tests/tt_metal/tt_metal/data_movement/test_data_movement.py
index 970b799b0f..2510d4b0c2 100755
--- a/tests/tt_metal/tt_metal/data_movement/test_data_movement.py
+++ b/tests/tt_metal/tt_metal/data_movement/test_data_movement.py
@@ -45,10 +45,6 @@ test_id_to_name = {
     21: "Conv Act with halo 3x3",
     22: "Conv Act with halo 3x3 Small",
     23: "Conv Halo Gather",
-    60: "All to All Packet Sizes",
-    61: "All to All Directed Ideal",
-    70: "All from All Packet Sizes",
-    71: "All from All Directed Ideal",
 }
 
 # Comments for each test explaining why we get the perf that we do
@@ -197,8 +193,8 @@ test_bounds = {
         16: {
             "riscv_0": {"latency": {"lower": 50, "upper": 30000}, "bandwidth": 0.4},
         },
-        30: {  # One from All Directed Ideal
-            "riscv_1": {"latency": {"lower": 33000, "upper": 35000}, "bandwidth": 30},
+        30: {
+            "riscv_1": {"latency": {"lower": 20000, "upper": 23500}, "bandwidth": 28},
         },
         50: {  # One to One Directed Ideal
             "riscv_0": {"latency": {"lower": 28000, "upper": 36000}, "bandwidth": 29},  # 33832
@@ -230,18 +226,6 @@ test_bounds = {
         23: {
             "riscv_1": {"latency": {"lower": 500, "upper": 1000}, "bandwidth": 10},
         },
-        # 60: { # All to All Packet Sizes NOT DONE
-        # "riscv_0": {"latency": {"lower": #, "upper": #}, "bandwidth": #},
-        # },
-        61: {  # All to All Directed Ideal
-            "riscv_0": {"latency": {"lower": 30000, "upper": 35000}, "bandwidth": 30},
-        },
-        # 70: { # All from All Packet Sizes NOT DONE
-        #    "riscv_0": {"latency": {"lower": #, "upper": #}, "bandwidth": #},
-        # },
-        # 71: { # All from All Directed Ideal NOT DONE
-        #    "riscv_0": {"latency": {"lower": 30000, "upper": 800000}, "bandwidth": 1.3}, # 33093-701498 cycles, 1.4714111800746403 Bytes/cycle
-        # },
     },
     "blackhole": {
         0: {
@@ -299,9 +283,7 @@ test_bounds = {
         16: {
             "riscv_0": {"latency": {"lower": 50, "upper": 30000}, "bandwidth": 0.4},
         },
-        30: {  # One from All Directed Ideal
-            "riscv_1": {"latency": {"lower": 16500, "upper": 17500}, "bandwidth": 60},
-        },
+        30: {"riscv_1": {"latency": {"lower": 10000, "upper": 11500}, "bandwidth": 57}},
         50: {  # One to One Directed Ideal
             "riscv_0": {"latency": {"lower": 12000, "upper": 19000}, "bandwidth": 59},  # 17000
         },
@@ -332,21 +314,6 @@ test_bounds = {
         23: {
             "riscv_1": {"latency": {"lower": 500, "upper": 1000}, "bandwidth": 20},
         },
-        # 60: { # All to All Packet Sizes NOT DONE
-        #    "riscv_0": {"latency": {"lower": 12000, "upper": 19000}, "bandwidth": 59},
-        # },
-        # 61: {  # All to All Directed Ideal
-        #    "riscv_0": {
-        #        "latency": {"lower": 30000, "upper": 35000},
-        #        "bandwidth": 30,
-        #    },  # 33154-33515 cycles, 30.79791138296285 Bytes/cycle
-        # },
-        # 70: { # All from All Packet Sizes NOT DONE
-        #    "riscv_0": {"latency": {"lower": #, "upper": #}, "bandwidth": #},
-        # },
-        # 71: { # All from All Directed Ideal NOT DONE
-        #    "riscv_0": {"latency": {"lower": 10000, "upper": 400000}, "bandwidth": 2.7}, # 18481-345478 cycles, 2.9955018843457473 Bytes/cycle
-        # },
     },
 }
 
diff --git a/tests/tt_metal/tt_metal/dispatch/CMakeLists.txt b/tests/tt_metal/tt_metal/dispatch/CMakeLists.txt
index ffe073dac6..5604d825c7 100644
--- a/tests/tt_metal/tt_metal/dispatch/CMakeLists.txt
+++ b/tests/tt_metal/tt_metal/dispatch/CMakeLists.txt
@@ -36,7 +36,6 @@ target_sources(
         dispatch_program/test_global_circular_buffers.cpp
         dispatch_program/test_program_reuse.cpp
         dispatch_trace/test_sub_device.cpp
-        dispatch_util/test_ringbuffer_cache.cpp
 )
 target_include_directories(
     unit_tests_dispatch_basic
diff --git a/tests/tt_metal/tt_metal/dispatch/dispatch_util/test_ringbuffer_cache.cpp b/tests/tt_metal/tt_metal/dispatch/dispatch_util/test_ringbuffer_cache.cpp
deleted file mode 100644
index bb4d0faf59..0000000000
--- a/tests/tt_metal/tt_metal/dispatch/dispatch_util/test_ringbuffer_cache.cpp
+++ /dev/null
@@ -1,181 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent Inc.
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include <gtest/gtest.h>
-#include "tt_metal/impl/dispatch/ringbuffer_cache.hpp"
-#include <memory>
-#include <numeric>
-#include <vector>
-#include <algorithm>
-#include <unordered_map>
-#include <map>
-#include <random>
-
-namespace tt::tt_metal {
-
-struct CacheTestParams {
-    size_t cache_block_sizeB;
-    size_t cache_size_blocks;
-    size_t initial_manager_size;
-    std::pair<int, int> pgm_ids;
-    std::pair<int, int> pgm_sizes;
-};
-class RingbufferCacheRandomizedTestsFixture : public ::testing::TestWithParam<CacheTestParams> {
-protected:
-    RingbufferCacheRandomizedTestsFixture() = default;
-    ~RingbufferCacheRandomizedTestsFixture() override = default;
-
-    std::unique_ptr<RingbufferCacheManager> rb_cache_;
-
-    // This function is called before each test in this test case.
-    void setup(CacheTestParams& params) {
-        rb_cache_ = std::make_unique<RingbufferCacheManager>(
-            params.cache_block_sizeB, params.cache_size_blocks, params.initial_manager_size);
-    }
-
-    // define accessors to the private members of the RingbufferCacheManager
-    auto get_next_block_offset() const { return rb_cache_->manager_.next_block_offset; }
-    auto get_oldest_block_offset() const { return rb_cache_->manager_.entry[rb_cache_->manager_.oldest_idx].offset; }
-    auto get_oldest_idx() const { return rb_cache_->manager_.oldest_idx; }
-    auto get_next_idx() const { return rb_cache_->manager_.next_idx; }
-    auto get_manager_entry_size() const { return rb_cache_->manager_.entry.size(); }
-    auto get_manager_entry(size_t idx) const { return rb_cache_->manager_.entry[idx]; }
-    auto get_valid_entry(size_t idx) const { return rb_cache_->valid_[idx]; }
-    constexpr static auto invalid_entry_ = RingbufferCacheManager::invalid_cache_entry_;
-};
-
-INSTANTIATE_TEST_SUITE_P(
-    RingbufferCacheRandomSuite,
-    RingbufferCacheRandomizedTestsFixture,
-    testing::Values(
-        CacheTestParams{
-            .cache_block_sizeB = 4,
-            .cache_size_blocks = 1024,
-            .initial_manager_size = 64,
-            .pgm_ids = std::make_pair(0, 1000),
-            .pgm_sizes = std::make_pair(1, 769)},
-        CacheTestParams{
-            .cache_block_sizeB = 4,
-            .cache_size_blocks = 1024,
-            .initial_manager_size = 32,
-            .pgm_ids = std::make_pair(0, 4000),
-            .pgm_sizes = std::make_pair(4, 20)},
-        CacheTestParams{
-            .cache_block_sizeB = 4,
-            .cache_size_blocks = 256,
-            .initial_manager_size = 64,
-            .pgm_ids = std::make_pair(0, 10000),
-            .pgm_sizes = std::make_pair(1, 4)},
-        CacheTestParams{
-            .cache_block_sizeB = 4,
-            .cache_size_blocks = 1024,
-            .initial_manager_size = 2,
-            .pgm_ids = std::make_pair(0, 10000),
-            .pgm_sizes = std::make_pair(1, 100)},
-        CacheTestParams{
-            .cache_block_sizeB = 4,
-            .cache_size_blocks = 512,
-            .initial_manager_size = 4,
-            .pgm_ids = std::make_pair(0, 1000),
-            .pgm_sizes = std::make_pair(1, 10)},
-        CacheTestParams{
-            .cache_block_sizeB = 4,
-            .cache_size_blocks = 1024,
-            .initial_manager_size = 1024,
-            .pgm_ids = std::make_pair(0, 4000),
-            .pgm_sizes = std::make_pair(4, 20)},
-        CacheTestParams{
-            .cache_block_sizeB = 4,
-            .cache_size_blocks = 1024,
-            .initial_manager_size = 2048,
-            .pgm_ids = std::make_pair(0, 4000),
-            .pgm_sizes = std::make_pair(16, 64)},
-        CacheTestParams{// high hits test
-                        .cache_block_sizeB = 4,
-                        .cache_size_blocks = 1024,
-                        .initial_manager_size = 2,
-                        .pgm_ids = std::make_pair(0, 512),
-                        .pgm_sizes = std::make_pair(1, 8)},
-        CacheTestParams{// high hits test
-                        .cache_block_sizeB = 4,
-                        .cache_size_blocks = 4096,
-                        .initial_manager_size = 32,
-                        .pgm_ids = std::make_pair(0, 512),
-                        .pgm_sizes = std::make_pair(4, 24)},
-        CacheTestParams{// high hits test
-                        .cache_block_sizeB = 4,
-                        .cache_size_blocks = 4096,
-                        .initial_manager_size = 16,
-                        .pgm_ids = std::make_pair(0, 256),
-                        .pgm_sizes = std::make_pair(4, 16)},
-        CacheTestParams{
-            .cache_block_sizeB = 4,
-            .cache_size_blocks = 512,
-            .initial_manager_size = 128,
-            .pgm_ids = std::make_pair(0, 4000),
-            .pgm_sizes = std::make_pair(100, 200)}));
-
-TEST_P(RingbufferCacheRandomizedTestsFixture, RandomizedQueries) {
-    CacheTestParams params = GetParam();
-    setup(params);
-    auto pgm_ids = params.pgm_ids;
-    auto pgm_sizes = params.pgm_sizes;
-
-    std::unordered_map<int, int> pgm_id_size_map;
-
-    int hits_count = 0;
-
-    std::random_device rd;
-    uint64_t rd1 = rd(), rd2 = rd();
-    std::mt19937 gen_pgm_id(rd1);
-    std::mt19937 gen_pgm_size(rd2);
-    std::uniform_int_distribution<uint64_t> dist_pgm_id(pgm_ids.first, pgm_ids.second - 1);
-    std::uniform_int_distribution<uint64_t> dist_pgm_size(pgm_sizes.first, pgm_sizes.second - 1);
-    uint64_t pgm_id, pgm_size;
-    constexpr size_t num_iterations = 10'000'000;
-    for (size_t i = 0; i < num_iterations; ++i) {
-        pgm_id = dist_pgm_id(gen_pgm_id);
-        if (pgm_id_size_map.find(pgm_id) != pgm_id_size_map.end()) {
-            pgm_size = pgm_id_size_map[pgm_id];
-        } else {
-            pgm_size = dist_pgm_size(gen_pgm_size);
-            pgm_id_size_map[pgm_id] = pgm_size;
-        }
-    }
-
-    gen_pgm_id.seed(rd1);  // restart from seed
-    auto start_rbcache = std::chrono::high_resolution_clock::now();
-    for (size_t i = 0; i < num_iterations; ++i) {
-        pgm_id = dist_pgm_id(gen_pgm_id);
-        pgm_size = pgm_id_size_map[pgm_id];
-
-        auto result = rb_cache_->get_cache_offset(pgm_id, pgm_size * params.cache_block_sizeB);
-        ASSERT_TRUE(result);
-        ASSERT_GE(result->offset, 0);
-        ASSERT_LT(result->offset, params.cache_size_blocks);
-        if (!result->is_cached) {
-            ASSERT_TRUE((result->offset + pgm_size) % params.cache_size_blocks == get_next_block_offset())
-                << "Failed check (iter:" << i << "): cache size: " << params.cache_size_blocks << ", pgm_id: " << pgm_id
-                << ", pgm_size: " << pgm_size << ", offset: " << result->offset
-                << ", next_block_offset: " << get_next_block_offset() << std::endl;
-        }
-        ASSERT_TRUE(get_manager_entry_size() >= std::min(params.initial_manager_size, params.cache_size_blocks))
-            << "Manager size: " << get_manager_entry_size() << ", cache size: " << params.cache_size_blocks
-            << ", initial manager size: " << params.initial_manager_size << ", oldest_idx: " << get_oldest_idx()
-            << ", next_index: " << get_next_idx() << ", oldest_block_offset: " << get_oldest_block_offset()
-            << ", next_block_offset: " << get_next_block_offset() << std::endl;
-        if (result->is_cached) {
-            ++hits_count;
-        }
-    }
-    auto end_rbcache = std::chrono::high_resolution_clock::now();
-    auto duration_rbcache = std::chrono::duration_cast<std::chrono::milliseconds>(end_rbcache - start_rbcache).count();
-    std::cout << "Ringbuffer cache runtime: " << duration_rbcache << " ms, hits: " << hits_count << std::endl;
-    ASSERT_TRUE(get_manager_entry_size() <= params.cache_size_blocks)
-        << "Manager size: " << get_manager_entry_size() << ", cache size: " << params.cache_size_blocks << std::endl;
-    std::cout << "Cache size: " << params.cache_size_blocks << ", initial manager size: " << params.initial_manager_size
-              << ", final manager size: " << get_manager_entry_size() << std::endl;
-}
-
-}  // namespace tt::tt_metal
diff --git a/tests/tt_metal/tt_metal/integration/matmul/test_matmul_multi_core_multi_dram_in0_mcast_in1_mcast.cpp b/tests/tt_metal/tt_metal/integration/matmul/test_matmul_multi_core_multi_dram_in0_mcast_in1_mcast.cpp
index 5d7a4ebfec..0195044aca 100644
--- a/tests/tt_metal/tt_metal/integration/matmul/test_matmul_multi_core_multi_dram_in0_mcast_in1_mcast.cpp
+++ b/tests/tt_metal/tt_metal/integration/matmul/test_matmul_multi_core_multi_dram_in0_mcast_in1_mcast.cpp
@@ -530,7 +530,7 @@ bool matmul_multi_core_multi_dram_in0_mcast_in1_mcast(tt_metal::IDevice* device)
                 convert_layout_tile_nfaces_to_tile_swizzled(tt::stl::make_const_span(result_bfp16));
 
             // log_info(LogTest, "Tile id {} on dram bank {}, address {}", tile_id, dram_bank, dram_address);
-            // print_vec_of_bfloat16(result_flat_layout, 1, "Result - tile#" + std::to_string(tile_id));
+            // print_vec(result_flat_layout, 32, 32, "Result - tile#" + std::to_string(tile_id));
             pass &= (golden_tile == result_flat_layout);
         }
     }
diff --git a/tests/tt_metal/tt_metal/integration/matmul/test_matmul_multi_core_multi_dram_inX_mcast.cpp b/tests/tt_metal/tt_metal/integration/matmul/test_matmul_multi_core_multi_dram_inX_mcast.cpp
index db301ed5bf..548119a24f 100644
--- a/tests/tt_metal/tt_metal/integration/matmul/test_matmul_multi_core_multi_dram_inX_mcast.cpp
+++ b/tests/tt_metal/tt_metal/integration/matmul/test_matmul_multi_core_multi_dram_inX_mcast.cpp
@@ -447,7 +447,7 @@ bool matmul_multi_core_multi_dram_inX_mcast(tt_metal::IDevice* device, int in1_o
                 convert_layout_tile_nfaces_to_tile_swizzled(tt::stl::make_const_span(result_bfp16));
 
             // log_info(LogTest, "Tile id {} on dram bank {}, address {}", tile_id, dram_bank, dram_address);
-            // print_vec_of_bfloat16(result_flat_layout, 1, "Result - tile#" + std::to_string(tile_id));
+            // print_vec(result_flat_layout, 32, 32, "Result - tile#" + std::to_string(tile_id));
             pass &= (golden_tile == result_flat_layout);
         }
     }
diff --git a/tests/tt_metal/tt_metal/perf_microbenchmark/1_compute_mm/test_compute_mm.cpp b/tests/tt_metal/tt_metal/perf_microbenchmark/1_compute_mm/test_compute_mm.cpp
index e6c52269d0..da2a95a4c4 100644
--- a/tests/tt_metal/tt_metal/perf_microbenchmark/1_compute_mm/test_compute_mm.cpp
+++ b/tests/tt_metal/tt_metal/perf_microbenchmark/1_compute_mm/test_compute_mm.cpp
@@ -1420,6 +1420,19 @@ std::vector<T> get_col_slice(std::vector<T> data, int start_col_index, int num_c
     return result;
 }
 
+void print_vec(const std::vector<float>& data, int rows, int cols, const string& name) {
+    std::cout << name << ": " << std::endl;
+    int index = 0;
+    for (int i = 0; i < rows; i++) {
+        for (int j = 0; j < cols; j++) {
+            std::cout << data.at(index) << " ";
+            index++;
+        }
+        std::cout << std::endl;
+    }
+    std::cout << std::endl;
+}
+
 void prepare_inputs(
     tt_metal::IDevice* device,
     CoreCoord core_range,
diff --git a/tests/tt_metal/tt_metal/perf_microbenchmark/dispatch/test_prefetcher.cpp b/tests/tt_metal/tt_metal/perf_microbenchmark/dispatch/test_prefetcher.cpp
index 61f84d21a6..188dcfa3b2 100644
--- a/tests/tt_metal/tt_metal/perf_microbenchmark/dispatch/test_prefetcher.cpp
+++ b/tests/tt_metal/tt_metal/perf_microbenchmark/dispatch/test_prefetcher.cpp
@@ -1073,7 +1073,6 @@ void gen_dram_ringbuffer_read_cmd(
         ringbuffer_cmd.log2_page_size = log_page_size;
         ringbuffer_cmd.base_addr = DRAM_DATA_BASE_ADDR + count * page_size;
         ringbuffer_cmd.length = length;
-        ringbuffer_cmd.wp_offset_update = length;
         count++;
 
         update_cmd_sizes(prefetch_cmds, cmd_sizes, [&]() { add_bare_prefetcher_cmd(prefetch_cmds, cmd, true); });
diff --git a/tests/tt_metal/tt_metal/perf_microbenchmark/old/matmul/matmul_local_l1.cpp b/tests/tt_metal/tt_metal/perf_microbenchmark/old/matmul/matmul_local_l1.cpp
index 81a33b7291..5a24d75a41 100644
--- a/tests/tt_metal/tt_metal/perf_microbenchmark/old/matmul/matmul_local_l1.cpp
+++ b/tests/tt_metal/tt_metal/perf_microbenchmark/old/matmul/matmul_local_l1.cpp
@@ -125,6 +125,19 @@ std::vector<T> slice(std::vector<T> const& v, int m, int n) {
     return vec;
 }
 
+void print_vec(const std::vector<bfloat16>& data, int rows, int cols, const std::string& name) {
+    std::cout << name << ": " << std::endl;
+    int index = 0;
+    for (int i = 0; i < rows; i++) {
+        for (int j = 0; j < cols; j++) {
+            std::cout << data.at(index).to_float() << " ";
+            index++;
+        }
+        std::cout << std::endl;
+    }
+    std::cout << std::endl;
+}
+
 int main(int argc, char** argv) {
     if (getenv("TT_METAL_SLOW_DISPATCH_MODE") != nullptr) {
         TT_THROW("Test not supported w/ slow dispatch, exiting");
@@ -233,8 +246,8 @@ int main(int argc, char** argv) {
         auto identity = create_identity_matrix(Kt * 32, Nt * 32, std::min(Kt, Nt) * 32);  // bflaot16 identity
 
         if (print_tensor) {
-            print_vec_of_bfloat16(tensor.get_values(), 1, "Activation first row");
-            print_vec_of_bfloat16(identity, 1, "Weights first row");
+            print_vec(tensor.get_values(), 2, Kt * 32, std::string("Activation first row"));
+            print_vec(identity, 2, Nt * 32, std::string("Weights first row"));
         }
 
         log_info(LogTest, "Slicing input tensors and copying them to L1");
@@ -350,8 +363,11 @@ int main(int argc, char** argv) {
                     auto result_untilized = untilize_swizzled(result_flat_layout, per_core_Mt * 32, per_core_Nt * 32);
 
                     if (print_tensor) {
-                        print_vec_of_bfloat16(
-                            result_untilized, 1, "result_untilized" + std::to_string(r) + " " + std::to_string(c));
+                        print_vec(
+                            result_untilized,
+                            2,
+                            Nt * 32,
+                            std::string("result_untilized" + std::to_string(r) + " " + std::to_string(c)));
                     }
 
                     if (!(per_core_golden == result_untilized)) {
diff --git a/tests/tt_metal/tt_metal/perf_microbenchmark/old/noc/test_noc_read_global_l1.cpp b/tests/tt_metal/tt_metal/perf_microbenchmark/old/noc/test_noc_read_global_l1.cpp
index c4fed0a225..3fcdf46f89 100644
--- a/tests/tt_metal/tt_metal/perf_microbenchmark/old/noc/test_noc_read_global_l1.cpp
+++ b/tests/tt_metal/tt_metal/perf_microbenchmark/old/noc/test_noc_read_global_l1.cpp
@@ -52,6 +52,19 @@ std::vector<T> slice_vec(std::vector<T> const& v, int m, int n) {
     return vec;
 }
 
+void print_vec(const std::vector<bfloat16>& data, int rows, int cols, const std::string& name) {
+    std::cout << name << ": " << std::endl;
+    int index = 0;
+    for (int i = 0; i < rows; i++) {
+        for (int j = 0; j < cols; j++) {
+            std::cout << data.at(index).to_float() << " ";
+            index++;
+        }
+        std::cout << std::endl;
+    }
+    std::cout << std::endl;
+}
+
 int main(int argc, char** argv) {
     if (getenv("TT_METAL_SLOW_DISPATCH_MODE") != nullptr) {
         TT_THROW("Test not supported w/ slow dispatch, exiting");
@@ -146,10 +159,11 @@ int main(int argc, char** argv) {
         if (print_tensor) {
             for (int r = 0; r < num_cores_r; ++r) {
                 for (int c = 0; c < num_cores_c; ++c) {
-                    print_vec_of_bfloat16(
+                    print_vec(
                         tensors[r * num_cores_c + c].get_values(),
                         1,
-                        "input tensor " + std::to_string(r) + " " + std::to_string(c));
+                        32,
+                        std::string("input tensor " + std::to_string(r) + " " + std::to_string(c)));
                     if (single_read || one_buffer_share) {
                         break;
                     }
@@ -236,12 +250,16 @@ int main(int argc, char** argv) {
                 auto result_bfp16 = unpack_uint32_vec_into_bfloat16_vec(result_vec);
 
                 if (print_tensor) {
-                    print_vec_of_bfloat16(
-                        result_bfp16, 1, "from l1 buffer " + std::to_string(r) + " " + std::to_string(c));
-                    print_vec_of_bfloat16(
+                    print_vec(
+                        result_bfp16,
+                        1,
+                        32,
+                        std::string("from l1 buffer " + std::to_string(r) + " " + std::to_string(c)));
+                    print_vec(
                         tensors[r * num_cores_c + c].get_values(),
                         1,
-                        "tensor " + std::to_string(r) + " " + std::to_string(c));
+                        32,
+                        std::string("tensor " + std::to_string(r) + " " + std::to_string(c)));
                 }
                 if (!(tensors[r * num_cores_c + c].get_values() == result_bfp16)) {
                     log_error(
@@ -325,11 +343,19 @@ int main(int argc, char** argv) {
                         slice_vec(tensors[tensors_idx].get_values(), (index - cb_tiles) * 1024, index * 1024 - 1);
 
                     if (print_tensor) {
-                        print_vec_of_bfloat16(
-                            result_bfp16, 1, "result_bfp16 " + std::to_string(r) + " " + std::to_string(c));
-                        print_vec_of_bfloat16(
-                            sliced_tensor, 1, "sliced_tensor " + std::to_string(r) + " " + std::to_string(c));
+                        print_vec(
+                            result_bfp16,
+                            32,
+                            32,
+                            std::string("result_bfp16 " + std::to_string(r) + " " + std::to_string(c)));
+
+                        print_vec(
+                            sliced_tensor,
+                            32,
+                            32,
+                            std::string("sliced_tensor " + std::to_string(r) + " " + std::to_string(c)));
                     }
+
                     if (sliced_tensor != result_bfp16) {
                         log_error(LogTest, "{}/{} - comparision failed ", r, c);
                         pass = false;
diff --git a/tests/tt_metal/tt_metal/perf_microbenchmark/old/noc/test_noc_read_local_l1.cpp b/tests/tt_metal/tt_metal/perf_microbenchmark/old/noc/test_noc_read_local_l1.cpp
index 8c86e9469b..b0c8e6b8cf 100644
--- a/tests/tt_metal/tt_metal/perf_microbenchmark/old/noc/test_noc_read_local_l1.cpp
+++ b/tests/tt_metal/tt_metal/perf_microbenchmark/old/noc/test_noc_read_local_l1.cpp
@@ -49,6 +49,19 @@ std::vector<T> slice_vec(std::vector<T> const& v, int m, int n) {
     return vec;
 }
 
+void print_vec(const std::vector<bfloat16>& data, int rows, int cols, const std::string& name) {
+    std::cout << name << ": " << std::endl;
+    int index = 0;
+    for (int i = 0; i < rows; i++) {
+        for (int j = 0; j < cols; j++) {
+            std::cout << data.at(index).to_float() << " ";
+            index++;
+        }
+        std::cout << std::endl;
+    }
+    std::cout << std::endl;
+}
+
 int main(int argc, char** argv) {
     if (getenv("TT_METAL_SLOW_DISPATCH_MODE") != nullptr) {
         TT_THROW("Test not supported w/ slow dispatch, exiting");
@@ -125,9 +138,10 @@ int main(int argc, char** argv) {
         if (print_tensor) {
             for (int r = 0; r < num_cores_r; ++r) {
                 for (int c = 0; c < num_cores_c; ++c) {
-                    print_vec_of_bfloat16(
+                    print_vec(
                         tensors[r * num_cores_c + c].get_values(),
                         1,
+                        32,
                         std::string("input tensor " + std::to_string(r) + " " + std::to_string(c)));
                 }
             }
@@ -248,13 +262,16 @@ int main(int argc, char** argv) {
                         slice_vec(tensors[r * num_cores_c + c].get_values(), (Nt - cb_tiles) * 1024, Nt * 1024 - 1);
 
                     if (print_tensor) {
-                        print_vec_of_bfloat16(
+                        print_vec(
                             result_bfp16,
-                            1,
+                            32,
+                            32,
                             std::string("result_bfp16 " + std::to_string(r) + " " + std::to_string(c)));
-                        print_vec_of_bfloat16(
+
+                        print_vec(
                             sliced_tensor,
-                            1,
+                            32,
+                            32,
                             std::string("sliced_tensor " + std::to_string(r) + " " + std::to_string(c)));
                     }
 
diff --git a/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_1d_rx.cpp b/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_1d_rx.cpp
index 874cac8cf4..919dd7a774 100644
--- a/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_1d_rx.cpp
+++ b/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_1d_rx.cpp
@@ -40,11 +40,7 @@ void kernel_main() {
         time_seed = prng_next(time_seed);
         uint32_t expected_val = time_seed + (packet_payload_size_bytes / 16) - 1;
 
-        WAYPOINT("FPW");
-        while (expected_val != *poll_addr) {
-            invalidate_l1_cache();
-        }
-        WAYPOINT("FPD");
+        while (expected_val != *poll_addr);
 
         // check for data correctness
         match = check_packet_data(
diff --git a/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_mux_ubench_drainer.cpp b/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_mux_ubench_drainer.cpp
index ec7923622f..3394c64c19 100644
--- a/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_mux_ubench_drainer.cpp
+++ b/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_mux_ubench_drainer.cpp
@@ -64,7 +64,6 @@ void kernel_main() {
 
     status_ptr[0] = tt::tt_fabric::DrainerStatus::READY_FOR_TRAFFIC;
     while (!got_immediate_termination_signal(termination_signal_ptr)) {
-        invalidate_l1_cache();
         bool has_unsent_payload = get_ptr_val(slots_free_stream_id) != NUM_BUFFERS;
         if (has_unsent_payload) {
             worker_interface.local_write_counter.increment();
diff --git a/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_traffic_gen.hpp b/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_traffic_gen.hpp
index e96d13c5a7..1de5314763 100644
--- a/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_traffic_gen.hpp
+++ b/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_traffic_gen.hpp
@@ -230,7 +230,6 @@ inline void fill_packet_data(tt_l1_ptr uint32_t* start_addr, uint32_t num_words,
 inline bool check_packet_data(tt_l1_ptr uint32_t* start_addr, uint32_t num_words, uint32_t start_val,
                               uint32_t& mismatch_addr, uint32_t& mismatch_val, uint32_t& expected_val) {
     tt_l1_ptr uint32_t* addr = start_addr + (PACKET_WORD_SIZE_BYTES/4 - 1);
-    invalidate_l1_cache();
     for (uint32_t i = 0; i < num_words; i++) {
         if (*addr != start_val) {
             mismatch_addr = reinterpret_cast<uint32_t>(addr);
diff --git a/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_traffic_gen_rx_socket.cpp b/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_traffic_gen_rx_socket.cpp
index c10de431bc..2690d6bc5c 100644
--- a/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_traffic_gen_rx_socket.cpp
+++ b/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_traffic_gen_rx_socket.cpp
@@ -75,7 +75,7 @@ void kernel_main() {
     zero_l1_buf((uint32_t*)client_pull_req_buf, sizeof(chan_req_buf));
     test_results[TT_FABRIC_MISC_INDEX] = 0xff000003;
 
-    client_interface->gk_interface_addr = get_noc_addr_helper(gk_interface_addr_h, gk_interface_addr_l);
+    client_interface->gk_interface_addr = ((uint64_t)gk_interface_addr_h << 32) | gk_interface_addr_l;
     client_interface->gk_msg_buf_addr = client_interface->gk_interface_addr + offsetof(gatekeeper_info_t, gk_msg_buf);
     client_interface->pull_req_buf_addr = xy_local_addr | client_pull_req_buf_addr;
     test_results[TT_FABRIC_MISC_INDEX] = 0xff000004;
diff --git a/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_tx_ubench.cpp b/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_tx_ubench.cpp
index 3cc3aba5da..3460216f72 100644
--- a/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_tx_ubench.cpp
+++ b/tests/tt_metal/tt_metal/perf_microbenchmark/routing/kernels/tt_fabric_tx_ubench.cpp
@@ -122,7 +122,7 @@ void kernel_main() {
     uint64_t data_words_sent = 0;
     uint32_t packet_count = 0;
 
-    uint64_t dst_addr = get_noc_addr_helper(noc_offset, target_address);
+    uint64_t dst_addr = ((uint64_t)noc_offset << 32 | target_address);
     if constexpr (mcast_data) {
         fabric_async_write_multicast_add_header(
             client_interface,
diff --git a/tests/tt_metal/tt_metal/perf_microbenchmark/routing/test_tt_fabric_sanity.cpp b/tests/tt_metal/tt_metal/perf_microbenchmark/routing/test_tt_fabric_sanity.cpp
index f02dcd6c00..9cf8cb0c41 100644
--- a/tests/tt_metal/tt_metal/perf_microbenchmark/routing/test_tt_fabric_sanity.cpp
+++ b/tests/tt_metal/tt_metal/perf_microbenchmark/routing/test_tt_fabric_sanity.cpp
@@ -169,7 +169,7 @@ struct test_board_t {
             throw std::runtime_error("Odd number of chips detected, not supported currently");
         }
 
-        tt::tt_metal::detail::SetFabricConfig(tt::tt_metal::FabricConfig::CUSTOM);
+        tt::tt_metal::detail::InitializeFabricConfig(tt::tt_metal::FabricConfig::CUSTOM);
 
         device_handle_map = tt::tt_metal::detail::CreateDevices(available_chip_ids);
         control_plane = &tt::tt_metal::MetalContext::instance().get_control_plane();
diff --git a/tests/tt_metal/tt_metal/test_datacopy_multi_core_multi_dram.cpp b/tests/tt_metal/tt_metal/test_datacopy_multi_core_multi_dram.cpp
index 8aec686fa5..fdfda086a0 100644
--- a/tests/tt_metal/tt_metal/test_datacopy_multi_core_multi_dram.cpp
+++ b/tests/tt_metal/tt_metal/test_datacopy_multi_core_multi_dram.cpp
@@ -17,6 +17,19 @@
 //////////////////////////////////////////////////////////////////////////////////////////
 using namespace tt;
 
+void print_vec(const std::vector<bfloat16>& data, int rows, int cols, string name) {
+    std::cout << name << ": " << std::endl;
+    int index = 0;
+    for (int i = 0; i < rows; i++) {
+        for (int j = 0; j < cols; j++) {
+            std::cout << data.at(index).to_float() << ", ";
+            index++;
+        }
+        std::cout << std::endl;
+    }
+    std::cout << std::endl;
+}
+
 std::vector<bfloat16> select_columns(std::vector<bfloat16> data, int M, int K, int N) {
     if (N == K) {
         return data;
@@ -402,11 +415,10 @@ int main(int argc, char** argv) {
                 tt_metal::detail::ReadFromDeviceDRAMChannel(
                     device, dram_bank, dram_address, single_tile_size, result_vec);
                 auto result_bfp16 = unpack_uint32_vec_into_bfloat16_vec(result_vec);
-                auto result_flat_layout =
-                    convert_layout_tile_nfaces_to_tile_swizzled(tt::stl::make_const_span(result_bfp16));
+                auto result_flat_layout = convert_to_flat_layout(tt::stl::make_const_span(result_bfp16));
 
                 // log_info(LogTest, "Tile id {} on dram bank {}, address {}", tile_id, dram_bank, dram_address);
-                // print_vec_of_bfloat16(result_flat_layout, 1, "Result " + std::to_string(tile_id));
+                // print_vec(result_flat_layout, 32, 32, "Result - tile#" + std::to_string(tile_id));
                 pass &= (golden_tile == result_flat_layout);
             }
         }
diff --git a/tests/tt_metal/tt_metal/test_generic_binary_reader_matmul_large_block.cpp b/tests/tt_metal/tt_metal/test_generic_binary_reader_matmul_large_block.cpp
index cbc9c75861..652e362c99 100644
--- a/tests/tt_metal/tt_metal/test_generic_binary_reader_matmul_large_block.cpp
+++ b/tests/tt_metal/tt_metal/test_generic_binary_reader_matmul_large_block.cpp
@@ -75,6 +75,19 @@ std::vector<std::uint32_t> transpose_tiles(
     return result;
 }
 
+void print_vec(const std::vector<bfloat16>& data, int rows, int cols, const std::string& name) {
+    std::cout << name << ": " << std::endl;
+    int index = 0;
+    for (int i = 0; i < rows; i++) {
+        for (int j = 0; j < cols; j++) {
+            std::cout << data.at(index).to_float() << ", ";
+            index++;
+        }
+        std::cout << std::endl;
+    }
+    std::cout << std::endl;
+}
+
 void print_faces(std::vector<bfloat16> data, const std::string& name) {
     std::cout << name << ": " << std::endl;
     int index = 0;
@@ -329,13 +342,13 @@ int main(int argc, char** argv) {
         auto result_flat_layout = convert_layout_tile_nfaces_to_tile_swizzled(tt::stl::make_const_span(result_bfp16));
         auto result_untilized = untilize_swizzled(result_flat_layout, M * 32, N * 32);
 
-        // print_vec_of_bfloat16(result_bfp16, 16, "Result bfp16");
+        // print_vec(result_bfp16, 128, 128, "Result bfp16");
         // print_faces(unpack_uint32_vec_into_bfloat16_vec(activations_tile_transposed), "Activations tile transpose");
         // print_faces(unpack_uint32_vec_into_bfloat16_vec(weights), "Weights tile transposed");
         // print_faces(result_bfp16, "Result bfp16");
         // print_vec_of_uint32_as_packed_bfloat16(weights, 16, "weights tile transposed");
-        // print_vec_of_bfloat16(result_untilized, M*N, "Result");
-        // print_vec_of_bfloat16(tensor.get_values(), 16, "Golden");
+        // print_vec(result_untilized, M*32, N*32, "Result");
+        // print_vec(tensor.get_values(), 128, 128, "Golden");
 
         pass &= (tensor.get_values() == result_untilized);
         pass &= tt_metal::CloseDevice(device);
diff --git a/tests/tt_metal/tt_metal/test_matmul_large_block.cpp b/tests/tt_metal/tt_metal/test_matmul_large_block.cpp
index 90fecb3026..5448e3f922 100644
--- a/tests/tt_metal/tt_metal/test_matmul_large_block.cpp
+++ b/tests/tt_metal/tt_metal/test_matmul_large_block.cpp
@@ -372,9 +372,8 @@ bool test_matmul_large_block(tt_metal::IDevice* device, bool activations_rm, boo
                 print_faces(result_bfp16, "Result");
             }
         } else {
-            auto result_flat_layout =
-                convert_layout_tile_nfaces_to_tile_swizzled(tt::stl::make_const_span(result_bfp16));
-            auto result_untilized = untilize_swizzled(result_flat_layout, M * 32, N * 32);
+            auto result_flat_layout = convert_to_flat_layout(tt::stl::make_const_span(result_bfp16));
+            auto result_untilized = untilize(result_flat_layout, M * 32, N * 32);
             pass &= (tensor.get_values() == result_untilized);
             if (not pass) {
                 print_faces(result_untilized, "Result");
diff --git a/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram.cpp b/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram.cpp
index 5d45d5ec37..9fb8636ed9 100644
--- a/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram.cpp
+++ b/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram.cpp
@@ -22,6 +22,19 @@ using std::vector;
 using namespace tt;
 using namespace tt::tt_metal;
 
+void print_vec(const std::vector<bfloat16>& data, int rows, int cols, string name) {
+    std::cout << name << ": " << std::endl;
+    int index = 0;
+    for (int i = 0; i < rows; i++) {
+        for (int j = 0; j < cols; j++) {
+            std::cout << data.at(index).to_float() << ", ";
+            index++;
+        }
+        std::cout << std::endl;
+    }
+    std::cout << std::endl;
+}
+
 std::vector<bfloat16> select_columns(std::vector<bfloat16> data, int M, int K, int N) {
     if (N == K) {
         return data;
@@ -419,8 +432,7 @@ int main(int argc, char** argv) {
                 result_vec.insert(result_vec.end(), result_iter, result_iter + 512);
                 result_iter += 512;
                 auto result_bfp16 = unpack_uint32_vec_into_bfloat16_vec(result_vec);
-                auto result_flat_layout =
-                    convert_layout_tile_nfaces_to_tile_swizzled(tt::stl::make_const_span(result_bfp16));
+                auto result_flat_layout = convert_to_flat_layout(tt::stl::make_const_span(result_bfp16));
 
                 pass &= (golden_tile == result_flat_layout);
             }
diff --git a/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram_in0_mcast.cpp b/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram_in0_mcast.cpp
index 969a7dd430..daec8edc17 100644
--- a/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram_in0_mcast.cpp
+++ b/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram_in0_mcast.cpp
@@ -19,6 +19,19 @@
 using std::vector;
 using namespace tt;
 
+void print_vec(const std::vector<bfloat16>& data, int rows, int cols, string name) {
+    std::cout << name << ": " << std::endl;
+    int index = 0;
+    for (int i = 0; i < rows; i++) {
+        for (int j = 0; j < cols; j++) {
+            std::cout << data.at(index).to_float() << ", ";
+            index++;
+        }
+        std::cout << std::endl;
+    }
+    std::cout << std::endl;
+}
+
 std::vector<bfloat16> select_columns(std::vector<bfloat16> data, int M, int K, int N) {
     if (N == K) {
         return data;
@@ -477,11 +490,10 @@ int main(int argc, char** argv) {
                 tt_metal::detail::ReadFromDeviceDRAMChannel(
                     device, dram_bank, dram_address, single_tile_size, result_vec);
                 auto result_bfp16 = unpack_uint32_vec_into_bfloat16_vec(result_vec);
-                auto result_flat_layout =
-                    convert_layout_tile_nfaces_to_tile_swizzled(tt::stl::make_const_span(result_bfp16));
+                auto result_flat_layout = convert_to_flat_layout(tt::stl::make_const_span(result_bfp16));
 
                 // log_info(LogTest, "Tile id {} on dram bank {}, address {}", tile_id, dram_bank, dram_address);
-                // print_vec_of_bfloat16(result_flat_layout, 1, "Result - tile#" + std::to_string(tile_id));
+                // print_vec(result_flat_layout, 32, 32, "Result - tile#" + std::to_string(tile_id));
                 pass &= (golden_tile == result_flat_layout);
             }
         }
diff --git a/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram_in0_mcast_in1_mcast.cpp b/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram_in0_mcast_in1_mcast.cpp
index 2b3e56a816..523e455caf 100644
--- a/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram_in0_mcast_in1_mcast.cpp
+++ b/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram_in0_mcast_in1_mcast.cpp
@@ -19,6 +19,19 @@
 using std::vector;
 using namespace tt;
 
+void print_vec(const std::vector<bfloat16>& data, int rows, int cols, string name) {
+    std::cout << name << ": " << std::endl;
+    int index = 0;
+    for (int i = 0; i < rows; i++) {
+        for (int j = 0; j < cols; j++) {
+            std::cout << data.at(index).to_float() << ", ";
+            index++;
+        }
+        std::cout << std::endl;
+    }
+    std::cout << std::endl;
+}
+
 std::vector<bfloat16> select_columns(std::vector<bfloat16> data, int M, int K, int N) {
     if (N == K) {
         return data;
@@ -572,11 +585,10 @@ int main(int argc, char** argv) {
                 tt_metal::detail::ReadFromDeviceDRAMChannel(
                     device, dram_bank, dram_address, single_tile_size, result_vec);
                 auto result_bfp16 = unpack_uint32_vec_into_bfloat16_vec(result_vec);
-                auto result_flat_layout =
-                    convert_layout_tile_nfaces_to_tile_swizzled(tt::stl::make_const_span(result_bfp16));
+                auto result_flat_layout = convert_to_flat_layout(tt::stl::make_const_span(result_bfp16));
 
                 // log_info(LogTest, "Tile id {} on dram bank {}, address {}", tile_id, dram_bank, dram_address);
-                // print_vec_of_bfloat16(result_flat_layout, 1, "Result - tile#" + std::to_string(tile_id));
+                // print_vec(result_flat_layout, 32, 32, "Result - tile#" + std::to_string(tile_id));
                 pass &= (golden_tile == result_flat_layout);
             }
         }
diff --git a/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram_in1_mcast.cpp b/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram_in1_mcast.cpp
index 9a26209f38..7bc7453806 100644
--- a/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram_in1_mcast.cpp
+++ b/tests/tt_metal/tt_metal/test_matmul_multi_core_multi_dram_in1_mcast.cpp
@@ -19,6 +19,19 @@
 using std::vector;
 using namespace tt;
 
+void print_vec(const std::vector<bfloat16>& data, int rows, int cols, string name) {
+    std::cout << name << ": " << std::endl;
+    int index = 0;
+    for (int i = 0; i < rows; i++) {
+        for (int j = 0; j < cols; j++) {
+            std::cout << data.at(index).to_float() << ", ";
+            index++;
+        }
+        std::cout << std::endl;
+    }
+    std::cout << std::endl;
+}
+
 std::vector<bfloat16> select_columns(std::vector<bfloat16> data, int M, int K, int N) {
     if (N == K) {
         return data;
@@ -471,11 +484,10 @@ int main(int argc, char** argv) {
                 tt_metal::detail::ReadFromDeviceDRAMChannel(
                     device, dram_bank, dram_address, single_tile_size, result_vec);
                 auto result_bfp16 = unpack_uint32_vec_into_bfloat16_vec(result_vec);
-                auto result_flat_layout =
-                    convert_layout_tile_nfaces_to_tile_swizzled(tt::stl::make_const_span(result_bfp16));
+                auto result_flat_layout = convert_to_flat_layout(tt::stl::make_const_span(result_bfp16));
 
                 // log_info(LogTest, "Tile id {} on dram bank {}, address {}", tile_id, dram_bank, dram_address);
-                // print_vec_of_bfloat16(result_flat_layout, 1, "Result - tile#" + std::to_string(tile_id));
+                // print_vec(result_flat_layout, 32, 32, "Result - tile#" + std::to_string(tile_id));
                 pass &= (golden_tile == result_flat_layout);
             }
         }
diff --git a/tests/tt_metal/tt_metal/test_matmul_multi_core_single_dram.cpp b/tests/tt_metal/tt_metal/test_matmul_multi_core_single_dram.cpp
index 91bfea77cd..5887fe2981 100644
--- a/tests/tt_metal/tt_metal/test_matmul_multi_core_single_dram.cpp
+++ b/tests/tt_metal/tt_metal/test_matmul_multi_core_single_dram.cpp
@@ -39,6 +39,19 @@ std::vector<std::uint32_t> transpose_tiles(
     return result;
 }
 
+void print_vec(const std::vector<bfloat16>& data, int rows, int cols, string name) {
+    std::cout << name << ": " << std::endl;
+    int index = 0;
+    for (int i = 0; i < rows; i++) {
+        for (int j = 0; j < cols; j++) {
+            std::cout << data.at(index).to_float() << ", ";
+            index++;
+        }
+        std::cout << std::endl;
+    }
+    std::cout << std::endl;
+}
+
 void print_faces(std::vector<bfloat16> data, string name) {
     std::cout << name << ": " << std::endl;
     int index = 0;
@@ -385,9 +398,8 @@ int main(int argc, char** argv) {
                     per_core_M * per_core_N * single_tile_size,
                     result_vec);
                 auto result_bfp16 = unpack_uint32_vec_into_bfloat16_vec(result_vec);
-                auto result_flat_layout =
-                    convert_layout_tile_nfaces_to_tile_swizzled(tt::stl::make_const_span(result_bfp16));
-                auto result_untilized = untilize_swizzled(result_flat_layout, per_core_M * 32, per_core_N * 32);
+                auto result_flat_layout = convert_to_flat_layout(tt::stl::make_const_span(result_bfp16));
+                auto result_untilized = untilize(result_flat_layout, per_core_M * 32, per_core_N * 32);
                 pass &= (per_core_golden == result_untilized);
             }
         }
diff --git a/tests/tt_metal/tt_metal/test_matmul_multi_tile.cpp b/tests/tt_metal/tt_metal/test_matmul_multi_tile.cpp
index 53e1d95e41..82d9d84fb6 100644
--- a/tests/tt_metal/tt_metal/test_matmul_multi_tile.cpp
+++ b/tests/tt_metal/tt_metal/test_matmul_multi_tile.cpp
@@ -235,8 +235,8 @@ bool run_matmul(const tt::ARCH& arch, const bool with_bias) {
         //                      Validation & Teardown
         ////////////////////////////////////////////////////////////////////////////
         auto result_bfp16 = unpack_uint32_vec_into_bfloat16_vec(result_vec);
-        auto result_flat_layout = convert_layout_tile_nfaces_to_tile_swizzled(tt::stl::make_const_span(result_bfp16));
-        auto result_untilized = untilize_swizzled(result_flat_layout, M * 32, N * 32);
+        auto result_flat_layout = convert_to_flat_layout(tt::stl::make_const_span(result_bfp16));
+        auto result_untilized = untilize(result_flat_layout, M * 32, N * 32);
 
         pass &= (tensor.get_values() == result_untilized);
 
diff --git a/tests/tt_metal/tt_metal/test_matmul_single_core.cpp b/tests/tt_metal/tt_metal/test_matmul_single_core.cpp
index f1ee53154c..1260c74fe0 100644
--- a/tests/tt_metal/tt_metal/test_matmul_single_core.cpp
+++ b/tests/tt_metal/tt_metal/test_matmul_single_core.cpp
@@ -39,6 +39,19 @@ std::vector<std::uint32_t> transpose_tiles(
     return result;
 }
 
+void print_vec(const std::vector<bfloat16>& data, int rows, int cols, string name) {
+    std::cout << name << ": " << std::endl;
+    int index = 0;
+    for (int i = 0; i < rows; i++) {
+        for (int j = 0; j < cols; j++) {
+            std::cout << data.at(index).to_float() << ", ";
+            index++;
+        }
+        std::cout << std::endl;
+    }
+    std::cout << std::endl;
+}
+
 void print_faces(std::vector<bfloat16> data, string name) {
     std::cout << name << ": " << std::endl;
     int index = 0;
@@ -291,15 +304,15 @@ int main(int argc, char** argv) {
         //                      Validation & Teardown
         ////////////////////////////////////////////////////////////////////////////
         auto result_bfp16 = unpack_uint32_vec_into_bfloat16_vec(result_vec);
-        auto result_flat_layout = convert_layout_tile_nfaces_to_tile_swizzled(tt::stl::make_const_span(result_bfp16));
-        auto result_untilized = untilize_swizzled(result_flat_layout, M * 32, N * 32);
-        // print_vec_of_bfloat16(result_bfp16, 16, "Result bfp16");
+        auto result_flat_layout = convert_to_flat_layout(tt::stl::make_const_span(result_bfp16));
+        auto result_untilized = untilize(result_flat_layout, M * 32, N * 32);
+        // print_vec(result_bfp16, 128, 128, "Result bfp16");
         // print_faces(unpack_uint32_vec_into_bfloat16_vec(activations_tile_transposed), "Activations tile transpose");
         // print_faces(unpack_uint32_vec_into_bfloat16_vec(weights), "Weights tile transposed");
         // print_faces(result_bfp16, "Result bfp16");
         // print_vec_of_uint32_as_packed_bfloat16(weights, 16, "weights tile transposed");
-        // print_vec_of_bfloat16(result_untilized, M*N, "Result");
-        // print_vec_of_bfloat16(tensor.get_values(), 16, "Golden");
+        // print_vec(result_untilized, M*32, N*32, "Result");
+        // print_vec(tensor.get_values(), 128, 128, "Golden");
         auto golden = select_columns(tensor.get_values(), M, K, std::min(K, N));
         // auto golden = tensor.get_values();
         pass &= (golden == result_untilized);
diff --git a/tests/tt_metal/tt_metal/test_matmul_single_core_small.cpp b/tests/tt_metal/tt_metal/test_matmul_single_core_small.cpp
index 2dff120377..42c2feb12c 100644
--- a/tests/tt_metal/tt_metal/test_matmul_single_core_small.cpp
+++ b/tests/tt_metal/tt_metal/test_matmul_single_core_small.cpp
@@ -40,6 +40,19 @@ std::vector<std::uint32_t> transpose_tiles(
     return result;
 }
 
+void print_vec(const std::vector<bfloat16>& data, int rows, int cols, string name) {
+    std::cout << name << ": " << std::endl;
+    int index = 0;
+    for (int i = 0; i < rows; i++) {
+        for (int j = 0; j < cols; j++) {
+            std::cout << data.at(index).to_float() << ", ";
+            index++;
+        }
+        std::cout << std::endl;
+    }
+    std::cout << std::endl;
+}
+
 void print_faces(std::vector<bfloat16> data, string name) {
     std::cout << name << ": " << std::endl;
     int index = 0;
@@ -293,16 +306,15 @@ int main(int argc, char** argv) {
         //                      Validation & Teardown
         ////////////////////////////////////////////////////////////////////////////
         auto result_bfp16 = unpack_uint32_vec_into_bfloat16_vec(result_vec);
-        auto result_flat_layout = convert_layout_tile_nfaces_to_tile_swizzled(tt::stl::make_const_span(result_bfp16));
-        auto result_untilized = untilize_swizzled(result_flat_layout, M * 32, N * 32);
-
-        // print_vec_of_bfloat16(result_bfp16, 16, "Result bfp16");
+        auto result_flat_layout = convert_to_flat_layout(tt::stl::make_const_span(result_bfp16));
+        auto result_untilized = untilize(result_flat_layout, M * 32, N * 32);
+        // print_vec(result_bfp16, 128, 128, "Result bfp16");
         // print_faces(unpack_uint32_vec_into_bfloat16_vec(activations_tile_transposed), "Activations tile transpose");
         // print_faces(unpack_uint32_vec_into_bfloat16_vec(weights), "Weights tile transposed");
         // print_faces(result_bfp16, "Result bfp16");
         // print_vec_of_uint32_as_packed_bfloat16(weights, 16, "weights tile transposed");
-        // print_vec_of_bfloat16(result_untilized, M * N, "Result bfloat16");
-        // print_vec_of_bfloat16(tensor.get_values(), 16, "Golden");
+        // print_vec(result_untilized, M*32, N*32, "Result");
+        // print_vec(tensor.get_values(), 128, 128, "Golden");
         auto golden = select_columns(tensor.get_values(), M, K, std::min(K, N));
         // auto golden = tensor.get_values();
         pass &= tt::test_utils::is_close_vectors<bfloat16>(
diff --git a/tests/tt_metal/tt_metal/test_matmul_single_tile.cpp b/tests/tt_metal/tt_metal/test_matmul_single_tile.cpp
index 490846e4e4..24b876c2ce 100644
--- a/tests/tt_metal/tt_metal/test_matmul_single_tile.cpp
+++ b/tests/tt_metal/tt_metal/test_matmul_single_tile.cpp
@@ -153,7 +153,7 @@ int main(int argc, char** argv) {
         //                      Validation & Teardown
         ////////////////////////////////////////////////////////////////////////////
         auto result_bfp16 = unpack_uint32_vec_into_bfloat16_vec(result_vec);
-        auto result_flat_layout = convert_layout_tile_nfaces_to_tile_swizzled(tt::stl::make_const_span(result_bfp16));
+        auto result_flat_layout = convert_to_flat_layout(tt::stl::make_const_span(result_bfp16));
         pass &= (tensor.get_values() == result_flat_layout);  // src1 is all 0's
         pass &= tt_metal::CloseDevice(device);
 
diff --git a/tests/ttnn/CMakeLists.txt b/tests/ttnn/CMakeLists.txt
index a9d168f426..5992ae3d29 100644
--- a/tests/ttnn/CMakeLists.txt
+++ b/tests/ttnn/CMakeLists.txt
@@ -1,3 +1,32 @@
+# Common function to set up target properties for TTNN tests
+function(setup_ttnn_test_target target_name)
+    target_link_libraries(
+        ${target_name}
+        PUBLIC
+            test_common_libs
+            ttnn
+            Metalium::Metal
+            GTest::gmock_main
+            Python3::Python
+    )
+    target_include_directories(
+        ${target_name}
+        PRIVATE
+            ${UMD_HOME}
+            ${PROJECT_SOURCE_DIR}
+            ${PROJECT_SOURCE_DIR}/tt_metal
+            ${PROJECT_SOURCE_DIR}/tests
+            ${CMAKE_CURRENT_SOURCE_DIR}
+            "$<TARGET_PROPERTY:TT::NN::CPP,INCLUDE_DIRECTORIES>"
+    )
+    set_target_properties(
+        ${target_name}
+        PROPERTIES
+            RUNTIME_OUTPUT_DIRECTORY
+                ${PROJECT_BINARY_DIR}/test/ttnn
+    )
+endfunction()
+
 set(Python3_FIND_STRATEGY LOCATION)
 find_package(Python3 REQUIRED COMPONENTS Development)
 
diff --git a/tests/ttnn/benchmark/cpp/host_alloc_on_tensor_readback.cpp b/tests/ttnn/benchmark/cpp/host_alloc_on_tensor_readback.cpp
index ebe3b03d2b..9918f52c96 100644
--- a/tests/ttnn/benchmark/cpp/host_alloc_on_tensor_readback.cpp
+++ b/tests/ttnn/benchmark/cpp/host_alloc_on_tensor_readback.cpp
@@ -37,8 +37,28 @@ void BM_host_alloc_on_tensor_readback(benchmark::State& state) {
 
 BENCHMARK(BM_host_alloc_on_tensor_readback)
     ->Unit(benchmark::kMicrosecond)
-    ->RangeMultiplier(2)
-    ->Range(1 << 10, 1 << 30);  // 1KB to 1GB, powers of 2
+    ->Iterations(5)
+    ->Arg(1 << 10)    // 1KB
+    ->Arg(2 << 10)    // 2KB
+    ->Arg(4 << 10)    // 4KB
+    ->Arg(8 << 10)    // 8KB
+    ->Arg(16 << 10)   // 16KB
+    ->Arg(32 << 10)   // 32KB
+    ->Arg(64 << 10)   // 64KB
+    ->Arg(128 << 10)  // 128KB
+    ->Arg(256 << 10)  // 256KB
+    ->Arg(512 << 10)  // 512KB
+    ->Arg(1 << 20)    // 1MB
+    ->Arg(2 << 20)    // 2MB
+    ->Arg(4 << 20)    // 4MB
+    ->Arg(8 << 20)    // 8MB
+    ->Arg(16 << 20)   // 16MB
+    ->Arg(32 << 20)   // 32MB
+    ->Arg(64 << 20)   // 64MB
+    ->Arg(128 << 20)  // 128MB
+    ->Arg(256 << 20)  // 256MB
+    ->Arg(512 << 20)  // 512MB
+    ->Arg(1 << 30);   // 1GB
 
 }  // namespace
 
diff --git a/tests/ttnn/unit_tests/gtests/CMakeLists.txt b/tests/ttnn/unit_tests/gtests/CMakeLists.txt
index d8d2886c25..acdfe9ac9a 100644
--- a/tests/ttnn/unit_tests/gtests/CMakeLists.txt
+++ b/tests/ttnn/unit_tests/gtests/CMakeLists.txt
@@ -1,171 +1,82 @@
-set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${PROJECT_BINARY_DIR}/test/ttnn)
-
-function(setup_ttnn_test_target target_name)
-    target_link_libraries(
-        ${target_name}
-        PUBLIC
-            test_common_libs
-            TTNN::CPP
-    )
-    target_include_directories(
-        ${target_name}
-        PRIVATE
-            ${PROJECT_SOURCE_DIR}/tests
-            ${CMAKE_CURRENT_SOURCE_DIR}
-    )
-endfunction()
-
-# unit_tests_ttnn
-
-add_library(unit_tests_ttnn_smoke OBJECT)
-add_library(TTNN::Test::Smoke ALIAS unit_tests_ttnn_smoke)
-TT_ENABLE_UNITY_BUILD(unit_tests_ttnn_smoke)
-target_sources(
-    unit_tests_ttnn_smoke
-    PRIVATE
-        test_matmul_benchmark.cpp
-        test_multiprod_queue.cpp
-        test_multi_cq_multi_dev.cpp
-        test_reflect.cpp
-        test_to_and_from_json.cpp
-        test_sliding_window_infra.cpp
-)
-target_include_directories(unit_tests_ttnn_smoke PRIVATE ${PROJECT_SOURCE_DIR}/tests)
-target_link_libraries(
-    unit_tests_ttnn_smoke
-    PRIVATE
-        test_common_libs
-        TTNN::CPP
+set(TTNN_UNIT_TESTS_SRC
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_add.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_graph_add.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_graph_basic.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_async_runtime.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_matmul_benchmark.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_multiprod_queue.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_multi_cq_multi_dev.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_launch_operation.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_graph_capture_arguments_morehdot.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_graph_capture_arguments_transpose.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_graph_query_op_constraints.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_graph_query_op_runtime.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_reflect.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_to_and_from_json.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_broadcast_to.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_sliding_window_infra.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_conv2d.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/common_test_utils.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/test_generic_op.cpp
 )
 
-add_library(unit_tests_ttnn_basic OBJECT)
-add_library(TTNN::Test::Basic ALIAS unit_tests_ttnn_basic)
-TT_ENABLE_UNITY_BUILD(unit_tests_ttnn_basic)
-target_sources(
-    unit_tests_ttnn_basic
-    PRIVATE
-        common_test_utils.cpp
-        test_add.cpp
-        test_async_runtime.cpp # TODO: Fix memory leak (LSan) then shift-left
-        test_broadcast_to.cpp
-        test_conv2d.cpp # TODO: Fix misaligned memory load (UBSan) then shift-left
-        test_generic_op.cpp
-        test_graph_add.cpp
-        test_graph_basic.cpp
-        test_graph_capture_arguments_morehdot.cpp
-        test_graph_capture_arguments_transpose.cpp
-        test_graph_query_op_constraints.cpp
-        test_graph_query_op_runtime.cpp
-        test_launch_operation.cpp # TODO: Fix data race (TSan) then shift-left
-)
-target_include_directories(unit_tests_ttnn_basic PRIVATE ${PROJECT_SOURCE_DIR}/tests)
-target_link_libraries(
-    unit_tests_ttnn_basic
-    PRIVATE
-        test_common_libs
-        TTNN::CPP
+set(TTNN_CCL_UNIT_TESTS_SRC
+    ${CMAKE_CURRENT_SOURCE_DIR}/ccl/test_erisc_data_mover_with_workers.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/ccl/test_fabric_erisc_data_mover_loopback_with_workers.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/ccl/test_ccl_commands.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/ccl/test_ccl_helpers.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/ccl/test_ccl_tensor_slicers.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/ccl/test_sharded_address_generators_new.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/ccl/test_sharded_address_generators.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/ccl/test_ccl_reduce_scatter_host_helpers.cpp
 )
 
-add_executable(unit_tests_ttnn)
-target_link_libraries(
-    unit_tests_ttnn
-    PRIVATE
-        TTNN::Test::Smoke
-        TTNN::Test::Basic
-)
+set(TTNN_CCL_OP_TESTS_SRC ${CMAKE_CURRENT_SOURCE_DIR}/ccl/test_persistent_fabric_ccl_ops.cpp)
 
-# unit_tests_ttnn_ccl
+set(TTNN_FABRIC_EDM_SRC ${CMAKE_CURRENT_SOURCE_DIR}/ccl/test_fabric_edm.cpp)
+set(TTNN_CCL_MULTI_TENSOR_UNIT_TESTS_SRC ${CMAKE_CURRENT_SOURCE_DIR}/ccl/test_multi_tensor_ccl.cpp)
+set(TTNN_1D_FABRIC_LATENCY_TEST_SRC ${CMAKE_CURRENT_SOURCE_DIR}/ccl/test_1d_fabric_loopback_latency.cpp)
 
-add_executable(unit_tests_ttnn_ccl)
-target_sources(
-    unit_tests_ttnn_ccl
-    PRIVATE
-        ccl/test_ccl_commands.cpp
-        ccl/test_ccl_helpers.cpp
-        ccl/test_ccl_reduce_scatter_host_helpers.cpp
-        ccl/test_ccl_tensor_slicers.cpp
-        ccl/test_erisc_data_mover_with_workers.cpp
-        ccl/test_fabric_erisc_data_mover_loopback_with_workers.cpp
-        ccl/test_sharded_address_generators.cpp
-        ccl/test_sharded_address_generators_new.cpp
-)
-target_include_directories(unit_tests_ttnn_ccl PRIVATE ${PROJECT_SOURCE_DIR}/tests)
-target_link_libraries(
-    unit_tests_ttnn_ccl
-    PRIVATE
-        test_common_libs
-        TTNN::CPP
+set(TTNN_ACCESSOR_UNIT_TESTS_SRC
+    ${CMAKE_CURRENT_SOURCE_DIR}/accessor/test_sharded_accessor.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/accessor/test_sharded_accessor_on_device.cpp
 )
 
-# unit_tests_ttnn_ccl_ops
-
-add_executable(unit_tests_ttnn_ccl_ops)
-target_sources(unit_tests_ttnn_ccl_ops PRIVATE ccl/test_persistent_fabric_ccl_ops.cpp)
-setup_ttnn_test_target(unit_tests_ttnn_ccl_ops)
-
-# unit_tests_ttnn_fabric_edm
-
-add_executable(unit_tests_ttnn_fabric_edm)
-target_sources(unit_tests_ttnn_fabric_edm PRIVATE ccl/test_fabric_edm.cpp)
-setup_ttnn_test_target(unit_tests_ttnn_fabric_edm)
-
-# unit_tests_ttnn_ccl_multi_tensor
-
-add_executable(unit_tests_ttnn_ccl_multi_tensor)
-target_sources(unit_tests_ttnn_ccl_multi_tensor PRIVATE ccl/test_multi_tensor_ccl.cpp)
-setup_ttnn_test_target(unit_tests_ttnn_ccl_multi_tensor)
-
-# unit_tests_ttnn_1d_fabric_latency
-
-add_executable(unit_tests_ttnn_1d_fabric_latency)
-target_sources(unit_tests_ttnn_1d_fabric_latency PRIVATE ccl/test_1d_fabric_loopback_latency.cpp)
-setup_ttnn_test_target(unit_tests_ttnn_1d_fabric_latency)
-
-# unit_tests_ttnn_accessor
-
-add_executable(unit_tests_ttnn_accessor)
-target_sources(
-    unit_tests_ttnn_accessor
-    PRIVATE
-        accessor/test_accessor_benchmarks.cpp
-        accessor/test_sharded_accessor.cpp
-        accessor/test_sharded_accessor_on_device.cpp
+set(TTNN_TENSOR_UNIT_TESTS_SRC
+    ${CMAKE_CURRENT_SOURCE_DIR}/tensor/common_tensor_test_utils.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/tensor/test_create_tensor.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/tensor/test_tensor_layout.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/tensor/test_create_tensor_multi_device.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/tensor/test_create_tensor_with_layout.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/tensor/test_distributed_tensor.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/tensor/test_mesh_tensor.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/tensor/test_partition.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/tensor/test_tensor_serialization.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/tensor/test_tensor_sharding.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/tensor/test_tensor_nd_sharding.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/tensor/test_vector_conversion.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/tensor/test_xtensor_conversion.cpp
 )
-setup_ttnn_test_target(unit_tests_ttnn_accessor)
-
-# unit_tests_ttnn_tensor
 
-add_executable(unit_tests_ttnn_tensor)
-target_sources(
-    unit_tests_ttnn_tensor
-    PRIVATE
-        tensor/common_tensor_test_utils.cpp
-        tensor/test_create_tensor.cpp
-        tensor/test_create_tensor_multi_device.cpp
-        tensor/test_create_tensor_with_layout.cpp
-        tensor/test_distributed_tensor.cpp
-        tensor/test_mesh_tensor.cpp
-        tensor/test_partition.cpp
-        tensor/test_tensor_layout.cpp
-        tensor/test_tensor_nd_sharding.cpp
-        tensor/test_tensor_serialization.cpp
-        tensor/test_tensor_sharding.cpp
-        tensor/test_vector_conversion.cpp
-        tensor/test_xtensor_conversion.cpp
-)
-setup_ttnn_test_target(unit_tests_ttnn_tensor)
+set(EMITC_UNIT_TESTS_SRC ${CMAKE_CURRENT_SOURCE_DIR}/emitc/test_sanity.cpp)
+
+add_executable(unit_tests_ttnn ${TTNN_UNIT_TESTS_SRC})
+TT_ENABLE_UNITY_BUILD(unit_tests_ttnn)
+add_executable(unit_tests_ttnn_ccl ${TTNN_CCL_UNIT_TESTS_SRC})
+add_executable(unit_tests_ttnn_ccl_ops ${TTNN_CCL_OP_TESTS_SRC})
+add_executable(unit_tests_ttnn_fabric_edm ${TTNN_FABRIC_EDM_SRC})
+add_executable(unit_tests_ttnn_ccl_multi_tensor ${TTNN_CCL_MULTI_TENSOR_UNIT_TESTS_SRC})
+add_executable(unit_tests_ttnn_1d_fabric_latency ${TTNN_1D_FABRIC_LATENCY_TEST_SRC})
+add_executable(unit_tests_ttnn_accessor ${TTNN_ACCESSOR_UNIT_TESTS_SRC})
+add_executable(unit_tests_ttnn_tensor ${TTNN_TENSOR_UNIT_TESTS_SRC})
 target_link_libraries(unit_tests_ttnn_tensor PRIVATE xtensor)
-
-# test_ccl_multi_cq_multi_device
-
-add_executable(test_ccl_multi_cq_multi_device)
-target_sources(
+add_executable(
     test_ccl_multi_cq_multi_device
-    PRIVATE
-        multi_thread/test_ccl_multi_cq_multi_device.cpp
-        multi_thread/test_utils.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/multi_thread/test_ccl_multi_cq_multi_device.cpp
+    ${CMAKE_CURRENT_SOURCE_DIR}/multi_thread/test_utils.cpp
 )
-setup_ttnn_test_target(test_ccl_multi_cq_multi_device)
+add_executable(unit_tests_ttnn_emitc ${EMITC_UNIT_TESTS_SRC})
+
 target_link_libraries(
     test_ccl_multi_cq_multi_device
     PRIVATE
@@ -173,8 +84,14 @@ target_link_libraries(
         Boost::lockfree
 )
 
-# unit_tests_ttnn_emitc
-
-add_executable(unit_tests_ttnn_emitc)
-target_sources(unit_tests_ttnn_emitc PRIVATE emitc/test_sanity.cpp)
+# Set up properties for all targets
+setup_ttnn_test_target(unit_tests_ttnn)
+setup_ttnn_test_target(unit_tests_ttnn_ccl)
+setup_ttnn_test_target(unit_tests_ttnn_ccl_ops)
+setup_ttnn_test_target(unit_tests_ttnn_fabric_edm)
+setup_ttnn_test_target(unit_tests_ttnn_ccl_multi_tensor)
+setup_ttnn_test_target(unit_tests_ttnn_1d_fabric_latency)
+setup_ttnn_test_target(unit_tests_ttnn_accessor)
+setup_ttnn_test_target(unit_tests_ttnn_tensor)
+setup_ttnn_test_target(test_ccl_multi_cq_multi_device)
 setup_ttnn_test_target(unit_tests_ttnn_emitc)
diff --git a/tests/ttnn/unit_tests/gtests/ccl/kernels/1D_fabric_latency_datapath_congestion_writer.cpp b/tests/ttnn/unit_tests/gtests/ccl/kernels/1D_fabric_latency_datapath_congestion_writer.cpp
index 7079e61932..191bff4090 100644
--- a/tests/ttnn/unit_tests/gtests/ccl/kernels/1D_fabric_latency_datapath_congestion_writer.cpp
+++ b/tests/ttnn/unit_tests/gtests/ccl/kernels/1D_fabric_latency_datapath_congestion_writer.cpp
@@ -4,8 +4,7 @@
 
 #include "tt_metal/api/tt-metalium/fabric_edm_packet_header.hpp"
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
+#include "ttnn/cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "dataflow_api.h"
 
 #include <cstdint>
diff --git a/tests/ttnn/unit_tests/gtests/ccl/kernels/1D_fabric_loopback_latency_test_writer.cpp b/tests/ttnn/unit_tests/gtests/ccl/kernels/1D_fabric_loopback_latency_test_writer.cpp
index 0f75cc6677..760a2c5fbb 100644
--- a/tests/ttnn/unit_tests/gtests/ccl/kernels/1D_fabric_loopback_latency_test_writer.cpp
+++ b/tests/ttnn/unit_tests/gtests/ccl/kernels/1D_fabric_loopback_latency_test_writer.cpp
@@ -4,8 +4,7 @@
 
 #include "tt_metal/api/tt-metalium/fabric_edm_packet_header.hpp"
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
+#include "ttnn/cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "dataflow_api.h"
 
 #include <cstdint>
diff --git a/tests/ttnn/unit_tests/gtests/ccl/kernels/edm_fabric_writer.cpp b/tests/ttnn/unit_tests/gtests/ccl/kernels/edm_fabric_writer.cpp
index 43574aca24..88bbac7928 100644
--- a/tests/ttnn/unit_tests/gtests/ccl/kernels/edm_fabric_writer.cpp
+++ b/tests/ttnn/unit_tests/gtests/ccl/kernels/edm_fabric_writer.cpp
@@ -3,10 +3,9 @@
 // SPDX-License-Identifier: Apache-2.0
 
 #include "tt_metal/api/tt-metalium/fabric_edm_packet_header.hpp"
+#include "ttnn/cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "dataflow_api.h"
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
 
 #include <cstdint>
 #include <cstddef>
@@ -348,7 +347,7 @@ void kernel_main() {
     fabric_connection.open();
 
     cb_reserve_back(source_l1_cb_index, 1);
-    cb_reserve_back(packet_header_cb, 1);
+    cb_reserve_back(packet_header_cb, packet_header_size_in_headers);
     const auto source_l1_buffer_address = get_write_ptr(source_l1_cb_index);
     const auto packet_header_buffer_address = get_write_ptr(packet_header_cb);
 
diff --git a/tests/ttnn/unit_tests/gtests/ccl/kernels/fabric_erisc_datamover_sender_worker_reader.cpp b/tests/ttnn/unit_tests/gtests/ccl/kernels/fabric_erisc_datamover_sender_worker_reader.cpp
index 7db9e70de3..75e2754ec8 100644
--- a/tests/ttnn/unit_tests/gtests/ccl/kernels/fabric_erisc_datamover_sender_worker_reader.cpp
+++ b/tests/ttnn/unit_tests/gtests/ccl/kernels/fabric_erisc_datamover_sender_worker_reader.cpp
@@ -5,6 +5,7 @@
 #include <cstdint>
 #include "dataflow_api.h"
 #include "debug/dprint.h"
+#include "tt_metal/api/tt-metalium/fabric_edm_packet_header.hpp"
 
 void kernel_main() {
     constexpr bool src_is_dram = get_compile_time_arg_val(0) == 1;
@@ -27,19 +28,16 @@ void kernel_main() {
         // How can I read ahead into the circular buffer so I don't have to do an async read barrier for
         // every page? I only want to block when the CB is full
         uint32_t pages_to_read = std::min<uint32_t>(pages_per_edm_buffer, num_pages_to_read_total - num_pages_read);
-
         cb_reserve_back(cb_id_in0, pages_to_read);
-
         uint32_t local_l1_read_addr = get_write_ptr(cb_id_in0);
+        local_l1_read_addr += sizeof(PACKET_HEADER_TYPE);
 
         for (uint32_t p = 0; p < pages_to_read; ++p) {
             uint64_t src_noc_addr = get_noc_addr(num_pages_read + p, source_address_generator);
             noc_async_read(src_noc_addr, local_l1_read_addr, page_size);
             local_l1_read_addr += page_size;
         }
-
         noc_async_read_barrier();
-
         cb_push_back(cb_id_in0, pages_to_read);
     }
 }
diff --git a/tests/ttnn/unit_tests/gtests/ccl/kernels/fabric_erisc_datamover_sender_worker_sender.cpp b/tests/ttnn/unit_tests/gtests/ccl/kernels/fabric_erisc_datamover_sender_worker_sender.cpp
index 489e812c42..0c97248e2f 100644
--- a/tests/ttnn/unit_tests/gtests/ccl/kernels/fabric_erisc_datamover_sender_worker_sender.cpp
+++ b/tests/ttnn/unit_tests/gtests/ccl/kernels/fabric_erisc_datamover_sender_worker_sender.cpp
@@ -7,9 +7,8 @@
 #include "dataflow_api.h"
 #include "tt_metal/api/tt-metalium/fabric_edm_packet_header.hpp"
 #include "tt_metal/fabric/hw/inc/edm_fabric/edm_fabric_worker_adapters.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
+#include "ttnn/cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_stream_regs.hpp"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
 
 struct unicast_mode {
     uint8_t distance;
@@ -73,7 +72,6 @@ void kernel_main() {
     ASSERT(worker_buffer_index_semaphore_addr != reinterpret_cast<size_t>(writer_send_sem_addr));
     ASSERT(worker_buffer_index_semaphore_addr != reinterpret_cast<size_t>(worker_teardown_sem_addr));
     ASSERT(worker_buffer_index_semaphore_addr != reinterpret_cast<size_t>(last_message_semaphore_address));
-    auto packet_header_buffer_cb_id = get_arg_val<uint32_t>(arg_idx++);
 
     transmit_config config;
     if (mcast_mode) {
@@ -98,7 +96,7 @@ void kernel_main() {
 
         edm_connection_handshake_id,
         edm_worker_location_info_addr,
-        edm_buffer_size_bytes + sizeof(PACKET_HEADER_TYPE),
+        edm_buffer_size_bytes,
         edm_buffer_index_id,
         writer_send_sem_addr,
         worker_teardown_sem_addr,
@@ -118,21 +116,17 @@ void kernel_main() {
 
     uint32_t buffer_index = 0;
     cb_wait_front(cb_id_in0, 1);
-
-    cb_reserve_back(packet_header_buffer_cb_id, 1);
-
-    auto packet_header_addr = get_write_ptr(packet_header_buffer_cb_id);
-    auto* packet_header = reinterpret_cast<volatile tt_l1_ptr PACKET_HEADER_TYPE*>(packet_header_addr);
+    auto a_packet_header_addr = get_read_ptr(cb_id_in0);
     for (uint32_t p = 0; p < total_pages_to_send; p += num_pages_per_send) {
         uint32_t pages_to_send = std::min<uint32_t>(num_pages_per_send, total_pages_to_send - p);
-
         sender.wait_for_empty_write_slot();
-
         cb_wait_front(cb_id_in0, pages_to_send);
 
         // bit of a hack to extract X/Y
         const auto dest_noc_address = get_noc_addr(p, dest_addr_gen, 0, NORMALIZED_NOC_INDEX);
-        auto payload_addr = get_read_ptr(cb_id_in0);
+        const size_t packet_size = page_size + sizeof(PACKET_HEADER_TYPE);
+        auto packet_addr = get_read_ptr(cb_id_in0);
+        auto* packet_header = reinterpret_cast<volatile PACKET_HEADER_TYPE*>(packet_addr);
         if constexpr (mcast_mode) {
             packet_header
                 ->to_chip_multicast(
@@ -145,9 +139,7 @@ void kernel_main() {
                     tt::tt_fabric::NocUnicastCommandHeader{dest_noc_address}, (pages_to_send * page_size));
         }
 
-        sender.send_payload_without_header_non_blocking_from_address(payload_addr, pages_to_send * page_size);
-        sender.send_payload_flush_non_blocking_from_address((uint32_t)packet_header, sizeof(PACKET_HEADER_TYPE));
-
+        sender.send_payload_blocking_from_address(packet_addr, packet_size);
         noc_async_writes_flushed();
         cb_pop_front(cb_id_in0, pages_to_send);
     }
@@ -155,14 +147,16 @@ void kernel_main() {
     if constexpr (!mcast_mode) {
         sender.wait_for_empty_write_slot();
 
+        auto& packet_header = *reinterpret_cast<PACKET_HEADER_TYPE*>(a_packet_header_addr);
         ASSERT(*last_message_semaphore_address == 0);
         uint64_t last_message_semaphore_noc0_addr =
             safe_get_noc_addr(my_x[0], my_y[0], (uint32_t)last_message_semaphore_address, 0);
-        packet_header->to_chip_unicast(2);
-        packet_header->to_noc_unicast_atomic_inc(
+        packet_header.to_chip_unicast(2);
+        packet_header.to_noc_unicast_atomic_inc(
             tt::tt_fabric::NocUnicastAtomicIncCommandHeader(last_message_semaphore_noc0_addr, 1, 32));
 
-        sender.send_payload_flush_non_blocking_from_address((uint32_t)packet_header, sizeof(PACKET_HEADER_TYPE));
+        sender.send_payload_blocking_from_address(
+            a_packet_header_addr, packet_header.get_payload_size_including_header());
 
         noc_semaphore_wait(last_message_semaphore_address, 1);
     }
diff --git a/tests/ttnn/unit_tests/gtests/ccl/test_1d_fabric_loopback_latency.cpp b/tests/ttnn/unit_tests/gtests/ccl/test_1d_fabric_loopback_latency.cpp
index 54cbe53095..dc13c4c91e 100644
--- a/tests/ttnn/unit_tests/gtests/ccl/test_1d_fabric_loopback_latency.cpp
+++ b/tests/ttnn/unit_tests/gtests/ccl/test_1d_fabric_loopback_latency.cpp
@@ -104,9 +104,7 @@ inline void RunPersistent1dFabricLatencyTest(
         }
     }
 
-    // Temporary until we move this to be under tt_metal and migrate to device init fabric
-    // OR packet header management is removed from user space, whichever comes first
-    constexpr size_t packet_header_size_bytes = sizeof(tt::tt_fabric::PacketHeader);
+    // static constexpr size_t source_l1_buffer_address = 1000000;
     static constexpr uint32_t packet_header_cb_index = tt::CB::c_in0;
     static constexpr uint32_t source_payload_cb_index = tt::CB::c_in1;
     static constexpr size_t packet_header_cb_size_in_headers = 4;
@@ -265,8 +263,9 @@ inline void RunPersistent1dFabricLatencyTest(
         // reserve CB
         tt_metal::CircularBufferConfig cb_src0_config =
             tt_metal::CircularBufferConfig(
-                packet_header_cb_size_in_headers * packet_header_size_bytes, {{packet_header_cb_index, cb_df}})
-                .set_page_size(packet_header_cb_index, packet_header_size_bytes);
+                packet_header_cb_size_in_headers * sizeof(tt::tt_fabric::PacketHeader),
+                {{packet_header_cb_index, cb_df}})
+                .set_page_size(packet_header_cb_index, sizeof(tt::tt_fabric::PacketHeader));
         CBHandle sender_workers_cb = CreateCircularBuffer(program, worker_cores, cb_src0_config);
 
         if (!use_device_init_fabric) {
diff --git a/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm.cpp b/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm.cpp
index c8e764b3a4..3b1cdb2687 100644
--- a/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm.cpp
+++ b/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm.cpp
@@ -155,8 +155,7 @@ static int run_single_test(
             auto& params = std::get<WriteThroughputStabilityTestWithPersistentFabricParams>(test_params.params);
             if (params.fabric_mode == FabricTestMode::Linear) {
                 Run1DFabricPacketSendTest<Fabric1DLineDeviceInitFixture>(test_fixture, test_specs, params);
-            } else if (
-                params.fabric_mode == FabricTestMode::HalfRing || params.fabric_mode == FabricTestMode::FullRing) {
+            } else if (params.fabric_mode == FabricTestMode::FullRing) {
                 Run1DFabricPacketSendTest<Fabric1DRingDeviceInitFixture>(test_fixture, test_specs, params);
             } else {
                 TT_THROW(
diff --git a/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp b/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp
index 344ee4f17e..d3e4d3a20d 100644
--- a/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp
+++ b/tests/ttnn/unit_tests/gtests/ccl/test_fabric_edm_common.hpp
@@ -14,7 +14,6 @@
 #include <tt-metalium/fabric.hpp>
 #include "tt_metal/test_utils/df/df.hpp"
 #include "tt_metal/test_utils/env_vars.hpp"
-#include "tt_metal/common/executor.hpp"
 #include <tt-metalium/fabric_edm_packet_header.hpp>
 
 #include "ttnn/common/queue_id.hpp"
@@ -98,7 +97,7 @@ protected:
         arch_ = tt::get_arch_from_string(tt::test_utils::get_umd_arch_name());
         num_devices_ = tt::tt_metal::GetNumAvailableDevices();
 
-        if (!(num_devices_ >= 8 &&
+        if (!(arch_ == tt::ARCH::WORMHOLE_B0 && num_devices_ >= 8 &&
               (tt::tt_metal::GetNumPCIeDevices() == 4 || tt::tt_metal::GetNumPCIeDevices() == GALAXY_6U_NUM_DEVICES))) {
             TT_THROW("This suite can only be run on T3000 or TG Wormhole devices");
         }
@@ -108,10 +107,10 @@ public:
     BaseFabricFixture() : device_open(false) {}
 
     BaseFabricFixture(tt::tt_metal::FabricConfig fabric_config) : device_open(false) {
-        tt::tt_metal::detail::SetFabricConfig(fabric_config);
+        tt::tt_metal::detail::InitializeFabricConfig(fabric_config);
     }
 
-    virtual ~BaseFabricFixture() { tt::tt_metal::detail::SetFabricConfig(tt::tt_metal::FabricConfig::DISABLED); }
+    virtual ~BaseFabricFixture() { tt::tt_metal::detail::InitializeFabricConfig(tt::tt_metal::FabricConfig::DISABLED); }
 
     virtual void SetupDevices() = 0;
     virtual void TearDown() = 0;
@@ -346,40 +345,32 @@ std::tuple<std::shared_ptr<Buffer>, std::vector<uint32_t>> build_input_buffer(
     return {local_input_buffer, inputs};
 }
 
-template <typename ProgramContainer>
 static void build_and_enqueue(
-    const std::vector<IDevice*>& devices, ProgramContainer& programs, bool enqueue_only = false) {
-    static_assert(
-        std::is_same_v<ProgramContainer, std::vector<Program*>> ||
-            std::is_same_v<ProgramContainer, std::vector<Program>>,
-        "programs must be a vector of Program* or Program");
+    const std::vector<IDevice*>& devices, std::vector<Program>& programs, bool enqueue_only = false) {
     TT_FATAL(
         devices.size() == programs.size(),
         "Number of devices must match number of programs when calling build_and_enqueue in test");
-
-    // Parallel compile and enqueue as a single atomic operation per device
-    std::vector<std::shared_future<void>> futures;
-    futures.reserve(devices.size());
-
+    if (!enqueue_only) {
+        for (size_t i = 0; i < devices.size(); i++) {
+            tt::tt_metal::detail::CompileProgram(devices[i], programs[i]);
+        }
+    }
     for (size_t i = 0; i < devices.size(); i++) {
-        futures.emplace_back(tt::tt_metal::detail::async([&devices, &programs, i, enqueue_only]() {
-            if constexpr (std::is_same_v<ProgramContainer, std::vector<Program*>>) {
-                if (!enqueue_only) {
-                    tt::tt_metal::detail::CompileProgram(devices[i], *programs[i]);
-                }
-                tt_metal::EnqueueProgram(devices[i]->command_queue(), *programs[i], false);
-            } else {
-                if (!enqueue_only) {
-                    tt::tt_metal::detail::CompileProgram(devices[i], programs[i]);
-                }
-                tt_metal::EnqueueProgram(devices[i]->command_queue(), programs[i], false);
-            }
-        }));
+        tt_metal::EnqueueProgram(devices[i]->command_queue(), programs[i], false);
     }
-
-    // Wait for all compile and enqueue operations to complete
-    for (const auto& future : futures) {
-        future.get();
+}
+static void build_and_enqueue(
+    const std::vector<IDevice*>& devices, std::vector<Program*>& program_ptrs, bool enqueue_only = false) {
+    TT_FATAL(
+        devices.size() == program_ptrs.size(),
+        "Number of devices must match number of programs when calling build_and_enqueue in test");
+    if (!enqueue_only) {
+        for (size_t i = 0; i < devices.size(); i++) {
+            tt::tt_metal::detail::CompileProgram(devices[i], *program_ptrs[i]);
+        }
+    }
+    for (size_t i = 0; i < devices.size(); i++) {
+        tt_metal::EnqueueProgram(devices[i]->command_queue(), *program_ptrs[i], false);
     }
 }
 
@@ -409,8 +400,8 @@ void generate_sender_worker_kernels(
     const CoreCoord& worker_core,
     const tt::tt_fabric::SenderWorkerAdapterSpec& worker_fabric_connection,
     const mode_variant_t& mode,
-    std::size_t edm_buffer_size_no_header,
-    uint32_t page_size,
+    std::size_t edm_buffer_size,
+    uint32_t page_plus_header_size,
     uint32_t num_pages_total,
     uint32_t num_pages_per_edm_buffer,
     uint32_t local_worker_fabric_semaphore_id,
@@ -420,13 +411,12 @@ void generate_sender_worker_kernels(
     bool src_is_dram,
     uint32_t dram_output_buffer_base_addr,
     bool dest_is_dram,
-    uint32_t worker_buffer_index_semaphore_id,
-    uint32_t packet_header_buffer_cb_id) {
+    uint32_t worker_buffer_index_semaphore_id) {
     const auto& edm_noc_core = CoreCoord(worker_fabric_connection.edm_noc_x, worker_fabric_connection.edm_noc_y);
     std::vector<uint32_t> sender_worker_reader_compile_args{
         src_is_dram,      //
         num_pages_total,  //
-        page_size,
+        page_plus_header_size - PACKET_HEADER_SIZE_BYTES,
         num_pages_per_edm_buffer};
     std::vector<uint32_t> sender_worker_reader_runtime_args{dram_input_buffer_base_addr};
 
@@ -442,7 +432,7 @@ void generate_sender_worker_kernels(
     std::vector<uint32_t> sender_worker_writer_compile_args{
         num_pages_per_edm_buffer,
         num_pages_total,
-        page_size,
+        page_plus_header_size - PACKET_HEADER_SIZE_BYTES,
         worker_fabric_connection.num_buffers_per_channel,
         dest_is_dram,
         std::holds_alternative<mcast_send>(mode) ? 1 : 0};
@@ -462,12 +452,11 @@ void generate_sender_worker_kernels(
 
         worker_fabric_connection.edm_connection_handshake_addr,
         worker_fabric_connection.edm_worker_location_info_addr,
-        edm_buffer_size_no_header,
+        edm_buffer_size,
         dram_output_buffer_base_addr,
         local_worker_last_message_semaphore_id,
         worker_buffer_index_semaphore_id,
-        worker_fabric_connection.buffer_index_semaphore_id,
-        packet_header_buffer_cb_id};
+        worker_fabric_connection.buffer_index_semaphore_id};
 
     if (std::holds_alternative<mcast_send>(mode)) {
         sender_worker_writer_runtime_args.push_back(std::get<mcast_send>(mode).distance);
@@ -487,12 +476,12 @@ void generate_sender_worker_kernels(
     }
 
     // Just want a dummy DF
-    tt::DataFormat df = page_size == 1024   ? tt::DataFormat::Bfp8
-                        : page_size == 2048 ? tt::DataFormat::Float16
-                                            : tt::DataFormat::Float32;
+    tt::DataFormat df = (page_plus_header_size - PACKET_HEADER_SIZE_BYTES) == 1024   ? tt::DataFormat::Bfp8
+                        : (page_plus_header_size - PACKET_HEADER_SIZE_BYTES) == 2048 ? tt::DataFormat::Float16
+                                                                                     : tt::DataFormat::Float32;
     tt_metal::CircularBufferConfig cb_src0_config =
-        tt_metal::CircularBufferConfig(2 * num_pages_per_edm_buffer * page_size, {{src0_cb_index, df}})
-            .set_page_size(src0_cb_index, page_size);
+        tt_metal::CircularBufferConfig(2 * num_pages_per_edm_buffer * page_plus_header_size, {{src0_cb_index, df}})
+            .set_page_size(src0_cb_index, page_plus_header_size);
     CBHandle sender_workers_cb = CreateCircularBuffer(program, worker_core, cb_src0_config);
     auto sender_worker_reader_kernel = tt_metal::CreateKernel(
         program,
@@ -529,6 +518,7 @@ bool RunLoopbackTest(
     tt::tt_fabric::FabricEriscDatamoverBuilder& chip_0_edm_builder,
     std::optional<SubdeviceInfo>& subdevice_managers) {
     auto& sender_program = programs.at(0);
+    std::size_t page_plus_header_size = page_size + sizeof(tt::tt_fabric::PacketHeader);
     std::size_t tensor_size_bytes = num_pages_total * page_size;
 
     std::vector<CoreCoord> worker_cores = {CoreCoord(0, 0)};
@@ -557,15 +547,6 @@ bool RunLoopbackTest(
     auto local_output_buffer = CreateBuffer(InterleavedBufferConfig{
         sender_device, test_config.size_bytes, test_config.page_size_bytes, test_config.output_buffer_type});
 
-    uint32_t packet_header_buffer_cb_id = tt::CBIndex::c_1;
-    // allocate a circular buffer of size 8k
-    constexpr size_t packet_header_buffer_size = 8192;
-    tt_metal::CircularBufferConfig packet_header_buffer_config =
-        tt_metal::CircularBufferConfig(
-            packet_header_buffer_size, {{packet_header_buffer_cb_id, tt::DataFormat::Float16}})
-            .set_page_size(packet_header_buffer_cb_id, page_size);
-    auto packet_header_buffer =
-        tt_metal::CreateCircularBuffer(sender_program, worker_cores.at(0), packet_header_buffer_config);
     tt_metal::detail::WriteToBuffer(local_output_buffer, all_zeros);
 
     auto local_input_buffer_address = local_input_buffer->address();
@@ -575,26 +556,29 @@ bool RunLoopbackTest(
     // EDM Builder Setup
     ////////////////////////////////////////////////////////////////////////////
 
-    const std::size_t edm_buffer_size_no_header =
-        tt::tt_fabric::FabricEriscDatamoverBuilder::default_packet_payload_size_bytes;
+    static constexpr std::size_t edm_buffer_size =
+        tt::tt_fabric::FabricEriscDatamoverBuilder::default_packet_payload_size_bytes + PACKET_HEADER_SIZE_BYTES;
 
     auto chip0_worker_fabric_connection = chip_0_edm_builder.build_connection_to_worker_channel();
     ////////////////////////////////////////////////////////////////////////////
     // Build Workers
     ////////////////////////////////////////////////////////////////////////////
     log_trace(tt::LogTest, "Generating local_sender -> remote_receiver workers");
-    const std::size_t pages_per_send = chip0_worker_fabric_connection.buffer_size_bytes / page_size;
+    const std::size_t pages_per_send =
+        (chip0_worker_fabric_connection.buffer_size_bytes - PACKET_HEADER_SIZE_BYTES) / page_size;
     const auto& worker_core = worker_cores.at(0);
     log_trace(tt::LogTest, "Worker {}. On Core x={},y={}", 0, worker_core.x, worker_core.y);
 
+    const auto& edm_config = tt::tt_fabric::FabricEriscDatamoverConfig(edm_buffer_size);
+
     generate_sender_worker_kernels(
         sender_program,
         sender_device,
         worker_core,
         chip0_worker_fabric_connection,
         unicast_send{2},  // 2 hops because we are looping back to ourselves
-        edm_buffer_size_no_header,
-        page_size,
+        edm_buffer_size,
+        page_plus_header_size,
         num_pages_total,
         pages_per_send,
         local_worker_fabric_semaphore_id,
@@ -604,8 +588,7 @@ bool RunLoopbackTest(
         src_is_dram,
         local_output_buffer_address,
         dest_is_dram,
-        worker_buffer_index_semaphore_id,
-        packet_header_buffer_cb_id);
+        worker_buffer_index_semaphore_id);
 
     ////////////////////////////////////////////////////////////////////////////
     //                      Compile and Execute Application
@@ -749,7 +732,7 @@ void generate_multi_input_test_worker_kernels_for_local_tensor_write(
     size_t first_cb_index,
     size_t second_cb_index,
     const CoreCoord& worker_core,
-    const uint32_t page_size,
+    const uint32_t page_plus_header_size,
     const uint32_t num_pages_per_edm_buffer,
     const ttnn::ccl::v2::TensorSlice& in0_tensor_slice,
     const ttnn::ccl::v2::TensorSlice& in1_tensor_slice,
@@ -760,20 +743,21 @@ void generate_multi_input_test_worker_kernels_for_local_tensor_write(
     std::optional<tt::tt_fabric::SenderWorkerAdapterSpec>& chip0_worker_backward_fabric_connection,
     const ttnn::ccl::cmd::CclCommandDestArgs& dest_args) {
     // Just want a dummy DF
-    tt::DataFormat df = page_size == 1024   ? tt::DataFormat::Bfp8
-                        : page_size == 2048 ? tt::DataFormat::Float16
-                                            : tt::DataFormat::Float32;
+    tt::DataFormat df = (page_plus_header_size - PACKET_HEADER_SIZE_BYTES) == 1024   ? tt::DataFormat::Bfp8
+                        : (page_plus_header_size - PACKET_HEADER_SIZE_BYTES) == 2048 ? tt::DataFormat::Float16
+                                                                                     : tt::DataFormat::Float32;
 
     {
         tt_metal::CircularBufferConfig cb_src0_config =
-            tt_metal::CircularBufferConfig(2 * num_pages_per_edm_buffer * page_size, {{first_cb_index, df}})
-                .set_page_size(first_cb_index, page_size);
+            tt_metal::CircularBufferConfig(2 * num_pages_per_edm_buffer * page_plus_header_size, {{first_cb_index, df}})
+                .set_page_size(first_cb_index, page_plus_header_size);
         CBHandle cb0 = CreateCircularBuffer(program, worker_core, cb_src0_config);
     }
     {
         tt_metal::CircularBufferConfig cb_src1_config =
-            tt_metal::CircularBufferConfig(2 * num_pages_per_edm_buffer * page_size, {{second_cb_index, df}})
-                .set_page_size(second_cb_index, page_size);
+            tt_metal::CircularBufferConfig(
+                2 * num_pages_per_edm_buffer * page_plus_header_size, {{second_cb_index, df}})
+                .set_page_size(second_cb_index, page_plus_header_size);
         CBHandle cb1 = CreateCircularBuffer(program, worker_core, cb_src1_config);
     }
 
@@ -782,7 +766,7 @@ void generate_multi_input_test_worker_kernels_for_local_tensor_write(
         {first_cb_index, second_cb_index},
         {&input_tensor0, &input_tensor1},
         device,
-        page_size,
+        page_plus_header_size - PACKET_HEADER_SIZE_BYTES,
         CoreRangeSet({CoreRange(worker_core)}),
         num_pages_per_edm_buffer,
         in0_tensor_slice,
@@ -799,7 +783,7 @@ void generate_multi_input_test_worker_kernels_for_local_tensor_write(
         {first_cb_index, second_cb_index},
         {&output_tensor0, &output_tensor1},
         device,
-        page_size,
+        page_plus_header_size - PACKET_HEADER_SIZE_BYTES,
         CoreRangeSet({CoreRange(worker_core)}),
         num_pages_per_edm_buffer,
         out0_tensor_slice,
@@ -871,6 +855,8 @@ bool RunLocalTestWithMultiInputReaders(
             std::holds_alternative<ttnn::ccl::cmd::DestTypeArgsNull>(dest_args), "Local command dest args expected");
     }
 
+    std::size_t page_plus_header_size = page_size + sizeof(tt::tt_fabric::PacketHeader);
+
     auto first_cb_index = tt::CB::c_in0;
     auto second_cb_index = tt::CB::c_in1;
 
@@ -920,7 +906,7 @@ bool RunLocalTestWithMultiInputReaders(
         first_cb_index,
         second_cb_index,
         worker_core,
-        page_size,
+        page_plus_header_size,
         num_pages_per_edm_buffer,
         in0_tensor_slice,
         in1_tensor_slice,
@@ -995,10 +981,11 @@ bool RunLineFabricTest(
 
     std::optional<SubdeviceInfo>& subdevice_managers,
     ttnn::ccl::EdmLineFabricOpInterface& line_fabric) {
+    std::size_t page_plus_header_size = page_size + sizeof(tt::tt_fabric::PacketHeader);
     std::size_t tensor_size_bytes = num_pages_total * page_size;
 
-    const std::size_t edm_buffer_size_no_header =
-        tt::tt_fabric::FabricEriscDatamoverBuilder::default_packet_payload_size_bytes;
+    static constexpr std::size_t edm_buffer_size =
+        tt::tt_fabric::FabricEriscDatamoverBuilder::default_packet_payload_size_bytes + PACKET_HEADER_SIZE_BYTES;
     const size_t local_chip_id = 0;
     const size_t remote_chip_id = 1;
     auto program_ptrs = std::vector<Program*>(devices.size());
@@ -1043,16 +1030,6 @@ bool RunLineFabricTest(
     });
     TT_ASSERT(all_same_addr, "All output buffers must have the same address");
 
-    uint32_t packet_header_buffer_cb_id = tt::CBIndex::c_1;
-    // allocate a circular buffer of size 8k
-    constexpr size_t packet_header_buffer_size = 8192;
-    tt_metal::CircularBufferConfig packet_header_buffer_config =
-        tt_metal::CircularBufferConfig(
-            packet_header_buffer_size, {{packet_header_buffer_cb_id, tt::DataFormat::Float16}})
-            .set_page_size(packet_header_buffer_cb_id, page_size);
-    auto packet_header_buffer =
-        tt_metal::CreateCircularBuffer(programs[0], worker_cores.at(0), packet_header_buffer_config);
-
     ////////////////////////////////////////////////////////////////////////////
     //   Setup Semaphores and Builders
     ////////////////////////////////////////////////////////////////////////////
@@ -1071,15 +1048,16 @@ bool RunLineFabricTest(
     auto chip0_worker_fabric_connection =
         line_fabric.uniquely_connect_worker(devices[0], ttnn::ccl::EdmLineFabricOpInterface::FORWARD);
 
-    const std::size_t pages_per_send = chip0_worker_fabric_connection.buffer_size_bytes / page_size;
+    const std::size_t pages_per_send =
+        (chip0_worker_fabric_connection.buffer_size_bytes - PACKET_HEADER_SIZE_BYTES) / page_size;
     generate_sender_worker_kernels(
         programs[0],
         devices[0],
         worker_core,
         chip0_worker_fabric_connection,
         mcast_send{mcast_first_chip, mcast_last_chip - mcast_first_chip + 1},
-        edm_buffer_size_no_header,
-        page_size,
+        edm_buffer_size,
+        page_plus_header_size,
         num_pages_total,
         pages_per_send,
         local_worker_fabric_semaphore_id,
@@ -1089,8 +1067,7 @@ bool RunLineFabricTest(
         src_is_dram,
         local_output_buffer_address,
         dest_is_dram,
-        worker_buffer_index_semaphore_id,
-        packet_header_buffer_cb_id);
+        worker_buffer_index_semaphore_id);
 
     ////////////////////////////////////////////////////////////////////////////
     //                      Compile and Execute Application
@@ -1312,13 +1289,11 @@ int TestLoopbackEntrypoint(
     IDevice* sender_device = device_0;
     IDevice* receiver_device = device_1;
 
-    const std::size_t edm_buffer_size = tt::tt_fabric::FabricEriscDatamoverBuilder::default_packet_payload_size_bytes;
+    static constexpr std::size_t edm_buffer_size =
+        tt::tt_fabric::FabricEriscDatamoverBuilder::default_packet_payload_size_bytes + PACKET_HEADER_SIZE_BYTES;
     const chip_id_t local_chip_id = 0;
     const chip_id_t remote_chip_id = 1;
-    // Note this is a fabric level test so we can keep the use of packet header here (we aren't using the
-    // fabric API yet because that requires us to use device init fabric, which doesn't implement the loopback
-    // that we take advantage of here)
-    const auto& edm_config = tt::tt_fabric::FabricEriscDatamoverConfig(edm_buffer_size + sizeof(PACKET_HEADER_TYPE));
+    const auto& edm_config = tt::tt_fabric::FabricEriscDatamoverConfig(edm_buffer_size);
     auto chip_0_edm_builder = tt::tt_fabric::FabricEriscDatamoverBuilder::build(
         sender_device, fabric_sender_program, eth_sender_core, local_chip_id, remote_chip_id, edm_config);
     chip_0_edm_builder.set_firmware_context_switch_interval(0);
@@ -2510,7 +2485,6 @@ void Run1DFabricPacketSendTest(
             params.fabric_mode == FabricTestMode::RingAsLinear,
         "This test can only be run with disable_end_workers_in_backward_direction set to true or fabric_mode set to "
         "Linear");
-    bool use_t3k = num_devices == 8;
     bool use_galaxy = num_devices == 32;
     bool use_tg = use_galaxy && tt::tt_metal::GetNumPCIeDevices() == 4;
     bool is_6u_galaxy = use_galaxy && tt::tt_metal::GetNumPCIeDevices() == 32;
@@ -2530,9 +2504,9 @@ void Run1DFabricPacketSendTest(
         !(params.num_fabric_rows > 0 && params.num_fabric_cols > 0),
         "Only one of num_fabric_rows and num_fabric_cols may be greater than 0. Test support for both axes live at the "
         "same time is not yet supported");
-    if (use_device_init_fabric && params.num_fabric_rows == 0 && params.num_fabric_cols == 0) {
-        TT_FATAL(use_t3k, "Using the full mesh as one ring topoplogy is only supported for T3K");
-    }
+    TT_FATAL(
+        use_device_init_fabric ^ (params.num_fabric_rows == 0 && params.num_fabric_cols == 0),
+        "Device init fabric is only supported in this test when launching with multiple fabric rows and/or columns");
 
     ttnn::ccl::Topology topology;
     FabricTestMode fabric_mode = params.fabric_mode;
@@ -2600,6 +2574,7 @@ void Run1DFabricPacketSendTest(
     std::optional<ttnn::ccl::EdmLineFabricOpInterface> fabric_handle = std::nullopt;
     std::optional<SubdeviceInfo> subdevice_managers = std::nullopt;
     std::optional<std::vector<Program>> fabric_programs = std::nullopt;
+    size_t packet_header_size_bytes = 0;
     if (!use_device_init_fabric) {
         std::vector<Program*> fabric_program_ptrs;
         TT_FATAL(
@@ -2619,7 +2594,12 @@ void Run1DFabricPacketSendTest(
             en_dateline_receiver_extra_buffer,
             en_dateline_upstream_sender_extra_buffer,
             en_dateline_upstream_receiver_extra_buffer);
+        packet_header_size_bytes = sizeof(tt::tt_fabric::PacketHeader);
+    } else {
+        // TODO: get packet header size from control plane after it adds APIs to present this information
+        packet_header_size_bytes = sizeof(tt::tt_fabric::PacketHeader);
     }
+    TT_FATAL(packet_header_size_bytes != 0, "Error in initializing local variable `packet_header_size_bytes`");
 
     // Other boiler plate setup
     std::vector<std::vector<CoreCoord>> worker_cores_vec_per_device;
@@ -2739,10 +2719,10 @@ void Run1DFabricPacketSendTest(
             }
 
             // reserve CB
-            constexpr size_t packet_header_buffer_size = 8192;
             tt_metal::CircularBufferConfig cb_src0_config =
-                tt_metal::CircularBufferConfig(packet_header_buffer_size, {{packet_header_cb_index, cb_df}})
-                    .set_page_size(packet_header_cb_index, packet_header_buffer_size);
+                tt_metal::CircularBufferConfig(
+                    packet_header_cb_size_in_headers * packet_header_size_bytes, {{packet_header_cb_index, cb_df}})
+                    .set_page_size(packet_header_cb_index, packet_header_size_bytes);
             CBHandle sender_workers_cb = CreateCircularBuffer(program, worker_cores, cb_src0_config);
 
             tt_metal::CircularBufferConfig cb_src1_config =
@@ -2777,8 +2757,6 @@ void Run1DFabricPacketSendTest(
                                              size_t link,
                                              bool is_connected_in_direction,
                                              IDevice* connected_device,
-                                             // not updated to CCL line direction because this is a metal/fabric level
-                                             // test
                                              ttnn::ccl::EdmLineFabricOpInterface::Direction direction,
                                              std::vector<uint32_t>& rt_args_out) {
                 rt_args_out.push_back(is_connected_in_direction);
@@ -3744,10 +3722,11 @@ void RunRingDeadlockStabilityTestWithPersistentFabric(
         mcast_bwd_hops = has_backward_connection ? line_size - 1 : 0;
 
         // reserve CB
-        constexpr size_t packet_header_buffer_size = 8192;
         tt_metal::CircularBufferConfig cb_src0_config =
-            tt_metal::CircularBufferConfig(packet_header_buffer_size, {{packet_header_cb_index, cb_df}})
-                .set_page_size(packet_header_cb_index, packet_header_buffer_size);
+            tt_metal::CircularBufferConfig(
+                packet_header_cb_size_in_headers * sizeof(tt::tt_fabric::PacketHeader),
+                {{packet_header_cb_index, cb_df}})
+                .set_page_size(packet_header_cb_index, sizeof(tt::tt_fabric::PacketHeader));
         CBHandle sender_workers_cb = CreateCircularBuffer(program, worker_cores, cb_src0_config);
 
         tt_metal::CircularBufferConfig cb_src1_config =
diff --git a/tests/ttnn/unit_tests/gtests/ccl/test_multi_tensor_ccl.cpp b/tests/ttnn/unit_tests/gtests/ccl/test_multi_tensor_ccl.cpp
index 914e7618bf..11da6a07c6 100644
--- a/tests/ttnn/unit_tests/gtests/ccl/test_multi_tensor_ccl.cpp
+++ b/tests/ttnn/unit_tests/gtests/ccl/test_multi_tensor_ccl.cpp
@@ -45,11 +45,11 @@ std::vector<IDevice*> get_line_devices(distributed::MeshDevice* mesh_device) {
 class T3000MultiCQFabricMeshDeviceFixture : public T3000MultiCQMeshDeviceFixture {
 protected:
     T3000MultiCQFabricMeshDeviceFixture() {
-        tt::tt_metal::detail::SetFabricConfig(tt::tt_metal::FabricConfig::FABRIC_1D);
+        tt::tt_metal::detail::InitializeFabricConfig(tt::tt_metal::FabricConfig::FABRIC_1D);
     }
     void TearDown() override {
         T3000MultiCQMeshDeviceFixture::TearDown();
-        tt::tt_metal::detail::SetFabricConfig(tt::tt_metal::FabricConfig::DISABLED);
+        tt::tt_metal::detail::InitializeFabricConfig(tt::tt_metal::FabricConfig::DISABLED);
     }
 };
 
diff --git a/tests/ttnn/unit_tests/gtests/ccl/test_sharded_address_generators_new.cpp b/tests/ttnn/unit_tests/gtests/ccl/test_sharded_address_generators_new.cpp
index 6bf3131943..df1bc368ad 100644
--- a/tests/ttnn/unit_tests/gtests/ccl/test_sharded_address_generators_new.cpp
+++ b/tests/ttnn/unit_tests/gtests/ccl/test_sharded_address_generators_new.cpp
@@ -25,12 +25,6 @@
 #define DYNAMIC_NOC_X(noc, x) NOC_0_X(noc, noc_size_x, (x))
 #define DYNAMIC_NOC_Y(noc, y) NOC_0_Y(noc, noc_size_y, (y))
 
-#define NOC_ADDR_COORD_SHIFT 36
-#define NUM_DRAM_BANKS 6
-#define NUM_NOCS 2
-int32_t bank_to_dram_offset[NUM_DRAM_BANKS];
-uint16_t dram_bank_to_noc_xy[NUM_NOCS][NUM_DRAM_BANKS];
-
 #endif
 #include "ttnn/cpp/ttnn/operations/ccl/kernel_common/sharding_addrgen.hpp"
 
@@ -123,12 +117,11 @@ void run_full_block_test(ADDRgen addrgen, ADDRgenInfo constants, uint32_t bank_b
 }
 
 TEST(CclnewWidthShardedTensorSliceIndexer_Wormhole, width_sharded_test) {
-    constexpr std::size_t shard_type = static_cast<std::size_t>(tt::tt_metal::TensorMemoryLayout::WIDTH_SHARDED);
+    constexpr std::size_t shard_type = static_cast<uint32_t>(tt::tt_metal::TensorMemoryLayout::WIDTH_SHARDED);
     constexpr std::size_t number_of_cores = 8;
     constexpr std::size_t page_size_jump = 1024;
     constexpr std::size_t pages_per_tensor_row = 32;
-    constexpr std::size_t contiguity =
-        static_cast<std::size_t>(shard_addr_gen_consts::ContiguityType::L1_NO_SHARD_PADDING);
+    constexpr std::size_t contiguity = static_cast<uint32_t>(shard_addr_gen_consts::ContiguityType::NO_SHARD_PADDING);
     constexpr std::size_t pages_per_shard_width = 6;
     constexpr std::size_t rows_per_shard_height = 1;
     constexpr std::size_t tensor_address = 0x100000;
@@ -147,13 +140,12 @@ TEST(CclnewWidthShardedTensorSliceIndexer_Wormhole, width_sharded_test) {
 }
 
 TEST(CclnewHeightShardedTensorSliceIndexer_Wormhole, height_sharded_test) {
-    static constexpr std::size_t shard_type =
-        static_cast<std::size_t>(tt::tt_metal::TensorMemoryLayout::HEIGHT_SHARDED);
+    static constexpr std::size_t shard_type = static_cast<uint32_t>(tt::tt_metal::TensorMemoryLayout::HEIGHT_SHARDED);
     static constexpr std::size_t number_of_cores = 4;
     static constexpr std::size_t page_size_jump = 1024;
     static constexpr std::size_t pages_per_tensor_row = 32;
     static constexpr std::size_t contiguity =
-        static_cast<std::size_t>(shard_addr_gen_consts::ContiguityType::L1_NO_SHARD_PADDING);
+        static_cast<uint32_t>(shard_addr_gen_consts::ContiguityType::NO_SHARD_PADDING);
     static constexpr std::size_t pages_per_shard_width = 1;
     static constexpr std::size_t rows_per_shard_height = 8;
     static constexpr std::size_t tensor_address = 0x100000;
@@ -172,12 +164,12 @@ TEST(CclnewHeightShardedTensorSliceIndexer_Wormhole, height_sharded_test) {
 }
 
 TEST(CclnewBlockShardedTensorSliceIndexer_Wormhole, block_sharded_test) {
-    static constexpr std::size_t shard_type = static_cast<std::size_t>(tt::tt_metal::TensorMemoryLayout::BLOCK_SHARDED);
+    static constexpr std::size_t shard_type = static_cast<uint32_t>(tt::tt_metal::TensorMemoryLayout::BLOCK_SHARDED);
     static constexpr std::size_t number_of_cores = 16;
     static constexpr std::size_t page_size_jump = 1024;
     static constexpr std::size_t pages_per_tensor_row = 32;
     static constexpr std::size_t contiguity =
-        static_cast<std::size_t>(shard_addr_gen_consts::ContiguityType::L1_NO_SHARD_PADDING);
+        static_cast<uint32_t>(shard_addr_gen_consts::ContiguityType::NO_SHARD_PADDING);
     static constexpr std::size_t pages_per_shard_width = 8;
     static constexpr std::size_t rows_per_shard_height = 8;
     static constexpr std::size_t tensor_address = 0x1000000;
diff --git a/tests/ttnn/unit_tests/gtests/multi_thread/test_ccl_multi_cq_multi_device.cpp b/tests/ttnn/unit_tests/gtests/multi_thread/test_ccl_multi_cq_multi_device.cpp
index a9703c7aea..16eaac18da 100644
--- a/tests/ttnn/unit_tests/gtests/multi_thread/test_ccl_multi_cq_multi_device.cpp
+++ b/tests/ttnn/unit_tests/gtests/multi_thread/test_ccl_multi_cq_multi_device.cpp
@@ -54,11 +54,11 @@ using tt::tt_metal::distributed::MeshShape;
 class T3000MultiCQFabricMeshDeviceFixture : public T3000MultiCQMeshDeviceFixture {
 protected:
     T3000MultiCQFabricMeshDeviceFixture() {
-        tt::tt_metal::detail::SetFabricConfig(tt::tt_metal::FabricConfig::FABRIC_1D);
+        tt::tt_metal::detail::InitializeFabricConfig(tt::tt_metal::FabricConfig::FABRIC_1D);
     }
     void TearDown() override {
         T3000MultiCQMeshDeviceFixture::TearDown();
-        tt::tt_metal::detail::SetFabricConfig(tt::tt_metal::FabricConfig::DISABLED);
+        tt::tt_metal::detail::InitializeFabricConfig(tt::tt_metal::FabricConfig::DISABLED);
     }
 };
 
diff --git a/tests/ttnn/unit_tests/operations/ccl/test_llama_reduce_scatter_async_TG.py b/tests/ttnn/unit_tests/operations/ccl/test_llama_reduce_scatter_async_TG.py
index f4311cda25..a2ed89848e 100644
--- a/tests/ttnn/unit_tests/operations/ccl/test_llama_reduce_scatter_async_TG.py
+++ b/tests/ttnn/unit_tests/operations/ccl/test_llama_reduce_scatter_async_TG.py
@@ -24,8 +24,8 @@ from tracy import signpost
 
 PACKET_WORKER_CRS = ttnn.CoreRangeSet(
     [
-        ttnn.CoreRange(ttnn.CoreCoord(1, 1), ttnn.CoreCoord(3, 2)),
-        ttnn.CoreRange(ttnn.CoreCoord(1, 3), ttnn.CoreCoord(2, 3)),
+        ttnn.CoreRange(ttnn.CoreCoord(1, 0), ttnn.CoreCoord(3, 1)),
+        ttnn.CoreRange(ttnn.CoreCoord(1, 2), ttnn.CoreCoord(2, 2)),
     ]
 )
 
diff --git a/tests/ttnn/unit_tests/operations/ccl/test_llama_reduce_scatter_create_heads_async_TG.py b/tests/ttnn/unit_tests/operations/ccl/test_llama_reduce_scatter_create_heads_async_TG.py
index 24b8aa8c18..4e50caf31c 100644
--- a/tests/ttnn/unit_tests/operations/ccl/test_llama_reduce_scatter_create_heads_async_TG.py
+++ b/tests/ttnn/unit_tests/operations/ccl/test_llama_reduce_scatter_create_heads_async_TG.py
@@ -24,8 +24,8 @@ from tracy import signpost
 
 PACKET_WORKER_CRS = ttnn.CoreRangeSet(
     [
-        ttnn.CoreRange(ttnn.CoreCoord(1, 1), ttnn.CoreCoord(3, 2)),
-        ttnn.CoreRange(ttnn.CoreCoord(1, 3), ttnn.CoreCoord(2, 3)),
+        ttnn.CoreRange(ttnn.CoreCoord(1, 0), ttnn.CoreCoord(3, 1)),
+        ttnn.CoreRange(ttnn.CoreCoord(1, 2), ttnn.CoreCoord(2, 2)),
     ]
 )
 
diff --git a/tests/ttnn/unit_tests/operations/eltwise/test_binary_bcast.py b/tests/ttnn/unit_tests/operations/eltwise/test_binary_bcast.py
index f005aa9549..7727083867 100644
--- a/tests/ttnn/unit_tests/operations/eltwise/test_binary_bcast.py
+++ b/tests/ttnn/unit_tests/operations/eltwise/test_binary_bcast.py
@@ -1912,46 +1912,14 @@ def test_binary_subtile_no_bcast(a_shape, b_shape, device):
     assert ttnn.pearson_correlation_coefficient(torch_output_tensor, output_tensor) >= 0.99988
 
 
-@pytest.mark.parametrize(
-    "a_shape, b_shape",
-    [
-        [[1, 1, 320, 320], [1, 1, 1, 320]],
-        [[1, 1, 1, 320], [1, 1, 320, 320]],
-        [[1, 4, 320, 320], [1, 1, 1, 320]],
-        [[1, 1, 1, 320], [1, 4, 320, 320]],
-        [[4, 1, 320, 320], [1, 1, 1, 320]],
-        [[1, 1, 1, 320], [4, 1, 320, 320]],
-        [[4, 4, 320, 320], [1, 1, 1, 320]],
-        [[1, 1, 1, 320], [4, 4, 320, 320]],
-        [[8192, 8192], [1, 8192]],
-        [[1, 8192], [8192, 8192]],
-    ],
-)
-def test_binary_subtile_row_bcast(a_shape, b_shape, device):
-    torch.manual_seed(0)
-
-    torch_input_tensor_a, input_tensor_a = rand_bf16_gen(a_shape, device)
-    torch_input_tensor_b, input_tensor_b = rand_bf16_gen(b_shape, device)
-
-    torch_output_tensor = torch_input_tensor_a + torch_input_tensor_b
-
-    output_tensor = ttnn.add(input_tensor_a, input_tensor_b, memory_config=ttnn.DRAM_MEMORY_CONFIG, use_legacy=False)
-    output_tensor = ttnn.to_torch(output_tensor)
-
-    assert output_tensor.shape == torch_output_tensor.shape
-    assert ttnn.pearson_correlation_coefficient(torch_output_tensor, output_tensor) >= 0.99988
-
-
 profile_a_b_shape_pairs = [
-    # [[8192, 8192], [8192, 8192]],
-    # [[1, 8192], [8192, 8192]],
-    # [[8192, 8192], [1, 8192]],
-    # [[8192, 1], [8192, 8192]],
-    # [[8192, 8192], [8192, 1]],
-    # [[1, 8192], [8192, 1]],
-    # [[8192, 1], [1, 8192]],
-    # [[1, 1], [8192, 8192]],
-    [[8192, 8192], [1, 1]],
+    # ((32, 32), (1, 32)),
+    # ((1280, 320), (1, 320)),
+    # ((8192, 8192), (1, 8192)),
+    # [[1, 1, 8192, 8192], [1, 1, 8192, 8192]],
+    # [[1, 4, 2048, 8192], [1, 1, 2048, 8192]],
+    # [[4, 1, 2048, 8192], [1, 1, 2048, 8192]],
+    [[4, 4, 2048, 2048], [1, 1, 2048, 2048]],
 ]
 
 
@@ -1996,124 +1964,6 @@ def test_binary_bcast_profile(device, dtype_pt, dtype_tt, a_and_b_shape, memory_
         ttnn.synchronize_device(device)
 
 
-@pytest.mark.parametrize(
-    "a_shape, b_shape",
-    [
-        [[1, 1, 320, 1], [1, 1, 320, 320]],
-        # a bcast, b no bcast
-        [[1, 1, 320, 1], [1, 4, 320, 320]],
-        [[1, 1, 320, 1], [4, 1, 320, 320]],
-        [[1, 1, 320, 1], [4, 4, 320, 320]],
-        [[4, 1, 320, 1], [1, 1, 320, 320]],
-        [[1, 4, 320, 1], [1, 1, 320, 320]],
-        [[4, 4, 320, 1], [1, 1, 320, 320]],
-        [[1, 4, 320, 1], [4, 1, 320, 320]],
-        [[4, 1, 320, 1], [1, 4, 320, 320]],
-        [[4, 4, 320, 1], [4, 4, 320, 320]],
-        # a no bcast, b bcast
-        [[1, 1, 320, 320], [1, 4, 320, 1]],
-        [[1, 1, 320, 320], [4, 1, 320, 1]],
-        [[1, 1, 320, 320], [4, 4, 320, 1]],
-        [[4, 1, 320, 320], [1, 1, 320, 1]],
-        [[1, 4, 320, 320], [1, 1, 320, 1]],
-        [[4, 4, 320, 320], [1, 1, 320, 1]],
-        [[1, 4, 320, 320], [4, 1, 320, 1]],
-        [[4, 1, 320, 320], [1, 4, 320, 1]],
-        [[4, 4, 320, 320], [4, 4, 320, 1]],
-    ],
-)
-def test_binary_subtile_col_bcast(a_shape, b_shape, device):
-    torch.manual_seed(0)
-
-    torch_input_tensor_a, input_tensor_a = rand_bf16_gen(a_shape, device)
-    torch_input_tensor_b, input_tensor_b = rand_bf16_gen(b_shape, device)
-
-    torch_output_tensor = torch_input_tensor_a + torch_input_tensor_b
-
-    output_tensor = ttnn.add(input_tensor_a, input_tensor_b, memory_config=ttnn.DRAM_MEMORY_CONFIG, use_legacy=False)
-    output_tensor = ttnn.to_torch(output_tensor)
-
-    assert output_tensor.shape == torch_output_tensor.shape
-    assert ttnn.pearson_correlation_coefficient(torch_output_tensor, output_tensor) >= 0.99988
-
-
-@pytest.mark.parametrize(
-    "a_shape, b_shape",
-    [
-        [[1, 1, 1, 1], [1, 1, 320, 320]],
-        [[1, 1, 320, 320], [1, 1, 1, 1]],
-        # a scalar, b no bcast
-        [[1, 1, 1, 1], [1, 4, 320, 320]],
-        [[1, 1, 1, 1], [4, 1, 320, 320]],
-        [[1, 1, 1, 1], [4, 4, 320, 320]],
-        [[1, 4, 1, 1], [1, 1, 320, 320]],
-        [[4, 1, 1, 1], [1, 1, 320, 320]],
-        [[4, 4, 1, 1], [1, 1, 320, 320]],
-        # # a no bast, b scalar
-        [[1, 1, 320, 320], [1, 4, 1, 1]],
-        [[1, 1, 320, 320], [4, 1, 1, 1]],
-        [[1, 1, 320, 320], [4, 4, 1, 1]],
-        [[1, 4, 320, 320], [1, 1, 1, 1]],
-        [[4, 1, 320, 320], [1, 1, 1, 1]],
-        [[4, 4, 320, 320], [1, 1, 1, 1]],
-    ],
-)
-def test_binary_subtile_scalar_bcast(a_shape, b_shape, device):
-    torch.manual_seed(0)
-
-    torch_input_tensor_a, input_tensor_a = rand_bf16_gen(a_shape, device)
-    torch_input_tensor_b, input_tensor_b = rand_bf16_gen(b_shape, device)
-
-    torch_output_tensor = torch_input_tensor_a + torch_input_tensor_b
-
-    output_tensor = ttnn.add(input_tensor_a, input_tensor_b, memory_config=ttnn.DRAM_MEMORY_CONFIG, use_legacy=False)
-    output_tensor = ttnn.to_torch(output_tensor)
-
-    assert output_tensor.shape == torch_output_tensor.shape
-    assert ttnn.pearson_correlation_coefficient(torch_output_tensor, output_tensor) >= 0.99988
-
-
-@pytest.mark.parametrize(
-    "a_shape, b_shape",
-    [
-        [[1, 1, 320, 1], [1, 1, 1, 320]],
-        # a col, b row
-        [[1, 1, 320, 1], [1, 4, 1, 320]],
-        [[1, 1, 320, 1], [4, 1, 1, 320]],
-        [[1, 1, 320, 1], [4, 4, 1, 320]],
-        [[4, 1, 320, 1], [1, 1, 1, 320]],
-        [[1, 4, 320, 1], [1, 1, 1, 320]],
-        [[4, 4, 320, 1], [1, 1, 1, 320]],
-        [[1, 4, 320, 1], [4, 1, 1, 320]],
-        [[4, 1, 320, 1], [1, 4, 1, 320]],
-        [[4, 4, 320, 1], [4, 4, 1, 320]],
-        # a row, b col
-        [[1, 1, 1, 320], [1, 4, 320, 1]],
-        [[1, 1, 1, 320], [4, 1, 320, 1]],
-        [[1, 1, 1, 320], [4, 4, 320, 1]],
-        [[4, 1, 1, 320], [1, 1, 320, 1]],
-        [[1, 4, 1, 320], [1, 1, 320, 1]],
-        [[4, 4, 1, 320], [1, 1, 320, 1]],
-        [[1, 4, 1, 320], [4, 1, 320, 1]],
-        [[4, 1, 1, 320], [1, 4, 320, 1]],
-        [[4, 4, 1, 320], [4, 4, 320, 1]],
-    ],
-)
-def test_binary_subtile_row_b_col_a_bcast(a_shape, b_shape, device):
-    torch.manual_seed(0)
-
-    torch_input_tensor_a, input_tensor_a = rand_bf16_gen(a_shape, device)
-    torch_input_tensor_b, input_tensor_b = rand_bf16_gen(b_shape, device)
-
-    torch_output_tensor = torch_input_tensor_a + torch_input_tensor_b
-
-    output_tensor = ttnn.add(input_tensor_a, input_tensor_b, memory_config=ttnn.DRAM_MEMORY_CONFIG, use_legacy=False)
-    output_tensor = ttnn.to_torch(output_tensor)
-
-    assert output_tensor.shape == torch_output_tensor.shape
-    assert ttnn.pearson_correlation_coefficient(torch_output_tensor, output_tensor) >= 0.99988
-
-
 @pytest.mark.parametrize(
     "input_shape_a",
     [
@@ -2376,22 +2226,15 @@ def rand_gen(shape, device, *, dtype, tt_dtype, min=0, max=1, memory_config):
     return pt, tt
 
 
-@pytest.mark.parametrize(
-    "dtype_pt_a, dtype_tt_a, dtype_pt_b, dtype_tt_b",
-    (
-        [torch.bfloat16, ttnn.bfloat16, torch.float32, ttnn.float32],
-        [torch.float32, ttnn.float32, torch.bfloat16, ttnn.bfloat16],
-    ),
-)
-def test_binary_mixed_add(dtype_pt_a, dtype_tt_a, dtype_pt_b, dtype_tt_b, device):
+def test_binary_mixed_add(device):
     torch.manual_seed(0)
     a_shape = torch.Size([1, 4, 2, 160])
     b_shape = torch.Size([1, 4, 1, 160])
     mem = ttnn.MemoryConfig(
         memory_layout=ttnn.TensorMemoryLayout.INTERLEAVED, buffer_type=ttnn.BufferType.L1, shard_spec=None
     )
-    a_pt, a_tt = rand_gen(a_shape, device, dtype=dtype_pt_a, tt_dtype=dtype_tt_a, memory_config=mem)
-    b_pt, b_tt = rand_gen(b_shape, device, dtype=dtype_pt_b, tt_dtype=dtype_tt_b, memory_config=mem)
+    a_pt, a_tt = rand_gen(a_shape, device, dtype=torch.bfloat16, tt_dtype=ttnn.bfloat16, memory_config=mem)
+    b_pt, b_tt = rand_gen(b_shape, device, dtype=torch.float32, tt_dtype=ttnn.float32, memory_config=mem)
 
     golden_fn = ttnn.get_golden_function(ttnn.add)
 
diff --git a/tests/ttnn/unit_tests/operations/eltwise/test_unary_int32.py b/tests/ttnn/unit_tests/operations/eltwise/test_unary_int32.py
deleted file mode 100644
index ef7846e1d6..0000000000
--- a/tests/ttnn/unit_tests/operations/eltwise/test_unary_int32.py
+++ /dev/null
@@ -1,152 +0,0 @@
-# SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-
-# SPDX-License-Identifier: Apache-2.0
-
-import torch
-import pytest
-import ttnn
-
-
-@pytest.mark.parametrize(
-    "input_shapes",
-    [
-        (torch.Size([1, 1, 32, 32])),
-        (torch.Size([1, 1, 320, 384])),
-        (torch.Size([1, 3, 320, 384])),
-        (torch.Size([1, 1, 1024, 1024])),
-        (torch.Size([1, 3, 1024, 1024])),
-    ],
-)
-@pytest.mark.parametrize(
-    "low_a, high_a",
-    [
-        (-1000, 1000),
-        (-1e4, 1e4),
-        (-1e6, 1e6),
-        (-2147483647, 0),  # max negative to zero
-        (0, 2147483647),  # zero to max positive
-        (2e9, 2147483647),  # large positive input
-        (-2147483647, -2e9),  # large negative input
-        (-2147483647, 2147483647),  # whole range
-    ],
-)
-@pytest.mark.parametrize(
-    "logical_op",
-    [
-        ttnn.logical_not,
-    ],
-)
-def test_unary_logical_int32(input_shapes, low_a, high_a, logical_op, device):
-    num_elements = max(int(torch.prod(torch.tensor(input_shapes)).item()), 1)
-    torch_input_tensor_a = torch.linspace(high_a, low_a, num_elements, dtype=torch.int32)
-    torch_input_tensor_a[::5] = 0  # every 5th element is zero
-    torch_input_tensor_a = torch_input_tensor_a[:num_elements].reshape(input_shapes).nan_to_num(0.0)
-
-    golden_function = ttnn.get_golden_function(logical_op)
-    torch_output_tensor = golden_function(torch_input_tensor_a, device=device)
-
-    input_tensor_a = ttnn.from_torch(
-        torch_input_tensor_a,
-        dtype=ttnn.int32,
-        device=device,
-        layout=ttnn.TILE_LAYOUT,
-        memory_config=ttnn.DRAM_MEMORY_CONFIG,
-    )
-
-    output_tensor = logical_op(input_tensor_a)
-    output_tensor = ttnn.to_torch(output_tensor)
-
-    assert torch.equal(output_tensor, torch_output_tensor)
-
-
-@pytest.mark.parametrize(
-    "logical_op",
-    [
-        ttnn.logical_not,
-    ],
-)
-def test_unary_logical_int32_edge_cases(logical_op, device):
-    # Note: torch.logical_not(-2147483648) returns False, whereas ttnn.logical_not(-2147483648) returns True.
-    # This discrepancy occurs because, in Wormhole, the value -2147483648 wraps around to 0 due to integer overflow.
-    torch_input_tensor_a = torch.tensor([0, 1, -1, 2147483647, -2147483647])
-    input_tensor_a = ttnn.from_torch(
-        torch_input_tensor_a,
-        dtype=ttnn.int32,
-        device=device,
-        layout=ttnn.TILE_LAYOUT,
-        memory_config=ttnn.DRAM_MEMORY_CONFIG,
-    )
-
-    golden_function = ttnn.get_golden_function(logical_op)
-    torch_output_tensor = golden_function(torch_input_tensor_a, device=device)
-
-    output_tensor = logical_op(input_tensor_a)
-    output_tensor = ttnn.to_torch(output_tensor)
-
-    assert torch.equal(output_tensor, torch_output_tensor)
-
-
-height_sharded_memory_config = ttnn.create_sharded_memory_config(
-    [128, 160],
-    core_grid=ttnn.CoreRangeSet({ttnn.CoreRange((1, 0), (1, 6)), ttnn.CoreRange((3, 0), (3, 6))}),
-    strategy=ttnn.ShardStrategy.HEIGHT,
-    orientation=ttnn.ShardOrientation.COL_MAJOR,
-    use_height_and_width_as_shard_shape=True,
-)
-
-width_sharded_memory_config = ttnn.create_sharded_memory_config(
-    [2240, 32],
-    core_grid=ttnn.CoreRangeSet({ttnn.CoreRange((2, 2), (2, 3)), ttnn.CoreRange((0, 0), (0, 1))}),
-    strategy=ttnn.ShardStrategy.WIDTH,
-    orientation=ttnn.ShardOrientation.ROW_MAJOR,
-    use_height_and_width_as_shard_shape=True,
-)
-
-block_sharded_memory_config = ttnn.create_sharded_memory_config(
-    [320, 32],
-    core_grid=ttnn.CoreRangeSet({ttnn.CoreRange((1, 0), (4, 6))}),
-    strategy=ttnn.ShardStrategy.BLOCK,
-    orientation=ttnn.ShardOrientation.ROW_MAJOR,
-    use_height_and_width_as_shard_shape=True,
-)
-
-
-@pytest.mark.parametrize(
-    "a_shape",
-    [(torch.Size([5, 7, 64, 128]))],
-)
-@pytest.mark.parametrize(
-    "sharded_config",
-    [
-        height_sharded_memory_config,
-        width_sharded_memory_config,
-        block_sharded_memory_config,
-    ],
-)
-@pytest.mark.parametrize(
-    "logical_op",
-    [
-        "logical_not",
-    ],
-)
-def test_unary_logical_int32_sharded(a_shape, sharded_config, logical_op, device):
-    ttnn_op = getattr(ttnn, logical_op)
-    num_elements = max(int(torch.prod(torch.tensor(a_shape)).item()), 1)
-    torch_input_tensor_a = torch.linspace(-100, 100, num_elements, dtype=torch.int32)
-    torch_input_tensor_a = torch_input_tensor_a[:num_elements].reshape(a_shape).nan_to_num(0.0)
-
-    input_tensor_a = ttnn.from_torch(
-        torch_input_tensor_a,
-        dtype=ttnn.int32,
-        device=device,
-        layout=ttnn.TILE_LAYOUT,
-        memory_config=sharded_config,
-    )
-
-    golden_function = ttnn.get_golden_function(ttnn_op)
-    torch_output_tensor = golden_function(torch_input_tensor_a, device=device)
-
-    output_tensor = ttnn_op(input_tensor_a, memory_config=sharded_config)
-    output_tensor = ttnn.to_torch(output_tensor)
-
-    assert torch.equal(output_tensor, torch_output_tensor)
diff --git a/tests/ttnn/unit_tests/operations/reduce/test_reduction.py b/tests/ttnn/unit_tests/operations/reduce/test_reduction.py
index 2119c4a571..2ad05d5356 100644
--- a/tests/ttnn/unit_tests/operations/reduce/test_reduction.py
+++ b/tests/ttnn/unit_tests/operations/reduce/test_reduction.py
@@ -9,7 +9,7 @@ import ttnn
 import sys
 
 from tests.ttnn.utils_for_testing import assert_with_pcc
-from models.utility_functions import skip_for_grayskull, skip_for_blackhole, is_blackhole, torch_random
+from models.utility_functions import skip_for_grayskull, skip_for_blackhole, torch_random
 
 
 @pytest.mark.parametrize("batch_size", [1, 16])
@@ -207,13 +207,10 @@ def test_sum_4d_tensor_dims(device, batch_size, c, h, w, dim, keepdim):
     assert_with_pcc(torch_output_tensor, output_tensor, pcc=0.99)
 
 
+@skip_for_blackhole("Bad CosineSimilarity on BH. Issue #21881")
 @pytest.mark.parametrize("dim1", [1])
-# This test picks the maximum dim2 that will pick the singlecore implementation.
-# TopK multicore uses 8 cores in blackhole, so we need to add support for bitonic sort with 8 cores
-# and non power of 2 dims as compared to wormhole. Issue #23465.
 @pytest.mark.parametrize(
-    "dim2",
-    [8192 - 64, pytest.param(50257, marks=pytest.mark.xfail(condition=is_blackhole(), reason="Issue #23465"))],
+    "dim2", [50257]
 )  # Need to resolve issue #20294 to verify 128256 for OXMIQ <- will need topk_local_sort to handle uint32_t
 @pytest.mark.parametrize("dim", [1])
 @pytest.mark.parametrize("k", [50, 3200])
@@ -225,6 +222,10 @@ def test_2d_topk(device, dim1, dim2, dim, k, largest, dtype):
     torch_dtype = torch.bfloat16
 
     input = torch.randn(shape, dtype=torch_dtype) * 0.9
+    # Make every 256th element 10000
+    # This produced a reproducible failure for WH/BH: https://github.com/tenstorrent/tt-metal/issues/21881
+    for i in range(256, input.numel(), 256):
+        input[0][i] = 10000
 
     pyt_topk_values, pyt_topk_indices = torch.topk(input, k, dim=dim, largest=largest, sorted=True)
 
diff --git a/tests/ttnn/unit_tests/operations/test_interleaved_to_sharded.py b/tests/ttnn/unit_tests/operations/test_interleaved_to_sharded.py
index 917f99848b..71152ce53f 100644
--- a/tests/ttnn/unit_tests/operations/test_interleaved_to_sharded.py
+++ b/tests/ttnn/unit_tests/operations/test_interleaved_to_sharded.py
@@ -63,164 +63,3 @@ def test_interleaved_to_sharded_hash(device, first_dtype, second_dtype, input_in
             input_tensor_device, sharded_mem_config, second_dtype, keep_l1_aligned=keep_l1_aligned
         )
         pcc_passed_b, pcc_message_b = assert_with_pcc(input_tensor_torch, ttnn.to_torch(output_tensor), pcc=0.9999)
-
-
-@pytest.mark.parametrize("dtype", [ttnn.bfloat16, ttnn.bfloat8_b])
-@pytest.mark.parametrize("layout", [ttnn.TILE_LAYOUT, ttnn.ROW_MAJOR_LAYOUT])
-@pytest.mark.parametrize(
-    "tensor_shape, shard_shape, shard_grid",
-    [
-        [
-            [2, 2, 128, 64],
-            (128, 64),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 0))}),
-        ],
-        [
-            [1, 1, 416, 64],
-            (128, 64),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 0))}),
-        ],
-    ],
-)
-@pytest.mark.parametrize("shard_orientation", [ttnn.ShardOrientation.ROW_MAJOR, ttnn.ShardOrientation.COL_MAJOR])
-def test_interleaved_to_dram_height_sharded(
-    device, dtype, layout, tensor_shape, shard_shape, shard_grid, shard_orientation
-):
-    if dtype == ttnn.bfloat8_b and layout == ttnn.ROW_MAJOR_LAYOUT:
-        pytest.skip("bfloat8_b not supported for i2s row-major")
-
-    # Output memory config
-    output_shard_spec = ttnn.ShardSpec(shard_grid, shard_shape, shard_orientation)
-    output_mem_config = ttnn.MemoryConfig(
-        ttnn.TensorMemoryLayout.HEIGHT_SHARDED, ttnn.BufferType.DRAM, output_shard_spec
-    )
-
-    # Test
-    torch_input_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    ttnn_input_tensor = ttnn.from_torch(torch_input_tensor, dtype=dtype, layout=layout)
-    ttnn_input_tensor = ttnn.to_device(ttnn_input_tensor, device)
-    ttnn_output_tensor = ttnn.interleaved_to_sharded(ttnn_input_tensor, output_mem_config)
-
-    assert_with_pcc(torch_input_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
-
-
-@pytest.mark.parametrize("dtype", [ttnn.bfloat16, ttnn.bfloat8_b])
-@pytest.mark.parametrize("layout", [ttnn.TILE_LAYOUT, ttnn.ROW_MAJOR_LAYOUT])
-@pytest.mark.parametrize(
-    "tensor_shape, shard_shape, shard_grid",
-    [
-        [
-            [2, 1, 32, 512],
-            (64, 128),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 0))}),
-        ],
-        [
-            [1, 1, 64, 416],
-            (64, 128),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 0))}),
-        ],
-    ],
-)
-@pytest.mark.parametrize("shard_orientation", [ttnn.ShardOrientation.ROW_MAJOR, ttnn.ShardOrientation.COL_MAJOR])
-def test_interleaved_to_dram_width_sharded(
-    device, dtype, layout, tensor_shape, shard_shape, shard_grid, shard_orientation
-):
-    if dtype == ttnn.bfloat8_b and layout == ttnn.ROW_MAJOR_LAYOUT:
-        pytest.skip("bfloat8_b not supported for i2s row-major")
-
-    # Output memory config
-    output_shard_spec = ttnn.ShardSpec(shard_grid, shard_shape, shard_orientation)
-    output_mem_config = ttnn.MemoryConfig(
-        ttnn.TensorMemoryLayout.WIDTH_SHARDED, ttnn.BufferType.DRAM, output_shard_spec
-    )
-
-    # Test
-    torch_input_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    ttnn_input_tensor = ttnn.from_torch(torch_input_tensor, dtype=dtype, layout=layout)
-    ttnn_input_tensor = ttnn.to_device(ttnn_input_tensor, device)
-    ttnn_output_tensor = ttnn.interleaved_to_sharded(ttnn_input_tensor, output_mem_config)
-
-    assert_with_pcc(torch_input_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
-
-
-@pytest.mark.parametrize(
-    "in_dtype, out_dtype",
-    [
-        [ttnn.bfloat8_b, ttnn.float32],
-        [ttnn.float32, ttnn.bfloat8_b],
-    ],
-)
-@pytest.mark.parametrize("layout", [ttnn.TILE_LAYOUT])
-@pytest.mark.parametrize(
-    "tensor_shape, shard_type, shard_shape, shard_grid",
-    [
-        [
-            [1, 1, 416, 64],
-            ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-            (128, 64),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 0))}),
-        ],
-        [
-            [1, 1, 64, 416],
-            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-            (64, 128),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 0))}),
-        ],
-    ],
-)
-@pytest.mark.parametrize("shard_orientation", [ttnn.ShardOrientation.ROW_MAJOR, ttnn.ShardOrientation.COL_MAJOR])
-def test_interleaved_to_dram_sharded_convert_dtype(
-    device, in_dtype, out_dtype, layout, tensor_shape, shard_type, shard_shape, shard_grid, shard_orientation
-):
-    # Output memory config
-    output_shard_spec = ttnn.ShardSpec(shard_grid, shard_shape, shard_orientation)
-    output_mem_config = ttnn.MemoryConfig(shard_type, ttnn.BufferType.DRAM, output_shard_spec)
-
-    # Test
-    torch_input_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    ttnn_input_tensor = ttnn.from_torch(torch_input_tensor, dtype=in_dtype, layout=layout)
-    ttnn_input_tensor = ttnn.to_device(ttnn_input_tensor, device)
-    ttnn_output_tensor = ttnn.interleaved_to_sharded(ttnn_input_tensor, output_mem_config, out_dtype)
-
-    assert_with_pcc(torch_input_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
-
-
-@pytest.mark.parametrize("dtype", [ttnn.bfloat8_b, ttnn.float32])
-@pytest.mark.parametrize("layout", [ttnn.TILE_LAYOUT, ttnn.ROW_MAJOR_LAYOUT])
-@pytest.mark.parametrize("input_mem_config", [ttnn.DRAM_MEMORY_CONFIG, ttnn.L1_MEMORY_CONFIG])
-@pytest.mark.parametrize(
-    "tensor_shape, shard_type, shard_shape, shard_grid",
-    [
-        [
-            [1, 1, 416, 64],
-            ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-            (128, 64),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 0))}),
-        ],
-        [
-            [1, 1, 64, 416],
-            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-            (64, 128),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 0))}),
-        ],
-    ],
-)
-@pytest.mark.parametrize("shard_orientation", [ttnn.ShardOrientation.ROW_MAJOR, ttnn.ShardOrientation.COL_MAJOR])
-def test_interleaved_to_dram_sharded_via_to_memory_layout(
-    device, dtype, layout, input_mem_config, tensor_shape, shard_type, shard_shape, shard_grid, shard_orientation
-):
-    if dtype == ttnn.bfloat8_b and layout == ttnn.ROW_MAJOR_LAYOUT:
-        pytest.skip("bfloat8_b not supported for i2s row-major")
-
-    # Output memory config
-    output_shard_spec = ttnn.ShardSpec(shard_grid, shard_shape, shard_orientation)
-    output_mem_config = ttnn.MemoryConfig(shard_type, ttnn.BufferType.DRAM, output_shard_spec)
-
-    # Test
-    torch_input_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    ttnn_input_tensor = ttnn.from_torch(
-        torch_input_tensor, dtype=dtype, layout=layout, device=device, memory_config=input_mem_config
-    )
-    ttnn_output_tensor = ttnn.to_memory_config(ttnn_input_tensor, output_mem_config)
-
-    assert_with_pcc(torch_input_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
diff --git a/tests/ttnn/unit_tests/operations/test_untilize.py b/tests/ttnn/unit_tests/operations/test_untilize.py
index 8805c08d24..daa79a9a7b 100644
--- a/tests/ttnn/unit_tests/operations/test_untilize.py
+++ b/tests/ttnn/unit_tests/operations/test_untilize.py
@@ -12,15 +12,17 @@ from tests.ttnn.utils_for_testing import assert_with_pcc
 
 @pytest.mark.parametrize("dtype", [ttnn.bfloat8_b, ttnn.bfloat16])
 @pytest.mark.parametrize("use_pack_untilize", [True, False])
-@pytest.mark.parametrize("tensor_shape", [[2, 2, 256, 512]])
+@pytest.mark.parametrize(
+    "tensor_shape",
+    [
+        [2, 256, 512],
+        [2, 2, 256, 512],
+    ],
+)
 def test_untilize_single_core_interleaved_to_interleaved(device, dtype, use_pack_untilize, tensor_shape):
-    # Input memory config
     input_memory_config = ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.L1)
-
-    # Output memory config
     output_memory_config = ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.L1)
 
-    # Test
     input_torch_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
     input_ttnn_tensor = ttnn.from_torch(input_torch_tensor, dtype=dtype, layout=ttnn.TILE_LAYOUT)
     input_ttnn_tensor = ttnn.to_device(input_ttnn_tensor, device, memory_config=input_memory_config)
@@ -33,7 +35,13 @@ def test_untilize_single_core_interleaved_to_interleaved(device, dtype, use_pack
 
 @pytest.mark.parametrize("dtype", [ttnn.bfloat16])
 @pytest.mark.parametrize("use_pack_untilize", [True])
-@pytest.mark.parametrize("tensor_shape", [[2, 2, 256, 512]])
+@pytest.mark.parametrize(
+    "tensor_shape",
+    [
+        [2, 256, 512],
+        [2, 2, 256, 512],
+    ],
+)
 @pytest.mark.parametrize(
     "output_memory_layout",
     [
@@ -61,8 +69,8 @@ def test_untilize_single_core_interleaved_to_interleaved(device, dtype, use_pack
             16,
             ttnn.CoreRangeSet(
                 {
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(7, 0)),
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 2), ttnn.CoreCoord(7, 2)),
+                    ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 7)),
+                    ttnn.CoreRange(ttnn.CoreCoord(2, 0), ttnn.CoreCoord(2, 7)),
                 }
             ),
             ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 3))}),
@@ -114,9 +122,9 @@ def test_untilize_single_core_interleaved_to_sharded(
     input_memory_config = ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.L1)
 
     # Output memory config
-    output_shard_memory_layout = shard_memory_layout_map[output_memory_layout]
+    shard_memory_layout = shard_memory_layout_map[output_memory_layout]
     output_shard_spec = ttnn.ShardSpec(
-        output_shard_memory_layout["shard_grid"], output_shard_memory_layout["shard_shape"], output_shard_orientation
+        shard_memory_layout["shard_grid"], shard_memory_layout["shard_shape"], output_shard_orientation
     )
     output_memory_config = ttnn.MemoryConfig(output_memory_layout, ttnn.BufferType.L1, output_shard_spec)
 
@@ -133,7 +141,13 @@ def test_untilize_single_core_interleaved_to_sharded(
 
 @pytest.mark.parametrize("dtype", [ttnn.bfloat16])
 @pytest.mark.parametrize("use_pack_untilize", [True])
-@pytest.mark.parametrize("tensor_shape", [[2, 2, 256, 512]])
+@pytest.mark.parametrize(
+    "tensor_shape",
+    [
+        [2, 256, 512],
+        [2, 2, 256, 512],
+    ],
+)
 @pytest.mark.parametrize(
     "input_memory_layout",
     [
@@ -161,8 +175,8 @@ def test_untilize_single_core_interleaved_to_sharded(
             16,
             ttnn.CoreRangeSet(
                 {
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(7, 0)),
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 2), ttnn.CoreCoord(7, 2)),
+                    ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 7)),
+                    ttnn.CoreRange(ttnn.CoreCoord(2, 0), ttnn.CoreCoord(2, 7)),
                 }
             ),
             ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 3))}),
@@ -211,9 +225,9 @@ def test_untilize_single_core_sharded_to_interleaved(
     }
 
     # Input memory config
-    input_shard_memory_layout = shard_memory_layout_map[input_memory_layout]
+    shard_memory_layout = shard_memory_layout_map[input_memory_layout]
     output_shard_spec = ttnn.ShardSpec(
-        input_shard_memory_layout["shard_grid"], input_shard_memory_layout["shard_shape"], input_shard_orientation
+        shard_memory_layout["shard_grid"], shard_memory_layout["shard_shape"], input_shard_orientation
     )
     input_memory_config = ttnn.MemoryConfig(input_memory_layout, ttnn.BufferType.L1, output_shard_spec)
 
@@ -233,7 +247,13 @@ def test_untilize_single_core_sharded_to_interleaved(
 
 @pytest.mark.parametrize("dtype", [ttnn.bfloat16])
 @pytest.mark.parametrize("use_pack_untilize", [True])
-@pytest.mark.parametrize("tensor_shape", [[2, 2, 256, 512]])
+@pytest.mark.parametrize(
+    "tensor_shape",
+    [
+        [2, 256, 512],
+        [2, 2, 256, 512],
+    ],
+)
 @pytest.mark.parametrize(
     "input_memory_layout",
     [
@@ -276,8 +296,8 @@ def test_untilize_single_core_sharded_to_interleaved(
             16,
             ttnn.CoreRangeSet(
                 {
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(7, 0)),
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 2), ttnn.CoreCoord(7, 2)),
+                    ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 7)),
+                    ttnn.CoreRange(ttnn.CoreCoord(2, 0), ttnn.CoreCoord(2, 7)),
                 }
             ),
             ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 3))}),
@@ -328,16 +348,16 @@ def test_untilize_single_core_sharded_to_sharded(
     }
 
     # Input memory config
-    input_shard_memory_layout = shard_memory_layout_map[input_memory_layout]
+    shard_memory_layout = shard_memory_layout_map[input_memory_layout]
     input_shard_spec = ttnn.ShardSpec(
-        input_shard_memory_layout["shard_grid"], input_shard_memory_layout["shard_shape"], input_shard_orientation
+        shard_memory_layout["shard_grid"], shard_memory_layout["shard_shape"], input_shard_orientation
     )
     input_memory_config = ttnn.MemoryConfig(input_memory_layout, ttnn.BufferType.L1, input_shard_spec)
 
     # Output memory config
-    output_shard_memory_layout = shard_memory_layout_map[output_memory_layout]
+    shard_memory_layout = shard_memory_layout_map[output_memory_layout]
     output_shard_spec = ttnn.ShardSpec(
-        output_shard_memory_layout["shard_grid"], output_shard_memory_layout["shard_shape"], output_shard_orientation
+        shard_memory_layout["shard_grid"], shard_memory_layout["shard_shape"], output_shard_orientation
     )
     output_memory_config = ttnn.MemoryConfig(output_memory_layout, ttnn.BufferType.L1, output_shard_spec)
 
@@ -350,970 +370,3 @@ def test_untilize_single_core_sharded_to_sharded(
     )
 
     assert_with_pcc(input_torch_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
-
-
-@pytest.mark.parametrize("dtype", [ttnn.bfloat16])
-@pytest.mark.parametrize("use_pack_untilize", [True])
-@pytest.mark.parametrize("tensor_shape", [[1, 1, 512, 512]])
-@pytest.mark.parametrize("input_buffer_type", [ttnn.BufferType.L1, ttnn.BufferType.DRAM])
-@pytest.mark.parametrize("output_buffer_type", [ttnn.BufferType.L1, ttnn.BufferType.DRAM])
-@pytest.mark.parametrize(
-    "input_memory_layout",
-    [
-        ttnn.TensorMemoryLayout.INTERLEAVED,
-        ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-    ],
-)
-@pytest.mark.parametrize(
-    "output_memory_layout",
-    [
-        ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-        ttnn.TensorMemoryLayout.INTERLEAVED,
-    ],
-)
-def test_untilize_single_core_buffer_type_variations(
-    device,
-    dtype,
-    use_pack_untilize,
-    tensor_shape,
-    input_buffer_type,
-    output_buffer_type,
-    input_memory_layout,
-    output_memory_layout,
-):
-    height_shard_spec = ttnn.ShardSpec(
-        ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 0))}),
-        (128, 512),
-        ttnn.ShardOrientation.ROW_MAJOR,
-    )
-
-    # Input memory config
-    if input_memory_layout == ttnn.TensorMemoryLayout.INTERLEAVED:
-        input_memory_config = ttnn.MemoryConfig(input_memory_layout, input_buffer_type)
-    else:
-        input_memory_config = ttnn.MemoryConfig(input_memory_layout, input_buffer_type, height_shard_spec)
-
-    # Output memory config
-    if output_memory_layout == ttnn.TensorMemoryLayout.INTERLEAVED:
-        output_memory_config = ttnn.MemoryConfig(output_memory_layout, output_buffer_type)
-    else:
-        output_memory_config = ttnn.MemoryConfig(output_memory_layout, output_buffer_type, height_shard_spec)
-
-    # Test
-    input_torch_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    input_ttnn_tensor = ttnn.from_torch(input_torch_tensor, dtype=dtype, layout=ttnn.TILE_LAYOUT)
-    input_ttnn_tensor = ttnn.to_device(input_ttnn_tensor, device, memory_config=input_memory_config)
-    ttnn_output_tensor = ttnn.untilize(
-        input_ttnn_tensor, memory_config=output_memory_config, use_multicore=False, use_pack_untilize=use_pack_untilize
-    )
-
-    assert_with_pcc(input_torch_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
-
-
-@pytest.mark.parametrize("dtype", [ttnn.bfloat8_b, ttnn.bfloat16])
-@pytest.mark.parametrize("use_pack_untilize", [True, False])
-@pytest.mark.parametrize(
-    "tensor_shape",
-    [
-        [2, 256, 512],
-        [4128, 512],  # multiple blocks per core and a cliff core
-    ],
-)
-def test_untilize_multi_core_interleaved_to_interleaved(device, dtype, use_pack_untilize, tensor_shape):
-    # Input memory config
-    input_memory_config = ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.L1)
-
-    # Output memory config
-    output_memory_config = ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.L1)
-
-    # Test
-    input_torch_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    input_ttnn_tensor = ttnn.from_torch(input_torch_tensor, dtype=dtype, layout=ttnn.TILE_LAYOUT)
-    input_ttnn_tensor = ttnn.to_device(input_ttnn_tensor, device, memory_config=input_memory_config)
-    ttnn_output_tensor = ttnn.untilize(
-        input_ttnn_tensor, memory_config=output_memory_config, use_multicore=True, use_pack_untilize=use_pack_untilize
-    )
-
-    assert_with_pcc(input_torch_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
-
-
-@pytest.mark.parametrize("dtype", [ttnn.bfloat16])
-@pytest.mark.parametrize("use_pack_untilize", [True])
-@pytest.mark.parametrize(
-    "tensor_shape",
-    [
-        [2, 256, 512],
-        [4128, 512],  # multiple blocks per core and a cliff core
-    ],
-)
-@pytest.mark.parametrize(
-    "output_memory_layout",
-    [
-        ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-        ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-        ttnn.TensorMemoryLayout.BLOCK_SHARDED,
-    ],
-)
-@pytest.mark.parametrize(
-    "output_shard_orientation",
-    [
-        ttnn.ShardOrientation.ROW_MAJOR,
-        ttnn.ShardOrientation.COL_MAJOR,
-    ],
-)
-@pytest.mark.parametrize(
-    "num_shard_cores, standard_shard_core_grid, block_shard_core_grid",
-    [
-        [
-            4,
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 3))}),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(1, 1))}),
-        ],
-        [
-            16,
-            ttnn.CoreRangeSet(
-                {
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(7, 0)),
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 2), ttnn.CoreCoord(7, 2)),
-                }
-            ),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 3))}),
-        ],
-    ],
-)
-def test_untilize_multi_core_interleaved_to_sharded(
-    device,
-    dtype,
-    use_pack_untilize,
-    tensor_shape,
-    output_memory_layout,
-    output_shard_orientation,
-    num_shard_cores,
-    standard_shard_core_grid,
-    block_shard_core_grid,
-):
-    num_tensor_dims = len(tensor_shape)
-    tensor_height = 1
-    for i in range(num_tensor_dims - 1):
-        tensor_height *= tensor_shape[i]
-    tensor_width = tensor_shape[num_tensor_dims - 1]
-
-    # Shard shapes
-    height_sharded_shard_shape = (tensor_height // num_shard_cores, tensor_width)
-    width_sharded_shard_shape = (tensor_height, tensor_width // num_shard_cores)
-    block_sharded_shard_shape = (
-        tensor_height // int(math.sqrt(num_shard_cores)),
-        tensor_width // int(math.sqrt(num_shard_cores)),
-    )
-
-    # Shard Memory Layout Map
-    shard_memory_layout_map = {
-        ttnn.TensorMemoryLayout.HEIGHT_SHARDED: {
-            "shard_grid": standard_shard_core_grid,
-            "shard_shape": height_sharded_shard_shape,
-        },
-        ttnn.TensorMemoryLayout.WIDTH_SHARDED: {
-            "shard_grid": standard_shard_core_grid,
-            "shard_shape": width_sharded_shard_shape,
-        },
-        ttnn.TensorMemoryLayout.BLOCK_SHARDED: {
-            "shard_grid": block_shard_core_grid,
-            "shard_shape": block_sharded_shard_shape,
-        },
-    }
-
-    # Input memory config
-    input_memory_config = ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.L1)
-
-    # Output memory config
-    input_shard_memory_layout = shard_memory_layout_map[output_memory_layout]
-    output_shard_spec = ttnn.ShardSpec(
-        input_shard_memory_layout["shard_grid"], input_shard_memory_layout["shard_shape"], output_shard_orientation
-    )
-    output_memory_config = ttnn.MemoryConfig(output_memory_layout, ttnn.BufferType.L1, output_shard_spec)
-
-    # Test
-    input_torch_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    input_ttnn_tensor = ttnn.from_torch(input_torch_tensor, dtype=dtype, layout=ttnn.TILE_LAYOUT)
-    input_ttnn_tensor = ttnn.to_device(input_ttnn_tensor, device, memory_config=input_memory_config)
-    ttnn_output_tensor = ttnn.untilize(
-        input_ttnn_tensor, memory_config=output_memory_config, use_multicore=True, use_pack_untilize=use_pack_untilize
-    )
-
-    assert_with_pcc(input_torch_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
-
-
-@pytest.mark.parametrize("dtype", [ttnn.bfloat16])
-@pytest.mark.parametrize("use_pack_untilize", [True])
-@pytest.mark.parametrize(
-    "tensor_shape",
-    [
-        [2, 256, 512],
-        [4, 4, 256, 512],
-    ],
-)
-@pytest.mark.parametrize(
-    "input_memory_layout",
-    [
-        ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-        ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-        ttnn.TensorMemoryLayout.BLOCK_SHARDED,
-    ],
-)
-@pytest.mark.parametrize(
-    "input_shard_orientation",
-    [
-        ttnn.ShardOrientation.ROW_MAJOR,
-        ttnn.ShardOrientation.COL_MAJOR,
-    ],
-)
-@pytest.mark.parametrize(
-    "num_shard_cores, standard_shard_core_grid, block_shard_core_grid",
-    [
-        [
-            4,
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 3))}),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(1, 1))}),
-        ],
-        [
-            16,
-            ttnn.CoreRangeSet(
-                {
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(7, 0)),
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 2), ttnn.CoreCoord(7, 2)),
-                }
-            ),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 3))}),
-        ],
-    ],
-)
-def test_untilize_multi_core_sharded_to_interleaved(
-    device,
-    dtype,
-    use_pack_untilize,
-    tensor_shape,
-    input_memory_layout,
-    input_shard_orientation,
-    num_shard_cores,
-    standard_shard_core_grid,
-    block_shard_core_grid,
-):
-    num_tensor_dims = len(tensor_shape)
-    tensor_height = 1
-    for i in range(num_tensor_dims - 1):
-        tensor_height *= tensor_shape[i]
-    tensor_width = tensor_shape[num_tensor_dims - 1]
-
-    # Shard shapes
-    height_sharded_shard_shape = (tensor_height // num_shard_cores, tensor_width)
-    width_sharded_shard_shape = (tensor_height, tensor_width // num_shard_cores)
-    block_sharded_shard_shape = (
-        tensor_height // int(math.sqrt(num_shard_cores)),
-        tensor_width // int(math.sqrt(num_shard_cores)),
-    )
-
-    # Shard Memory Layout Map
-    shard_memory_layout_map = {
-        ttnn.TensorMemoryLayout.HEIGHT_SHARDED: {
-            "shard_grid": standard_shard_core_grid,
-            "shard_shape": height_sharded_shard_shape,
-        },
-        ttnn.TensorMemoryLayout.WIDTH_SHARDED: {
-            "shard_grid": standard_shard_core_grid,
-            "shard_shape": width_sharded_shard_shape,
-        },
-        ttnn.TensorMemoryLayout.BLOCK_SHARDED: {
-            "shard_grid": block_shard_core_grid,
-            "shard_shape": block_sharded_shard_shape,
-        },
-    }
-
-    # Input memory config
-    input_shard_memory_layout = shard_memory_layout_map[input_memory_layout]
-    output_shard_spec = ttnn.ShardSpec(
-        input_shard_memory_layout["shard_grid"], input_shard_memory_layout["shard_shape"], input_shard_orientation
-    )
-    input_memory_config = ttnn.MemoryConfig(input_memory_layout, ttnn.BufferType.L1, output_shard_spec)
-
-    # Output memory config
-    output_memory_config = ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.L1)
-
-    # Test
-    input_torch_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    input_ttnn_tensor = ttnn.from_torch(input_torch_tensor, dtype=dtype, layout=ttnn.TILE_LAYOUT)
-    input_ttnn_tensor = ttnn.to_device(input_ttnn_tensor, device, memory_config=input_memory_config)
-    ttnn_output_tensor = ttnn.untilize(
-        input_ttnn_tensor, memory_config=output_memory_config, use_multicore=True, use_pack_untilize=use_pack_untilize
-    )
-
-    assert_with_pcc(input_torch_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
-
-
-@pytest.mark.parametrize("dtype", [ttnn.bfloat16])
-@pytest.mark.parametrize("use_pack_untilize", [True, False])
-@pytest.mark.parametrize("tensor_shape", [[160, 160]])
-@pytest.mark.parametrize(
-    "input_memory_layout, input_shard_shape, input_shard_core_grid",
-    [
-        [
-            ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-            (128, 160),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 1))}),
-        ],
-        [
-            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-            (160, 128),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 1))}),
-        ],
-        [
-            ttnn.TensorMemoryLayout.BLOCK_SHARDED,
-            (128, 128),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(1, 1))}),
-        ],
-    ],
-)
-@pytest.mark.parametrize(
-    "input_shard_orientation",
-    [
-        ttnn.ShardOrientation.ROW_MAJOR,
-        ttnn.ShardOrientation.COL_MAJOR,
-    ],
-)
-def test_untilize_multi_core_sharded_to_interleaved_uneven_input_shard_spec(
-    device,
-    dtype,
-    use_pack_untilize,
-    tensor_shape,
-    input_memory_layout,
-    input_shard_shape,
-    input_shard_core_grid,
-    input_shard_orientation,
-):
-    # Input Memory config
-    input_shard_spec = ttnn.ShardSpec(input_shard_core_grid, input_shard_shape, input_shard_orientation)
-    input_memory_config = ttnn.MemoryConfig(input_memory_layout, ttnn.BufferType.L1, input_shard_spec)
-
-    # Output memory config
-    output_memory_config = ttnn.MemoryConfig(ttnn.TensorMemoryLayout.INTERLEAVED, ttnn.BufferType.L1)
-
-    # Test
-    input_torch_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    input_ttnn_tensor = ttnn.from_torch(input_torch_tensor, dtype=dtype, layout=ttnn.TILE_LAYOUT)
-    input_ttnn_tensor = ttnn.to_device(input_ttnn_tensor, device, memory_config=input_memory_config)
-    ttnn_output_tensor = ttnn.untilize(
-        input_ttnn_tensor, memory_config=output_memory_config, use_multicore=True, use_pack_untilize=use_pack_untilize
-    )
-
-    assert_with_pcc(input_torch_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
-
-
-@pytest.mark.parametrize("dtype", [ttnn.bfloat16])
-@pytest.mark.parametrize("use_pack_untilize", [True])
-@pytest.mark.parametrize("tensor_shape", [[2, 2, 128, 512]])
-@pytest.mark.parametrize(
-    "input_memory_layout, output_memory_layout",
-    [
-        [ttnn.TensorMemoryLayout.HEIGHT_SHARDED, ttnn.TensorMemoryLayout.WIDTH_SHARDED],
-        [ttnn.TensorMemoryLayout.HEIGHT_SHARDED, ttnn.TensorMemoryLayout.BLOCK_SHARDED],
-        [ttnn.TensorMemoryLayout.WIDTH_SHARDED, ttnn.TensorMemoryLayout.HEIGHT_SHARDED],
-        [ttnn.TensorMemoryLayout.WIDTH_SHARDED, ttnn.TensorMemoryLayout.BLOCK_SHARDED],
-        [ttnn.TensorMemoryLayout.BLOCK_SHARDED, ttnn.TensorMemoryLayout.HEIGHT_SHARDED],
-        [ttnn.TensorMemoryLayout.BLOCK_SHARDED, ttnn.TensorMemoryLayout.WIDTH_SHARDED],
-    ],
-)
-@pytest.mark.parametrize("input_shard_orientation", [ttnn.ShardOrientation.ROW_MAJOR, ttnn.ShardOrientation.COL_MAJOR])
-@pytest.mark.parametrize("output_shard_orientation", [ttnn.ShardOrientation.ROW_MAJOR, ttnn.ShardOrientation.COL_MAJOR])
-@pytest.mark.parametrize(
-    "num_shard_cores, standard_shard_core_grid, block_shard_core_grid",
-    [
-        [
-            4,
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 3))}),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(1, 1))}),
-        ],
-        [
-            16,
-            ttnn.CoreRangeSet(
-                {
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(7, 0)),
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 2), ttnn.CoreCoord(7, 2)),
-                }
-            ),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 3))}),
-        ],
-    ],
-)
-def test_untilize_multi_core_sharded_to_sharded_different_shard_types(
-    device,
-    dtype,
-    use_pack_untilize,
-    tensor_shape,
-    input_memory_layout,
-    output_memory_layout,
-    input_shard_orientation,
-    output_shard_orientation,
-    num_shard_cores,
-    standard_shard_core_grid,
-    block_shard_core_grid,
-):
-    num_tensor_dims = len(tensor_shape)
-    tensor_height = 1
-    for i in range(num_tensor_dims - 1):
-        tensor_height *= tensor_shape[i]
-    tensor_width = tensor_shape[num_tensor_dims - 1]
-
-    # Shard shapes
-    height_sharded_shard_shape = (tensor_height // num_shard_cores, tensor_width)
-    width_sharded_shard_shape = (tensor_height, tensor_width // num_shard_cores)
-    block_sharded_shard_shape = (
-        tensor_height // int(math.sqrt(num_shard_cores)),
-        tensor_width // int(math.sqrt(num_shard_cores)),
-    )
-
-    # Shard Memory Layout Map
-    shard_memory_layout_map = {
-        ttnn.TensorMemoryLayout.HEIGHT_SHARDED: {
-            "shard_grid": standard_shard_core_grid,
-            "shard_shape": height_sharded_shard_shape,
-        },
-        ttnn.TensorMemoryLayout.WIDTH_SHARDED: {
-            "shard_grid": standard_shard_core_grid,
-            "shard_shape": width_sharded_shard_shape,
-        },
-        ttnn.TensorMemoryLayout.BLOCK_SHARDED: {
-            "shard_grid": block_shard_core_grid,
-            "shard_shape": block_sharded_shard_shape,
-        },
-    }
-
-    # Input memory config
-    input_shard_memory_layout = shard_memory_layout_map[input_memory_layout]
-    input_shard_spec = ttnn.ShardSpec(
-        input_shard_memory_layout["shard_grid"], input_shard_memory_layout["shard_shape"], input_shard_orientation
-    )
-    input_memory_config = ttnn.MemoryConfig(input_memory_layout, ttnn.BufferType.L1, input_shard_spec)
-
-    # Output memory config
-    output_shard_memory_layout = shard_memory_layout_map[output_memory_layout]
-    output_shard_spec = ttnn.ShardSpec(
-        output_shard_memory_layout["shard_grid"], output_shard_memory_layout["shard_shape"], output_shard_orientation
-    )
-    output_memory_config = ttnn.MemoryConfig(output_memory_layout, ttnn.BufferType.L1, output_shard_spec)
-
-    # Test
-    input_torch_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    input_ttnn_tensor = ttnn.from_torch(input_torch_tensor, dtype=dtype, layout=ttnn.TILE_LAYOUT)
-    input_ttnn_tensor = ttnn.to_device(input_ttnn_tensor, device, memory_config=input_memory_config)
-    ttnn_output_tensor = ttnn.untilize(
-        input_ttnn_tensor, memory_config=output_memory_config, use_multicore=True, use_pack_untilize=use_pack_untilize
-    )
-
-    assert_with_pcc(input_torch_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
-
-
-@pytest.mark.parametrize("dtype", [ttnn.bfloat16])
-@pytest.mark.parametrize("use_pack_untilize", [True, False])
-@pytest.mark.parametrize("tensor_shape", [[160, 160]])
-@pytest.mark.parametrize(
-    "input_memory_layout, input_shard_shape, input_shard_core_grid, output_memory_layout, output_shard_shape, output_shard_core_grid",
-    [
-        [
-            ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-            (128, 160),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 1))}),
-            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-            (160, 32),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 4))}),
-        ],
-        [
-            ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-            (128, 160),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 1))}),
-            ttnn.TensorMemoryLayout.BLOCK_SHARDED,
-            (32, 32),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(4, 4))}),
-        ],
-        [
-            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-            (160, 128),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 1))}),
-            ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-            (32, 160),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 4))}),
-        ],
-        [
-            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-            (160, 128),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 1))}),
-            ttnn.TensorMemoryLayout.BLOCK_SHARDED,
-            (32, 32),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(4, 4))}),
-        ],
-        [
-            ttnn.TensorMemoryLayout.BLOCK_SHARDED,
-            (128, 128),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(1, 1))}),
-            ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-            (32, 160),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 4))}),
-        ],
-        [
-            ttnn.TensorMemoryLayout.BLOCK_SHARDED,
-            (128, 128),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(1, 1))}),
-            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-            (160, 32),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 4))}),
-        ],
-    ],
-)
-@pytest.mark.parametrize("input_shard_orientation", [ttnn.ShardOrientation.ROW_MAJOR, ttnn.ShardOrientation.COL_MAJOR])
-@pytest.mark.parametrize("output_shard_orientation", [ttnn.ShardOrientation.ROW_MAJOR, ttnn.ShardOrientation.COL_MAJOR])
-def test_untilize_multi_core_sharded_to_sharded_different_shard_types_uneven_input_shard_spec(
-    device,
-    dtype,
-    use_pack_untilize,
-    tensor_shape,
-    input_memory_layout,
-    input_shard_shape,
-    input_shard_core_grid,
-    output_memory_layout,
-    output_shard_shape,
-    output_shard_core_grid,
-    input_shard_orientation,
-    output_shard_orientation,
-):
-    # Input Memory config
-    input_shard_spec = ttnn.ShardSpec(input_shard_core_grid, input_shard_shape, input_shard_orientation)
-    input_memory_config = ttnn.MemoryConfig(input_memory_layout, ttnn.BufferType.L1, input_shard_spec)
-
-    # Output memory config
-    output_shard_spec = ttnn.ShardSpec(output_shard_core_grid, output_shard_shape, output_shard_orientation)
-    output_memory_config = ttnn.MemoryConfig(output_memory_layout, ttnn.BufferType.L1, output_shard_spec)
-
-    # Test
-    input_torch_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    input_ttnn_tensor = ttnn.from_torch(input_torch_tensor, dtype=dtype, layout=ttnn.TILE_LAYOUT)
-    input_ttnn_tensor = ttnn.to_device(input_ttnn_tensor, device, memory_config=input_memory_config)
-    ttnn_output_tensor = ttnn.untilize(
-        input_ttnn_tensor, memory_config=output_memory_config, use_multicore=True, use_pack_untilize=use_pack_untilize
-    )
-
-    assert_with_pcc(input_torch_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
-
-
-@pytest.mark.parametrize("dtype", [ttnn.bfloat16])
-@pytest.mark.parametrize("use_pack_untilize", [True])
-@pytest.mark.parametrize("tensor_shape", [[2, 2, 128, 512]])
-@pytest.mark.parametrize(
-    "memory_layout",
-    [
-        ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-        ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-        ttnn.TensorMemoryLayout.BLOCK_SHARDED,
-    ],
-)
-@pytest.mark.parametrize("input_shard_orientation", [ttnn.ShardOrientation.ROW_MAJOR])
-@pytest.mark.parametrize("output_shard_orientation", [ttnn.ShardOrientation.ROW_MAJOR])
-@pytest.mark.parametrize(
-    "input_num_shard_cores, input_standard_shard_core_grid, input_block_shard_core_grid",
-    [
-        [
-            4,
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 3))}),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(1, 1))}),
-        ],
-        [
-            16,
-            ttnn.CoreRangeSet(
-                {
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(7, 0)),
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 2), ttnn.CoreCoord(7, 2)),
-                }
-            ),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 3))}),
-        ],
-    ],
-)
-@pytest.mark.parametrize(
-    "output_num_shard_cores, output_standard_shard_core_grid, output_block_shard_core_grid",
-    [
-        [
-            4,
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 3))}),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(1, 1))}),
-        ],
-        [
-            16,
-            ttnn.CoreRangeSet(
-                {
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(7, 0)),
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 2), ttnn.CoreCoord(7, 2)),
-                }
-            ),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 3))}),
-        ],
-    ],
-)
-def test_untilize_multi_core_sharded_to_sharded_same_shard_type_different_shard_spec(
-    device,
-    dtype,
-    use_pack_untilize,
-    tensor_shape,
-    memory_layout,
-    input_shard_orientation,
-    output_shard_orientation,
-    input_num_shard_cores,
-    input_standard_shard_core_grid,
-    input_block_shard_core_grid,
-    output_num_shard_cores,
-    output_standard_shard_core_grid,
-    output_block_shard_core_grid,
-):
-    num_tensor_dims = len(tensor_shape)
-    tensor_height = 1
-    for i in range(num_tensor_dims - 1):
-        tensor_height *= tensor_shape[i]
-    tensor_width = tensor_shape[num_tensor_dims - 1]
-
-    # Input Shard shapes
-    input_height_sharded_shard_shape = (tensor_height // input_num_shard_cores, tensor_width)
-    input_width_sharded_shard_shape = (tensor_height, tensor_width // input_num_shard_cores)
-    input_block_sharded_shard_shape = (
-        tensor_height // int(math.sqrt(input_num_shard_cores)),
-        tensor_width // int(math.sqrt(input_num_shard_cores)),
-    )
-
-    # Input Shard shapes
-    output_height_sharded_shard_shape = (tensor_height // output_num_shard_cores, tensor_width)
-    output_width_sharded_shard_shape = (tensor_height, tensor_width // output_num_shard_cores)
-    output_block_sharded_shard_shape = (
-        tensor_height // int(math.sqrt(output_num_shard_cores)),
-        tensor_width // int(math.sqrt(output_num_shard_cores)),
-    )
-
-    # Input Shard Memory Layout Map
-    input_shard_memory_layout_map = {
-        ttnn.TensorMemoryLayout.HEIGHT_SHARDED: {
-            "shard_grid": input_standard_shard_core_grid,
-            "shard_shape": input_height_sharded_shard_shape,
-        },
-        ttnn.TensorMemoryLayout.WIDTH_SHARDED: {
-            "shard_grid": input_standard_shard_core_grid,
-            "shard_shape": input_width_sharded_shard_shape,
-        },
-        ttnn.TensorMemoryLayout.BLOCK_SHARDED: {
-            "shard_grid": input_block_shard_core_grid,
-            "shard_shape": input_block_sharded_shard_shape,
-        },
-    }
-
-    # Output Shard Memory Layout Map
-    output_shard_memory_layout_map = {
-        ttnn.TensorMemoryLayout.HEIGHT_SHARDED: {
-            "shard_grid": output_standard_shard_core_grid,
-            "shard_shape": output_height_sharded_shard_shape,
-        },
-        ttnn.TensorMemoryLayout.WIDTH_SHARDED: {
-            "shard_grid": output_standard_shard_core_grid,
-            "shard_shape": output_width_sharded_shard_shape,
-        },
-        ttnn.TensorMemoryLayout.BLOCK_SHARDED: {
-            "shard_grid": output_block_shard_core_grid,
-            "shard_shape": output_block_sharded_shard_shape,
-        },
-    }
-
-    # Input memory config
-    input_shard_memory_layout = input_shard_memory_layout_map[memory_layout]
-    input_shard_spec = ttnn.ShardSpec(
-        input_shard_memory_layout["shard_grid"], input_shard_memory_layout["shard_shape"], input_shard_orientation
-    )
-    input_memory_config = ttnn.MemoryConfig(memory_layout, ttnn.BufferType.L1, input_shard_spec)
-
-    # Output memory config
-    output_shard_memory_layout = output_shard_memory_layout_map[memory_layout]
-    output_shard_spec = ttnn.ShardSpec(
-        output_shard_memory_layout["shard_grid"], output_shard_memory_layout["shard_shape"], output_shard_orientation
-    )
-    output_memory_config = ttnn.MemoryConfig(memory_layout, ttnn.BufferType.L1, output_shard_spec)
-
-    # Test
-    input_torch_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    input_ttnn_tensor = ttnn.from_torch(input_torch_tensor, dtype=dtype, layout=ttnn.TILE_LAYOUT)
-    input_ttnn_tensor = ttnn.to_device(input_ttnn_tensor, device, memory_config=input_memory_config)
-    ttnn_output_tensor = ttnn.untilize(
-        input_ttnn_tensor, memory_config=output_memory_config, use_multicore=True, use_pack_untilize=use_pack_untilize
-    )
-
-    assert_with_pcc(input_torch_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
-
-
-@pytest.mark.parametrize("dtype", [ttnn.bfloat16])
-@pytest.mark.parametrize("use_pack_untilize", [True, False])
-@pytest.mark.parametrize("tensor_shape", [[160, 160]])
-@pytest.mark.parametrize(
-    "memory_layout, input_shard_shape, input_shard_core_grid, output_shard_shape, output_shard_core_grid",
-    [
-        [
-            ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-            (128, 160),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 1))}),
-            (32, 160),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 4))}),
-        ],
-        [
-            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-            (160, 128),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 1))}),
-            (160, 32),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 4))}),
-        ],
-        [
-            ttnn.TensorMemoryLayout.BLOCK_SHARDED,
-            (128, 128),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(1, 1))}),
-            (32, 32),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(4, 4))}),
-        ],
-    ],
-)
-@pytest.mark.parametrize("input_shard_orientation", [ttnn.ShardOrientation.ROW_MAJOR])
-@pytest.mark.parametrize("output_shard_orientation", [ttnn.ShardOrientation.ROW_MAJOR])
-def test_untilize_multi_core_sharded_to_sharded_same_shard_type_different_shard_spec_uneven_input_shard_spec(
-    device,
-    dtype,
-    use_pack_untilize,
-    tensor_shape,
-    memory_layout,
-    input_shard_shape,
-    input_shard_core_grid,
-    output_shard_shape,
-    output_shard_core_grid,
-    input_shard_orientation,
-    output_shard_orientation,
-):
-    # Input Memory config
-    input_shard_spec = ttnn.ShardSpec(input_shard_core_grid, input_shard_shape, input_shard_orientation)
-    input_memory_config = ttnn.MemoryConfig(memory_layout, ttnn.BufferType.L1, input_shard_spec)
-
-    # Output memory config
-    output_shard_spec = ttnn.ShardSpec(output_shard_core_grid, output_shard_shape, output_shard_orientation)
-    output_memory_config = ttnn.MemoryConfig(memory_layout, ttnn.BufferType.L1, output_shard_spec)
-
-    # Test
-    input_torch_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    input_ttnn_tensor = ttnn.from_torch(input_torch_tensor, dtype=dtype, layout=ttnn.TILE_LAYOUT)
-    input_ttnn_tensor = ttnn.to_device(input_ttnn_tensor, device, memory_config=input_memory_config)
-    ttnn_output_tensor = ttnn.untilize(
-        input_ttnn_tensor, memory_config=output_memory_config, use_multicore=True, use_pack_untilize=use_pack_untilize
-    )
-
-    assert_with_pcc(input_torch_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
-
-
-@pytest.mark.parametrize("dtype", [ttnn.bfloat16])
-@pytest.mark.parametrize("use_pack_untilize", [True])
-@pytest.mark.parametrize("tensor_shape", [[2, 2, 128, 512]])
-@pytest.mark.parametrize(
-    "memory_layout",
-    [
-        ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-        ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-        ttnn.TensorMemoryLayout.BLOCK_SHARDED,
-    ],
-)
-@pytest.mark.parametrize("shard_orientation", [ttnn.ShardOrientation.ROW_MAJOR])
-@pytest.mark.parametrize(
-    "num_shard_cores, standard_shard_core_grid, block_shard_core_grid",
-    [
-        [
-            4,
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 3))}),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(1, 1))}),
-        ],
-        [
-            16,
-            ttnn.CoreRangeSet(
-                {
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(7, 0)),
-                    ttnn.CoreRange(ttnn.CoreCoord(0, 2), ttnn.CoreCoord(7, 2)),
-                }
-            ),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 3))}),
-        ],
-    ],
-)
-def test_untilize_multi_core_sharded_to_sharded_same_shard_type_and_shard_spec(
-    device,
-    dtype,
-    use_pack_untilize,
-    tensor_shape,
-    memory_layout,
-    shard_orientation,
-    num_shard_cores,
-    standard_shard_core_grid,
-    block_shard_core_grid,
-):
-    # This test targets a special case implementation for when
-    # the input and output shard types and shard specs are identical
-
-    num_tensor_dims = len(tensor_shape)
-    tensor_height = 1
-    for i in range(num_tensor_dims - 1):
-        tensor_height *= tensor_shape[i]
-    tensor_width = tensor_shape[num_tensor_dims - 1]
-
-    # Shard shapes
-    height_sharded_shard_shape = (tensor_height // num_shard_cores, tensor_width)
-    width_sharded_shard_shape = (tensor_height, tensor_width // num_shard_cores)
-    block_sharded_shard_shape = (
-        tensor_height // int(math.sqrt(num_shard_cores)),
-        tensor_width // int(math.sqrt(num_shard_cores)),
-    )
-
-    # Shard Memory Layout Map
-    shard_memory_layout_map = {
-        ttnn.TensorMemoryLayout.HEIGHT_SHARDED: {
-            "shard_grid": standard_shard_core_grid,
-            "shard_shape": height_sharded_shard_shape,
-        },
-        ttnn.TensorMemoryLayout.WIDTH_SHARDED: {
-            "shard_grid": standard_shard_core_grid,
-            "shard_shape": width_sharded_shard_shape,
-        },
-        ttnn.TensorMemoryLayout.BLOCK_SHARDED: {
-            "shard_grid": block_shard_core_grid,
-            "shard_shape": block_sharded_shard_shape,
-        },
-    }
-
-    # Memory config
-    shard_memory_layout = shard_memory_layout_map[memory_layout]
-    shard_spec = ttnn.ShardSpec(
-        shard_memory_layout["shard_grid"], shard_memory_layout["shard_shape"], shard_orientation
-    )
-    memory_config = ttnn.MemoryConfig(memory_layout, ttnn.BufferType.L1, shard_spec)
-
-    # Test
-    input_torch_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    input_ttnn_tensor = ttnn.from_torch(input_torch_tensor, dtype=dtype, layout=ttnn.TILE_LAYOUT)
-    input_ttnn_tensor = ttnn.to_device(input_ttnn_tensor, device, memory_config=memory_config)
-    ttnn_output_tensor = ttnn.untilize(
-        input_ttnn_tensor, memory_config=memory_config, use_multicore=True, use_pack_untilize=use_pack_untilize
-    )
-
-    assert_with_pcc(input_torch_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
-
-
-@pytest.mark.parametrize("dtype", [ttnn.bfloat16])
-@pytest.mark.parametrize("use_pack_untilize", [True, False])
-@pytest.mark.parametrize("tensor_shape", [[160, 160]])
-@pytest.mark.parametrize(
-    "memory_layout, shard_shape, shard_core_grid",
-    [
-        [
-            ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-            (128, 160),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 1))}),
-        ],
-        [
-            ttnn.TensorMemoryLayout.WIDTH_SHARDED,
-            (160, 128),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(0, 1))}),
-        ],
-        [
-            ttnn.TensorMemoryLayout.BLOCK_SHARDED,
-            (128, 128),
-            ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(1, 1))}),
-        ],
-    ],
-)
-@pytest.mark.parametrize("shard_orientation", [ttnn.ShardOrientation.ROW_MAJOR])
-def test_untilize_multi_core_sharded_to_sharded_same_shard_type_and_shard_spec_uneven_shard_spec(
-    device,
-    dtype,
-    use_pack_untilize,
-    tensor_shape,
-    memory_layout,
-    shard_shape,
-    shard_core_grid,
-    shard_orientation,
-):
-    # This test targets a special case implementation for when
-    # the input and output shard types and shard specs are identical
-
-    # Memory config
-    shard_spec = ttnn.ShardSpec(shard_core_grid, shard_shape, shard_orientation)
-    memory_config = ttnn.MemoryConfig(memory_layout, ttnn.BufferType.L1, shard_spec)
-
-    # Test
-    input_torch_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    input_ttnn_tensor = ttnn.from_torch(input_torch_tensor, dtype=dtype, layout=ttnn.TILE_LAYOUT)
-    input_ttnn_tensor = ttnn.to_device(input_ttnn_tensor, device, memory_config=memory_config)
-    ttnn_output_tensor = ttnn.untilize(
-        input_ttnn_tensor, memory_config=memory_config, use_multicore=True, use_pack_untilize=use_pack_untilize
-    )
-
-    assert_with_pcc(input_torch_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
-
-
-@pytest.mark.parametrize("dtype", [ttnn.bfloat16])
-@pytest.mark.parametrize("use_pack_untilize", [True])
-@pytest.mark.parametrize("tensor_shape", [[1, 1, 512, 512]])
-@pytest.mark.parametrize("input_buffer_type", [ttnn.BufferType.L1, ttnn.BufferType.DRAM])
-@pytest.mark.parametrize("output_buffer_type", [ttnn.BufferType.L1, ttnn.BufferType.DRAM])
-@pytest.mark.parametrize(
-    "input_memory_layout",
-    [
-        ttnn.TensorMemoryLayout.INTERLEAVED,
-        ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-    ],
-)
-@pytest.mark.parametrize(
-    "output_memory_layout",
-    [
-        ttnn.TensorMemoryLayout.HEIGHT_SHARDED,
-        ttnn.TensorMemoryLayout.INTERLEAVED,
-    ],
-)
-def test_untilize_multi_core_buffer_type_variations(
-    device,
-    dtype,
-    use_pack_untilize,
-    tensor_shape,
-    input_buffer_type,
-    output_buffer_type,
-    input_memory_layout,
-    output_memory_layout,
-):
-    if input_buffer_type == ttnn.BufferType.DRAM and input_memory_layout != ttnn.TensorMemoryLayout.INTERLEAVED:
-        pytest.skip("Untilize multicore does not support input DRAM sharded")
-
-    height_shard_spec = ttnn.ShardSpec(
-        ttnn.CoreRangeSet({ttnn.CoreRange(ttnn.CoreCoord(0, 0), ttnn.CoreCoord(3, 0))}),
-        (128, 512),
-        ttnn.ShardOrientation.ROW_MAJOR,
-    )
-
-    # Input memory config
-    if input_memory_layout == ttnn.TensorMemoryLayout.INTERLEAVED:
-        input_memory_config = ttnn.MemoryConfig(input_memory_layout, input_buffer_type)
-    else:
-        input_memory_config = ttnn.MemoryConfig(input_memory_layout, input_buffer_type, height_shard_spec)
-
-    # Output memory config
-    if output_memory_layout == ttnn.TensorMemoryLayout.INTERLEAVED:
-        output_memory_config = ttnn.MemoryConfig(output_memory_layout, output_buffer_type)
-    else:
-        output_memory_config = ttnn.MemoryConfig(output_memory_layout, output_buffer_type, height_shard_spec)
-
-    # Test
-    input_torch_tensor = torch.randn(tensor_shape, dtype=torch.bfloat16)
-    input_ttnn_tensor = ttnn.from_torch(input_torch_tensor, dtype=dtype, layout=ttnn.TILE_LAYOUT)
-    input_ttnn_tensor = ttnn.to_device(input_ttnn_tensor, device, memory_config=input_memory_config)
-    ttnn_output_tensor = ttnn.untilize(
-        input_ttnn_tensor, memory_config=output_memory_config, use_multicore=True, use_pack_untilize=use_pack_untilize
-    )
-
-    assert_with_pcc(input_torch_tensor, ttnn.to_torch(ttnn_output_tensor), 0.9999)
diff --git a/tt-train/configs/training_mnist_mlp.yaml b/tt-train/configs/training_mnist_mlp.yaml
index adbda5fd87..8975066088 100644
--- a/tt-train/configs/training_mnist_mlp.yaml
+++ b/tt-train/configs/training_mnist_mlp.yaml
@@ -10,5 +10,5 @@ training_config:
   model_path: "/tmp/mnist_mlp.msgpack"
   mlp_config:
     input_features: 784
-    hidden_features: [128, 128]
+    hidden_features: [128]
     output_features: 10
diff --git a/tt-train/configs/training_shakespear_gpt2xl.yaml b/tt-train/configs/training_shakespear_gpt2xl.yaml
index b58d7a6feb..6e47e67ac9 100644
--- a/tt-train/configs/training_shakespear_gpt2xl.yaml
+++ b/tt-train/configs/training_shakespear_gpt2xl.yaml
@@ -23,7 +23,3 @@ training_config:
     max_sequence_length: 1024
     experimental:
       use_composite_layernorm: false
-
-device_config:
-  enable_tp: true
-  mesh_shape: [1,2]
diff --git a/tt-train/configs/training_shakespeare_llama3_tensor_parallel.yaml b/tt-train/configs/training_shakespeare_llama3_tensor_parallel.yaml
index 3562a10d3d..0382793610 100644
--- a/tt-train/configs/training_shakespeare_llama3_tensor_parallel.yaml
+++ b/tt-train/configs/training_shakespeare_llama3_tensor_parallel.yaml
@@ -26,6 +26,3 @@ eval_config:
   temperature: 0.7
   top_k: 50
   top_p: 1.0
-device_config:
-  enable_tp: true
-  mesh_shape: [1,2]
diff --git a/tt-train/configs/training_shakespeare_nanogpt_ddp_n300.yaml b/tt-train/configs/training_shakespeare_nanogpt_ddp_n300.yaml
deleted file mode 100644
index da2d8c9e38..0000000000
--- a/tt-train/configs/training_shakespeare_nanogpt_ddp_n300.yaml
+++ /dev/null
@@ -1,34 +0,0 @@
-training_config:
-    project_name: "tt_train_nano_gpt"
-    model_type: "gpt2"
-    seed: 5489
-    model_save_interval: 500
-    batch_size: 64
-    num_epochs: 1
-    max_steps: 5000
-    learning_rate: 0.0003
-    weight_decay: 0.01
-    use_moreh_adamw: true
-    use_kahan_summation: false
-    use_clip_grad_norm: false
-    clip_grad_norm_max_norm: 1.0
-    transformer_config:
-      num_heads: 6
-      embedding_dim: 384
-      dropout_prob: 0.2
-      num_blocks: 6
-      vocab_size: 96
-      max_sequence_length: 256
-      positional_embedding_type: trainable
-      runner_type: default
-      experimental:
-        use_composite_layernorm: false
-eval_config:
-    repetition_penalty: 1.0
-    temperature: 0.7
-    top_k: 50
-    top_p: 1.0
-device_config:
-    enable_ddp: true
-    mesh_shape: [1,2]
-    device_ids: [0,1]
diff --git a/tt-train/configs/training_shakespeare_tiny_llama3_tensor_parallel.yaml b/tt-train/configs/training_shakespeare_tiny_llama3_tensor_parallel.yaml
index e97900ec34..ad0bfe6ed9 100644
--- a/tt-train/configs/training_shakespeare_tiny_llama3_tensor_parallel.yaml
+++ b/tt-train/configs/training_shakespeare_tiny_llama3_tensor_parallel.yaml
@@ -28,6 +28,3 @@ eval_config:
   temperature: 0.7
   top_k: 50
   top_p: 1.0
-device_config:
-  enable_tp: true
-  mesh_shape: [1,2]
diff --git a/tt-train/sources/examples/linear_regression_ddp/main.cpp b/tt-train/sources/examples/linear_regression_ddp/main.cpp
index 57497f0b24..dcdd70e762 100644
--- a/tt-train/sources/examples/linear_regression_ddp/main.cpp
+++ b/tt-train/sources/examples/linear_regression_ddp/main.cpp
@@ -32,7 +32,7 @@ int main() {
     const size_t num_targets = 32;
     const float noise = 0.0F;
     const bool bias = true;
-    ttml::autograd::ctx().open_device(tt::tt_metal::distributed::MeshShape(1, 2));
+    ttml::autograd::ctx().set_mesh_shape(tt::tt_metal::distributed::MeshShape(1, 2));
 
     auto training_params = ttml::datasets::MakeRegressionParams{
         .n_samples = training_samples_count,
diff --git a/tt-train/sources/examples/mnist_mlp/main.cpp b/tt-train/sources/examples/mnist_mlp/main.cpp
index b7951d4a23..91291bab46 100644
--- a/tt-train/sources/examples/mnist_mlp/main.cpp
+++ b/tt-train/sources/examples/mnist_mlp/main.cpp
@@ -5,7 +5,6 @@
 #include <yaml-cpp/node/node.h>
 
 #include <CLI/CLI.hpp>
-#include <chrono>
 #include <core/ttnn_all_includes.hpp>
 #include <cstdint>
 #include <functional>
@@ -69,10 +68,7 @@ TrainingConfig parse_config(const YAML::Node &yaml_config) {
 void initialize_device(bool enable_tp) {
     if (enable_tp) {
         // we support only N300 for now
-        ttml::autograd::ctx().open_device(tt::tt_metal::distributed::MeshShape(1, 2));
-    } else {
-        // use single device defaults
-        ttml::autograd::ctx().open_device();
+        ttml::autograd::ctx().set_mesh_shape(tt::tt_metal::distributed::MeshShape(1, 2));
     }
 }
 
@@ -174,7 +170,6 @@ int main(int argc, char **argv) {
         dataset.test_images, dataset.test_labels);
 
     auto *device = &ttml::autograd::ctx().get_device();
-    device->enable_program_cache();
     std::function<BatchType(std::vector<DatasetSample> && samples)> collate_fn =
         [num_features, num_targets, device](std::vector<DatasetSample> &&samples) {
             const uint32_t batch_size = samples.size();
@@ -255,8 +250,6 @@ int main(int argc, char **argv) {
     };
 
     for (size_t epoch = 0; epoch < config.num_epochs; ++epoch) {
-        auto start_time = std::chrono::steady_clock::now();
-        uint32_t num_steps_in_epoch = 0;
         for (const auto &[data, target] : train_dataloader) {
             optimizer.zero_grad();
             auto output = run_model(model, data);
@@ -275,13 +268,8 @@ int main(int argc, char **argv) {
             optimizer.step();
             ttml::autograd::ctx().reset_graph();
             training_step++;
-            num_steps_in_epoch++;
         }
 
-        auto end_time = std::chrono::steady_clock::now();
-        auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);
-        fmt::println("per step ms: {} ms", duration.count() / num_steps_in_epoch);
-
         const float test_accuracy = evaluate(test_dataloader, model, num_targets);
         fmt::print(
             "Epoch: {:3d} | Average Loss: {:.4f} | Accuracy: {:.4f}%\n",
diff --git a/tt-train/sources/examples/nano_gpt/3tier/common.cpp b/tt-train/sources/examples/nano_gpt/3tier/common.cpp
index 2f13d40df3..8a72ac8b8a 100644
--- a/tt-train/sources/examples/nano_gpt/3tier/common.cpp
+++ b/tt-train/sources/examples/nano_gpt/3tier/common.cpp
@@ -113,11 +113,8 @@ uint32_t round_up_to_tile(uint32_t value, uint32_t tile_size) {
 
 void initialize_device(bool ddp, bool tp) {
     if (ddp || tp) {
-        // FIXME: currently hardcoded for n300
-        ttml::autograd::ctx().open_device(tt::tt_metal::distributed::MeshShape(1, 2));
-    } else {
-        // use single device defaults
-        ttml::autograd::ctx().open_device();
+        // currently supports only N300 device
+        ttml::autograd::ctx().set_mesh_shape(tt::tt_metal::distributed::MeshShape(1, 2));
     }
 }
 
diff --git a/tt-train/sources/examples/nano_gpt/main.cpp b/tt-train/sources/examples/nano_gpt/main.cpp
index ea189fb516..a5dff22d40 100644
--- a/tt-train/sources/examples/nano_gpt/main.cpp
+++ b/tt-train/sources/examples/nano_gpt/main.cpp
@@ -62,7 +62,7 @@ ttml::serialization::NamedParameters get_model_parameters(Model &model) {
     return model->parameters();
 }
 
-uint64_t get_number_of_parameters(Model &model, bool tp) {
+uint64_t get_number_of_parameters(Model &model, bool enable_tp) {
     auto *device = &ttml::autograd::ctx().get_device();
     auto num_devices = static_cast<uint32_t>(device->num_devices());
 
@@ -75,7 +75,7 @@ uint64_t get_number_of_parameters(Model &model, bool tp) {
     for (const auto &[name, tensor_ptr] : parameters) {
         auto tensor = tensor_ptr->get_value();
         auto params_in_tensor = tensor.logical_volume();
-        if (tp && (contains(name, "fc") || contains(name, "linear"))) {
+        if (enable_tp && (contains(name, "fc") || contains(name, "linear"))) {
             num_params += params_in_tensor * num_devices;
         } else {
             num_params += params_in_tensor;
@@ -258,7 +258,6 @@ void generate(
     auto pad_token_id = 0U;
     auto original_vocab_size = tokenizer.get_vocab_size();
     fmt::println("Original tokenizer vocab size: {}", original_vocab_size);
-
     auto *device = &ttml::autograd::ctx().get_device();
     auto num_devices = static_cast<uint32_t>(device->num_devices());
     // this is workaround for tensor parallel case, we need to have vocab size divisible by 32 per device
@@ -435,49 +434,6 @@ TrainingConfig parse_config(const YAML::Node &yaml_config) {
     return config;
 }
 
-struct DeviceConfig {
-    // multidevice config: default to single device with default mapping of
-    // physical devices onto the mesh shape.
-    tt::tt_metal::distributed::MeshShape mesh_shape{1, 1};
-    std::vector<int> device_ids{};
-
-    bool enable_ddp = false;
-    bool enable_tp = false;
-};
-
-DeviceConfig parse_device_config(const YAML::Node &yaml_config) {
-    DeviceConfig config;
-    auto device_node = yaml_config["device_config"];
-    if (!device_node) {
-        return config;
-    }
-
-    config.enable_ddp = device_node["enable_ddp"].as<bool>(false);
-    config.enable_tp = device_node["enable_tp"].as<bool>(false);
-
-    if (config.enable_ddp && config.enable_tp) {
-        throw std::runtime_error("DDP and TP cannot be enabled at the same time. Disable DDP or TP.");
-    }
-
-    auto mesh_shape_node = device_node["mesh_shape"];
-    bool multidevice = config.enable_ddp || config.enable_tp;
-    if (multidevice && !mesh_shape_node) {
-        throw std::runtime_error("Mesh shape is required for multidevice training");
-    }
-    if (mesh_shape_node) {
-        assert(mesh_shape_node.size() == 2);
-        auto mesh_shape = mesh_shape_node.as<std::vector<int>>();
-        config.mesh_shape = tt::tt_metal::distributed::MeshShape(mesh_shape[0], mesh_shape[1]);
-    }
-
-    auto device_ids_node = device_node["device_ids"];
-    if (device_ids_node) {
-        config.device_ids = device_ids_node.as<std::vector<int>>();
-    }
-
-    return config;
-}
-
 const std::unordered_map<
     std::string,
     std::function<std::unique_ptr<ttml::schedulers::LRSchedulerBase>(ttml::optimizers::OptimizerBase *, size_t)>>
@@ -493,20 +449,27 @@ int main(int argc, char **argv) {
     bool is_eval = false;
     bool add_time_to_name = true;
     bool enable_wandb = true;
+    bool ddp = false;
+    bool enable_tp = false;
     std::string save_and_exit_path = "";
     app.add_option("-c,--config", config_name, "Yaml Config name")->default_val(config_name);
     app.add_option("-e,--eval", is_eval, "Is evaluation")->default_val(is_eval);
     app.add_option("-t,--add_time_to_name", add_time_to_name, "Add time to run name")->default_val(add_time_to_name);
     app.add_option("-w,--wandb", enable_wandb, "Enable wandb logging")->default_val(enable_wandb);
+    app.add_option("-d,--ddp", ddp, "Enable DDP")->default_val(ddp);
+    app.add_option("-p,--tp", enable_tp, "Enable TP")->default_val(enable_tp);
     app.add_option("-n,--name", run_name, "Run name")->default_val(run_name);
     app.add_option("-s,--save_and_exit", save_and_exit_path, "Save and exit (path to dumped msgpack)")
         ->default_val(save_and_exit_path);
     CLI11_PARSE(app, argc, argv);
 
+    if (ddp && enable_tp) {
+        throw std::logic_error("DDP and TP cannot be enabled at the same time. Disable DDP or TP.");
+    }
+
     auto yaml_config = YAML::LoadFile(config_name);
     TrainingConfig config = parse_config(yaml_config);
     EvalConfig eval_config = parse_eval_config(yaml_config);
-    DeviceConfig device_config = parse_device_config(yaml_config);
 
     if (config.enable_mpi) {
         auto &ctx = ttml::autograd::ctx();
@@ -519,12 +482,12 @@ int main(int argc, char **argv) {
         enable_wandb = false;
     }
 
-    if (device_config.enable_ddp || device_config.enable_tp) {
-        fmt::println("Device config:");
-        fmt::println("  Tensor parallel enabled: {}", device_config.enable_tp);
-        fmt::println("  Distributed data-parallel enabled: {}", device_config.enable_ddp);
-        fmt::println("  Mesh shape: {}", device_config.mesh_shape);
-        fmt::println("  Device IDs: {}", device_config.device_ids);
+    // needs more validation for TP
+    if (ddp) {
+        fmt::println("Distributed data parallel is enabled");
+    }
+    if (enable_tp) {
+        fmt::println("Tensor parallel is enabled");
     }
 
     if (config.enable_mpi) {
@@ -543,7 +506,9 @@ int main(int argc, char **argv) {
         }
     }
 
-    if (device_config.enable_tp) {
+    initialize_device(ddp, enable_tp);
+
+    if (enable_tp) {
         if (!config.model_path.empty()) {
             throw std::runtime_error("Save and load is not supported with Tensor Parallel model");
         }
@@ -659,8 +624,6 @@ int main(int argc, char **argv) {
     fmt::print("Vocab size: {}\n", tokenizer->get_vocab_size());
     fmt::print("Tokenizer type: {}\n", config.tokenizer_type);
 
-    initialize_device(device_config.mesh_shape, device_config.device_ids);
-
     auto *device = &ttml::autograd::ctx().get_device();
     device->enable_program_cache();
 
@@ -682,7 +645,7 @@ int main(int argc, char **argv) {
         ttml::core::from_vector(mask, ttml::core::create_shape({1, 1, sequence_length, sequence_length}), device));
 
     std::function<BatchType(std::vector<DatasetSample> && samples)> collate_fn =
-        [sequence_length, num_heads, device, &cached_data, &device_config](std::vector<DatasetSample> &&samples) {
+        [sequence_length, num_heads, device, &cached_data, ddp](std::vector<DatasetSample> &&samples) {
             auto start_timer = std::chrono::high_resolution_clock::now();
             const uint32_t batch_size = samples.size();
             std::vector<uint32_t> &data = cached_data.data;
@@ -702,7 +665,7 @@ int main(int argc, char **argv) {
             fmt::print("dataloader host only step time {} ms\n", (double)duration / 1000.);
 
             auto create_data_and_targets = [&]() -> std::tuple<TensorPtr, TensorPtr> {
-                if (device_config.enable_ddp) {
+                if (ddp) {
                     auto data_xtensor = xt::adapt(data, {batch_size, 1U, 1U, sequence_length});
                     auto data_composer = ttml::core::ShardXTensorToMesh<uint32_t>(device->shape(), 0);
                     auto data_tensor =
@@ -746,7 +709,7 @@ int main(int argc, char **argv) {
     std::visit(
         [&](auto &&arg) {
             if constexpr (requires { arg.vocab_size; }) {
-                arg.vocab_size = round_up_to_tile(tokenizer->get_vocab_size(), (device_config.enable_tp ? num_devices : 1U) * 32U);
+                arg.vocab_size = round_up_to_tile(tokenizer->get_vocab_size(), (enable_tp ? num_devices : 1U) * 32U);
             } else {
                 throw std::runtime_error(
                     "Unsupported transformer configuration type: " + std::string(typeid(arg).name()));
@@ -755,15 +718,15 @@ int main(int argc, char **argv) {
         config.transformer_config);
 
     Model model = std::visit(
-        [&device_config](auto &&arg) -> Model {
+        [enable_tp](auto &&arg) -> Model {
             if constexpr (std::is_same_v<std::decay_t<decltype(arg)>, ttml::models::llama::LlamaConfig>) {
-                if (device_config.enable_tp) {
+                if (enable_tp) {
                     return ttml::models::distributed::llama::create(arg);
                 } else {
                     return ttml::models::llama::create(arg);
                 }
             } else if constexpr (std::is_same_v<std::decay_t<decltype(arg)>, ttml::models::gpt2::TransformerConfig>) {
-                if (device_config.enable_tp) {
+                if (enable_tp) {
                     return ttml::models::distributed::gpt2::create(arg);
                 } else {
                     return ttml::models::gpt2::create(arg);
@@ -806,7 +769,7 @@ int main(int argc, char **argv) {
                 std::visit([](auto &&arg) { return arg.max_sequence_length; }, config.transformer_config),
                 num_heads,
                 sequence_length,
-                device_config.enable_tp,
+                enable_tp,
                 eval_config.temperature,
                 eval_config.repetition_penalty,
                 eval_config.top_k,
@@ -829,7 +792,7 @@ int main(int argc, char **argv) {
         fmt::println("Remote optimizer configured!");
     }
 
-    fmt::print("Number of parameters: {}\n", get_number_of_parameters(model, device_config.enable_tp));
+    fmt::print("Number of parameters: {}\n", get_number_of_parameters(model, enable_tp));
 
     auto select_optimizer =
         [&model, &adamw_params, &config](bool use_moreh_adamw) -> std::unique_ptr<ttml::optimizers::OptimizerBase> {
@@ -911,12 +874,12 @@ int main(int argc, char **argv) {
             if (gradient_accumulator_helper.should_step()) {
                 // synchronize gradients for multi-device case, no-op if single device
                 auto parameters = get_model_parameters(model);
-                if (!device_config.enable_tp) {
+                if (!enable_tp) {
                     ttml::core::distributed::synchronize_parameters(parameters);
                 }
 
                 if (config.use_clip_grad_norm) {
-                    if (device_config.enable_tp) {
+                    if (enable_tp) {
                         throw std::logic_error("Clip grad norm is not supported with TP");
                     }
                     ttml::core::clip_grad_norm(parameters, config.clip_grad_norm_max_norm);
diff --git a/tt-train/sources/examples/nano_gpt/utils.cpp b/tt-train/sources/examples/nano_gpt/utils.cpp
index f6697c1e6d..3cb60072c1 100644
--- a/tt-train/sources/examples/nano_gpt/utils.cpp
+++ b/tt-train/sources/examples/nano_gpt/utils.cpp
@@ -94,6 +94,9 @@ std::unique_ptr<ttml::schedulers::LRSchedulerBase> create_warmup_with_linear_sch
     return std::make_unique<ttml::schedulers::SequentialScheduler>(optimizer, std::move(schedulers), std::move(steps));
 }
 
-void initialize_device(const tt::tt_metal::distributed::MeshShape &mesh_shape, const std::vector<int> &device_ids) {
-    ttml::autograd::ctx().open_device(mesh_shape, device_ids);
+void initialize_device(bool ddp, bool tp) {
+    if (ddp || tp) {
+        // currently supports only N300 device
+        ttml::autograd::ctx().set_mesh_shape(tt::tt_metal::distributed::MeshShape(1, 2));
+    }
 }
diff --git a/tt-train/sources/examples/nano_gpt/utils.hpp b/tt-train/sources/examples/nano_gpt/utils.hpp
index 7594359467..3d4f843b91 100644
--- a/tt-train/sources/examples/nano_gpt/utils.hpp
+++ b/tt-train/sources/examples/nano_gpt/utils.hpp
@@ -175,6 +175,4 @@ std::string generate_run_name(const std::string &run_name, const TrainingConfig
     return ss.str();
 }
 
-void initialize_device(
-    const tt::tt_metal::distributed::MeshShape &mesh_shape,
-    const std::vector<int> &device_ids);
+void initialize_device(bool ddp, bool tp);
diff --git a/tt-train/sources/ttml/autograd/auto_context.cpp b/tt-train/sources/ttml/autograd/auto_context.cpp
index e33c61e469..3ba412bcd2 100644
--- a/tt-train/sources/ttml/autograd/auto_context.cpp
+++ b/tt-train/sources/ttml/autograd/auto_context.cpp
@@ -46,13 +46,11 @@ void AutoContext::reset_graph() {
     m_graph.reset();
 }
 
-void AutoContext::open_device(
-    const tt::tt_metal::distributed::MeshShape& mesh_shape, const std::vector<int>& device_ids) {
+void AutoContext::open_device() {
     if (m_device) {
         throw std::runtime_error("open_device was called after the device was created.");
     }
-    m_mesh_shape = mesh_shape;
-    m_device = std::make_unique<core::MeshDevice>(m_mesh_shape, device_ids);
+    m_device = std::make_unique<core::MeshDevice>(m_mesh_shape);
 }
 
 void AutoContext::close_device() {
@@ -70,6 +68,13 @@ ttnn::distributed::MeshDevice& AutoContext::get_device() {
 AutoContext::AutoContext() : m_generator(m_seed) {
 }
 
+void AutoContext::set_mesh_shape(tt::tt_metal::distributed::MeshShape shape) {
+    if (m_device) {
+        throw std::runtime_error("set_mesh_shape was called after the device was created.");
+    }
+    m_mesh_shape = shape;
+}
+
 tt::tt_metal::distributed::MeshShape AutoContext::get_mesh_shape() const {
     return m_mesh_shape;
 }
diff --git a/tt-train/sources/ttml/autograd/auto_context.hpp b/tt-train/sources/ttml/autograd/auto_context.hpp
index ff0f2e2de8..50682bf789 100644
--- a/tt-train/sources/ttml/autograd/auto_context.hpp
+++ b/tt-train/sources/ttml/autograd/auto_context.hpp
@@ -46,11 +46,10 @@ public:
 
     ttnn::distributed::MeshDevice& get_device();
 
+    void set_mesh_shape(tt::tt_metal::distributed::MeshShape shape);
     [[nodiscard]] tt::tt_metal::distributed::MeshShape get_mesh_shape() const;
 
-    void open_device(
-        const tt::tt_metal::distributed::MeshShape& mesh_shape = tt::tt_metal::distributed::MeshShape(1, 1),
-        const std::vector<int>& device_ids = std::vector<int>{});
+    void open_device();
 
     void close_device();
 
diff --git a/tt-train/sources/ttml/core/mesh_device.cpp b/tt-train/sources/ttml/core/mesh_device.cpp
index ba6194ee7e..eb48d64dd2 100644
--- a/tt-train/sources/ttml/core/mesh_device.cpp
+++ b/tt-train/sources/ttml/core/mesh_device.cpp
@@ -6,15 +6,13 @@
 
 namespace ttml::core {
 
-MeshDevice::MeshDevice(const tt::tt_metal::distributed::MeshShape& shape, const std::vector<int>& device_ids) :
+MeshDevice::MeshDevice(tt::tt_metal::distributed::MeshShape shape) :
     m_mesh_device(ttnn::distributed::open_mesh_device(
         shape,
         DEFAULT_L1_SMALL_SIZE,
         DEFAULT_TRACE_REGION_SIZE,
-        /* num_command_queues=*/1,
-        tt::tt_metal::DispatchCoreConfig{},
-        /*offset=*/std::nullopt,
-        /*physical_device_ids=*/device_ids)) {
+        /* num_command_queues*/ 1,
+        tt::tt_metal::DispatchCoreConfig{})) {
     assert(m_mesh_device);
 }
 
diff --git a/tt-train/sources/ttml/core/mesh_device.hpp b/tt-train/sources/ttml/core/mesh_device.hpp
index 6a0b8778db..490f9d5b45 100644
--- a/tt-train/sources/ttml/core/mesh_device.hpp
+++ b/tt-train/sources/ttml/core/mesh_device.hpp
@@ -11,7 +11,7 @@ namespace ttml::core {
 // should I implement pimpl or its fine
 class MeshDevice {
 public:
-    explicit MeshDevice(const tt::tt_metal::distributed::MeshShape& shape, const std::vector<int>& device_ids);
+    explicit MeshDevice(tt::tt_metal::distributed::MeshShape shape);
     MeshDevice(MeshDevice&& device) = default;
     MeshDevice(const MeshDevice&) = delete;
 
diff --git a/tt-train/sources/ttml/core/tt_tensor_utils.cpp b/tt-train/sources/ttml/core/tt_tensor_utils.cpp
index d5f00f616b..3f29c90cca 100644
--- a/tt-train/sources/ttml/core/tt_tensor_utils.cpp
+++ b/tt-train/sources/ttml/core/tt_tensor_utils.cpp
@@ -96,10 +96,12 @@ std::vector<tt::tt_metal::HostBuffer> get_as(const ttnn::Tensor& tensor) {
             if constexpr (std::is_same_v<StorageType, tt::tt_metal::HostStorage>) {
                 return {storage.buffer};
             } else if constexpr (std::is_same_v<StorageType, tt::tt_metal::MultiDeviceHostStorage>) {
+                auto num_buffers = storage.num_buffers();
                 std::vector<tt::tt_metal::HostBuffer> buffers;
-                buffers.reserve(storage.distributed_buffer().shard_coords().size());
-                storage.distributed_buffer().apply(
-                    [&buffers](const tt::tt_metal::HostBuffer& shard) { buffers.push_back(shard); });
+                buffers.reserve(num_buffers);
+                for (uint32_t i = 0; i < num_buffers; ++i) {
+                    buffers.push_back(storage.get_buffer(i));
+                }
                 return buffers;
             } else {
                 throw std::runtime_error("Tensor must be on host");
diff --git a/tt-train/tests/core/n300_utils_test.cpp b/tt-train/tests/core/n300_utils_test.cpp
index c15a797119..0895027178 100644
--- a/tt-train/tests/core/n300_utils_test.cpp
+++ b/tt-train/tests/core/n300_utils_test.cpp
@@ -26,7 +26,8 @@ protected:
         if (!check_board_is_n300()) {
             GTEST_SKIP() << "Skipping N300 specific tests";
         }
-        ttml::autograd::ctx().open_device(tt::tt_metal::distributed::MeshShape(1, 2));
+        ttml::autograd::ctx().set_mesh_shape(tt::tt_metal::distributed::MeshShape(1, 2));
+        ttml::autograd::ctx().open_device();
     }
 
     void TearDown() override {
diff --git a/tt-train/tests/model/linear_regression_ddp_test.cpp b/tt-train/tests/model/linear_regression_ddp_test.cpp
index 73487a02a4..47beee0efd 100644
--- a/tt-train/tests/model/linear_regression_ddp_test.cpp
+++ b/tt-train/tests/model/linear_regression_ddp_test.cpp
@@ -34,7 +34,8 @@ protected:
         if (!check_board_is_n300()) {
             GTEST_SKIP() << "Skipping N300 specific tests";
         }
-        ttml::autograd::ctx().open_device(tt::tt_metal::distributed::MeshShape(1, 2));
+        ttml::autograd::ctx().set_mesh_shape(tt::tt_metal::distributed::MeshShape(1, 2));
+        ttml::autograd::ctx().open_device();
     }
 
     void TearDown() override {
diff --git a/tt-train/tests/modules/distributed/linear_test.cpp b/tt-train/tests/modules/distributed/linear_test.cpp
index e4f99f3987..67cef30b5c 100644
--- a/tt-train/tests/modules/distributed/linear_test.cpp
+++ b/tt-train/tests/modules/distributed/linear_test.cpp
@@ -39,7 +39,8 @@ protected:
         if (!check_board_is_n300()) {
             GTEST_SKIP() << "Skipping N300 specific tests";
         }
-        ttml::autograd::ctx().open_device(tt::tt_metal::distributed::MeshShape(1, 2));
+        ttml::autograd::ctx().set_mesh_shape(tt::tt_metal::distributed::MeshShape(1, 2));
+        ttml::autograd::ctx().open_device();
     }
 
     void TearDown() override {
diff --git a/tt-train/tests/ops/distributed/comm_ops_test.cpp b/tt-train/tests/ops/distributed/comm_ops_test.cpp
index 30321fbfe3..69175e35b7 100644
--- a/tt-train/tests/ops/distributed/comm_ops_test.cpp
+++ b/tt-train/tests/ops/distributed/comm_ops_test.cpp
@@ -29,7 +29,8 @@ protected:
         if (!check_board_is_n300()) {
             GTEST_SKIP() << "Skipping N300 specific tests";
         }
-        ttml::autograd::ctx().open_device(tt::tt_metal::distributed::MeshShape(1, 2));
+        ttml::autograd::ctx().set_mesh_shape(tt::tt_metal::distributed::MeshShape(1, 2));
+        ttml::autograd::ctx().open_device();
     }
 
     void TearDown() override {
diff --git a/tt-train/tests/ttnn_fixed/distributed/distributed_ttnn_ops_test.cpp b/tt-train/tests/ttnn_fixed/distributed/distributed_ttnn_ops_test.cpp
index c0086eb7dc..fee96b81d2 100644
--- a/tt-train/tests/ttnn_fixed/distributed/distributed_ttnn_ops_test.cpp
+++ b/tt-train/tests/ttnn_fixed/distributed/distributed_ttnn_ops_test.cpp
@@ -27,7 +27,8 @@ protected:
         if (!check_board_is_n300()) {
             GTEST_SKIP() << "Skipping N300 specific tests";
         }
-        ttml::autograd::ctx().open_device(tt::tt_metal::distributed::MeshShape(1, 2));
+        ttml::autograd::ctx().set_mesh_shape(tt::tt_metal::distributed::MeshShape(1, 2));
+        ttml::autograd::ctx().open_device();
     }
 
     void TearDown() override {
diff --git a/tt_metal/CMakeLists.txt b/tt_metal/CMakeLists.txt
index 2ecb22cffe..f1b9ab3c1a 100644
--- a/tt_metal/CMakeLists.txt
+++ b/tt_metal/CMakeLists.txt
@@ -127,7 +127,6 @@ target_sources(
             api/tt-metalium/fabric_edm_packet_header.hpp
             core_descriptors/blackhole_140_arch.yaml
             core_descriptors/wormhole_b0_80_arch.yaml
-            core_descriptors/wormhole_b0_80_arch_eth_dispatch.yaml
             fabric/mesh_graph_descriptors/n150_mesh_graph_descriptor.yaml
             fabric/mesh_graph_descriptors/n300_mesh_graph_descriptor.yaml
             fabric/mesh_graph_descriptors/p100_mesh_graph_descriptor.yaml
@@ -138,38 +137,24 @@ target_sources(
             fabric/mesh_graph_descriptors/t3k_mesh_graph_descriptor.yaml
             fabric/mesh_graph_descriptors/tg_mesh_graph_descriptor.yaml
             fabric/hw/inc/fabric_routing_mode.h
-            fabric/hw/inc/noc_addr.h
             impl/dispatch/kernels/cq_commands.hpp
             impl/dispatch/kernels/cq_common.hpp
             impl/dispatch/kernels/cq_helpers.hpp
             impl/dispatch/kernels/packet_queue.hpp
             impl/dispatch/kernels/packet_queue_ctrl.hpp
             include/compute_kernel_api.h
-            include/compute_kernel_api/bcast.h
             include/compute_kernel_api/blank.h
             include/compute_kernel_api/cb_api.h
             include/compute_kernel_api/common.h
             include/compute_kernel_api/common_globals.h
-            include/compute_kernel_api/eltwise_binary.h
             include/compute_kernel_api/eltwise_unary/eltwise_unary.h
             include/compute_kernel_api/eltwise_unary/exp.h
-            include/compute_kernel_api/eltwise_unary/negative.h
-            include/compute_kernel_api/eltwise_unary/recip.h
-            include/compute_kernel_api/eltwise_unary/relu.h
             include/compute_kernel_api/eltwise_unary/sfpu_split_includes.h
-            include/compute_kernel_api/eltwise_unary/sqrt.h
-            include/compute_kernel_api/mask.h
             include/compute_kernel_api/matmul.h
             include/compute_kernel_api/pack.h
-            include/compute_kernel_api/pack_untilize.h
             include/compute_kernel_api/reconfig_data_format.h
-            include/compute_kernel_api/reduce.h
             include/compute_kernel_api/reg_api.h
-            include/compute_kernel_api/softmax.h
             include/compute_kernel_api/tile_move_copy.h
-            include/compute_kernel_api/tilize.h
-            include/compute_kernel_api/transpose_wh.h
-            include/compute_kernel_api/untilize.h
             soc_descriptors/blackhole_140_arch.yaml
             soc_descriptors/wormhole_b0_80_arch.yaml
             third_party/tt_llk/tt_llk_blackhole/common/inc/ckernel.h
diff --git a/tt_metal/api/tt-metalium/device_pool.hpp b/tt_metal/api/tt-metalium/device_pool.hpp
index cdbae622dc..349c40a1ff 100644
--- a/tt_metal/api/tt-metalium/device_pool.hpp
+++ b/tt_metal/api/tt-metalium/device_pool.hpp
@@ -100,7 +100,6 @@ private:
     void initialize_host(tt_metal::IDevice* dev) const;
 
     // Initialize state for activated devices
-    void init_fabric(const std::vector<tt_metal::IDevice*>& active_devices) const;
     void initialize_active_devices() const;
     void add_devices_to_pool(const std::vector<chip_id_t>& device_ids);
     void wait_for_fabric_router_sync() const;
diff --git a/tt_metal/api/tt-metalium/fabric_edm_packet_header.hpp b/tt_metal/api/tt-metalium/fabric_edm_packet_header.hpp
index 810ad9da95..1c68748318 100644
--- a/tt_metal/api/tt-metalium/fabric_edm_packet_header.hpp
+++ b/tt_metal/api/tt-metalium/fabric_edm_packet_header.hpp
@@ -10,9 +10,9 @@
 #include <limits>
 
 #if defined(KERNEL_BUILD) || defined(FW_BUILD)
+#include "ttnn/cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "tt_metal/fabric/hw/inc/edm_fabric/edm_fabric_utils.hpp"
 #include "tt_metal/fabric/hw/inc/fabric_routing_mode.h"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
 #else
 #include <tt-metalium/assert.hpp>
 #endif
diff --git a/tt_metal/api/tt-metalium/host_buffer.hpp b/tt_metal/api/tt-metalium/host_buffer.hpp
index 18d43d0acd..38e39ea05f 100644
--- a/tt_metal/api/tt-metalium/host_buffer.hpp
+++ b/tt_metal/api/tt-metalium/host_buffer.hpp
@@ -1,4 +1,4 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
+// SPDX-FileCopyrightText: Â© 2024 Tenstorrent AI ULC
 //
 // SPDX-License-Identifier: Apache-2.0
 
@@ -18,8 +18,8 @@
 
 namespace tt::tt_metal {
 
-// HostBuffer is a wrapper around contiguous data, which can either be owned or borrowed from external sources (Python
-// objects, mmap-ed regions, etc).
+// HostBuffer is wrapper around data, which can either be owned or borrowed from external sources (Python objects,
+// mmap-ed regions, etc).
 class HostBuffer {
 public:
     HostBuffer();
@@ -62,10 +62,21 @@ public:
     template <typename T>
     tt::stl::Span<const T> view_as() const&& = delete;
 
+    // Returns true if the data buffer is borrowed.
+    bool is_borrowed() const;
+
+    // Makes a deep copy of the data buffer.
+    HostBuffer deep_copy() const;
+
+    // Returns a pin for the data buffer.
+    // The data won't be freed until the pin is destroyed.
+    MemoryPin pin() const;
+
 private:
     MemoryPin pin_;
     tt::stl::Span<std::byte> view_;
     const std::type_info* type_info_ = nullptr;
+    bool is_borrowed_ = false;
 };
 
 template <typename T>
@@ -74,6 +85,7 @@ HostBuffer::HostBuffer(std::shared_ptr<std::vector<T>> data) {
     view_ = tt::stl::Span<std::byte>(reinterpret_cast<std::byte*>(data->data()), size_bytes);
     pin_ = MemoryPin(data);
     type_info_ = &typeid(T);
+    is_borrowed_ = false;
 }
 
 template <typename T>
@@ -89,7 +101,8 @@ HostBuffer::HostBuffer(tt::stl::Span<T> borrowed_data, MemoryPin pin) :
     view_(
         tt::stl::Span<std::byte>(reinterpret_cast<std::byte*>(borrowed_data.data()), borrowed_data.size() * sizeof(T))),
     pin_(std::move(pin)),
-    type_info_(&typeid(T)) {}
+    type_info_(&typeid(T)),
+    is_borrowed_(true) {}
 
 template <typename T>
 tt::stl::Span<T> HostBuffer::view_as() & {
diff --git a/tt_metal/api/tt-metalium/mesh_coord.hpp b/tt_metal/api/tt-metalium/mesh_coord.hpp
index cb26be1ae8..91b4770f1e 100644
--- a/tt_metal/api/tt-metalium/mesh_coord.hpp
+++ b/tt_metal/api/tt-metalium/mesh_coord.hpp
@@ -145,7 +145,6 @@ public:
     class Iterator {
     public:
         Iterator& operator++();
-        Iterator operator++(int);
         const MeshCoordinate& operator*() const;
         bool operator==(const Iterator& other) const;
         bool operator!=(const Iterator& other) const;
diff --git a/tt_metal/api/tt-metalium/program.hpp b/tt_metal/api/tt-metalium/program.hpp
index 4bd3aabbcf..1b7fa5146c 100644
--- a/tt_metal/api/tt-metalium/program.hpp
+++ b/tt_metal/api/tt-metalium/program.hpp
@@ -109,7 +109,7 @@ public:
 
     void compile(IDevice* device, bool force_slow_dispatch = false);
 
-    void generate_dispatch_commands(IDevice* device, bool use_prefetcher_cache);
+    void generate_dispatch_commands(IDevice* device);
 
     void invalidate_circular_buffer_allocation();
 
diff --git a/tt_metal/api/tt-metalium/tt_metal.hpp b/tt_metal/api/tt-metalium/tt_metal.hpp
index 59f3dc23d0..02e7ef6060 100644
--- a/tt_metal/api/tt-metalium/tt_metal.hpp
+++ b/tt_metal/api/tt-metalium/tt_metal.hpp
@@ -40,22 +40,8 @@ namespace detail {
 
 bool DispatchStateCheck(bool isFastDispatch);
 
-/**
- * Call before CreateDevices to enable fabric, which uses the specified number of routing planes.
- * Currently, setting num_routing_planes dictates how many routing planes the fabric should be active on
- * for that init sequence. The number of routing planes fabric will be initialized on will be the max
- * of all the values specified by different clients. If a client wants to initialize fabric on all the
- * available routing planes, num_routing_planes can be left unspecifed.
- * NOTE: This does not 'reserve' routing planes for any clients, but is rather a global setting.
- *
- * Return value: void
- *
- * | Argument           | Description                         | Data type         | Valid range | Required |
- * |--------------------|-------------------------------------|-------------------|-------------|----------|
- * | fabric_config      | Fabric config to set                | FabricConfig      |             | Yes      |
- * | num_routing_planes | Number of routing planes for fabric | optional<uint8_t> |             | No       |
- */
-void SetFabricConfig(FabricConfig fabric_config, std::optional<uint8_t> num_routing_planes = std::nullopt);
+// Call before CreateDevices to enable fabric, which uses all free ethernet cores
+void InitializeFabricConfig(FabricConfig fabric_config);
 
 std::map<chip_id_t, IDevice*> CreateDevices(
     // TODO: delete this in favour of DevicePool
diff --git a/tt_metal/common/host_buffer.cpp b/tt_metal/common/host_buffer.cpp
index 9b2a619fb3..c2d35fcedb 100644
--- a/tt_metal/common/host_buffer.cpp
+++ b/tt_metal/common/host_buffer.cpp
@@ -14,7 +14,7 @@
 
 namespace tt::tt_metal {
 
-HostBuffer::HostBuffer() : pin_(), view_(tt::stl::Span<std::byte>()), type_info_(nullptr) {}
+HostBuffer::HostBuffer() : pin_(), view_(tt::stl::Span<std::byte>()), type_info_(nullptr), is_borrowed_(false) {}
 
 HostBuffer::HostBuffer(const HostBuffer& other) = default;
 
@@ -36,12 +36,27 @@ void HostBuffer::swap(HostBuffer& other) noexcept {
     swap(pin_, other.pin_);
     swap(view_, other.view_);
     swap(type_info_, other.type_info_);
+    swap(is_borrowed_, other.is_borrowed_);
 }
 
 tt::stl::Span<std::byte> HostBuffer::view_bytes() & noexcept { return view_; }
 
 tt::stl::Span<const std::byte> HostBuffer::view_bytes() const& noexcept { return view_; }
 
+bool HostBuffer::is_borrowed() const { return is_borrowed_; }
+
+HostBuffer HostBuffer::deep_copy() const {
+    auto copied_data = std::make_shared<std::vector<std::byte>>(view_bytes().begin(), view_bytes().end());
+    HostBuffer copy;
+    copy.view_ = tt::stl::Span<std::byte>(copied_data->data(), copied_data->size());
+    copy.pin_ = MemoryPin(copied_data);
+    copy.type_info_ = type_info_;
+    copy.is_borrowed_ = false;
+    return copy;
+}
+
+MemoryPin HostBuffer::pin() const { return pin_; }
+
 bool operator==(const HostBuffer& a, const HostBuffer& b) noexcept {
     auto a_view = a.view_bytes();
     auto b_view = b.view_bytes();
diff --git a/tt_metal/common/mesh_coord.cpp b/tt_metal/common/mesh_coord.cpp
index d0e19a4c8c..30ffa04b1e 100644
--- a/tt_metal/common/mesh_coord.cpp
+++ b/tt_metal/common/mesh_coord.cpp
@@ -231,12 +231,6 @@ MeshCoordinateRange::Iterator::Iterator(
     const MeshCoordinateRange* range, const MeshCoordinate& current, size_t linear_index) :
     range_(range), current_coord_(current), linear_index_(linear_index) {}
 
-MeshCoordinateRange::Iterator MeshCoordinateRange::Iterator::operator++(int) {
-    Iterator tmp = *this;
-    ++(*this);
-    return tmp;
-}
-
 MeshCoordinateRange::Iterator& MeshCoordinateRange::Iterator::operator++() {
     ++linear_index_;
 
diff --git a/tt_metal/distributed/distributed_host_buffer.cpp b/tt_metal/distributed/distributed_host_buffer.cpp
index f880e34130..216cfa5b6c 100644
--- a/tt_metal/distributed/distributed_host_buffer.cpp
+++ b/tt_metal/distributed/distributed_host_buffer.cpp
@@ -110,6 +110,7 @@ DistributedHostBuffer DistributedHostBuffer::transform(
     const std::vector<size_t> indices_to_process = get_populated_local_shard_indices();
     const auto& local_shards = local_shards_.values();
     std::vector<Shard> transformed_shards(local_shards.size());
+
     if (policy == ProcessShardExecutionPolicy::SEQUENTIAL || indices_to_process.size() < 2) {
         std::for_each(indices_to_process.begin(), indices_to_process.end(), [&](size_t i) {
             transformed_shards[i] = Shard{.buffer = fn(local_shards[i].buffer), .is_populated = true};
@@ -121,6 +122,7 @@ DistributedHostBuffer DistributedHostBuffer::transform(
         });
         detail::GetExecutor().run(taskflow).wait();
     }
+
     return DistributedHostBuffer(
         global_shape_,
         local_offset_,
diff --git a/tt_metal/distributed/fd_mesh_command_queue.cpp b/tt_metal/distributed/fd_mesh_command_queue.cpp
index 2e22e2d2d3..881913ba9a 100644
--- a/tt_metal/distributed/fd_mesh_command_queue.cpp
+++ b/tt_metal/distributed/fd_mesh_command_queue.cpp
@@ -75,23 +75,14 @@ FDMeshCommandQueue::FDMeshCommandQueue(
     std::shared_ptr<CQSharedState>& cq_shared_state) :
     MeshCommandQueueBase(mesh_device, id, dispatch_thread_pool),
     reader_thread_pool_(reader_thread_pool),
-    cq_shared_state_(cq_shared_state),
-    dispatch_core_type_(MetalContext::instance().get_dispatch_core_manager().get_dispatch_core_type()),
-    prefetcher_dram_aligned_block_size_(MetalContext::instance().hal().get_alignment(HalMemType::DRAM)),
-    prefetcher_cache_sizeB_(MetalContext::instance().dispatch_mem_map(this->dispatch_core_type_).ringbuffer_size()),
-    prefetcher_dram_aligned_num_blocks_(prefetcher_cache_sizeB_ / prefetcher_dram_aligned_block_size_),
-    prefetcher_cache_manager_size_(
-        1 << (std::bit_width(std::min(1024u, std::max(2u, prefetcher_dram_aligned_num_blocks_ >> 4))) - 1)),
-    prefetcher_cache_manager_(std::make_unique<RingbufferCacheManager>(
-        prefetcher_dram_aligned_block_size_, prefetcher_dram_aligned_num_blocks_, prefetcher_cache_manager_size_)),
-    dummy_prefetcher_cache_manager_(std::make_unique<RingbufferCacheManager>(
-        prefetcher_dram_aligned_block_size_, prefetcher_dram_aligned_num_blocks_, prefetcher_cache_manager_size_)) {
+    cq_shared_state_(cq_shared_state) {
     program_dispatch::reset_config_buf_mgrs_and_expected_workers(
         config_buffer_mgr_,
         expected_num_workers_completed_,
         DispatchSettings::DISPATCH_MESSAGE_ENTRIES,
         mesh_device_->allocator()->get_config().l1_unreserved_base);
     this->populate_virtual_program_dispatch_core();
+    this->populate_dispatch_core_type();
     this->populate_read_descriptor_queue();
     completion_queue_reader_thread_ = std::thread(&FDMeshCommandQueue::read_completion_queue, this);
 }
@@ -146,6 +137,21 @@ void FDMeshCommandQueue::populate_virtual_program_dispatch_core() {
     }
 }
 
+void FDMeshCommandQueue::populate_dispatch_core_type() {
+    uint32_t device_idx = 0;
+    for (auto device : this->mesh_device_->get_devices()) {
+        if (device_idx) {
+            TT_FATAL(
+                this->dispatch_core_type_ ==
+                    MetalContext::instance().get_dispatch_core_manager().get_dispatch_core_type(),
+                "Expected the Dispatch Core Type to match across device in a Mesh");
+        } else {
+            this->dispatch_core_type_ = MetalContext::instance().get_dispatch_core_manager().get_dispatch_core_type();
+        }
+        device_idx++;
+    }
+}
+
 CoreCoord FDMeshCommandQueue::virtual_program_dispatch_core() const { return this->dispatch_core_; }
 
 CoreType FDMeshCommandQueue::dispatch_core_type() const { return this->dispatch_core_type_; }
@@ -238,37 +244,11 @@ void FDMeshCommandQueue::enqueue_mesh_workload(MeshWorkload& mesh_workload, bool
 
     std::unordered_set<uint32_t> chip_ids_in_workload = {};
     std::vector<MeshCoordinateRange> active_sub_grids = {};
-
-    auto max_program_kernels_sizeB = mesh_workload.impl().max_program_kernels_sizeB_;
-    bool use_prefetcher_cache = mesh_workload.impl().use_prefetcher_cache_;
-    if (use_prefetcher_cache) {
-        bool is_cached;
-        uint32_t cache_offset;
-        std::tie(is_cached, cache_offset) =
-            this->query_prefetcher_cache(mesh_workload.impl().get_id(), max_program_kernels_sizeB);
-        TT_ASSERT(
-            cache_offset + max_program_kernels_sizeB <= this->prefetcher_cache_sizeB_,
-            "Prefetcher cache offset: {}, max_program_kernels_sizeB: {}, prefetcher_cache_sizeB: {}",
-            cache_offset,
-            max_program_kernels_sizeB,
-            this->prefetcher_cache_sizeB_);
-        dispatch_metadata.prefetcher_cache_info.is_cached = is_cached;
-        dispatch_metadata.prefetcher_cache_info.offset = cache_offset;
-        dispatch_metadata.prefetcher_cache_info.mesh_max_program_kernels_sizeB = max_program_kernels_sizeB;
-    } else {
-        // prefetcher cache will be overwritten, reset for next workload
-        this->reset_prefetcher_cache_manager();
-    }
     // Iterate over all programs. Update dispatch commands per program to reflect
     // current device state. Write the finalized program command sequence to each
     // physical device tied to the program.
     for (auto& [device_range, program] : mesh_workload.get_programs()) {
         auto& program_cmd_seq = mesh_workload.impl().get_dispatch_cmds_for_program(program, command_hash);
-        TT_ASSERT(
-            use_prefetcher_cache == program_cmd_seq.prefetcher_cache_used,
-            "use_prefetcher_cache: {}, program_cmd_seq.prefetcher_cache_used: {}",
-            use_prefetcher_cache,
-            program_cmd_seq.prefetcher_cache_used);
         program_dispatch::update_program_dispatch_commands(
             program.impl(),
             program_cmd_seq,
@@ -303,12 +283,7 @@ void FDMeshCommandQueue::enqueue_mesh_workload(MeshWorkload& mesh_workload, bool
     // Send go signals to devices not running a program to ensure consistent global state
     if (not sysmem_manager.get_bypass_mode()) {
         this->write_go_signal_to_unused_sub_grids(
-            chip_ids_in_workload,
-            sub_device_id,
-            expected_num_workers_completed,
-            mcast_go_signals,
-            unicast_go_signals,
-            dispatch_metadata);
+            chip_ids_in_workload, sub_device_id, expected_num_workers_completed, mcast_go_signals, unicast_go_signals);
     } else {
         MeshCoordinateRangeSet active_sub_grids_set;
         for (const auto& sub_grid : active_sub_grids) {
@@ -320,8 +295,7 @@ void FDMeshCommandQueue::enqueue_mesh_workload(MeshWorkload& mesh_workload, bool
             sub_device_id,
             expected_num_workers_completed,
             mcast_go_signals,
-            unicast_go_signals,
-            dispatch_metadata);
+            unicast_go_signals);
     }
     // Increment Launch Message Buffer Write Pointers
     if (mcast_go_signals) {
@@ -401,7 +375,6 @@ void FDMeshCommandQueue::enqueue_read_shard_from_core(
     sub_device_ids = buffer_dispatch::select_sub_device_ids(mesh_device_, sub_device_ids);
 
     if (size_bytes > 0) {
-        this->reset_prefetcher_cache_manager();
         device_dispatch::CoreReadDispatchParams dispatch_params{
             address.virtual_core_coord,
             address.address,
@@ -467,8 +440,6 @@ void FDMeshCommandQueue::read_shard_from_device(
 
     auto device = shard_view->device();
     sub_device_ids = buffer_dispatch::select_sub_device_ids(mesh_device_, sub_device_ids);
-    // Reading from device would clobber prefetcher cache, so reset it now
-    this->reset_prefetcher_cache_manager();
 
     if (is_sharded(shard_view->buffer_layout())) {
         auto dispatch_params = buffer_dispatch::initialize_sharded_buf_read_dispatch_params(
@@ -774,8 +745,7 @@ void FDMeshCommandQueue::write_go_signal_to_unused_sub_grids(
     const SubDeviceId& sub_device_id,
     uint32_t expected_num_workers_completed,
     bool mcast_go_signals,
-    bool unicast_go_signals,
-    const program_dispatch::ProgramDispatchMetadata& dispatch_md) {
+    bool unicast_go_signals) {
     for (auto& device : this->mesh_device_->get_devices()) {
         if (chip_ids_in_workload.find(device->id()) == chip_ids_in_workload.end()) {
             write_go_signal(
@@ -786,8 +756,7 @@ void FDMeshCommandQueue::write_go_signal_to_unused_sub_grids(
                 expected_num_workers_completed,
                 this->virtual_program_dispatch_core(),
                 mcast_go_signals,
-                unicast_go_signals,
-                dispatch_md);
+                unicast_go_signals);
         }
     }
 }
@@ -842,8 +811,7 @@ void FDMeshCommandQueue::capture_go_signal_trace_on_unused_subgrids(
     const SubDeviceId& sub_device_id,
     uint32_t expected_num_workers_completed,
     bool mcast_go_signals,
-    bool unicast_go_signals,
-    const program_dispatch::ProgramDispatchMetadata& dispatch_md) {
+    bool unicast_go_signals) {
     MeshCoordinateRange full_grid(mesh_device_->shape());
     MeshCoordinateRangeSet unused_grids = subtract(full_grid, active_grid);
     for (const auto& unused_grid : unused_grids.ranges()) {
@@ -857,8 +825,7 @@ void FDMeshCommandQueue::capture_go_signal_trace_on_unused_subgrids(
             expected_num_workers_completed,
             this->virtual_program_dispatch_core(),
             mcast_go_signals,
-            unicast_go_signals,
-            dispatch_md);
+            unicast_go_signals);
         auto mesh_trace_md = MeshTraceStagingMetadata{
             unused_grid,
             unused_grid.start_coord(),
@@ -894,11 +861,6 @@ void FDMeshCommandQueue::enqueue_trace(const MeshTraceId& trace_id, bool blockin
         trace_dispatch::issue_trace_commands(
             mesh_device_, device->sysmem_manager(), dispatch_md, id_, expected_num_workers_completed_, dispatch_core_);
     }
-
-    // Reset the prefetcher cache manager, since trace capture modifies the state on host for subsequent non-trace
-    // programs
-    this->reset_prefetcher_cache_manager();
-
     trace_dispatch::update_worker_state_post_trace_execution(
         trace_inst->desc->descriptors,
         cq_shared_state_->worker_launch_message_buffer_state,
@@ -925,8 +887,6 @@ void FDMeshCommandQueue::record_begin(const MeshTraceId& trace_id, const std::sh
     for (auto device : mesh_device_->get_devices()) {
         device->sysmem_manager().set_bypass_mode(/*enable*/ true, /*clear*/ true);
     }
-
-    swap(this->dummy_prefetcher_cache_manager_, this->prefetcher_cache_manager_);
 }
 
 void FDMeshCommandQueue::record_end() {
@@ -947,11 +907,6 @@ void FDMeshCommandQueue::record_end() {
     for (auto device : mesh_device_->get_devices()) {
         device->sysmem_manager().set_bypass_mode(/*enable*/ false, /*clear*/ true);
     }
-
-    // Trace has modified the prefetcher cache manager so reset it first and then swap to restore the state as before
-    // the recording
-    this->reset_prefetcher_cache_manager();
-    swap(this->dummy_prefetcher_cache_manager_, this->prefetcher_cache_manager_);
 }
 
 SystemMemoryManager& FDMeshCommandQueue::reference_sysmem_manager() {
@@ -968,20 +923,4 @@ void FDMeshCommandQueue::update_launch_messages_for_device_profiler(
 #endif
 }
 
-std::pair<bool, size_t> FDMeshCommandQueue::query_prefetcher_cache(uint64_t workload_id, uint32_t lengthB) {
-    auto result = prefetcher_cache_manager_->get_cache_offset(workload_id, lengthB);
-    TT_FATAL(
-        result.has_value(),
-        "Prefetcher cache query failed. Cache size: {}, requested: {}",
-        this->prefetcher_cache_manager_->get_cache_sizeB(),
-        lengthB);
-    return std::make_pair(result.value().is_cached, result.value().offset * this->prefetcher_dram_aligned_block_size_);
-}
-
-void FDMeshCommandQueue::reset_prefetcher_cache_manager() { prefetcher_cache_manager_->reset(); }
-
-int FDMeshCommandQueue::get_prefetcher_cache_sizeB() const {
-    return this->prefetcher_cache_manager_->get_cache_sizeB();
-}
-
 }  // namespace tt::tt_metal::distributed
diff --git a/tt_metal/distributed/fd_mesh_command_queue.hpp b/tt_metal/distributed/fd_mesh_command_queue.hpp
index 200c280626..08e9d7d962 100644
--- a/tt_metal/distributed/fd_mesh_command_queue.hpp
+++ b/tt_metal/distributed/fd_mesh_command_queue.hpp
@@ -14,8 +14,6 @@
 #include "dispatch/launch_message_ring_buffer_state.hpp"
 #include "dispatch/worker_config_buffer.hpp"
 #include "mesh_trace.hpp"
-#include "tt_metal/impl/dispatch/ringbuffer_cache.hpp"
-#include "tt_metal/impl/program/dispatch.hpp"
 
 namespace tt::tt_metal::distributed {
 
@@ -36,6 +34,7 @@ class FDMeshCommandQueue final : public MeshCommandQueueBase {
 private:
     void populate_read_descriptor_queue();
     void populate_virtual_program_dispatch_core();
+    void populate_dispatch_core_type();
     CoreCoord virtual_program_dispatch_core() const;
     CoreType dispatch_core_type() const;
 
@@ -62,8 +61,7 @@ private:
         const SubDeviceId& sub_device_id,
         uint32_t expected_num_workers_completed,
         bool mcast_go_signals,
-        bool unicast_go_signals,
-        const program_dispatch::ProgramDispatchMetadata& dispatch_md);
+        bool unicast_go_signals);
     // Workload dispatch utility functions
     // Write dispatch commands associated with running a program on a Virtual Mesh subgrid
     void write_program_cmds_to_subgrid(
@@ -81,8 +79,7 @@ private:
         const SubDeviceId& sub_device_id,
         uint32_t expected_num_workers_completed,
         bool mcast_go_signals,
-        bool unicast_go_signals,
-        const program_dispatch::ProgramDispatchMetadata& dispatch_md);
+        bool unicast_go_signals);
     // When the device profiler is not enabled, launch messages are identical across all physical devices running the
     // same program, to reduce state managed on host. When the profiler is enabled, the host_assigned_id field in the
     // launch message must be unique across physical devices to accurately capture program execution time on host and
@@ -124,7 +121,7 @@ private:
     std::vector<MeshTraceStagingMetadata> ordered_mesh_trace_md_;
 
     CoreCoord dispatch_core_;
-    const CoreType dispatch_core_type_;
+    CoreType dispatch_core_type_ = CoreType::WORKER;
     // MeshCommandQueues and the MeshDevice share thread-pools for dispatching to and reading from the Mesh
     std::shared_ptr<ThreadPool>
         reader_thread_pool_;  // Thread pool used to read from the Mesh (used by the Completion Queue Reader thread)
@@ -156,15 +153,6 @@ private:
     // This is temporary - will not be needed when we MeshCommandQueue is the only dispatch interface.
     std::atomic<bool> in_use_ = false;
 
-    const uint32_t prefetcher_dram_aligned_block_size_;
-    const uint64_t prefetcher_cache_sizeB_;
-    const uint32_t prefetcher_dram_aligned_num_blocks_;
-    const uint32_t prefetcher_cache_manager_size_;
-    // The prefetcher cache manager is used to track the state of the prefetcher cache.
-    std::unique_ptr<RingbufferCacheManager> prefetcher_cache_manager_;
-    // The backup prefetcher cache manager is used to stash away the prefetcher cache state during trace recording.
-    std::unique_ptr<RingbufferCacheManager> dummy_prefetcher_cache_manager_;
-
 protected:
     void write_shard_to_device(
         const MeshBuffer& buffer,
@@ -234,11 +222,6 @@ public:
     void copy_buffer_data_to_user_space(MeshBufferReadDescriptor& read_buffer_descriptor);
     // Helper function - read L1 data from Completion Queue
     void read_l1_data_from_completion_queue(MeshCoreDataReadDescriptor& read_l1_data_descriptor);
-
-    // Prefetcher Cache Manager APIs
-    std::pair<bool, size_t> query_prefetcher_cache(uint64_t workload_id, uint32_t lengthB);
-    void reset_prefetcher_cache_manager();
-    int get_prefetcher_cache_sizeB() const;
 };
 
 }  // namespace tt::tt_metal::distributed
diff --git a/tt_metal/distributed/mesh_workload.cpp b/tt_metal/distributed/mesh_workload.cpp
index 5e89534ecd..163004ebb7 100644
--- a/tt_metal/distributed/mesh_workload.cpp
+++ b/tt_metal/distributed/mesh_workload.cpp
@@ -16,7 +16,6 @@
 #include <unordered_set>
 #include <utility>
 #include <vector>
-#include <atomic>
 
 #include "assert.hpp"
 #include "buffer.hpp"
@@ -36,7 +35,6 @@
 #include "tt_metal/impl/dispatch/device_command.hpp"
 #include "util.hpp"
 #include "tracy/Tracy.hpp"
-#include "tt_metal/distributed/fd_mesh_command_queue.hpp"
 
 enum class CoreType;
 namespace tt {
@@ -47,13 +45,7 @@ enum class HalProgrammableCoreType;
 }  // namespace tt_metal
 }  // namespace tt
 
-static uint64_t get_next_counter() {
-    static std::atomic<uint64_t> workload_counter = 0;
-    return workload_counter++;
-}
-
 namespace tt::tt_metal::distributed {
-
 namespace {
 
 // Returns an intersecting range from `programs` if it exists, otherwise returns std::nullopt.
@@ -69,7 +61,7 @@ std::optional<MeshCoordinateRange> find_intersection(
 
 }  // namespace
 
-MeshWorkloadImpl::MeshWorkloadImpl() : id(get_next_counter()) {
+MeshWorkloadImpl::MeshWorkloadImpl() {
     ZoneScoped;
     // A MeshWorkload tracks maintains its own handles to kernels across all
     // encapsulated programs
@@ -205,15 +197,9 @@ void MeshWorkloadImpl::generate_dispatch_commands(MeshCommandQueue& mesh_cq) {
     // These commands will be updated based on MeshDevice state when the
     // workload is enqueued.
     auto mesh_device = mesh_cq.device();
-    auto dispatch_core_type = MetalContext::instance().get_dispatch_core_manager().get_dispatch_core_type();
-    uint32_t prefetcher_cache_sizeB = MetalContext::instance().dispatch_mem_map(dispatch_core_type).ringbuffer_size();
-
-    bool use_prefetcher_cache =
-        this->max_program_kernels_sizeB_ and this->max_program_kernels_sizeB_ <= prefetcher_cache_sizeB;
     for (auto& [device_range, program] : programs_) {
-        program.generate_dispatch_commands(mesh_device, use_prefetcher_cache);
+        program.generate_dispatch_commands(mesh_device);
     }
-    this->use_prefetcher_cache_ = use_prefetcher_cache;
 }
 
 bool MeshWorkloadImpl::runs_on_noc_multicast_only_cores() {
@@ -431,7 +417,7 @@ void MeshWorkloadImpl::finalize_offsets(MeshDevice* mesh_device) {
     }
     tt::stl::Span<tt::tt_metal::detail::ProgramImpl*> programs(program_impls.data(), program_impls.size());
 
-    this->max_program_kernels_sizeB_ = tt::tt_metal::detail::ProgramImpl::finalize_program_offsets(
+    tt::tt_metal::detail::ProgramImpl::finalize_program_offsets(
         mesh_device, kernels_getter, kernel_groups_getter, semaphores_getter, programs);
 
     set_finalized();
diff --git a/tt_metal/distributed/mesh_workload_impl.hpp b/tt_metal/distributed/mesh_workload_impl.hpp
index 064ebb195c..c0671200b2 100644
--- a/tt_metal/distributed/mesh_workload_impl.hpp
+++ b/tt_metal/distributed/mesh_workload_impl.hpp
@@ -24,9 +24,6 @@ class MeshWorkloadImpl {
     //  - Multi Program Multi Device (Completely Heterogeneous MeshWorkload)
     // Support for configurable runtime arguments will be added in future versions.
 private:
-    uint64_t id;
-
-    uint64_t get_id() const { return id; }
     bool runs_on_noc_multicast_only_cores();
     bool runs_on_noc_unicast_only_cores();
     void compile(MeshDevice* mesh_device);
@@ -63,9 +60,6 @@ private:
     friend FDMeshCommandQueue;
     friend class tt::tt_metal::Program;
 
-    uint32_t max_program_kernels_sizeB_ = 0;
-    bool use_prefetcher_cache_ = false;
-
 public:
     // Main User-Facing API building blocks
     MeshWorkloadImpl();
diff --git a/tt_metal/distributed/mesh_workload_utils.cpp b/tt_metal/distributed/mesh_workload_utils.cpp
index e153afa5c5..6180801c06 100644
--- a/tt_metal/distributed/mesh_workload_utils.cpp
+++ b/tt_metal/distributed/mesh_workload_utils.cpp
@@ -30,13 +30,10 @@ void write_go_signal(
     uint32_t expected_num_workers_completed,
     CoreCoord dispatch_core,
     bool send_mcast,
-    bool send_unicasts,
-    const program_dispatch::ProgramDispatchMetadata& dispatch_md) {
+    bool send_unicasts) {
     uint32_t pcie_alignment = MetalContext::instance().hal().get_alignment(HalMemType::HOST);
     uint32_t cmd_sequence_sizeB = align(sizeof(CQPrefetchCmd) + sizeof(CQDispatchCmd), pcie_alignment) +
                                   MetalContext::instance().hal().get_alignment(HalMemType::HOST);
-    cmd_sequence_sizeB +=
-        dispatch_md.prefetcher_cache_info.is_cached ? 0 : align(sizeof(CQPrefetchCmd), pcie_alignment);
 
     void* cmd_region = sysmem_manager.issue_queue_reserve(cmd_sequence_sizeB, cq_id);
 
@@ -44,13 +41,6 @@ void write_go_signal(
     auto sub_device_index = *sub_device_id;
 
     HugepageDeviceCommand go_signal_cmd_sequence(cmd_region, cmd_sequence_sizeB);
-
-    if (not dispatch_md.prefetcher_cache_info.is_cached) {
-        go_signal_cmd_sequence.add_prefetch_set_ringbuffer_offset(
-            dispatch_md.prefetcher_cache_info.offset + dispatch_md.prefetcher_cache_info.mesh_max_program_kernels_sizeB,
-            true);
-    }
-
     go_msg_t run_program_go_signal;
     run_program_go_signal.signal = RUN_MSG_GO;
     run_program_go_signal.master_x = dispatch_core.x;
@@ -82,8 +72,6 @@ void write_go_signal(
         device->noc_data_start_index(sub_device_id, send_mcast, send_unicasts), /* noc_data_start_idx */
         dispatcher_for_go_signal);
 
-    TT_ASSERT(go_signal_cmd_sequence.size_bytes() == go_signal_cmd_sequence.write_offset_bytes());
-
     sysmem_manager.issue_queue_push_back(cmd_sequence_sizeB, cq_id);
 
     sysmem_manager.fetch_queue_reserve_back(cq_id);
diff --git a/tt_metal/distributed/mesh_workload_utils.hpp b/tt_metal/distributed/mesh_workload_utils.hpp
index 0f53fee245..7d5fd08184 100644
--- a/tt_metal/distributed/mesh_workload_utils.hpp
+++ b/tt_metal/distributed/mesh_workload_utils.hpp
@@ -7,7 +7,6 @@
 
 #include "core_coord.hpp"
 #include "sub_device_types.hpp"
-#include "tt_metal/impl/program/dispatch.hpp"
 
 namespace tt {
 namespace tt_metal {
@@ -28,7 +27,6 @@ void write_go_signal(
     uint32_t expected_num_workers_completed,
     CoreCoord dispatch_core,
     bool send_mcast,
-    bool send_unicasts,
-    const program_dispatch::ProgramDispatchMetadata& dispatch_md);
+    bool send_unicasts);
 
 }  // namespace tt::tt_metal::distributed
diff --git a/tt_metal/distributed/multihost/mpi_distributed_context.hpp b/tt_metal/distributed/multihost/mpi_distributed_context.hpp
index 469eec3507..7a190202c7 100644
--- a/tt_metal/distributed/multihost/mpi_distributed_context.hpp
+++ b/tt_metal/distributed/multihost/mpi_distributed_context.hpp
@@ -104,7 +104,7 @@ public:
     [[nodiscard]] ContextPtr create_sub_context(tt::stl::Span<int> ranks) const override;
     void abort(int error_code) const override;
     void revoke_and_shrink() override;
-    [[nodiscard]] bool is_revoked() override;
+    [[nodiscard]] virtual bool is_revoked() override;
 
     /* ----------------- mpi constructors ---------------- */
     explicit MPIContext(MPI_Comm comm);
diff --git a/tt_metal/fabric/CMakeLists.txt b/tt_metal/fabric/CMakeLists.txt
index fece1b87cd..1e67faceaa 100644
--- a/tt_metal/fabric/CMakeLists.txt
+++ b/tt_metal/fabric/CMakeLists.txt
@@ -32,6 +32,7 @@ target_sources(
             hw/inc/tt_fabric_interface.h
             hw/inc/tt_fabric_mux.hpp
             hw/inc/tt_fabric_mux_interface.hpp
+            hw/inc/eth_chan_noc_mapping.h
             hw/inc/fabric_routing_mode.h
             hw/inc/edm_fabric/edm_fabric_flow_control_helpers.hpp
             hw/inc/edm_fabric/edm_fabric_utils.hpp
diff --git a/tt_metal/fabric/erisc_datamover_builder.cpp b/tt_metal/fabric/erisc_datamover_builder.cpp
index 7324552323..075d9ffc3d 100644
--- a/tt_metal/fabric/erisc_datamover_builder.cpp
+++ b/tt_metal/fabric/erisc_datamover_builder.cpp
@@ -61,8 +61,7 @@ FabricRiscConfig::FabricRiscConfig(uint32_t risc_id) :
         this->is_receiver_channel_serviced_.fill(true);
     } else if (arch == tt::ARCH::BLACKHOLE) {
         this->is_sender_channel_serviced_.fill(risc_id == 0);
-        // TODO: set this to be risc_id == 1 when we want to split sender/receiver on the two eriscs
-        this->is_receiver_channel_serviced_.fill(risc_id == 0);
+        this->is_receiver_channel_serviced_.fill(risc_id == 1);
         this->enable_context_switch_ = false;
         this->enable_interrupts_ = false;
     } else {
diff --git a/tt_metal/fabric/fabric.cpp b/tt_metal/fabric/fabric.cpp
index 96e232b9c0..5832b201e2 100644
--- a/tt_metal/fabric/fabric.cpp
+++ b/tt_metal/fabric/fabric.cpp
@@ -60,11 +60,6 @@ size_t get_tt_fabric_channel_buffer_size_bytes() {
     return control_plane.get_fabric_context().get_fabric_channel_buffer_size_bytes();
 }
 
-size_t get_tt_fabric_packet_header_size_bytes() {
-    const auto& control_plane = tt::tt_metal::MetalContext::instance().get_control_plane();
-    return control_plane.get_fabric_context().get_fabric_packet_header_size_bytes();
-}
-
 void append_fabric_connection_rt_args(
     const chip_id_t src_chip_id,
     const chip_id_t dst_chip_id,
diff --git a/tt_metal/fabric/fabric_context.cpp b/tt_metal/fabric/fabric_context.cpp
index 6b1ef1fb50..bacc9a9765 100644
--- a/tt_metal/fabric/fabric_context.cpp
+++ b/tt_metal/fabric/fabric_context.cpp
@@ -8,7 +8,6 @@
 #include <tt-metalium/fabric_edm_types.hpp>
 #include <tt-metalium/fabric_types.hpp>
 #include <tt-metalium/assert.hpp>
-#include <tt-metalium/host_api.hpp>
 #include <tt-metalium/erisc_datamover_builder.hpp>
 #include <magic_enum/magic_enum.hpp>
 #include <umd/device/types/cluster_descriptor_types.h>  // chip_id_t
@@ -75,25 +74,6 @@ size_t FabricContext::get_max_payload_size_bytes() const {
     }
 }
 
-std::unique_ptr<tt::tt_fabric::FabricEriscDatamoverConfig> FabricContext::get_edm_config_options(
-    tt::tt_fabric::FabricEriscDatamoverType edm_type) {
-    constexpr bool enable_dateline_sender_extra_buffer_slots = true;
-    constexpr bool enable_dateline_receiver_extra_buffer_slots = true;
-    constexpr bool enable_dateline_upstream_sender_extra_buffer_slots = true;
-    constexpr bool enable_dateline_upstream_receiver_extra_buffer_slots = true;
-
-    auto edm_options = tt::tt_fabric::FabricEriscDatamoverOptions{
-        .edm_type = edm_type,
-        .enable_dateline_sender_extra_buffer_slots = enable_dateline_sender_extra_buffer_slots,
-        .enable_dateline_receiver_extra_buffer_slots = enable_dateline_receiver_extra_buffer_slots,
-        .enable_dateline_upstream_sender_extra_buffer_slots = enable_dateline_upstream_sender_extra_buffer_slots,
-        .enable_dateline_upstream_receiver_extra_buffer_slots = enable_dateline_upstream_receiver_extra_buffer_slots,
-    };
-
-    return std::make_unique<tt::tt_fabric::FabricEriscDatamoverConfig>(
-        this->channel_buffer_size_bytes_, this->topology_, edm_options);
-}
-
 FabricContext::FabricContext(tt::tt_metal::FabricConfig fabric_config) {
     TT_FATAL(
         fabric_config != tt::tt_metal::FabricConfig::DISABLED,
@@ -108,22 +88,18 @@ FabricContext::FabricContext(tt::tt_metal::FabricConfig fabric_config) {
     this->max_payload_size_bytes_ = this->get_max_payload_size_bytes();
     this->channel_buffer_size_bytes_ = this->packet_header_size_bytes_ + this->max_payload_size_bytes_;
 
-    this->router_config_ = get_edm_config_options(tt::tt_fabric::FabricEriscDatamoverType::Default);
-    this->dateline_router_config_ = get_edm_config_options(tt::tt_fabric::FabricEriscDatamoverType::Dateline);
-    this->dateline_upstream_router_config_ =
-        get_edm_config_options(tt::tt_fabric::FabricEriscDatamoverType::DatelineUpstream);
-    this->dateline_upstream_adjcent_router_config_ =
-        get_edm_config_options(tt::tt_fabric::FabricEriscDatamoverType::DatelineUpstreamAdjacentDevice);
-
-    this->num_devices = tt::tt_metal::GetNumAvailableDevices();
-    auto num_pcie_devices = tt::tt_metal::GetNumPCIeDevices();
-    if (this->num_devices != 4 && num_pcie_devices == 4) {
-        // adding TG's 4 dispatch devices
-        this->num_devices += num_pcie_devices;
-    }
-    this->master_router_chans_.resize(num_devices, UNINITIALIZED_MASTER_ROUTER_CHAN);
-    this->num_initialized_routers_.resize(num_devices, UNINITIALIZED_ROUTERS);
-
+    this->router_config_ =
+        std::make_unique<tt::tt_fabric::FabricEriscDatamoverConfig>(this->channel_buffer_size_bytes_, this->topology_);
+    // disable upstream buffering optimization for now for device init.
+    auto dateline_edm_options = tt::tt_fabric::FabricEriscDatamoverOptions{
+        .edm_type = tt::tt_fabric::FabricEriscDatamoverType::Dateline,
+        .enable_dateline_sender_extra_buffer_slots = false,
+        .enable_dateline_receiver_extra_buffer_slots = true,
+        .enable_dateline_upstream_sender_extra_buffer_slots = false,
+        .enable_dateline_upstream_receiver_extra_buffer_slots = false,
+    };
+    this->dateline_router_config_ = std::make_unique<tt::tt_fabric::FabricEriscDatamoverConfig>(
+        this->channel_buffer_size_bytes_, this->topology_, dateline_edm_options);
     set_routing_mode(this->topology_, this->fabric_config_);
 }
 
@@ -141,65 +117,42 @@ size_t FabricContext::get_fabric_max_payload_size_bytes() const { return this->m
 
 size_t FabricContext::get_fabric_channel_buffer_size_bytes() const { return this->channel_buffer_size_bytes_; }
 
-tt::tt_fabric::FabricEriscDatamoverConfig& FabricContext::get_fabric_router_config(
-    tt::tt_fabric::FabricEriscDatamoverType fabric_edm_type) const {
-    switch (fabric_edm_type) {
-        case tt::tt_fabric::FabricEriscDatamoverType::Default:
-            TT_FATAL(this->router_config_ != nullptr, "Error, fabric router config is uninitialized");
-            return *this->router_config_.get();
-            break;
-        case tt::tt_fabric::FabricEriscDatamoverType::Dateline:
-            TT_FATAL(this->dateline_router_config_ != nullptr, "Error, fabric dateline router config is uninitialized");
-            return *this->dateline_router_config_.get();
-            break;
-        case tt::tt_fabric::FabricEriscDatamoverType::DatelineUpstream:
-            TT_FATAL(
-                this->dateline_upstream_router_config_ != nullptr,
-                "Error, fabric dateline upstream router config is uninitialized");
-            return *this->dateline_upstream_router_config_.get();
-        case tt::tt_fabric::FabricEriscDatamoverType::DatelineUpstreamAdjacentDevice:
-            TT_FATAL(
-                this->dateline_upstream_adjcent_router_config_ != nullptr,
-                "Error, fabric dateline upstream adjacent device router config is uninitialized");
-            return *this->dateline_upstream_adjcent_router_config_.get();
-        default: TT_FATAL(false, "Error, invalid fabric edm type");
+tt::tt_fabric::FabricEriscDatamoverConfig& FabricContext::get_fabric_router_config(bool is_dateline) const {
+    if (is_dateline) {
+        TT_FATAL(this->dateline_router_config_ != nullptr, "Error, fabric dateline router config is uninitialized");
+        return *this->dateline_router_config_.get();
+    } else {
+        TT_FATAL(this->router_config_ != nullptr, "Error, fabric router config is uninitialized");
+        return *this->router_config_.get();
     }
 };
 
 void FabricContext::set_num_fabric_initialized_routers(chip_id_t chip_id, size_t num_routers) {
-    TT_FATAL(chip_id < num_devices, "Device ID {} exceeds maximum supported devices {}", chip_id, num_devices);
+    auto it = this->num_initialized_routers_.find(chip_id);
     TT_FATAL(
-        this->num_initialized_routers_[chip_id] == UNINITIALIZED_ROUTERS,
-        "Error, tried to set num initialized routers again for device {}",
-        chip_id);
+        it == this->num_initialized_routers_.end(),
+        "Error, tried to set num initialized routers again for the same device");
     this->num_initialized_routers_[chip_id] = num_routers;
 }
 
 uint32_t FabricContext::get_num_fabric_initialized_routers(chip_id_t chip_id) const {
-    TT_FATAL(chip_id < num_devices, "Device ID {} exceeds maximum supported devices {}", chip_id, num_devices);
+    auto it = this->num_initialized_routers_.find(chip_id);
     TT_FATAL(
-        this->num_initialized_routers_[chip_id] != UNINITIALIZED_ROUTERS,
-        "Error, querying num initialized routers for an unknown device {}",
-        chip_id);
-    return this->num_initialized_routers_[chip_id];
+        it != this->num_initialized_routers_.end(), "Error, querying num initialized routers for an unknown device");
+    return it->second;
 }
 
 void FabricContext::set_fabric_master_router_chan(chip_id_t chip_id, chan_id_t chan_id) {
-    TT_FATAL(chip_id < num_devices, "Device ID {} exceeds maximum supported devices {}", chip_id, num_devices);
+    auto it = this->master_router_chans_.find(chip_id);
     TT_FATAL(
-        this->master_router_chans_[chip_id] == UNINITIALIZED_MASTER_ROUTER_CHAN,
-        "Error, tried to set master router channel again for the same device {}",
-        chip_id);
+        it == this->master_router_chans_.end(), "Error, tried to set master router channel again for the same device");
     this->master_router_chans_[chip_id] = chan_id;
 }
 
 chan_id_t FabricContext::get_fabric_master_router_chan(chip_id_t chip_id) const {
-    TT_FATAL(chip_id < num_devices, "Device ID {} exceeds maximum supported devices {}", chip_id, num_devices);
-    TT_FATAL(
-        this->master_router_chans_[chip_id] != UNINITIALIZED_MASTER_ROUTER_CHAN,
-        "Error, querying master router channel for an unknown device {}",
-        chip_id);
-    return this->master_router_chans_[chip_id];
+    auto it = this->master_router_chans_.find(chip_id);
+    TT_FATAL(it != this->master_router_chans_.end(), "Error, querying master router channel for an unknown device");
+    return it->second;
 }
 
 std::vector<size_t> FabricContext::get_fabric_router_addresses_to_clear() const {
diff --git a/tt_metal/fabric/fabric_context.hpp b/tt_metal/fabric/fabric_context.hpp
index 61b4cafa88..727d5088ee 100644
--- a/tt_metal/fabric/fabric_context.hpp
+++ b/tt_metal/fabric/fabric_context.hpp
@@ -10,7 +10,6 @@
 #include <umd/device/types/cluster_descriptor_types.h>  // chip_id_t
 #include <tt-metalium/erisc_datamover_builder.hpp>
 #include <vector>
-#include <limits>
 #include "tt_metal/fabric/fabric_host_utils.hpp"
 
 namespace tt::tt_fabric {
@@ -31,9 +30,7 @@ public:
     size_t get_fabric_max_payload_size_bytes() const;
     size_t get_fabric_channel_buffer_size_bytes() const;
 
-    tt::tt_fabric::FabricEriscDatamoverConfig& get_fabric_router_config(
-        tt::tt_fabric::FabricEriscDatamoverType fabric_edm_type =
-            tt::tt_fabric::FabricEriscDatamoverType::Default) const;
+    tt::tt_fabric::FabricEriscDatamoverConfig& get_fabric_router_config(bool is_dateline = false) const;
 
     void set_num_fabric_initialized_routers(chip_id_t chip_id, size_t num_routers);
     uint32_t get_num_fabric_initialized_routers(chip_id_t chip_id) const;
@@ -54,8 +51,6 @@ private:
     tt::tt_fabric::Topology get_topology() const;
     size_t get_packet_header_size_bytes() const;
     size_t get_max_payload_size_bytes() const;
-    std::unique_ptr<tt::tt_fabric::FabricEriscDatamoverConfig> get_edm_config_options(
-        tt::tt_fabric::FabricEriscDatamoverType edm_type);
 
     bool initialized_ = false;
     tt::tt_metal::FabricConfig fabric_config_{};
@@ -67,16 +62,8 @@ private:
     size_t channel_buffer_size_bytes_ = 0;
     std::unique_ptr<tt::tt_fabric::FabricEriscDatamoverConfig> router_config_ = nullptr;
     std::unique_ptr<tt::tt_fabric::FabricEriscDatamoverConfig> dateline_router_config_ = nullptr;
-    std::unique_ptr<tt::tt_fabric::FabricEriscDatamoverConfig> dateline_upstream_router_config_ = nullptr;
-    std::unique_ptr<tt::tt_fabric::FabricEriscDatamoverConfig> dateline_upstream_adjcent_router_config_ = nullptr;
-
-    // Using vectors. Use Device IDs as indices
-    size_t num_devices = 0;
-    static constexpr chan_id_t UNINITIALIZED_MASTER_ROUTER_CHAN = std::numeric_limits<chan_id_t>::max();
-    static constexpr uint32_t UNINITIALIZED_ROUTERS = std::numeric_limits<uint32_t>::max();
-    // Use vector instead of unordered_map to be thread safe
-    std::vector<chan_id_t> master_router_chans_;
-    std::vector<uint32_t> num_initialized_routers_;
+    std::unordered_map<chip_id_t, chan_id_t> master_router_chans_{};
+    std::unordered_map<chip_id_t, uint32_t> num_initialized_routers_{};
 };
 
 }  // namespace tt::tt_fabric
diff --git a/tt_metal/fabric/hw/inc/edm_fabric/1d_fabric_transaction_id_tracker.hpp b/tt_metal/fabric/hw/inc/edm_fabric/1d_fabric_transaction_id_tracker.hpp
index 7208997bba..b4360aab66 100644
--- a/tt_metal/fabric/hw/inc/edm_fabric/1d_fabric_transaction_id_tracker.hpp
+++ b/tt_metal/fabric/hw/inc/edm_fabric/1d_fabric_transaction_id_tracker.hpp
@@ -25,7 +25,7 @@ private:
     uint8_t next_trid = 0;
 };
 
-template <uint8_t NUM_CHANNELS, size_t MAX_TRANSACTION_IDS, size_t OFFSET>
+template <size_t NUM_CHANNELS, size_t MAX_TRANSACTION_IDS, size_t OFFSET>
 struct WriteTransactionIdTracker {
     static constexpr size_t NUM_CHANNELS_PARAM = NUM_CHANNELS;
     static constexpr size_t MAX_TRANSACTION_IDS_PARAM = MAX_TRANSACTION_IDS;
diff --git a/tt_metal/fabric/hw/inc/edm_fabric/edm_fabric_utils.hpp b/tt_metal/fabric/hw/inc/edm_fabric/edm_fabric_utils.hpp
index 7bf3414467..5e6f38ffa7 100644
--- a/tt_metal/fabric/hw/inc/edm_fabric/edm_fabric_utils.hpp
+++ b/tt_metal/fabric/hw/inc/edm_fabric/edm_fabric_utils.hpp
@@ -30,7 +30,7 @@ FORCE_INLINE void send_chunk_from_address_with_trid(
     } else {
         noc_async_write_one_packet_with_trid<false, false>(
             local_l1_address,
-            get_noc_addr_helper(remote_l1_write_addr_h, remote_l1_write_addr_l),
+            ((uint64_t)remote_l1_write_addr_h << 32) | remote_l1_write_addr_l,
             page_size * num_pages,
             trid,
             cmd_buf,
diff --git a/tt_metal/fabric/hw/inc/edm_fabric/edm_fabric_worker_adapters.hpp b/tt_metal/fabric/hw/inc/edm_fabric/edm_fabric_worker_adapters.hpp
index c07408b27d..c2f5d34eb4 100644
--- a/tt_metal/fabric/hw/inc/edm_fabric/edm_fabric_worker_adapters.hpp
+++ b/tt_metal/fabric/hw/inc/edm_fabric/edm_fabric_worker_adapters.hpp
@@ -15,6 +15,7 @@
 #include "edm_fabric_flow_control_helpers.hpp"
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_stream_regs.hpp"
 #include "tt_metal/hw/inc/utils/utils.h"
+#include "tt_metal/hw/inc/wormhole/core_config.h"
 #include "debug/assert.h"
 
 #include <cstdint>
@@ -225,7 +226,6 @@ struct WorkerToFabricEdmSenderImpl {
     }
 
     FORCE_INLINE bool edm_has_space_for_packet() const {
-        invalidate_l1_cache();
         if constexpr (!I_USE_STREAM_REG_FOR_CREDIT_RECEIVE) {
             return (this->buffer_slot_write_counter.counter - *this->from_remote_buffer_free_slots_ptr) <
                    this->num_buffers_per_channel;
@@ -501,7 +501,7 @@ private:
         } else {
             const uint64_t noc_sem_addr =
                 get_noc_addr(this->edm_noc_x, this->edm_noc_y, this->edm_buffer_remote_free_slots_update_addr, noc);
-            noc_inline_dw_write<true>(noc_sem_addr, (-1) << REMOTE_DEST_BUF_WORDS_FREE_INC, 0xf, noc);
+            noc_inline_dw_write(noc_sem_addr, (-1) << REMOTE_DEST_BUF_WORDS_FREE_INC, 0xf, noc);
         }
         if constexpr (I_USE_STREAM_REG_FOR_CREDIT_RECEIVE) {
             // Write to the atomic increment stream register (write of -1 will subtract 1)
@@ -525,6 +525,7 @@ private:
                     BufferIndex{wrap_increment(this->buffer_slot_index.get(), this->num_buffers_per_channel)};
                 this->edm_buffer_addr =
                     this->edm_buffer_base_addr + (this->get_buffer_slot_index() * this->buffer_size_bytes);
+                ;
             } else {
                 this->buffer_slot_index = BufferIndex{wrap_increment(this->buffer_slot_index.get(), this->num_buffers_per_channel)};
                 this->edm_buffer_addr =
@@ -588,7 +589,7 @@ private:
                 source_address,
                 1,
                 size_bytes,
-                get_noc_addr(this->edm_noc_x, this->edm_noc_y, 0) >> NOC_ADDR_COORD_SHIFT,
+                get_noc_addr(this->edm_noc_x, this->edm_noc_y, 0) >> 32,
                 this->edm_buffer_slot_addrs[this->get_buffer_slot_index()],
                 trid,
                 EDM_TO_DOWNSTREAM_NOC,
@@ -598,7 +599,7 @@ private:
                 source_address,
                 1,
                 size_bytes,
-                get_noc_addr(this->edm_noc_x, this->edm_noc_y, 0) >> NOC_ADDR_COORD_SHIFT,
+                get_noc_addr(this->edm_noc_x, this->edm_noc_y, 0) >> 32,
                 this->edm_buffer_addr,
                 trid,
                 EDM_TO_DOWNSTREAM_NOC,
diff --git a/tt_metal/fabric/hw/inc/edm_fabric/fabric_edm_packet_transmission.hpp b/tt_metal/fabric/hw/inc/edm_fabric/fabric_edm_packet_transmission.hpp
index ea939b7275..53324831d1 100644
--- a/tt_metal/fabric/hw/inc/edm_fabric/fabric_edm_packet_transmission.hpp
+++ b/tt_metal/fabric/hw/inc/edm_fabric/fabric_edm_packet_transmission.hpp
@@ -255,7 +255,7 @@ FORCE_INLINE void update_packet_header_for_next_hop(
 // !!!WARNING!!! * ENSURE DOWNSTREAM EDM HAS SPACE FOR PACKET BEFORE CALLING
 // !!!WARNING!!!
 // This function does a write, so needs to be volatile to avoid compiler optimizations
-template <bool enable_ring_support, bool stateful_api, uint8_t NUM_SENDER_BUFFERS>
+template <uint8_t NUM_SENDER_BUFFERS, bool enable_ring_support, bool stateful_api>
 FORCE_INLINE void forward_payload_to_downstream_edm(
     volatile tt_l1_ptr PACKET_HEADER_TYPE* packet_header,
     uint16_t payload_size_bytes,
diff --git a/tt_metal/fabric/hw/inc/edm_fabric/fabric_erisc_datamover_channels.hpp b/tt_metal/fabric/hw/inc/edm_fabric/fabric_erisc_datamover_channels.hpp
index 978be79bff..46cb566c16 100644
--- a/tt_metal/fabric/hw/inc/edm_fabric/fabric_erisc_datamover_channels.hpp
+++ b/tt_metal/fabric/hw/inc/edm_fabric/fabric_erisc_datamover_channels.hpp
@@ -223,7 +223,7 @@ struct EdmChannelWorkerInterface {
     }
 
     FORCE_INLINE void notify_worker_of_read_counter_update() {
-        noc_inline_dw_write<true, true>(
+        noc_inline_dw_write<false, true>(
             this->cached_worker_semaphore_address,
             local_read_counter.counter,
             0xf,
@@ -242,7 +242,6 @@ struct EdmChannelWorkerInterface {
     //
     template <bool posted = false>
     FORCE_INLINE void teardown_worker_connection() const {
-        invalidate_l1_cache();
         const auto& worker_info = *worker_location_info_ptr;
         uint64_t worker_semaphore_address = get_noc_addr(
             (uint32_t)worker_info.worker_xy.x,
@@ -258,7 +257,6 @@ struct EdmChannelWorkerInterface {
     }
 
     FORCE_INLINE void cache_producer_noc_addr() {
-        invalidate_l1_cache();
         const auto& worker_info = *worker_location_info_ptr;
         uint64_t worker_semaphore_address = get_noc_addr(
             (uint32_t)worker_info.worker_xy.x, (uint32_t)worker_info.worker_xy.y, worker_info.worker_semaphore_address);
@@ -266,11 +264,9 @@ struct EdmChannelWorkerInterface {
     }
 
     [[nodiscard]] FORCE_INLINE bool has_worker_teardown_request() const {
-        invalidate_l1_cache();
         return *connection_live_semaphore == tt::tt_fabric::EdmToEdmSender<0>::close_connection_request_value;
     }
     [[nodiscard]] FORCE_INLINE bool connection_is_live() const {
-        invalidate_l1_cache();
         return *connection_live_semaphore == tt::tt_fabric::EdmToEdmSender<0>::open_connection_value;
     }
 
diff --git a/tt_metal/fabric/hw/inc/edm_fabric/fabric_stream_regs.hpp b/tt_metal/fabric/hw/inc/edm_fabric/fabric_stream_regs.hpp
index 1e9f008a5c..5d9f7c9d08 100644
--- a/tt_metal/fabric/hw/inc/edm_fabric/fabric_stream_regs.hpp
+++ b/tt_metal/fabric/hw/inc/edm_fabric/fabric_stream_regs.hpp
@@ -14,22 +14,10 @@ using StreamId = tt::tt_fabric::NamedType<uint32_t, struct StreamIdType>;
 // This will be an atomic register read to the register
 template <uint32_t stream_id>
 FORCE_INLINE int32_t get_ptr_val() {
-#ifdef ARCH_WORMHOLE
     return NOC_STREAM_READ_REG(stream_id, STREAM_REMOTE_DEST_BUF_SPACE_AVAILABLE_REG_INDEX);
-#else
-    return (
-        NOC_STREAM_READ_REG(stream_id, STREAM_REMOTE_DEST_BUF_SPACE_AVAILABLE_REG_INDEX) &
-        ((1 << REMOTE_DEST_WORDS_FREE_WIDTH) - 1));
-#endif
 }
 FORCE_INLINE int32_t get_ptr_val(uint8_t stream_id) {
-#ifdef ARCH_WORMHOLE
     return NOC_STREAM_READ_REG(stream_id, STREAM_REMOTE_DEST_BUF_SPACE_AVAILABLE_REG_INDEX);
-#else
-    return (
-        NOC_STREAM_READ_REG(stream_id, STREAM_REMOTE_DEST_BUF_SPACE_AVAILABLE_REG_INDEX) &
-        ((1 << REMOTE_DEST_WORDS_FREE_WIDTH) - 1));
-#endif
 }
 
 // Writing to this register will leverage the built-in stream hardware which will automatically perform an atomic
diff --git a/tt_metal/hw/inc/wormhole/eth_chan_noc_mapping.h b/tt_metal/fabric/hw/inc/eth_chan_noc_mapping.h
similarity index 100%
rename from tt_metal/hw/inc/wormhole/eth_chan_noc_mapping.h
rename to tt_metal/fabric/hw/inc/eth_chan_noc_mapping.h
diff --git a/tt_metal/fabric/hw/inc/tt_fabric.h b/tt_metal/fabric/hw/inc/tt_fabric.h
index 14e69393bf..652c992ac2 100644
--- a/tt_metal/fabric/hw/inc/tt_fabric.h
+++ b/tt_metal/fabric/hw/inc/tt_fabric.h
@@ -9,9 +9,9 @@
 #include "dataflow_api.h"
 #include "noc_overlay_parameters.h"
 #include "ethernet/dataflow_api.h"
-#include "eth_chan_noc_mapping.h"
 #include <fabric_host_interface.h>
 #include "tt_metal/fabric/hw/inc/tt_fabric_interface.h"
+#include "tt_metal/fabric/hw/inc/eth_chan_noc_mapping.h"
 
 using namespace tt::tt_fabric;
 
@@ -173,10 +173,7 @@ struct fvc_outbound_push_state_t {
         } else {
             // relay the credits to noc data sender to replenish buffer space in noc sender
             noc_inline_dw_write_with_state<false, true, true, true, true>(
-                slots_cleared << REMOTE_DEST_BUF_WORDS_FREE_INC,
-                *slots_cleared_ack_addr >> NOC_ADDR_COORD_SHIFT,
-                write_at_cmd_buf,
-                1);
+                slots_cleared << REMOTE_DEST_BUF_WORDS_FREE_INC, *slots_cleared_ack_addr >> 32, write_at_cmd_buf, 1);
             // clear the credits receied from ethernet receiver.
             *update_sender_slots_cleared = (-slots_cleared) << REMOTE_DEST_BUF_WORDS_FREE_INC;
         }
@@ -373,7 +370,7 @@ struct fvc_inbound_push_state_t {
                 // Write lower 4 Bytes of 8 Byte entry.
                 noc_inline_dw_write(router_addr, (uint32_t)update_router_space);
                 // Write upper 4 Bytes of 8 Byte entry.
-                noc_inline_dw_write(router_addr + (sizeof(uint32_t)), xy_local_addr >> NOC_ADDR_COORD_SHIFT);
+                noc_inline_dw_write(router_addr + (sizeof(uint32_t)), xy_local_addr >> 32);
             }
         } else {
             uint32_t router_direction = get_next_hop_router_direction(mesh_id, device_id);
@@ -385,7 +382,7 @@ struct fvc_inbound_push_state_t {
             router_addr += router_direction * sizeof(uint64_t);
             // stream register to receive router buffer space available updates.
             noc_inline_dw_write(router_addr, (uint32_t)update_router_space);
-            noc_inline_dw_write(router_addr + sizeof(uint32_t), xy_local_addr >> NOC_ADDR_COORD_SHIFT);
+            noc_inline_dw_write(router_addr + sizeof(uint32_t), xy_local_addr >> 32);
             uint32_t remote_buffer_start =
                 FABRIC_ROUTER_DATA_BUF_START + router_direction * FABRIC_ROUTER_OUTBOUND_BUF_SIZE;
             for (uint32_t i = 0; i < FABRIC_ROUTER_OUTBOUND_BUF_SLOTS; i++) {
@@ -600,7 +597,7 @@ struct fvc_inbound_push_state_t {
             advance_remote_wrptr(1, remote_wrptr_direction);
             advance_out_rdptr<fvc_mode>(1);
             uint64_t push_addr = get_noc_addr_helper(dest_addr, router_push_addr);
-            noc_inline_dw_write<true, true>(push_addr, 1 << REMOTE_DEST_BUF_WORDS_FREE_INC);
+            noc_inline_dw_write<false, true>(push_addr, 1 << REMOTE_DEST_BUF_WORDS_FREE_INC);
 
             *update_router_space = (-1) << REMOTE_DEST_BUF_WORDS_FREE_INC;
             uint32_t words_available = packet_words_remaining;
@@ -642,7 +639,7 @@ struct fvc_inbound_push_state_t {
         noc_async_write_one_packet(get_local_buffer_read_addr(), buffer_wr_addr, FABRIC_ROUTER_BUF_SLOT_SIZE);
         advance_remote_wrptr(1, direction);
         uint64_t push_addr = get_noc_addr_helper(mcast_router_noc_xy[direction], router_push_addr);
-        noc_inline_dw_write<true, true>(push_addr, 1 << REMOTE_DEST_BUF_WORDS_FREE_INC);
+        noc_inline_dw_write<false, true>(push_addr, 1 << REMOTE_DEST_BUF_WORDS_FREE_INC);
         *update_router_space = (-1) << REMOTE_DEST_BUF_WORDS_FREE_INC;
     }
 
@@ -771,7 +768,7 @@ struct fvc_inbound_push_state_t {
         advance_remote_wrptr(1);
         advance_out_rdptr<fvc_mode>(1);
         uint64_t push_addr = get_noc_addr_helper(dest_addr, router_push_addr);
-        noc_inline_dw_write<true>(push_addr, 1 << REMOTE_DEST_BUF_WORDS_FREE_INC);
+        noc_inline_dw_write(push_addr, 1 << REMOTE_DEST_BUF_WORDS_FREE_INC);
 
         *update_router_space = (-1) << REMOTE_DEST_BUF_WORDS_FREE_INC;
         uint32_t words_available = packet_words_remaining;
@@ -971,7 +968,6 @@ struct fvc_outbound_pull_state_t {
         uint32_t num_words_before_wrap = words_before_buffer_wrap(fvc_out_rdptr);
         uint32_t chunk_to_forward = std::min(num_words_before_wrap, words_remaining);
         while (words_remaining) {
-            invalidate_l1_cache();
             src_addr = get_local_buffer_read_addr();
             dest_addr = src_addr - buffer_start + remote_buffer_start;
             if constexpr (barrier) {
@@ -1818,7 +1814,6 @@ struct fvcc_outbound_state_t {
     inline void advance_fvcc_rdptr() {
         uint32_t rd_ptr = remote_rdptr.ptr;
         while (rd_ptr != fvcc_buf->rdptr.ptr) {
-            invalidate_l1_cache();
             uint32_t msg_index = fvcc_buf->rdptr.ptr & FVCC_SIZE_MASK;
             fvcc_buf->msg_buf[msg_index].packet_header.routing.flags = 0;
             fvcc_buf->rdptr.ptr = inc_ptr_with_wrap(fvcc_buf->rdptr.ptr);
@@ -2027,7 +2022,6 @@ struct fvcc_inbound_state_t {
         uint32_t wrptr = fvcc_buf->wrptr.ptr;
         noc_addr = dest_addr + offsetof(ctrl_chan_msg_buf, rdptr);
         while (1) {
-            invalidate_l1_cache();
             noc_async_read_one_packet(noc_addr, (uint32_t)(&fvcc_buf->rdptr.ptr), 4);
             noc_async_read_barrier();
             if (!fvcc_buf_ptrs_full(wrptr, fvcc_buf->rdptr.ptr)) {
@@ -2065,7 +2059,7 @@ struct fvcc_inbound_state_t {
             }
         } else {
             // Control message is not meant for local chip. Forward to next router enroute to destination.
-            uint64_t dest_addr = get_noc_addr_helper(get_next_hop_router_noc_xy(), FVCC_OUT_BUF_START);
+            uint64_t dest_addr = ((uint64_t)get_next_hop_router_noc_xy() << 32) | FVCC_OUT_BUF_START;
             forward_message(dest_addr);
         }
         curr_packet_valid = false;
@@ -2266,7 +2260,6 @@ struct socket_reader_state_t {
             uint32_t dest_addr = 0;  // should be second half of fvc buffer.
             uint32_t words_remaining = total_words_to_forward;
             while (words_remaining) {
-                invalidate_l1_cache();
                 uint32_t num_words_before_local_wrap = words_before_pull_buffer_wrap(buffer_size, fvc_out_rdptr);
                 uint32_t num_words_before_remote_wrap = words_before_pull_buffer_wrap(buffer_size, fvc_out_wrptr);
                 uint32_t words_to_forward = std::min(num_words_before_local_wrap, num_words_before_remote_wrap);
@@ -2407,7 +2400,6 @@ bool wait_all_src_dest_ready(volatile router_state_t* router_state, uint32_t tim
     router_state->scratch[0] = 0xAA;
 
     while (!src_ready or !dest_ready) {
-        invalidate_l1_cache();
         if (router_state->sync_out != 0xAA) {
             internal_::eth_send_packet(0, scratch_addr, sync_in_addr, 1);
         } else {
@@ -2461,7 +2453,6 @@ template <bool blocking_mode = false>
 inline bool tt_fabric_check_pull_request_slot(uint64_t dest_addr, volatile local_pull_request_t* local_pull_request, uint32_t wrptr) {
     uint64_t noc_addr = dest_addr + offsetof(chan_req_buf, rdptr);
     do {
-        invalidate_l1_cache();
         noc_async_read_one_packet(noc_addr, (uint32_t)(&local_pull_request->rdptr.ptr), 4);
         noc_async_read_barrier();
         if (!req_buf_ptrs_full(wrptr, local_pull_request->rdptr.ptr)) {
diff --git a/tt_metal/fabric/hw/inc/tt_fabric_api.h b/tt_metal/fabric/hw/inc/tt_fabric_api.h
index 5cb3299b3c..e7a86672bb 100644
--- a/tt_metal/fabric/hw/inc/tt_fabric_api.h
+++ b/tt_metal/fabric/hw/inc/tt_fabric_api.h
@@ -167,7 +167,6 @@ inline void fabric_send_pull_request(
 FORCE_INLINE void fabric_wait_for_pull_request_words_flushed(
     volatile tt_l1_ptr fabric_pull_client_interface_t* client_interface, uint32_t words) {
     while (client_interface->local_pull_request.pull_request.words_read < words) {
-        invalidate_l1_cache();
 #pragma GCC unroll 4
         for (int i = 0; i < 4; i++) {
             asm("nop");
@@ -556,7 +555,7 @@ inline void fabric_client_connect(
         router_addr,
         (STREAM_REG_ADDR(
             STREAM_ID_NOC_RECEIVER_BUFFER_SPACE, STREAM_REMOTE_DEST_BUF_SPACE_AVAILABLE_UPDATE_REG_INDEX)));
-    noc_inline_dw_write(router_addr + sizeof(uint32_t), xy_local_addr >> NOC_ADDR_COORD_SHIFT);
+    noc_inline_dw_write(router_addr + sizeof(uint32_t), xy_local_addr >> 32);
     client_interface->router_addr_h = router_addr_h;
     client_interface->buffer_size = FABRIC_ROUTER_OUTBOUND_BUF_SLOTS;
     client_interface->wr_ptr = local_req_entry->remote_router_wr_ptr.ptr;
@@ -573,9 +572,7 @@ inline void fabric_client_connect(
 
 inline void fabric_client_disconnect(volatile tt_l1_ptr fabric_push_client_interface_t* client_interface) {
     // wait for slots to drain
-    while (*(uint32_t*)(client_interface->router_space) != FABRIC_ROUTER_OUTBOUND_BUF_SLOTS) {
-        invalidate_l1_cache();
-    }
+    while (*(uint32_t*)(client_interface->router_space) != FABRIC_ROUTER_OUTBOUND_BUF_SLOTS);
 
     uint64_t client_q_addr = get_noc_addr_helper(client_interface->router_addr_h, FABRIC_ROUTER_CLIENT_QUEUE_START);
 
@@ -605,7 +602,6 @@ inline void fabric_async_write_push_data(
     uint64_t push_addr = get_noc_addr_helper(client_interface->router_addr_h, client_interface->router_push_addr);
     uint32_t router_buf_space = *(volatile uint32_t*)client_interface->router_space;
     while (router_buf_space == 0) {
-        invalidate_l1_cache();
         router_buf_space = *(volatile uint32_t*)client_interface->router_space;
     }
 
@@ -619,7 +615,7 @@ inline void fabric_async_write_push_data(
         size -= PACKET_HEADER_SIZE_BYTES;
     }
     noc_async_write_one_packet(src_addr, buffer_wr_addr, size, noc_index);
-    noc_inline_dw_write<true>(push_addr, 1 << REMOTE_DEST_BUF_WORDS_FREE_INC);
+    noc_inline_dw_write(push_addr, 1 << REMOTE_DEST_BUF_WORDS_FREE_INC);
     client_interface->wr_ptr++;
     *(volatile uint32_t*)client_interface->update_router_space = (-1) << REMOTE_DEST_BUF_WORDS_FREE_INC;
     if (client_interface->wr_ptr >= client_interface->buffer_size) {
diff --git a/tt_metal/fabric/hw/inc/tt_fabric_utils.h b/tt_metal/fabric/hw/inc/tt_fabric_utils.h
index df90a7d629..0693a38b4f 100644
--- a/tt_metal/fabric/hw/inc/tt_fabric_utils.h
+++ b/tt_metal/fabric/hw/inc/tt_fabric_utils.h
@@ -69,7 +69,6 @@ FORCE_INLINE void check_worker_connections(
 inline void wait_for_notification(uint32_t address, uint32_t value) {
     volatile tt_l1_ptr uint32_t* poll_addr = (volatile tt_l1_ptr uint32_t*)address;
     while (*poll_addr != value) {
-        invalidate_l1_cache();
         // context switch while waiting to allow slow dispatch traffic to go through
         run_routing();
     }
@@ -99,8 +98,7 @@ inline void notify_master_router(uint32_t master_eth_chan, uint32_t address) {
 inline void notify_subordinate_routers(
     uint32_t router_eth_chans_mask, uint32_t exclude_eth_chan, uint32_t address, uint32_t notification) {
     uint32_t remaining_cores = router_eth_chans_mask;
-    constexpr uint32_t num_routers = sizeof(eth_chan_to_noc_xy[0]) / sizeof(eth_chan_to_noc_xy[0][0]);
-    for (uint32_t i = 0; i < num_routers; i++) {
+    for (uint32_t i = 0; i < 16; i++) {
         if (remaining_cores == 0) {
             break;
         }
diff --git a/tt_metal/fabric/impl/kernels/edm_fabric/fabric_erisc_datamover.cpp b/tt_metal/fabric/impl/kernels/edm_fabric/fabric_erisc_datamover.cpp
index 700de07f26..e3e19e1a4f 100644
--- a/tt_metal/fabric/impl/kernels/edm_fabric/fabric_erisc_datamover.cpp
+++ b/tt_metal/fabric/impl/kernels/edm_fabric/fabric_erisc_datamover.cpp
@@ -372,17 +372,13 @@ bool did_something;
 //   SENDER SIDE HELPERS
 /////////////////////////////////////////////
 
-template <
-    uint8_t sender_channel_index,
-    uint8_t to_receiver_pkts_sent_id,
-    bool SKIP_CONNECTION_LIVENESS_CHECK,
-    uint8_t SENDER_NUM_BUFFERS,
-    uint8_t RECEIVER_NUM_BUFFERS>
+template <uint8_t SENDER_NUM_BUFFERS, uint8_t RECEIVER_NUM_BUFFERS, uint8_t to_receiver_pkts_sent_id, bool SKIP_CONNECTION_LIVENESS_CHECK>
 FORCE_INLINE void send_next_data(
     tt::tt_fabric::EthChannelBuffer<SENDER_NUM_BUFFERS>& sender_buffer_channel,
     tt::tt_fabric::EdmChannelWorkerInterface<SENDER_NUM_BUFFERS>& sender_worker_interface,
     OutboundReceiverChannelPointers<RECEIVER_NUM_BUFFERS>& outbound_to_receiver_channel_pointers,
-    tt::tt_fabric::EthChannelBuffer<RECEIVER_NUM_BUFFERS>& receiver_buffer_channel) {
+    tt::tt_fabric::EthChannelBuffer<RECEIVER_NUM_BUFFERS>& receiver_buffer_channel,
+    uint8_t sender_channel_index) {
     auto& remote_receiver_buffer_index = outbound_to_receiver_channel_pointers.remote_receiver_buffer_index;
     auto& remote_receiver_num_free_slots = outbound_to_receiver_channel_pointers.num_free_slots;
     auto& local_sender_write_counter = sender_worker_interface.local_write_counter;
@@ -412,8 +408,7 @@ FORCE_INLINE void send_next_data(
     if constexpr (SKIP_CONNECTION_LIVENESS_CHECK) {
         // For persistent connections, we don't need to increment the counter, we only care about the
         // buffer index, so we only increment it directly
-        local_sender_write_counter.index =
-            BufferIndex{wrap_increment<SENDER_NUM_BUFFERS>(local_sender_write_counter.index.get())};
+        local_sender_write_counter.index = BufferIndex{wrap_increment<SENDER_NUM_BUFFERS>(local_sender_write_counter.index.get())};
     } else {
         local_sender_write_counter.increment();
     }
@@ -449,7 +444,6 @@ FORCE_INLINE void receiver_send_received_ack(
     BufferIndex receiver_buffer_index,
     const tt::tt_fabric::EthChannelBuffer<RECEIVER_NUM_BUFFERS>& local_receiver_buffer_channel) {
     // Set the acknowledgement bits
-    invalidate_l1_cache();
     volatile tt_l1_ptr auto* pkt_header = reinterpret_cast<volatile tt_l1_ptr PACKET_HEADER_TYPE*>(
         local_receiver_buffer_channel.get_buffer_address(receiver_buffer_index));
     const auto src_id = pkt_header->src_ch_id;
@@ -488,7 +482,6 @@ FORCE_INLINE bool can_forward_packet_completely(
     tt_l1_ptr MeshPacketHeader* packet_header,
     std::array<tt::tt_fabric::EdmToEdmSender<SENDER_NUM_BUFFERS>, NUM_USED_RECEIVER_CHANNELS>& downstream_edm_interface,
     std::array<uint8_t, num_eth_ports>& port_direction_table) {
-    invalidate_l1_cache();
     if (packet_header->is_mcast_active) {
         // mcast downstream needs to check if downstream has space (lookup from set direction field)
         // forward to local and remote
@@ -597,20 +590,20 @@ FORCE_INLINE __attribute__((optimize("jump-tables"))) bool can_forward_packet_co
 }
 
 // !!!WARNING!!! - MAKE SURE CONSUMER HAS SPACE BEFORE CALLING
-template <uint8_t rx_channel_id, uint8_t SENDER_NUM_BUFFERS>
+template <uint8_t SENDER_NUM_BUFFERS>
 FORCE_INLINE void receiver_forward_packet(
     // TODO: have a separate cached copy of the packet header to save some additional L1 loads
     tt_l1_ptr PACKET_HEADER_TYPE* packet_start,
     ROUTING_FIELDS_TYPE cached_routing_fields,
     tt::tt_fabric::EdmToEdmSender<SENDER_NUM_BUFFERS>& downstream_edm_interface,
-    uint8_t transaction_id) {
+    uint8_t transaction_id,
+    uint8_t rx_channel_id) {
     constexpr bool ENABLE_STATEFUL_NOC_APIS =
 #if !defined(DEBUG_PRINT_ENABLED) and !defined(WATCHER_ENABLED)
         true;
 #else
         false;
 #endif
-    invalidate_l1_cache();  // Make sure we have the latest packet header in L1
     if constexpr (std::is_same_v<ROUTING_FIELDS_TYPE, tt::tt_fabric::RoutingFields>) {
         // If the packet is a terminal packet, then we can just deliver it locally
         bool start_distance_is_terminal_value =
@@ -620,7 +613,7 @@ FORCE_INLINE void receiver_forward_packet(
         bool not_last_destination_device = cached_routing_fields.value != tt::tt_fabric::RoutingFields::LAST_MCAST_VAL;
         // disable when dprint enabled due to noc cmd buf usage of DPRINT
         if (not_last_destination_device) {
-            forward_payload_to_downstream_edm<enable_ring_support, ENABLE_STATEFUL_NOC_APIS>(
+            forward_payload_to_downstream_edm<SENDER_NUM_BUFFERS, enable_ring_support, ENABLE_STATEFUL_NOC_APIS>(
                 packet_start, payload_size_bytes, cached_routing_fields, downstream_edm_interface, transaction_id);
         }
         if (start_distance_is_terminal_value) {
@@ -634,11 +627,11 @@ FORCE_INLINE void receiver_forward_packet(
                 execute_chip_unicast_to_local_chip(packet_start, payload_size_bytes, transaction_id, rx_channel_id);
                 break;
             case tt::tt_fabric::LowLatencyRoutingFields::FORWARD_ONLY:
-                forward_payload_to_downstream_edm<enable_ring_support, ENABLE_STATEFUL_NOC_APIS>(
+                forward_payload_to_downstream_edm<SENDER_NUM_BUFFERS, enable_ring_support, ENABLE_STATEFUL_NOC_APIS>(
                     packet_start, payload_size_bytes, cached_routing_fields, downstream_edm_interface, transaction_id);
                 break;
             case tt::tt_fabric::LowLatencyRoutingFields::WRITE_AND_FORWARD:
-                forward_payload_to_downstream_edm<enable_ring_support, ENABLE_STATEFUL_NOC_APIS>(
+                forward_payload_to_downstream_edm<SENDER_NUM_BUFFERS, enable_ring_support, ENABLE_STATEFUL_NOC_APIS>(
                     packet_start, payload_size_bytes, cached_routing_fields, downstream_edm_interface, transaction_id);
                 execute_chip_unicast_to_local_chip(packet_start, payload_size_bytes, transaction_id, rx_channel_id);
                 break;
@@ -651,12 +644,13 @@ FORCE_INLINE void receiver_forward_packet(
 
 #if defined(FABRIC_2D) && defined(DYNAMIC_ROUTING_ENABLED)
 // !!!WARNING!!! - MAKE SURE CONSUMER HAS SPACE BEFORE CALLING
-template <uint8_t rx_channel_id, uint8_t SENDER_NUM_BUFFERS>
+template <uint8_t SENDER_NUM_BUFFERS>
 FORCE_INLINE __attribute__((optimize("jump-tables"))) void receiver_forward_packet(
     tt_l1_ptr PACKET_HEADER_TYPE* packet_start,
     ROUTING_FIELDS_TYPE cached_routing_fields,
     std::array<tt::tt_fabric::EdmToEdmSender<SENDER_NUM_BUFFERS>, NUM_USED_RECEIVER_CHANNELS>& downstream_edm_interface,
     uint8_t transaction_id,
+    uint8_t rx_channel_id,
     std::array<uint8_t, num_eth_ports>& port_direction_table) {
     auto dest_mesh_id = packet_start->dst_start_mesh_id;
     auto dest_chip_id = packet_start->dst_start_chip_id;
@@ -670,7 +664,7 @@ FORCE_INLINE __attribute__((optimize("jump-tables"))) void receiver_forward_pack
         uint32_t downstream_channel = routing_table->inter_mesh_table.dest_entry[dest_mesh_id];
         ASSERT(downstream_channel != INVALID_DIRECTION);
         auto downstream_direction = port_direction_table[downstream_channel];
-        forward_payload_to_downstream_edm<enable_ring_support, false>(
+        forward_payload_to_downstream_edm<SENDER_NUM_BUFFERS, enable_ring_support, false>(
             packet_start,
             payload_size_bytes,
             cached_routing_fields,
@@ -684,7 +678,7 @@ FORCE_INLINE __attribute__((optimize("jump-tables"))) void receiver_forward_pack
                 for (size_t i = eth_chan_directions::EAST; i < eth_chan_directions::COUNT; i++) {
                     if (packet_start->mcast_params[i] and i != my_direction) {
                         packet_start->mcast_params[i]--;
-                        forward_payload_to_downstream_edm<enable_ring_support, false>(
+                        forward_payload_to_downstream_edm<SENDER_NUM_BUFFERS, enable_ring_support, false>(
                             packet_start,
                             payload_size_bytes,
                             cached_routing_fields,
@@ -698,7 +692,7 @@ FORCE_INLINE __attribute__((optimize("jump-tables"))) void receiver_forward_pack
             auto downstream_channel = routing_table->intra_mesh_table.dest_entry[dest_chip_id];
             ASSERT(downstream_channel != INVALID_DIRECTION);
             auto downstream_direction = port_direction_table[downstream_channel];
-            forward_payload_to_downstream_edm<enable_ring_support, false>(
+            forward_payload_to_downstream_edm<SENDER_NUM_BUFFERS, enable_ring_support, false>(
                 packet_start,
                 payload_size_bytes,
                 cached_routing_fields,
@@ -710,12 +704,13 @@ FORCE_INLINE __attribute__((optimize("jump-tables"))) void receiver_forward_pack
 #endif
 
 // !!!WARNING!!! - MAKE SURE CONSUMER HAS SPACE BEFORE CALLING
-template <uint8_t rx_channel_id, uint8_t SENDER_NUM_BUFFERS>
+template <uint8_t SENDER_NUM_BUFFERS>
 FORCE_INLINE __attribute__((optimize("jump-tables"))) void receiver_forward_packet(
     tt_l1_ptr PACKET_HEADER_TYPE* packet_start,
     ROUTING_FIELDS_TYPE cached_routing_fields,
     std::array<tt::tt_fabric::EdmToEdmSender<SENDER_NUM_BUFFERS>, NUM_USED_RECEIVER_CHANNELS>& downstream_edm_interface,
     uint8_t transaction_id,
+    uint8_t rx_channel_id,
     uint32_t hop_cmd) {
     uint16_t payload_size_bytes = packet_start->payload_size_bytes;
 
@@ -725,7 +720,7 @@ FORCE_INLINE __attribute__((optimize("jump-tables"))) void receiver_forward_pack
             if constexpr (my_direction == eth_chan_directions::EAST) {
                 execute_chip_unicast_to_local_chip(packet_start, payload_size_bytes, transaction_id, rx_channel_id);
             } else {
-                forward_payload_to_downstream_edm<enable_ring_support, false>(
+                forward_payload_to_downstream_edm<SENDER_NUM_BUFFERS, enable_ring_support, false>(
                     packet_start,
                     payload_size_bytes,
                     cached_routing_fields,
@@ -737,7 +732,7 @@ FORCE_INLINE __attribute__((optimize("jump-tables"))) void receiver_forward_pack
             if constexpr (my_direction == eth_chan_directions::WEST) {
                 execute_chip_unicast_to_local_chip(packet_start, payload_size_bytes, transaction_id, rx_channel_id);
             } else {
-                forward_payload_to_downstream_edm<enable_ring_support, false>(
+                forward_payload_to_downstream_edm<SENDER_NUM_BUFFERS, enable_ring_support, false>(
                     packet_start,
                     payload_size_bytes,
                     cached_routing_fields,
@@ -747,14 +742,14 @@ FORCE_INLINE __attribute__((optimize("jump-tables"))) void receiver_forward_pack
             break;
         case LowLatencyMeshRoutingFields::WRITE_AND_FORWARD_EW:
             if constexpr (my_direction == eth_chan_directions::WEST) {
-                forward_payload_to_downstream_edm<enable_ring_support, false>(
+                forward_payload_to_downstream_edm<SENDER_NUM_BUFFERS, enable_ring_support, false>(
                     packet_start,
                     payload_size_bytes,
                     cached_routing_fields,
                     downstream_edm_interface[eth_chan_directions::EAST],
                     transaction_id);
             } else {
-                forward_payload_to_downstream_edm<enable_ring_support, false>(
+                forward_payload_to_downstream_edm<SENDER_NUM_BUFFERS, enable_ring_support, false>(
                     packet_start,
                     payload_size_bytes,
                     cached_routing_fields,
@@ -767,7 +762,7 @@ FORCE_INLINE __attribute__((optimize("jump-tables"))) void receiver_forward_pack
             if constexpr (my_direction == eth_chan_directions::NORTH) {
                 execute_chip_unicast_to_local_chip(packet_start, payload_size_bytes, transaction_id, rx_channel_id);
             } else {
-                forward_payload_to_downstream_edm<enable_ring_support, false>(
+                forward_payload_to_downstream_edm<SENDER_NUM_BUFFERS, enable_ring_support, false>(
                     packet_start,
                     payload_size_bytes,
                     cached_routing_fields,
@@ -779,7 +774,7 @@ FORCE_INLINE __attribute__((optimize("jump-tables"))) void receiver_forward_pack
             if constexpr (my_direction == eth_chan_directions::SOUTH) {
                 execute_chip_unicast_to_local_chip(packet_start, payload_size_bytes, transaction_id, rx_channel_id);
             } else {
-                forward_payload_to_downstream_edm<enable_ring_support, false>(
+                forward_payload_to_downstream_edm<SENDER_NUM_BUFFERS, enable_ring_support, false>(
                     packet_start,
                     payload_size_bytes,
                     cached_routing_fields,
@@ -789,14 +784,14 @@ FORCE_INLINE __attribute__((optimize("jump-tables"))) void receiver_forward_pack
             break;
         case LowLatencyMeshRoutingFields::WRITE_AND_FORWARD_NS:
             if constexpr (my_direction == eth_chan_directions::SOUTH) {
-                forward_payload_to_downstream_edm<enable_ring_support, false>(
+                forward_payload_to_downstream_edm<SENDER_NUM_BUFFERS, enable_ring_support, false>(
                     packet_start,
                     payload_size_bytes,
                     cached_routing_fields,
                     downstream_edm_interface[eth_chan_directions::NORTH],
                     transaction_id);
             } else {
-                forward_payload_to_downstream_edm<enable_ring_support, false>(
+                forward_payload_to_downstream_edm<SENDER_NUM_BUFFERS, enable_ring_support, false>(
                     packet_start,
                     payload_size_bytes,
                     cached_routing_fields,
@@ -822,18 +817,20 @@ FORCE_INLINE void establish_edm_connection(
 ////////////////////////////////////
 template <
     bool enable_packet_header_recording,
-    uint8_t sender_channel_index,
-    uint8_t to_receiver_pkts_sent_id,
-    bool SKIP_CONNECTION_LIVENESS_CHECK,
+    bool enable_fabric_counters,
+    uint8_t RECEIVER_NUM_BUFFERS,
     uint8_t SENDER_NUM_BUFFERS,
-    uint8_t RECEIVER_NUM_BUFFERS>
-void run_sender_channel_step_impl(
+    uint8_t to_receiver_pkts_sent_id,
+    bool SKIP_CONNECTION_LIVENESS_CHECK>
+void run_sender_channel_step(
     tt::tt_fabric::EthChannelBuffer<SENDER_NUM_BUFFERS>& local_sender_channel,
     tt::tt_fabric::EdmChannelWorkerInterface<SENDER_NUM_BUFFERS>& local_sender_channel_worker_interface,
     OutboundReceiverChannelPointers<RECEIVER_NUM_BUFFERS>& outbound_to_receiver_channel_pointers,
     tt::tt_fabric::EthChannelBuffer<RECEIVER_NUM_BUFFERS>& remote_receiver_channel,
+    volatile tt::tt_fabric::EdmFabricSenderChannelCounters* sender_channel_counters,
     PacketHeaderRecorder& packet_header_recorder,
     bool& channel_connection_established,
+    uint8_t sender_channel_index,
     uint32_t sender_channel_free_slots_stream_id) {
     // If the receiver has space, and we have one or more packets unsent from producer, then send one
     // TODO: convert to loop to send multiple packets back to back (or support sending multiple packets in one shot)
@@ -858,11 +855,12 @@ void run_sender_channel_step_impl(
             tt::tt_fabric::validate(*packet_header);
             packet_header_recorder.record_packet_header(reinterpret_cast<volatile uint32_t*>(packet_header));
         }
-        send_next_data<sender_channel_index, to_receiver_pkts_sent_id, SKIP_CONNECTION_LIVENESS_CHECK>(
+        send_next_data<SENDER_NUM_BUFFERS, RECEIVER_NUM_BUFFERS, to_receiver_pkts_sent_id,SKIP_CONNECTION_LIVENESS_CHECK>(
             local_sender_channel,
             local_sender_channel_worker_interface,
             outbound_to_receiver_channel_pointers,
-            remote_receiver_channel);
+            remote_receiver_channel,
+            sender_channel_index);
         increment_local_update_ptr_val(sender_channel_free_slots_stream_id, 1);
     }
 
@@ -874,9 +872,8 @@ void run_sender_channel_step_impl(
             to_sender_packets_completed_streams[sender_channel_index], -completions_since_last_check);
         if constexpr (!enable_first_level_ack) {
             if constexpr (SKIP_CONNECTION_LIVENESS_CHECK) {
-                local_sender_channel_worker_interface
-                    .template update_persistent_connection_copy_of_free_slots<enable_ring_support>(
-                        completions_since_last_check);
+                local_sender_channel_worker_interface.template update_persistent_connection_copy_of_free_slots<enable_ring_support>(
+                    completions_since_last_check);
             } else {
                 // Connection liveness checks are only done for connections that are not persistent
                 // For those connections, it's unsafe to use free-slots counters held in stream registers
@@ -902,8 +899,7 @@ void run_sender_channel_step_impl(
         auto acks_since_last_check = get_ptr_val(to_sender_packets_acked_streams[sender_channel_index]);
         if (acks_since_last_check > 0) {
             if constexpr (SKIP_CONNECTION_LIVENESS_CHECK) {
-                local_sender_channel_worker_interface
-                    .template update_persistent_connection_copy_of_free_slots<enable_ring_support>();
+                local_sender_channel_worker_interface.template update_persistent_connection_copy_of_free_slots<enable_ring_support>();
             } else {
                 if (channel_connection_established) {
                     local_sender_channel_worker_interface.notify_worker_of_read_counter_update();
@@ -936,51 +932,22 @@ void run_sender_channel_step_impl(
 
 template <
     bool enable_packet_header_recording,
-    uint8_t VC_RECEIVER_CHANNEL,
-    uint8_t sender_channel_index,
-    typename EthSenderChannels,
-    typename EdmChannelWorkerIFs,
-    typename RemoteEthReceiverChannels,
+    bool enable_fabric_counters,
     uint8_t RECEIVER_NUM_BUFFERS,
+    uint8_t DOWNSTREAM_SENDER_NUM_BUFFERS,
     size_t NUM_SENDER_CHANNELS,
-    size_t MAX_NUM_SENDER_CHANNELS>
-FORCE_INLINE void run_sender_channel_step(
-    EthSenderChannels& local_sender_channels,
-    EdmChannelWorkerIFs& local_sender_channel_worker_interfaces,
-    OutboundReceiverChannelPointers<RECEIVER_NUM_BUFFERS>& outbound_to_receiver_channel_pointers,
-    RemoteEthReceiverChannels& remote_receiver_channels,
-    std::array<PacketHeaderRecorder, MAX_NUM_SENDER_CHANNELS>& sender_channel_packet_recorders,
-    std::array<bool, NUM_SENDER_CHANNELS>& channel_connection_established,
-    std::array<uint32_t, NUM_SENDER_CHANNELS>& local_sender_channel_free_slots_stream_ids_ordered) {
-    if constexpr (is_sender_channel_serviced[sender_channel_index]) {
-        run_sender_channel_step_impl<
-            enable_packet_header_recording,
-            sender_channel_index,
-            to_receiver_packets_sent_streams[VC_RECEIVER_CHANNEL],
-            sender_ch_live_check_skip[sender_channel_index],
-            SENDER_NUM_BUFFERS_ARRAY[sender_channel_index]>(
-            local_sender_channels.template get<sender_channel_index>(),
-            local_sender_channel_worker_interfaces.template get<sender_channel_index>(),
-            outbound_to_receiver_channel_pointers,
-            remote_receiver_channels.template get<VC_RECEIVER_CHANNEL>(),
-            sender_channel_packet_recorders[sender_channel_index],
-            channel_connection_established[sender_channel_index],
-            local_sender_channel_free_slots_stream_ids_ordered[sender_channel_index]);
-    }
-}
-
-template <
-    uint8_t receiver_channel,
     uint8_t to_receiver_pkts_sent_id,
-    typename WriteTridTracker,
-    uint8_t RECEIVER_NUM_BUFFERS,
-    uint8_t DOWNSTREAM_SENDER_NUM_BUFFERS>
-void run_receiver_channel_step_impl(
+    uint8_t receiver_channel,
+    typename WriteTridTracker>
+void run_receiver_channel_step(
     tt::tt_fabric::EthChannelBuffer<RECEIVER_NUM_BUFFERS>& local_receiver_channel,
     std::array<tt::tt_fabric::EdmToEdmSender<DOWNSTREAM_SENDER_NUM_BUFFERS>, NUM_USED_RECEIVER_CHANNELS>&
         downstream_edm_interface,
+    volatile tt::tt_fabric::EdmFabricReceiverChannelCounters* receiver_channel_counters_ptr,
     ReceiverChannelPointers<RECEIVER_NUM_BUFFERS>& receiver_channel_pointers,
+    PacketHeaderRecorder& packet_header_recorder,
     WriteTridTracker& receiver_channel_trid_tracker,
+    uint8_t rx_channel_id,
     std::array<uint8_t, num_eth_ports>& port_direction_table) {
     auto& ack_counter = receiver_channel_pointers.ack_counter;
     auto pkts_received_since_last_check = get_ptr_val<to_receiver_pkts_sent_id>();
@@ -1003,7 +970,6 @@ void run_receiver_channel_step_impl(
     auto& wr_sent_counter = receiver_channel_pointers.wr_sent_counter;
     bool unwritten_packets = !wr_sent_counter.is_caught_up_to(ack_counter);
     if (unwritten_packets) {
-        invalidate_l1_cache();
         auto receiver_buffer_index = wr_sent_counter.get_buffer_index();
         tt_l1_ptr PACKET_HEADER_TYPE* packet_header = const_cast<PACKET_HEADER_TYPE*>(
             local_receiver_channel.template get_packet_header<PACKET_HEADER_TYPE>(receiver_buffer_index));
@@ -1051,15 +1017,24 @@ void run_receiver_channel_step_impl(
                 receiver_buffer_index);
             if constexpr (is_2d_fabric) {
 #if defined(DYNAMIC_ROUTING_ENABLED)
-                receiver_forward_packet<receiver_channel>(
-                    packet_header, cached_routing_fields, downstream_edm_interface, trid, port_direction_table);
+                receiver_forward_packet(
+                    packet_header,
+                    cached_routing_fields,
+                    downstream_edm_interface,
+                    trid,
+                    rx_channel_id,
+                    port_direction_table);
 #else
-                receiver_forward_packet<receiver_channel>(
-                    packet_header, cached_routing_fields, downstream_edm_interface, trid, hop_cmd);
+                receiver_forward_packet(
+                    packet_header, cached_routing_fields, downstream_edm_interface, trid, rx_channel_id, hop_cmd);
 #endif
             } else {
-                receiver_forward_packet<receiver_channel>(
-                    packet_header, cached_routing_fields, downstream_edm_interface[receiver_channel], trid);
+                receiver_forward_packet(
+                    packet_header,
+                    cached_routing_fields,
+                    downstream_edm_interface[receiver_channel],
+                    trid,
+                    rx_channel_id);
             }
             wr_sent_counter.increment();
         }
@@ -1108,29 +1083,6 @@ void run_receiver_channel_step_impl(
     }
 };
 
-template <
-    uint8_t receiver_channel,
-    typename EthReceiverChannels,
-    typename WriteTridTracker,
-    uint8_t RECEIVER_NUM_BUFFERS,
-    uint8_t DOWNSTREAM_SENDER_NUM_BUFFERS>
-FORCE_INLINE void run_receiver_channel_step(
-    EthReceiverChannels& local_receiver_channels,
-    std::array<tt::tt_fabric::EdmToEdmSender<DOWNSTREAM_SENDER_NUM_BUFFERS>, NUM_USED_RECEIVER_CHANNELS>&
-        downstream_edm_interface,
-    ReceiverChannelPointers<RECEIVER_NUM_BUFFERS>& receiver_channel_pointers,
-    WriteTridTracker& receiver_channel_trid_tracker,
-    std::array<uint8_t, num_eth_ports>& port_direction_table) {
-    if constexpr (is_receiver_channel_serviced[receiver_channel]) {
-        run_receiver_channel_step_impl<receiver_channel, to_receiver_packets_sent_streams[receiver_channel]>(
-            local_receiver_channels.template get<receiver_channel>(),
-            downstream_edm_interface,
-            receiver_channel_pointers,
-            receiver_channel_trid_tracker,
-            port_direction_table);
-    }
-}
-
 /*
  * Main control loop for fabric EDM. Run indefinitely until a termination signal is received
  *
@@ -1139,10 +1091,12 @@ FORCE_INLINE void run_receiver_channel_step(
  */
 template <
     bool enable_packet_header_recording,
-    size_t NUM_RECEIVER_CHANNELS,
-    uint8_t DOWNSTREAM_SENDER_NUM_BUFFERS,
+    bool enable_fabric_counters,
+    uint8_t NUM_RECEIVER_CHANNELS,
+    size_t DOWNSTREAM_SENDER_NUM_BUFFERS,
     size_t NUM_SENDER_CHANNELS,
     size_t MAX_NUM_SENDER_CHANNELS,
+    size_t MAX_NUM_RECEIVER_CHANNELS,
     typename EthSenderChannels,
     typename EthReceiverChannels,
     typename RemoteEthReceiverChannels,
@@ -1157,6 +1111,11 @@ void run_fabric_edm_main_loop(
         downstream_edm_noc_interfaces,
     RemoteEthReceiverChannels& remote_receiver_channels,
     volatile tt::tt_fabric::TerminationSignal* termination_signal_ptr,
+    std::array<volatile tt::tt_fabric::EdmFabricReceiverChannelCounters*, MAX_NUM_RECEIVER_CHANNELS>
+        receiver_channel_counters_ptrs,
+    std::array<volatile tt::tt_fabric::EdmFabricSenderChannelCounters*, MAX_NUM_SENDER_CHANNELS>
+        sender_channel_counters_ptrs,
+    std::array<PacketHeaderRecorder, MAX_NUM_RECEIVER_CHANNELS>& receiver_channel_packet_recorders,
     std::array<PacketHeaderRecorder, MAX_NUM_SENDER_CHANNELS>& sender_channel_packet_recorders,
     TransactionIdTrackerCH0& receiver_channel_0_trid_tracker,
     TransactionIdTrackerCH1& receiver_channel_1_trid_tracker,
@@ -1193,7 +1152,6 @@ void run_fabric_edm_main_loop(
     // improve performance. The value of 32 was chosen somewhat empirically and then raised up slightly.
 
     while (!got_immediate_termination_signal(termination_signal_ptr)) {
-        invalidate_l1_cache();
         bool got_graceful_termination = got_graceful_termination_signal(termination_signal_ptr);
         if (got_graceful_termination) {
             DPRINT << "EDM Graceful termination\n";
@@ -1205,68 +1163,140 @@ void run_fabric_edm_main_loop(
 
             // There are some cases, mainly for performance, where we don't want to switch between sender channels
             // so we interoduce this to provide finer grain control over when we disable the automatic switching
-            run_sender_channel_step<enable_packet_header_recording, VC0_RECEIVER_CHANNEL, 0>(
-                local_sender_channels,
-                local_sender_channel_worker_interfaces,
-                outbound_to_receiver_channel_pointer_ch0,
-                remote_receiver_channels,
-                sender_channel_packet_recorders,
-                channel_connection_established,
-                local_sender_channel_free_slots_stream_ids_ordered);
-            if constexpr (!dateline_connection) {
-                run_receiver_channel_step<0>(
-                    local_receiver_channels,
-                    downstream_edm_noc_interfaces,
-                    receiver_channel_pointers_ch0,
-                    receiver_channel_0_trid_tracker,
-                    port_direction_table);
+            if constexpr (is_sender_channel_serviced[0]) {
+                run_sender_channel_step<
+                    enable_packet_header_recording,
+                    enable_fabric_counters,
+                    REMOTE_RECEIVER_NUM_BUFFERS_ARRAY[VC0_RECEIVER_CHANNEL],
+                    SENDER_NUM_BUFFERS_ARRAY[0],
+                    to_receiver_packets_sent_streams[VC0_RECEIVER_CHANNEL],
+                    sender_ch_live_check_skip[0]>(
+                    local_sender_channels.template get<0>(),
+                    local_sender_channel_worker_interfaces.template get<0>(),
+                    outbound_to_receiver_channel_pointer_ch0,
+                    remote_receiver_channels.template get<VC0_RECEIVER_CHANNEL>(),
+                    sender_channel_counters_ptrs[0],
+                    sender_channel_packet_recorders[0],
+                    channel_connection_established[0],
+                    0,
+                    local_sender_channel_free_slots_stream_ids_ordered[0]);
             }
-            if constexpr (enable_ring_support && !skip_receiver_channel_1_connection) {
-                run_receiver_channel_step<1>(
-                    local_receiver_channels,
-                    downstream_edm_noc_interfaces,
-                    receiver_channel_pointers_ch1,
-                    receiver_channel_1_trid_tracker,
-                    port_direction_table);
+            if constexpr (is_receiver_channel_serviced[0]) {
+                if constexpr (!dateline_connection) {
+                    run_receiver_channel_step<
+                        enable_packet_header_recording,
+                        enable_fabric_counters,
+                        RECEIVER_NUM_BUFFERS_ARRAY[0],
+                        DOWNSTREAM_SENDER_NUM_BUFFERS,
+                        NUM_SENDER_CHANNELS,
+                        to_receiver_packets_sent_streams[0],
+                        0>(
+                        local_receiver_channels.template get<0>(),
+                        downstream_edm_noc_interfaces,
+                        receiver_channel_counters_ptrs[0],
+                        receiver_channel_pointers_ch0,
+                        receiver_channel_packet_recorders[0],
+                        receiver_channel_0_trid_tracker,
+                        0,
+                        port_direction_table);
+                }
+            }
+            if constexpr (is_receiver_channel_serviced[1]) {
+                if constexpr (enable_ring_support && !skip_receiver_channel_1_connection) {
+                    run_receiver_channel_step<
+                        enable_packet_header_recording,
+                        enable_fabric_counters,
+                        RECEIVER_NUM_BUFFERS_ARRAY[1],
+                        DOWNSTREAM_SENDER_NUM_BUFFERS,
+                        NUM_SENDER_CHANNELS,
+                        to_receiver_packets_sent_streams[1],
+                        1>(
+                        local_receiver_channels.template get<1>(),
+                        downstream_edm_noc_interfaces,
+                        receiver_channel_counters_ptrs[1],
+                        receiver_channel_pointers_ch1,
+                        receiver_channel_packet_recorders[1],
+                        receiver_channel_1_trid_tracker,
+                        1,
+                        port_direction_table);
+                }
             }
 
             if constexpr (is_sender_channel_serviced[1] && !skip_sender_channel_1_connection) {
-                run_sender_channel_step<enable_packet_header_recording, VC0_RECEIVER_CHANNEL, 1>(
-                    local_sender_channels,
-                    local_sender_channel_worker_interfaces,
+                run_sender_channel_step<
+                    enable_packet_header_recording,
+                    enable_fabric_counters,
+                    REMOTE_RECEIVER_NUM_BUFFERS_ARRAY[VC0_RECEIVER_CHANNEL],
+                    SENDER_NUM_BUFFERS_ARRAY[1],
+                    to_receiver_packets_sent_streams[VC0_RECEIVER_CHANNEL],
+                    sender_ch_live_check_skip[1]>(
+                    local_sender_channels.template get<1>(),
+                    local_sender_channel_worker_interfaces.template get<1>(),
                     outbound_to_receiver_channel_pointer_ch0,
-                    remote_receiver_channels,
-                    sender_channel_packet_recorders,
-                    channel_connection_established,
-                    local_sender_channel_free_slots_stream_ids_ordered);
+                    remote_receiver_channels.template get<VC0_RECEIVER_CHANNEL>(),
+                    sender_channel_counters_ptrs[1],
+                    sender_channel_packet_recorders[1],
+                    channel_connection_established[1],
+                    1,
+                    local_sender_channel_free_slots_stream_ids_ordered[1]);
             }
             if constexpr (is_2d_fabric) {
-                run_sender_channel_step<enable_packet_header_recording, VC0_RECEIVER_CHANNEL, 2>(
-                    local_sender_channels,
-                    local_sender_channel_worker_interfaces,
-                    outbound_to_receiver_channel_pointer_ch0,
-                    remote_receiver_channels,
-                    sender_channel_packet_recorders,
-                    channel_connection_established,
-                    local_sender_channel_free_slots_stream_ids_ordered);
-                run_sender_channel_step<enable_packet_header_recording, VC0_RECEIVER_CHANNEL, 3>(
-                    local_sender_channels,
-                    local_sender_channel_worker_interfaces,
-                    outbound_to_receiver_channel_pointer_ch0,
-                    remote_receiver_channels,
-                    sender_channel_packet_recorders,
-                    channel_connection_established,
-                    local_sender_channel_free_slots_stream_ids_ordered);
+                if constexpr (is_sender_channel_serviced[2]) {
+                    run_sender_channel_step<
+                        enable_packet_header_recording,
+                        enable_fabric_counters,
+                        REMOTE_RECEIVER_NUM_BUFFERS_ARRAY[VC0_RECEIVER_CHANNEL],
+                        SENDER_NUM_BUFFERS_ARRAY[2],
+                        to_receiver_packets_sent_streams[VC0_RECEIVER_CHANNEL],
+                        sender_ch_live_check_skip[2]>(
+                        local_sender_channels.template get<2>(),
+                        local_sender_channel_worker_interfaces.template get<2>(),
+                        outbound_to_receiver_channel_pointer_ch0,
+                        remote_receiver_channels.template get<VC0_RECEIVER_CHANNEL>(),
+                        sender_channel_counters_ptrs[2],
+                        sender_channel_packet_recorders[2],
+                        channel_connection_established[2],
+                        2,
+                        local_sender_channel_free_slots_stream_ids_ordered[2]);
+                }
+                if constexpr (is_sender_channel_serviced[3]) {
+                    run_sender_channel_step<
+                        enable_packet_header_recording,
+                        enable_fabric_counters,
+                        REMOTE_RECEIVER_NUM_BUFFERS_ARRAY[VC0_RECEIVER_CHANNEL],
+                        SENDER_NUM_BUFFERS_ARRAY[3],
+                        to_receiver_packets_sent_streams[VC0_RECEIVER_CHANNEL],
+                        sender_ch_live_check_skip[3]>(
+                        local_sender_channels.template get<3>(),
+                        local_sender_channel_worker_interfaces.template get<3>(),
+                        outbound_to_receiver_channel_pointer_ch0,
+                        remote_receiver_channels.template get<VC0_RECEIVER_CHANNEL>(),
+                        sender_channel_counters_ptrs[3],
+                        sender_channel_packet_recorders[3],
+                        channel_connection_established[3],
+                        3,
+                        local_sender_channel_free_slots_stream_ids_ordered[3]);
+                }
             }
             if constexpr (enable_ring_support && !dateline_connection) {
-                run_sender_channel_step<enable_packet_header_recording, VC1_RECEIVER_CHANNEL, NUM_SENDER_CHANNELS - 1>(
-                    local_sender_channels,
-                    local_sender_channel_worker_interfaces,
-                    outbound_to_receiver_channel_pointer_ch1,
-                    remote_receiver_channels,
-                    sender_channel_packet_recorders,
-                    channel_connection_established,
-                    local_sender_channel_free_slots_stream_ids_ordered);
+                if constexpr (is_sender_channel_serviced[NUM_SENDER_CHANNELS - 1]) {
+                    run_sender_channel_step<
+                        enable_packet_header_recording,
+                        enable_fabric_counters,
+                        REMOTE_RECEIVER_NUM_BUFFERS_ARRAY[VC1_RECEIVER_CHANNEL],
+                        SENDER_NUM_BUFFERS_ARRAY[NUM_SENDER_CHANNELS - 1],
+                        to_receiver_packets_sent_streams[VC1_RECEIVER_CHANNEL],
+                        sender_ch_live_check_skip[NUM_SENDER_CHANNELS - 1]>(
+                        local_sender_channels.template get<NUM_SENDER_CHANNELS - 1>(),
+                        local_sender_channel_worker_interfaces.template get<NUM_SENDER_CHANNELS - 1>(),
+                        outbound_to_receiver_channel_pointer_ch1,
+                        remote_receiver_channels.template get<VC1_RECEIVER_CHANNEL>(),
+                        sender_channel_counters_ptrs[NUM_SENDER_CHANNELS - 1],
+                        sender_channel_packet_recorders[NUM_SENDER_CHANNELS - 1],
+                        channel_connection_established[NUM_SENDER_CHANNELS - 1],
+                        NUM_SENDER_CHANNELS - 1,
+                        local_sender_channel_free_slots_stream_ids_ordered[NUM_SENDER_CHANNELS - 1]);
+                }
             }
         }
 
@@ -1293,9 +1323,7 @@ void __attribute__((noinline)) wait_for_static_connection_to_ready(
         if (!sender_ch_live_check_skip[idx]) {
             return;
         }
-        while (!connect_is_requested(*interface.connection_live_semaphore)) {
-            invalidate_l1_cache();
-        }
+        while (!connect_is_requested(*interface.connection_live_semaphore));
         establish_edm_connection(interface, local_sender_channel_free_slots_stream_ids_ordered[idx]);
     });
 }
@@ -2008,23 +2036,35 @@ void kernel_main() {
     }
 #endif
 
-    WAYPOINT("FSCW");
     wait_for_static_connection_to_ready(
         local_sender_channel_worker_interfaces, local_sender_channel_free_slots_stream_ids_ordered);
-    WAYPOINT("FSCD");
 
     //////////////////////////////
     //////////////////////////////
     //        MAIN LOOP
     //////////////////////////////
     //////////////////////////////
-    run_fabric_edm_main_loop<enable_packet_header_recording, NUM_RECEIVER_CHANNELS>(
+    run_fabric_edm_main_loop<
+        enable_packet_header_recording,
+        enable_fabric_counters,
+        NUM_RECEIVER_CHANNELS,
+        DOWNSTREAM_SENDER_NUM_BUFFERS,
+        NUM_SENDER_CHANNELS,
+        MAX_NUM_SENDER_CHANNELS,
+        MAX_NUM_RECEIVER_CHANNELS>(
         local_receiver_channels,
         local_sender_channels,
         local_sender_channel_worker_interfaces,
         downstream_edm_noc_interfaces,
         remote_receiver_channels,
         termination_signal_ptr,
+        {receiver_0_channel_counters_ptr, receiver_1_channel_counters_ptr},
+        {sender_channel_0_counters_ptr,
+         sender_channel_1_counters_ptr,
+         sender_channel_2_counters_ptr,
+         sender_channel_3_counters_ptr,
+         sender_channel_4_counters_ptr},
+        receiver_channel_packet_recorders,
         sender_channel_packet_recorders,
         receiver_channel_0_trid_tracker,
         receiver_channel_1_trid_tracker,
diff --git a/tt_metal/fabric/impl/kernels/tt_fabric_gatekeeper.cpp b/tt_metal/fabric/impl/kernels/tt_fabric_gatekeeper.cpp
index dd19f5f347..a0f9ce56f4 100644
--- a/tt_metal/fabric/impl/kernels/tt_fabric_gatekeeper.cpp
+++ b/tt_metal/fabric/impl/kernels/tt_fabric_gatekeeper.cpp
@@ -34,13 +34,12 @@ uint32_t gk_message_pending;
 
 inline void notify_all_routers(uint32_t notification) {
     uint32_t remaining_cores = router_mask;
-    constexpr uint32_t num_routers = sizeof(eth_chan_to_noc_xy[0]) / sizeof(eth_chan_to_noc_xy[0][0]);
-    for (uint32_t i = 0; i < num_routers; i++) {
+    for (uint32_t i = 0; i < 16; i++) {
         if (remaining_cores == 0) {
             break;
         }
         if (remaining_cores & (0x1 << i)) {
-            uint64_t dest_addr = get_noc_addr_helper(eth_chan_to_noc_xy[noc_index][i], FABRIC_ROUTER_SYNC_SEM);
+            uint64_t dest_addr = ((uint64_t)eth_chan_to_noc_xy[noc_index][i] << 32) | FABRIC_ROUTER_SYNC_SEM;
             noc_inline_dw_write(dest_addr, notification);
             remaining_cores &= ~(0x1 << i);
         }
@@ -64,8 +63,8 @@ inline void get_routing_tables() {
     if (temp_mask) {
         for (uint32_t i = 0; i < 4; i++) {
             if (temp_mask & 0x1) {
-                uint64_t router_config_addr = get_noc_addr_helper(
-                    eth_chan_to_noc_xy[noc_index][channel], eth_l1_mem::address_map::FABRIC_ROUTER_CONFIG_BASE);
+                uint64_t router_config_addr = ((uint64_t)eth_chan_to_noc_xy[noc_index][channel] << 32) |
+                                              eth_l1_mem::address_map::FABRIC_ROUTER_CONFIG_BASE;
                 noc_async_read_one_packet(
                     router_config_addr,
                     (uint32_t)&routing_table[routing_plane],
@@ -170,7 +169,7 @@ inline void socket_open(packet_header_t* packet) {
             // If remote receive socket already opened,
             // set send socket state to active and return.
             handle->status_notification_addr =
-                get_noc_addr_helper(packet->session.ack_offset_h, packet->session.ack_offset_l);
+                ((uint64_t)packet->session.ack_offset_h << 32) | packet->session.ack_offset_l;
             set_socket_active(handle);
             DPRINT << "GK: Receiver Available " << (uint32_t)handle->socket_id << ENDL();
             return;
@@ -187,12 +186,12 @@ inline void socket_open(packet_header_t* packet) {
             handle->socket_direction = packet->packet_parameters.socket_parameters.socket_direction;
             handle->routing_plane = packet->packet_parameters.socket_parameters.routing_plane;
             handle->status_notification_addr =
-                get_noc_addr_helper(packet->session.ack_offset_h, packet->session.ack_offset_l);
+                ((uint64_t)packet->session.ack_offset_h << 32) | packet->session.ack_offset_l;
             handle->socket_state = SocketState::OPENING;
 
             if (handle->socket_direction == SOCKET_DIRECTION_RECV) {
                 handle->pull_notification_adddr =
-                    get_noc_addr_helper(packet->session.target_offset_h, packet->session.target_offset_l);
+                    ((uint64_t)packet->session.target_offset_h << 32) | packet->session.target_offset_l;
                 handle->sender_dev_id = packet->routing.src_dev_id;
                 handle->sender_mesh_id = packet->routing.src_mesh_id;
                 handle->rcvr_dev_id = routing_table[0].my_device_id;
@@ -242,7 +241,7 @@ inline void socket_open_for_connect(packet_header_t* packet) {
             handle->socket_type = packet->packet_parameters.socket_parameters.socket_type;
             handle->socket_direction = SOCKET_DIRECTION_SEND;
             handle->pull_notification_adddr =
-                get_noc_addr_helper(packet->session.target_offset_h, packet->session.target_offset_l);
+                ((uint64_t)packet->session.target_offset_h << 32) | packet->session.target_offset_l;
             handle->routing_plane = packet->packet_parameters.socket_parameters.routing_plane;
             handle->socket_state = SocketState::OPENING;
             handle->sender_dev_id = routing_table[0].my_device_id;
@@ -284,7 +283,7 @@ inline void socket_connect(packet_header_t* packet) {
             }
             found_sender = true;
             handle->pull_notification_adddr =
-                get_noc_addr_helper(packet->session.target_offset_h, packet->session.target_offset_l);
+                ((uint64_t)packet->session.target_offset_h << 32) | packet->session.target_offset_l;
             handle->socket_state = SocketState::ACTIVE;
             set_socket_active(handle);
             DPRINT << "GK: Found Send Socket " << (uint32_t)handle->socket_id << ENDL();
@@ -398,8 +397,9 @@ inline void process_pending_socket() {
                 message->packet_header.packet_parameters.socket_parameters.routing_plane = handle->routing_plane;
                 tt_fabric_add_header_checksum((packet_header_t*)&message->packet_header);
 
-                router_addr = get_noc_addr_helper(
-                    get_next_hop_router_noc_xy(&message->packet_header, handle->routing_plane), FVCC_OUT_BUF_START);
+                router_addr =
+                    ((uint64_t)get_next_hop_router_noc_xy(&message->packet_header, handle->routing_plane) << 32) |
+                    FVCC_OUT_BUF_START;
 
                 if (send_gk_message(router_addr, &message->packet_header)) {
                     DPRINT << "GK: Sending Connect to " << (uint32_t)handle->sender_dev_id << ENDL();
diff --git a/tt_metal/fabric/impl/kernels/tt_fabric_router.cpp b/tt_metal/fabric/impl/kernels/tt_fabric_router.cpp
index 2481728380..a947cafaf9 100644
--- a/tt_metal/fabric/impl/kernels/tt_fabric_router.cpp
+++ b/tt_metal/fabric/impl/kernels/tt_fabric_router.cpp
@@ -106,7 +106,7 @@ void kernel_main() {
     fvcc_inbound_state.init(
         FVCC_IN_BUF_START,
         (uint32_t)&fvcc_outbound_state.remote_rdptr,
-        get_noc_addr_helper(gk_message_addr_h, gk_message_addr_l) + offsetof(gatekeeper_info_t, gk_msg_buf));
+        (((uint64_t)gk_message_addr_h << 32) | gk_message_addr_l) + offsetof(gatekeeper_info_t, gk_msg_buf));
 #endif
 
     if (!wait_all_src_dest_ready(&router_state, timeout_cycles)) {
diff --git a/tt_metal/hw/CMakeLists.txt b/tt_metal/hw/CMakeLists.txt
index eedd786db4..373657af94 100644
--- a/tt_metal/hw/CMakeLists.txt
+++ b/tt_metal/hw/CMakeLists.txt
@@ -32,8 +32,7 @@ set(IRAM_OPTIONS
 file(STRINGS "../sfpi-version.sh" SFPI)
 set(SFPI_arch_os "${CMAKE_HOST_SYSTEM_PROCESSOR}_${CMAKE_HOST_SYSTEM_NAME}")
 foreach(tuple ${SFPI})
-    string(STRIP ${tuple} pair)
-    string(REPLACE "=" ";" pair ${pair})
+    string(REPLACE "=" ";" pair ${tuple})
     list(LENGTH pair count)
     if(count EQUAL 2)
         list(GET pair 0 key)
@@ -291,9 +290,7 @@ if(CMAKE_VERSION VERSION_GREATER_EQUAL 3.23)
                 ckernels/blackhole/metal/llk_api/llk_sfpu/ckernel_sfpu_log1p.h
                 ckernels/blackhole/metal/llk_api/llk_sfpu/ckernel_sfpu_max.h
                 ckernels/blackhole/metal/llk_api/llk_sfpu/ckernel_sfpu_recip.h
-                ckernels/blackhole/metal/llk_api/llk_sfpu/ckernel_sfpu_relu.h
                 ckernels/blackhole/metal/llk_api/llk_sfpu/ckernel_sfpu_unary_max_min.h
-                ckernels/blackhole/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_relu.h
                 ckernels/blackhole/metal/llk_api/llk_unpack_untilize_api.h
                 ckernels/wormhole_b0/metal/llk_api/llk_math_binary_api.h
                 ckernels/wormhole_b0/metal/llk_api/llk_math_reduce_api.h
@@ -416,54 +413,48 @@ if(CMAKE_VERSION VERSION_GREATER_EQUAL 3.23)
                 ckernels/wormhole_b0/metal/llk_io/llk_io_unpack.h
                 ckernels/wormhole_b0/metal/llk_io/llk_operands.h
                 ckernels/wormhole_b0/metal/llk_io/llk_outputs.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_init.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_abs.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_alt_complex_rotate90.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_bitwise_and.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_bitwise_not.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_bitwise_or.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_bitwise_xor.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_comp.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_unary_max_min.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_exp2.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_expm1.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_fill.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_fmod.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_heaviside.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_i1.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_init.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_left_shift.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_log.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_log1p.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_mask.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_max.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_negative.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_power.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_prelu.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_recip.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_relu.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_remainder.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_right_shift.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_rounding_ops.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_rsqrt.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_tiled_prod.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_sigmoid.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_sign.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_signbit.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_silu.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_sqrt.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_square.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_tanh.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_tiled_prod.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_topk.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_trigonometry.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_unary_comp.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_trigonometry.h
                 ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_unary_comp.h
-                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_unary_max_min.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_remainder.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_rounding_ops.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_bitwise_xor.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_bitwise_not.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_fmod.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_bitwise_and.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_bitwise_or.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_right_shift.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_left_shift.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_fill.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_alt_complex_rotate90.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_prelu.h
+                ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_i1.h
                 inc/atomic_rwptr.h
                 inc/bit_utils.h
                 inc/blackhole/c_tensix_core.h
                 inc/blackhole/cfg_defines.h
                 inc/blackhole/core_config.h
                 inc/blackhole/dev_mem_map.h
-                inc/blackhole/eth_chan_noc_mapping.h
                 inc/blackhole/eth_l1_address_map.h
                 inc/blackhole/noc/noc.h
                 inc/blackhole/noc/noc_overlay_parameters.h
@@ -484,7 +475,6 @@ if(CMAKE_VERSION VERSION_GREATER_EQUAL 3.23)
                 inc/debug/assert.h
                 inc/debug/dprint.h
                 inc/debug/dprint_buffer.h
-                inc/debug/dprint_pages.h
                 inc/debug/dprint_tile.h
                 inc/debug/fw_debug.h
                 inc/debug/noc_logging.h
@@ -505,13 +495,11 @@ if(CMAKE_VERSION VERSION_GREATER_EQUAL 3.23)
                 inc/risc_attribs.h
                 inc/risc_common.h
                 inc/tensix_functions.h
-                inc/utils/bfloat16.h
                 inc/utils/utils.h
                 inc/vptr_uint.h
                 inc/wormhole/c_tensix_core.h
                 inc/wormhole/core_config.h
                 inc/wormhole/dev_mem_map.h
-                inc/wormhole/eth_chan_noc_mapping.h
                 inc/wormhole/eth_l1_address_map.h
                 inc/wormhole/noc/noc.h
                 inc/wormhole/noc/noc_overlay_parameters.h
diff --git a/tt_metal/hw/ckernels/blackhole/metal/llk_api/llk_sfpu/ckernel_sfpu_logical_not_noti.h b/tt_metal/hw/ckernels/blackhole/metal/llk_api/llk_sfpu/ckernel_sfpu_logical_not_noti.h
index 0c09426cb8..00220f0982 100644
--- a/tt_metal/hw/ckernels/blackhole/metal/llk_api/llk_sfpu/ckernel_sfpu_logical_not_noti.h
+++ b/tt_metal/hw/ckernels/blackhole/metal/llk_api/llk_sfpu/ckernel_sfpu_logical_not_noti.h
@@ -7,19 +7,20 @@
 #include "ckernel.h"
 #include "ckernel_defs.h"
 #include "sfpi.h"
+using namespace sfpi;
 
 namespace ckernel {
 namespace sfpu {
 
-template <typename V, typename T>
+template <bool APPROXIMATION_MODE>
 inline void calculate_logical_not_unary() {
 #pragma GCC unroll 0
     for (int d = 0; d < 8; d++) {
-        V v = sfpi::dst_reg[0];
-        v_if(v == 0) { sfpi::dst_reg[0] = T(1); }
-        v_else { sfpi::dst_reg[0] = T(0); }
+        vFloat v = dst_reg[0];
+        v_if(v == 0) { dst_reg[0] = 1.0f; }
+        v_else { dst_reg[0] = 0.0f; }
         v_endif;
-        sfpi::dst_reg++;
+        dst_reg++;
     }
 }
 
diff --git a/tt_metal/hw/ckernels/blackhole/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_logical_not_noti.h b/tt_metal/hw/ckernels/blackhole/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_logical_not_noti.h
index 7e95014ae9..9b630efdfe 100644
--- a/tt_metal/hw/ckernels/blackhole/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_logical_not_noti.h
+++ b/tt_metal/hw/ckernels/blackhole/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_logical_not_noti.h
@@ -20,13 +20,7 @@ inline void llk_math_eltwise_unary_sfpu_logical_not_unary_init() {
 template <bool APPROXIMATE>
 inline void llk_math_eltwise_unary_sfpu_logical_not_unary_op(uint dst_index) {
     llk_math_eltwise_unary_sfpu_params<APPROXIMATE>(
-        ckernel::sfpu::calculate_logical_not_unary<sfpi::vFloat, float>, dst_index, static_cast<int>(VectorMode::RC));
-}
-
-template <bool APPROXIMATE>
-inline void llk_math_eltwise_unary_sfpu_logical_not_unary_op_int32(uint dst_index) {
-    llk_math_eltwise_unary_sfpu_params<APPROXIMATE>(
-        ckernel::sfpu::calculate_logical_not_unary<sfpi::vInt, int16_t>, dst_index, static_cast<int>(VectorMode::RC));
+        ckernel::sfpu::calculate_logical_not_unary<APPROXIMATE>, dst_index, (int)VectorMode::RC);
 }
 
 }  // namespace ckernel
diff --git a/tt_metal/hw/ckernels/wormhole_b0/metal/llk_api/llk_sfpu/ckernel_sfpu_logical_not_noti.h b/tt_metal/hw/ckernels/wormhole_b0/metal/llk_api/llk_sfpu/ckernel_sfpu_logical_not_noti.h
index aa910a8715..a99dae1264 100644
--- a/tt_metal/hw/ckernels/wormhole_b0/metal/llk_api/llk_sfpu/ckernel_sfpu_logical_not_noti.h
+++ b/tt_metal/hw/ckernels/wormhole_b0/metal/llk_api/llk_sfpu/ckernel_sfpu_logical_not_noti.h
@@ -9,19 +9,20 @@
 #include "noc_nonblocking_api.h"
 
 #include "sfpi.h"
+using namespace sfpi;
 
 namespace ckernel {
 namespace sfpu {
 
-template <typename V, typename T>
+template <bool APPROXIMATION_MODE>
 inline void calculate_logical_not_unary() {
 #pragma GCC unroll 0
     for (int d = 0; d < 8; d++) {
-        V v = sfpi::dst_reg[0];
-        v_if(v == 0) { sfpi::dst_reg[0] = T(1); }
-        v_else { sfpi::dst_reg[0] = T(0); }
+        vFloat v = dst_reg[0];
+        v_if(v == 0) { dst_reg[0] = 1.0f; }
+        v_else { dst_reg[0] = 0.0f; }
         v_endif;
-        sfpi::dst_reg++;
+        dst_reg++;
     }
 }
 
diff --git a/tt_metal/hw/ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_logical_not_noti.h b/tt_metal/hw/ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_logical_not_noti.h
index 490cae471a..9b630efdfe 100644
--- a/tt_metal/hw/ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_logical_not_noti.h
+++ b/tt_metal/hw/ckernels/wormhole_b0/metal/llk_api/llk_sfpu/llk_math_eltwise_unary_sfpu_logical_not_noti.h
@@ -16,16 +16,11 @@ template <bool APPROXIMATE>
 inline void llk_math_eltwise_unary_sfpu_logical_not_unary_init() {
     llk_math_eltwise_unary_sfpu_init<SfpuType::logical_not_unary, APPROXIMATE>();
 }
-template <bool APPROXIMATE>
-inline void llk_math_eltwise_unary_sfpu_logical_not_unary_op(uint dst_index) {
-    llk_math_eltwise_unary_sfpu_params<APPROXIMATE>(
-        ckernel::sfpu::calculate_logical_not_unary<sfpi::vFloat, float>, dst_index, static_cast<int>(VectorMode::RC));
-}
 
 template <bool APPROXIMATE>
-inline void llk_math_eltwise_unary_sfpu_logical_not_unary_op_int32(uint dst_index) {
+inline void llk_math_eltwise_unary_sfpu_logical_not_unary_op(uint dst_index) {
     llk_math_eltwise_unary_sfpu_params<APPROXIMATE>(
-        ckernel::sfpu::calculate_logical_not_unary<sfpi::vInt, int16_t>, dst_index, static_cast<int>(VectorMode::RC));
+        ckernel::sfpu::calculate_logical_not_unary<APPROXIMATE>, dst_index, (int)VectorMode::RC);
 }
 
 }  // namespace ckernel
diff --git a/tt_metal/hw/firmware/src/brisc.cc b/tt_metal/hw/firmware/src/brisc.cc
index db8d6357e5..870adc2d9a 100644
--- a/tt_metal/hw/firmware/src/brisc.cc
+++ b/tt_metal/hw/firmware/src/brisc.cc
@@ -249,7 +249,7 @@ void device_setup() {
 
 inline void deassert_ncrisc_trisc() {
     // Below sets ncrisc to go so we can wait until it is cleared on first iteration
-    mailboxes->subordinate_sync.all = RUN_SYNC_MSG_ALL_INIT;
+    mailboxes->subordinate_sync.all = RUN_SYNC_MSG_ALL_SUBORDINATES_DONE;
 
     // Bring ncrisc/triscs out of reset
     deassert_all_reset();
@@ -341,10 +341,9 @@ int main() {
 
     // Set ncrisc's resume address to 0 so we know when ncrisc has overwritten it
     mailboxes->ncrisc_halt.resume_addr = 0;
+    mailboxes->subordinate_sync.dm1 = RUN_SYNC_MSG_GO;
     deassert_ncrisc_trisc();
 
-    // Wait for all cores to be finished initializing before reporting initialization done.
-    wait_ncrisc_trisc();
     mailboxes->go_message.signal = RUN_MSG_DONE;
 
     // Initialize the NoCs to a safe state
diff --git a/tt_metal/hw/firmware/src/idle_erisc.cc b/tt_metal/hw/firmware/src/idle_erisc.cc
index 38569c00e5..da4daa2da0 100644
--- a/tt_metal/hw/firmware/src/idle_erisc.cc
+++ b/tt_metal/hw/firmware/src/idle_erisc.cc
@@ -118,9 +118,6 @@ int main() {
     risc_init();
 
     mailboxes->subordinate_sync.all = RUN_SYNC_MSG_ALL_SUBORDINATES_DONE;
-#ifdef ARCH_BLACKHOLE
-    mailboxes->subordinate_sync.dm1 = RUN_SYNC_MSG_INIT;
-#endif
     set_deassert_addresses();
     //device_setup();
 
@@ -130,8 +127,6 @@ int main() {
     }
 
     deassert_all_reset(); // Bring all riscs on eth cores out of reset
-    // Wait for all subordinate ERISCs to be ready before reporting the core is done initializing.
-    wait_subordinate_eriscs(heartbeat);
     mailboxes->go_message.signal = RUN_MSG_DONE;
     mailboxes->launch_msg_rd_ptr = 0; // Initialize the rdptr to 0
     // Cleanup profiler buffer incase we never get the go message
diff --git a/tt_metal/hw/firmware/src/ncrisc.cc b/tt_metal/hw/firmware/src/ncrisc.cc
index 2c006a3693..280ef269af 100644
--- a/tt_metal/hw/firmware/src/ncrisc.cc
+++ b/tt_metal/hw/firmware/src/ncrisc.cc
@@ -109,8 +109,6 @@ int main(int argc, char *argv[]) {
     my_logical_x_ = mailboxes->core_info.absolute_logical_x;
     my_logical_y_ = mailboxes->core_info.absolute_logical_y;
 
-    signal_ncrisc_completion();
-
     // Cleanup profiler buffer incase we never get the go message
     while (1) {
         WAYPOINT("W");
diff --git a/tt_metal/hw/firmware/src/subordinate_idle_erisc.cc b/tt_metal/hw/firmware/src/subordinate_idle_erisc.cc
index 97103d36e3..afac65fdd1 100644
--- a/tt_metal/hw/firmware/src/subordinate_idle_erisc.cc
+++ b/tt_metal/hw/firmware/src/subordinate_idle_erisc.cc
@@ -73,7 +73,6 @@ int main(int argc, char *argv[]) {
     my_logical_x_ = mailboxes->core_info.absolute_logical_x;
     my_logical_y_ = mailboxes->core_info.absolute_logical_y;
     risc_init();
-    signal_subordinate_idle_erisc_completion();
 
     // Cleanup profiler buffer incase we never get the go message
     while (1) {
diff --git a/tt_metal/hw/firmware/src/trisc.cc b/tt_metal/hw/firmware/src/trisc.cc
index 2b9f1ed033..ed01e93492 100644
--- a/tt_metal/hw/firmware/src/trisc.cc
+++ b/tt_metal/hw/firmware/src/trisc.cc
@@ -106,7 +106,6 @@ int main(int argc, char *argv[]) {
 
     my_logical_x_ = mailboxes->core_info.absolute_logical_x;
     my_logical_y_ = mailboxes->core_info.absolute_logical_y;
-    *trisc_run = RUN_SYNC_MSG_DONE;
 
     // Cleanup profiler buffer incase we never get the go message
     while (1) {
diff --git a/tt_metal/hw/inc/blackhole/eth_chan_noc_mapping.h b/tt_metal/hw/inc/blackhole/eth_chan_noc_mapping.h
deleted file mode 100644
index 2f83cf0116..0000000000
--- a/tt_metal/hw/inc/blackhole/eth_chan_noc_mapping.h
+++ /dev/null
@@ -1,38 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#pragma once
-
-uint16_t eth_chan_to_noc_xy[2][12] __attribute__((used)) = {
-    {
-        // noc=0
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 20) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 21) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 22) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 23) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 24) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 25) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 26) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 27) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 28) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 29) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 30) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 31) << NOC_COORD_REG_OFFSET),
-    },
-    {
-        // noc=1
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 20) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 21) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 22) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 23) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 24) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 25) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 26) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 27) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 28) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 29) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 30) << NOC_COORD_REG_OFFSET),
-        (((25 << NOC_ADDR_NODE_ID_BITS) | 31) << NOC_COORD_REG_OFFSET),
-    },
-};
diff --git a/tt_metal/hw/inc/dataflow_api.h b/tt_metal/hw/inc/dataflow_api.h
index 92aa55833c..74c9f73153 100644
--- a/tt_metal/hw/inc/dataflow_api.h
+++ b/tt_metal/hw/inc/dataflow_api.h
@@ -837,7 +837,6 @@ inline void noc_async_write_multicast(
         noc_async_write_multicast_one_packet(src_local_l1_addr, dst_noc_addr_multicast, size, num_dests, linked);
     } else {
         WAYPOINT("NMWW");
-        NOC_TRACE_QUICK_PUSH_IF_LINKED(write_cmd_buf, linked);
         DEBUG_SANITIZE_NOC_MULTI_WRITE_TRANSACTION(noc, dst_noc_addr_multicast, src_local_l1_addr, size);
         ncrisc_noc_fast_write_any_len<noc_mode>(
             noc,
@@ -1557,7 +1556,7 @@ FORCE_INLINE void noc_inline_dw_write(
             be,  // byte-enable
             vc,
             false,  // mcast
-            posted  // posted
+            false   // posted
         );
         WAYPOINT("NWID");
         return;
@@ -1580,7 +1579,7 @@ FORCE_INLINE void noc_inline_dw_write(
         false,  // linked
         1,      // num_dests
         true,   // multicast_path_reserve
-        posted  // posted
+        false   // posted
     );
     noc_async_writes_flushed(noc);
 #else
@@ -1625,11 +1624,7 @@ FORCE_INLINE void noc_inline_dw_write_set_state(
     }
     NOC_CMD_BUF_WRITE_REG(noc, cmd_buf, NOC_CTRL, noc_cmd_field);
     NOC_CMD_BUF_WRITE_REG(noc, cmd_buf, NOC_TARG_ADDR_LO, addr & 0xFFFFFFFF);
-#ifdef ARCH_BLACKHOLE
-    NOC_CMD_BUF_WRITE_REG(noc, cmd_buf, NOC_TARG_ADDR_MID, (uint32_t)(addr >> 32) & 0x1000000F);
-#endif
-    NOC_CMD_BUF_WRITE_REG(
-        noc, cmd_buf, NOC_TARG_ADDR_COORDINATE, (uint32_t)(addr >> NOC_ADDR_COORD_SHIFT) & NOC_COORDINATE_MASK);
+    NOC_CMD_BUF_WRITE_REG(noc, cmd_buf, NOC_TARG_ADDR_COORDINATE, (uint32_t)(addr >> NOC_ADDR_COORD_SHIFT));
     NOC_CMD_BUF_WRITE_REG(noc, cmd_buf, NOC_AT_LEN_BE, be32);
     WAYPOINT("NWID");
 }
@@ -1725,6 +1720,9 @@ inline void RISC_POST_HEARTBEAT(uint32_t& heartbeat) {
     ptr[0] = 0xAABB0000 | (heartbeat & 0xFFFF);
 }
 
+FORCE_INLINE
+uint32_t min(uint32_t a, uint32_t b) { return (a < b) ? a : b; }
+
 template <bool use_vc>
 FORCE_INLINE uint32_t noc_async_read_tile_dram_sharded_set_state(
     uint32_t bank_base_address,
diff --git a/tt_metal/hw/inc/dev_msgs.h b/tt_metal/hw/inc/dev_msgs.h
index 4bcbf81f30..2410f6a381 100644
--- a/tt_metal/hw/inc/dev_msgs.h
+++ b/tt_metal/hw/inc/dev_msgs.h
@@ -64,7 +64,6 @@ constexpr uint32_t RUN_SYNC_MSG_WAITING_FOR_RESET = 0x2;
 constexpr uint32_t RUN_SYNC_MSG_INIT_SYNC_REGISTERS = 0x3;
 constexpr uint32_t RUN_SYNC_MSG_DONE = 0;
 constexpr uint32_t RUN_SYNC_MSG_ALL_GO = 0x80808080;
-constexpr uint32_t RUN_SYNC_MSG_ALL_INIT = 0x40404040;
 constexpr uint32_t RUN_SYNC_MSG_ALL_SUBORDINATES_DONE = 0;
 
 struct ncrisc_halt_msg_t {
diff --git a/tt_metal/impl/CMakeLists.txt b/tt_metal/impl/CMakeLists.txt
index 3f48f6d521..b31e3c1240 100644
--- a/tt_metal/impl/CMakeLists.txt
+++ b/tt_metal/impl/CMakeLists.txt
@@ -49,7 +49,6 @@ set(IMPL_SRC
     ${CMAKE_CURRENT_SOURCE_DIR}/dispatch/launch_message_ring_buffer_state.cpp
     ${CMAKE_CURRENT_SOURCE_DIR}/dispatch/worker_config_buffer.cpp
     ${CMAKE_CURRENT_SOURCE_DIR}/dispatch/data_collection.cpp
-    ${CMAKE_CURRENT_SOURCE_DIR}/dispatch/ringbuffer_cache.cpp
     ${CMAKE_CURRENT_SOURCE_DIR}/dispatch/topology.cpp
     ${CMAKE_CURRENT_SOURCE_DIR}/dispatch/kernel_config/fd_kernel.cpp
     ${CMAKE_CURRENT_SOURCE_DIR}/dispatch/kernel_config/prefetch.cpp
diff --git a/tt_metal/impl/allocator/algorithms/free_list.hpp b/tt_metal/impl/allocator/algorithms/free_list.hpp
index 071535acaf..0857217da1 100644
--- a/tt_metal/impl/allocator/algorithms/free_list.hpp
+++ b/tt_metal/impl/allocator/algorithms/free_list.hpp
@@ -27,29 +27,28 @@ public:
         DeviceAddr min_allocation_size,
         DeviceAddr alignment,
         SearchPolicy search_policy);
-    ~FreeList() override;
-    void init() override;
+    ~FreeList();
+    void init();
 
-    std::vector<std::pair<DeviceAddr, DeviceAddr>> available_addresses(DeviceAddr size_bytes) const override;
+    std::vector<std::pair<DeviceAddr, DeviceAddr>> available_addresses(DeviceAddr size_bytes) const;
 
-    std::optional<DeviceAddr> allocate(
-        DeviceAddr size_bytes, bool bottom_up = true, DeviceAddr address_limit = 0) override;
+    std::optional<DeviceAddr> allocate(DeviceAddr size_bytes, bool bottom_up = true, DeviceAddr address_limit = 0);
 
-    std::optional<DeviceAddr> allocate_at_address(DeviceAddr absolute_start_address, DeviceAddr size_bytes) override;
+    std::optional<DeviceAddr> allocate_at_address(DeviceAddr absolute_start_address, DeviceAddr size_bytes);
 
-    void deallocate(DeviceAddr absolute_address) override;
+    void deallocate(DeviceAddr absolute_address);
 
-    void clear() override;
+    void clear();
 
-    Statistics get_statistics() const override;
+    Statistics get_statistics() const;
 
-    void dump_blocks(std::ostream& out) const override;
+    void dump_blocks(std::ostream& out) const;
 
-    MemoryBlockTable get_memory_block_table() const override;
+    MemoryBlockTable get_memory_block_table() const;
 
-    void shrink_size(DeviceAddr shrink_size, bool bottom_up = true) override;
+    void shrink_size(DeviceAddr shrink_size, bool bottom_up = true);
 
-    void reset_size() override;
+    void reset_size();
 
 private:
     struct Block {
diff --git a/tt_metal/impl/context/metal_context.cpp b/tt_metal/impl/context/metal_context.cpp
index b8c1369a12..6a68aeb10b 100644
--- a/tt_metal/impl/context/metal_context.cpp
+++ b/tt_metal/impl/context/metal_context.cpp
@@ -256,7 +256,7 @@ void MetalContext::set_custom_control_plane_mesh_graph(
 
     global_control_plane_ = std::make_unique<tt::tt_fabric::GlobalControlPlane>(
         mesh_graph_desc_file, logical_mesh_chip_id_to_physical_chip_id_mapping);
-    this->set_fabric_config(fabric_config_);
+    this->initialize_fabric_config(fabric_config_);
 }
 
 void MetalContext::set_default_control_plane_mesh_graph() {
@@ -264,72 +264,23 @@ void MetalContext::set_default_control_plane_mesh_graph() {
         !DevicePool::is_initialized() || DevicePool::instance().get_all_active_devices().size() == 0,
         "Modifying control plane requires no devices to be active");
     global_control_plane_.reset();
-    this->set_fabric_config(fabric_config_);
+    this->initialize_fabric_config(fabric_config_);
 }
 
-void MetalContext::teardown_fabric_config() {
-    this->cluster_->configure_ethernet_cores_for_fabric_routers(tt_metal::FabricConfig::DISABLED);
-    this->get_control_plane().clear_fabric_context();
-}
-
-void MetalContext::set_fabric_config(
-    const tt_metal::FabricConfig fabric_config, std::optional<uint8_t> num_routing_planes) {
-    if (this->fabric_config_ == tt_metal::FabricConfig::DISABLED || fabric_config == tt_metal::FabricConfig::DISABLED) {
-        this->fabric_config_ = fabric_config;
-    } else {
-        TT_FATAL(
-            this->fabric_config_ == fabric_config,
-            "Tried to override previous value of fabric config: {}, with: {}",
-            this->fabric_config_,
-            fabric_config);
-    }
+void MetalContext::initialize_fabric_config(tt_metal::FabricConfig fabric_config) {
+    fabric_config_ = fabric_config;
+    cluster_->configure_ethernet_cores_for_fabric_routers(fabric_config);
 
-    if (this->fabric_config_ == tt_metal::FabricConfig::DISABLED) {
-        if (num_routing_planes.has_value()) {
-            log_warning(
-                tt::LogMetal,
-                "Got num_routing_planes while disabling fabric, ignoring it and disabling all active routing planes");
+    // Initialize fabric context in control plane if fabric is enabled
+    if (fabric_config != tt_metal::FabricConfig::DISABLED) {
+        if (tt::tt_fabric::is_tt_fabric_config(fabric_config)) {
+            this->get_control_plane().initialize_fabric_context(fabric_config);
         }
-
-        this->teardown_fabric_config();
-        return;
-    }
-
-    if (num_routing_planes.has_value() && num_routing_planes.value() < this->num_fabric_active_routing_planes_) {
-        log_warning(
-            tt::LogMetal,
-            "Got num_routing_planes: {}, which is less than current value: {}, ignoring the override",
-            num_routing_planes.value(),
-            this->num_fabric_active_routing_planes_);
-        return;
-    }
-
-    // if num_routing_planes is not specified, use max available number of routing planes
-    // ideally the highest value should be the maximum number of eth cores in a direction across all chips
-    const auto new_val = std::max(
-        this->num_fabric_active_routing_planes_, num_routing_planes.value_or(std::numeric_limits<uint8_t>::max()));
-    if (new_val != this->num_fabric_active_routing_planes_ && this->num_fabric_active_routing_planes_ > 0) {
-        log_info(
-            tt::LogMetal,
-            "Overriding the number of routing planes to activate from {} to {}",
-            this->num_fabric_active_routing_planes_,
-            new_val);
-    }
-    this->num_fabric_active_routing_planes_ = new_val;
-}
-
-void MetalContext::initialize_fabric_config() {
-    if (this->fabric_config_ == tt_metal::FabricConfig::DISABLED) {
-        return;
+    } else {
+        this->get_control_plane().clear_fabric_context();
     }
 
-    this->cluster_->configure_ethernet_cores_for_fabric_routers(
-        this->fabric_config_, this->num_fabric_active_routing_planes_);
-    auto& control_plane = this->get_control_plane();
-    if (tt::tt_fabric::is_tt_fabric_config(this->fabric_config_)) {
-        control_plane.initialize_fabric_context(this->fabric_config_);
-    }
-    control_plane.configure_routing_tables_for_fabric_ethernet_channels();
+    this->get_control_plane().configure_routing_tables_for_fabric_ethernet_channels();
 }
 
 tt_metal::FabricConfig MetalContext::get_fabric_config() const {
diff --git a/tt_metal/impl/context/metal_context.hpp b/tt_metal/impl/context/metal_context.hpp
index a4cb458a3a..652f736b6e 100644
--- a/tt_metal/impl/context/metal_context.hpp
+++ b/tt_metal/impl/context/metal_context.hpp
@@ -67,9 +67,7 @@ public:
         const std::string& mesh_graph_desc_file,
         const std::map<tt_fabric::FabricNodeId, chip_id_t>& logical_mesh_chip_id_to_physical_chip_id_mapping);
     void set_default_control_plane_mesh_graph();
-    void set_fabric_config(
-        tt_metal::FabricConfig fabric_config, std::optional<uint8_t> num_routing_planes = std::nullopt);
-    void initialize_fabric_config();
+    void initialize_fabric_config(tt_metal::FabricConfig fabric_config);
     tt_metal::FabricConfig get_fabric_config() const;
 
 private:
@@ -82,7 +80,6 @@ private:
     void clear_dram_state(chip_id_t device_id);
     void clear_launch_messages_on_eth_cores(chip_id_t device_id);
     void initialize_control_plane();
-    void teardown_fabric_config();
 
     bool initialized_ = false;
     bool teardown_registered_ = false;
@@ -104,7 +101,6 @@ private:
     std::array<std::unique_ptr<DispatchMemMap>, magic_enum::enum_count<CoreType>()> dispatch_mem_map_;
     std::unique_ptr<tt::tt_fabric::GlobalControlPlane> global_control_plane_;
     tt_metal::FabricConfig fabric_config_ = tt_metal::FabricConfig::DISABLED;
-    uint8_t num_fabric_active_routing_planes_ = 0;
 };
 
 }  // namespace tt::tt_metal
diff --git a/tt_metal/impl/debug/inspector_impl.cpp b/tt_metal/impl/debug/inspector_impl.cpp
index b4e0cb9f2f..0e4e079215 100644
--- a/tt_metal/impl/debug/inspector_impl.cpp
+++ b/tt_metal/impl/debug/inspector_impl.cpp
@@ -76,7 +76,14 @@ void Logger::log_program_compile_started(const ProgramData& program_data) noexce
 }
 
 void Logger::log_program_compile_already_exists(const ProgramData& program_data) noexcept {
-    // Long running programs call this log entry too many times, so we don't want to log it.
+    try {
+        programs_ostream << "- program_compile_already_exists:\n";
+        programs_ostream << "    id: " << program_data.program_id << "\n";
+        programs_ostream << "    timestamp_ns: " << convert_timestamp(program_data.compile_started_timestamp) << "\n";
+        programs_ostream.flush();
+    } catch (const std::exception& e) {
+        TT_INSPECTOR_LOG("Failed to log program compile already exists: {}", e.what());
+    }
 }
 
 void Logger::log_program_kernel_compile_finished(const ProgramData& program_data, const KernelData& kernel_data) noexcept {
diff --git a/tt_metal/impl/device/device_pool.cpp b/tt_metal/impl/device/device_pool.cpp
index d70db1e292..c1279577f1 100644
--- a/tt_metal/impl/device/device_pool.cpp
+++ b/tt_metal/impl/device/device_pool.cpp
@@ -11,7 +11,6 @@
 #include <unistd.h>  // Warning Linux Only, needed for _SC_NPROCESSORS_ONLN
 #include <algorithm>
 #include <cstdlib>
-#include <future>
 #include <set>
 #include <utility>
 
@@ -39,7 +38,6 @@
 #include "tt_metal/impl/debug/watcher_server.hpp"
 #include "tt_metal/impl/dispatch/topology.hpp"
 #include "tt_metal/impl/dispatch/system_memory_manager.hpp"
-#include "tt_metal/common/executor.hpp"
 #include <umd/device/tt_core_coordinates.h>
 
 using namespace tt::tt_metal;
@@ -274,11 +272,6 @@ void DevicePool::initialize(
             "consider using CreateDevices API.");
     }
 
-    // Need to reserve eth cores for fabric before we initialize individual devices to maintain consistent state
-    // while initializing default sub device state.
-    // This call will be a no-op if fabric is disabled.
-    tt::tt_metal::MetalContext::instance().initialize_fabric_config();
-
     _inst->skip_remote_devices = skip;
     _inst->use_max_eth_core_count_on_all_devices_ = use_max_eth_core_count_on_all_devices;
     _inst->add_devices_to_pool(device_ids);
@@ -318,18 +311,6 @@ void DevicePool::initialize_host(IDevice* dev) const {
     watcher_attach(dev->id());
 }
 
-void DevicePool::init_fabric(const std::vector<tt_metal::IDevice*>& active_devices) const {
-    std::vector<std::shared_future<void>> events;
-    for (uint32_t i = 0; i < active_devices.size(); i++) {
-        auto& dev = active_devices[i];
-        events.emplace_back(detail::async([dev]() { dev->init_fabric(); }));
-    }
-    for (const auto& event : events) {
-        // Wait for all fabric programs to be initialized
-        event.get();
-    }
-}
-
 void DevicePool::initialize_active_devices() const {
     const auto& active_devices = this->get_all_active_devices();
 
@@ -345,7 +326,9 @@ void DevicePool::initialize_active_devices() const {
         }
 
         // Initialize fabric on mmio device
-        init_fabric(active_devices);
+        for (const auto& dev : active_devices) {
+            dev->init_fabric();
+        }
         log_info(tt::LogMetal, "Fabric Initialized with config {}", fabric_config);
     }
 
diff --git a/tt_metal/impl/dispatch/device_command.cpp b/tt_metal/impl/dispatch/device_command.cpp
index be1da0f3ff..8f4b0a855d 100644
--- a/tt_metal/impl/dispatch/device_command.cpp
+++ b/tt_metal/impl/dispatch/device_command.cpp
@@ -232,70 +232,6 @@ void DeviceCommand<hugepage_write>::add_prefetch_relay_paged_packed(
     this->memcpy((char*)relay_paged_cmd_dst + sizeof(CQPrefetchCmd), &sub_cmds[offset_idx], sub_cmds_sizeB);
 }
 
-template <bool hugepage_write>
-void DeviceCommand<hugepage_write>::add_prefetch_paged_to_ringbuffer(
-    const CQPrefetchPagedToRingbufferCmd& paged_to_ringbuffer_info) {
-    uint32_t increment_sizeB = tt::align(sizeof(CQPrefetchCmd), this->pcie_alignment);
-    auto initialize_paged_to_ringbuffer_cmd = [&](CQPrefetchCmd* paged_to_ringbuffer_cmd) {
-        paged_to_ringbuffer_cmd->base.cmd_id = CQ_PREFETCH_CMD_PAGED_TO_RINGBUFFER;
-        paged_to_ringbuffer_cmd->paged_to_ringbuffer = paged_to_ringbuffer_info;
-    };
-    CQPrefetchCmd* paged_to_ringbuffer_cmd_dst = this->reserve_space<CQPrefetchCmd*>(increment_sizeB);
-
-    if constexpr (hugepage_write) {
-        alignas(MEMCPY_ALIGNMENT) CQPrefetchCmd paged_to_ringbuffer_cmd;
-        initialize_paged_to_ringbuffer_cmd(&paged_to_ringbuffer_cmd);
-        this->memcpy(paged_to_ringbuffer_cmd_dst, &paged_to_ringbuffer_cmd, sizeof(CQPrefetchCmd));
-    } else {
-        initialize_paged_to_ringbuffer_cmd(paged_to_ringbuffer_cmd_dst);
-    }
-}
-
-template <bool hugepage_write>
-void DeviceCommand<hugepage_write>::add_prefetch_set_ringbuffer_offset(uint32_t offset, bool update_wp) {
-    uint32_t increment_sizeB = tt::align(sizeof(CQPrefetchCmd), this->pcie_alignment);
-    auto initialize_set_ringbuffer_offset_cmd = [&](CQPrefetchCmd* set_ringbuffer_offset_cmd) {
-        set_ringbuffer_offset_cmd->base.cmd_id = CQ_PREFETCH_CMD_SET_RINGBUFFER_OFFSET;
-        set_ringbuffer_offset_cmd->set_ringbuffer_offset.offset = offset;
-        set_ringbuffer_offset_cmd->set_ringbuffer_offset.update_wp = update_wp;
-    };
-    CQPrefetchCmd* set_ringbuffer_offset_cmd_dst = this->reserve_space<CQPrefetchCmd*>(increment_sizeB);
-
-    if constexpr (hugepage_write) {
-        alignas(MEMCPY_ALIGNMENT) CQPrefetchCmd set_ringbuffer_offset_cmd;
-        initialize_set_ringbuffer_offset_cmd(&set_ringbuffer_offset_cmd);
-        this->memcpy(set_ringbuffer_offset_cmd_dst, &set_ringbuffer_offset_cmd, sizeof(CQPrefetchCmd));
-    } else {
-        initialize_set_ringbuffer_offset_cmd(set_ringbuffer_offset_cmd_dst);
-    }
-}
-
-template <bool hugepage_write>
-void DeviceCommand<hugepage_write>::add_prefetch_relay_ringbuffer(
-    uint32_t num_sub_cmds, const std::vector<CQPrefetchRelayRingbufferSubCmd>& sub_cmds, uint32_t offset_idx) {
-    static_assert(sizeof(CQPrefetchRelayRingbufferSubCmd) % sizeof(uint32_t) == 0);
-
-    uint32_t sub_cmds_sizeB = num_sub_cmds * sizeof(CQPrefetchRelayRingbufferSubCmd);
-    uint32_t increment_sizeB = tt::align(sub_cmds_sizeB + sizeof(CQPrefetchCmd), this->pcie_alignment);
-    auto initialize_relay_ringbuffer_cmd = [&](CQPrefetchCmd* relay_ringbuffer_cmd) {
-        relay_ringbuffer_cmd->base.cmd_id = CQ_PREFETCH_CMD_RELAY_RINGBUFFER;
-        relay_ringbuffer_cmd->relay_ringbuffer.count = num_sub_cmds;
-        relay_ringbuffer_cmd->relay_ringbuffer.stride = increment_sizeB;
-    };
-
-    CQPrefetchCmd* relay_ringbuffer_cmd_dst = this->reserve_space<CQPrefetchCmd*>(increment_sizeB);
-
-    if constexpr (hugepage_write) {
-        alignas(MEMCPY_ALIGNMENT) CQPrefetchCmd relay_ringbuffer_cmd;
-        initialize_relay_ringbuffer_cmd(&relay_ringbuffer_cmd);
-        this->memcpy(relay_ringbuffer_cmd_dst, &relay_ringbuffer_cmd, sizeof(CQPrefetchCmd));
-    } else {
-        initialize_relay_ringbuffer_cmd(relay_ringbuffer_cmd_dst);
-    }
-
-    this->memcpy((char*)relay_ringbuffer_cmd_dst + sizeof(CQPrefetchCmd), &sub_cmds[offset_idx], sub_cmds_sizeB);
-}
-
 template <bool hugepage_write>
 template <bool flush_prefetch, bool inline_data>
 void DeviceCommand<hugepage_write>::add_dispatch_write_linear(
diff --git a/tt_metal/impl/dispatch/device_command.hpp b/tt_metal/impl/dispatch/device_command.hpp
index be9524d118..0873ef74e9 100644
--- a/tt_metal/impl/dispatch/device_command.hpp
+++ b/tt_metal/impl/dispatch/device_command.hpp
@@ -73,13 +73,6 @@ public:
         uint16_t num_sub_cmds,
         uint32_t offset_idx = 0);
 
-    void add_prefetch_paged_to_ringbuffer(const CQPrefetchPagedToRingbufferCmd& paged_to_ringbuffer_info);
-
-    void add_prefetch_set_ringbuffer_offset(uint32_t offset, bool update_wp = false);
-
-    void add_prefetch_relay_ringbuffer(
-        uint32_t num_sub_cmds, const std::vector<CQPrefetchRelayRingbufferSubCmd>& sub_cmds, uint32_t offset_idx = 0);
-
     template <bool flush_prefetch = true, bool inline_data = false>
     void add_dispatch_write_linear(
         uint8_t num_mcast_dests,
diff --git a/tt_metal/impl/dispatch/device_command_calculator.hpp b/tt_metal/impl/dispatch/device_command_calculator.hpp
index 34eb9f3567..16a8441716 100644
--- a/tt_metal/impl/dispatch/device_command_calculator.hpp
+++ b/tt_metal/impl/dispatch/device_command_calculator.hpp
@@ -129,22 +129,6 @@ public:
         this->cmd_write_offsetB += increment_sizeB;
     }
 
-    void add_prefetch_relay_ringbuffer(uint16_t num_sub_cmds) {
-        static_assert(sizeof(CQPrefetchRelayRingbufferSubCmd) % sizeof(uint32_t) == 0);
-
-        uint32_t sub_cmds_sizeB = num_sub_cmds * sizeof(CQPrefetchRelayRingbufferSubCmd);
-        uint32_t increment_sizeB = tt::align(sub_cmds_sizeB + sizeof(CQPrefetchCmd), this->pcie_alignment);
-        this->cmd_write_offsetB += increment_sizeB;
-    }
-
-    void add_prefetch_set_ringbuffer_offset() {
-        this->cmd_write_offsetB += tt::align(sizeof(CQPrefetchCmd), this->pcie_alignment);
-    }
-
-    void add_prefetch_paged_to_ringbuffer() {
-        this->cmd_write_offsetB += tt::align(sizeof(CQPrefetchCmd), this->pcie_alignment);
-    }
-
     template <typename PackedSubCmd>
     void add_dispatch_write_packed(
         uint16_t num_sub_cmds,
diff --git a/tt_metal/impl/dispatch/dispatch_mem_map.cpp b/tt_metal/impl/dispatch/dispatch_mem_map.cpp
index 209816e405..14cf85ae03 100644
--- a/tt_metal/impl/dispatch/dispatch_mem_map.cpp
+++ b/tt_metal/impl/dispatch/dispatch_mem_map.cpp
@@ -205,9 +205,4 @@ std::pair<uint32_t, uint32_t> DispatchMemMap::get_device_l1_info(const CoreType&
     return {l1_base, l1_size};
 }
 
-uint32_t DispatchMemMap::get_prefetcher_l1_size() const {
-    auto [l1_base, l1_size] = this->get_device_l1_info(this->settings.core_type_);
-    return l1_size;
-}
-
 }  // namespace tt::tt_metal
diff --git a/tt_metal/impl/dispatch/dispatch_mem_map.hpp b/tt_metal/impl/dispatch/dispatch_mem_map.hpp
index fd22726b21..8b4646e4bf 100644
--- a/tt_metal/impl/dispatch/dispatch_mem_map.hpp
+++ b/tt_metal/impl/dispatch/dispatch_mem_map.hpp
@@ -81,8 +81,6 @@ public:
     // Offset to be passed in the go message.
     uint8_t get_dispatch_message_update_offset(uint32_t index) const;
 
-    uint32_t get_prefetcher_l1_size() const;
-
 private:
     // Reset the instance using the settings for the core_type and num_hw_cqs.
     void reset(const CoreType& core_type, const uint32_t num_hw_cqs);
diff --git a/tt_metal/impl/dispatch/hardware_command_queue.cpp b/tt_metal/impl/dispatch/hardware_command_queue.cpp
index 41643611de..e1fdf812e1 100644
--- a/tt_metal/impl/dispatch/hardware_command_queue.cpp
+++ b/tt_metal/impl/dispatch/hardware_command_queue.cpp
@@ -40,8 +40,6 @@
 #include "tt_metal/impl/trace/dispatch.hpp"
 #include <umd/device/tt_xy_pair.h>
 #include "data_collection.hpp"
-#include "ringbuffer_cache.hpp"
-#include "program/dispatch.hpp"
 
 namespace tt {
 namespace tt_metal {
@@ -84,17 +82,7 @@ HWCommandQueue::HWCommandQueue(
     uint32_t completion_queue_reader_core) :
     manager_(device->sysmem_manager()),
     completion_queue_thread_{},
-    completion_queue_reader_core_(completion_queue_reader_core),
-    prefetcher_dram_aligned_block_size_(MetalContext::instance().hal().get_alignment(HalMemType::DRAM)),
-    prefetcher_cache_sizeB_(
-        MetalContext::instance().dispatch_mem_map(this->get_dispatch_core_type()).ringbuffer_size()),
-    prefetcher_dram_aligned_num_blocks_(prefetcher_cache_sizeB_ / prefetcher_dram_aligned_block_size_),
-    prefetcher_cache_manager_size_(
-        1 << (std::bit_width(std::min(1024u, std::max(2u, prefetcher_dram_aligned_num_blocks_ >> 4))) - 1)),
-    prefetcher_cache_manager_(std::make_unique<RingbufferCacheManager>(
-        prefetcher_dram_aligned_block_size_, prefetcher_dram_aligned_num_blocks_, prefetcher_cache_manager_size_)),
-    dummy_prefetcher_cache_manager_(std::make_unique<RingbufferCacheManager>(
-        prefetcher_dram_aligned_block_size_, prefetcher_dram_aligned_num_blocks_, prefetcher_cache_manager_size_)) {
+    completion_queue_reader_core_(completion_queue_reader_core) {
     ZoneScopedN("CommandQueue_constructor");
     this->device_ = device;
     this->cq_shared_state_ = std::move(cq_shared_state);
@@ -249,9 +237,6 @@ void HWCommandQueue::enqueue_read_buffer(
     TT_FATAL(!this->manager_.get_bypass_mode(), "Enqueue Read Buffer cannot be used with tracing");
     Buffer& buffer_obj = get_buffer_object(buffer);
 
-    // Reading from device would clobber prefetcher cache, so reset it now
-    this->reset_prefetcher_cache_manager();
-
     // This is to make sure we block on the same sub_device_ids at the end
     // TODO: enqueue_read_from_core will call select_sub_device_ids every loop which will have minor overhead
     sub_device_ids = buffer_dispatch::select_sub_device_ids(this->device_, sub_device_ids);
@@ -433,14 +418,11 @@ void HWCommandQueue::enqueue_program(Program& program, bool blocking) {
     // Lower the program to device: Generate dispatch commands.
     // Values in these commands will get updated based on kernel config ring
     // buffer state at runtime.
-    auto program_sizeB = program.impl().kernel_bins_sizeB;
-    bool use_prefetcher_cache = program_sizeB and program_sizeB <= this->prefetcher_cache_sizeB_;
-    program.generate_dispatch_commands(device_, use_prefetcher_cache);
+    program.generate_dispatch_commands(device_);
     program.set_last_used_command_queue_for_testing(this);
 
     if (this->manager_.get_bypass_mode()) {
-        this->trace_nodes_.push_back(
-            program_dispatch::create_trace_node(program.impl(), device_, use_prefetcher_cache));
+        this->trace_nodes_.push_back(program_dispatch::create_trace_node(program.impl(), device_));
         return;
     }
 
@@ -473,26 +455,6 @@ void HWCommandQueue::enqueue_program(Program& program, bool blocking) {
 
     auto& worker_launch_message_buffer_state =
         this->cq_shared_state_->worker_launch_message_buffer_state[*sub_device_id];
-
-    // Dispatch metadata contains runtime information based on
-    // the kernel config ring buffer state
-    program_dispatch::ProgramDispatchMetadata dispatch_metadata;
-    if (use_prefetcher_cache) {
-        bool& is_cached = dispatch_metadata.prefetcher_cache_info.is_cached;
-        uint32_t& cache_offset = dispatch_metadata.prefetcher_cache_info.offset;
-        std::tie(is_cached, cache_offset) = this->query_prefetcher_cache(program.get_id(), program_sizeB);
-        TT_ASSERT(
-            cache_offset + program_sizeB <= this->prefetcher_cache_sizeB_,
-            "Prefetcher cache overflow: offset: {}, program size: {}, cache size: {}",
-            cache_offset,
-            program_sizeB,
-            this->prefetcher_cache_sizeB_);
-        dispatch_metadata.prefetcher_cache_info.mesh_max_program_kernels_sizeB = program_sizeB;
-    } else {
-        // prefetcher cache will be overwritten, reset for next program
-        this->reset_prefetcher_cache_manager();
-    }
-
     auto command = EnqueueProgramCommand(
         this->id_,
         this->device_,
@@ -505,8 +467,7 @@ void HWCommandQueue::enqueue_program(Program& program, bool blocking) {
         // The assembled program command will encode the location of the launch messages in the ring buffer
         worker_launch_message_buffer_state.get_mcast_wptr(),
         worker_launch_message_buffer_state.get_unicast_wptr(),
-        sub_device_id,
-        dispatch_metadata);
+        sub_device_id);
     // Update wptrs for tensix and eth launch message in the device class
     if (program.runs_on_noc_multicast_only_cores()) {
         worker_launch_message_buffer_state.inc_mcast_wptr(1);
@@ -613,10 +574,6 @@ void HWCommandQueue::enqueue_trace(const uint32_t trace_id, bool blocking) {
         this->expected_num_workers_completed_,
         virtual_enqueue_program_dispatch_core_);
 
-    // Reset the prefetcher cache manager, since trace capture modifies the state on host for subsequent non-trace
-    // programs
-    this->reset_prefetcher_cache_manager();
-
     trace_dispatch::update_worker_state_post_trace_execution(
         trace_inst->desc->descriptors,
         this->cq_shared_state_->worker_launch_message_buffer_state,
@@ -748,8 +705,6 @@ void HWCommandQueue::record_begin(const uint32_t tid, const std::shared_ptr<Trac
     this->tid_ = tid;
     this->trace_ctx_ = std::move(ctx);
     this->manager_.set_bypass_mode(true, true);  // start trace capture
-
-    swap(this->dummy_prefetcher_cache_manager_, this->prefetcher_cache_manager_);
 }
 
 // Allocate space for program binaries and other data in the worker config ring buffer.
@@ -820,29 +775,6 @@ void HWCommandQueue::record_end() {
         auto& cached_program_command_sequence = program.get_trace_cached_program_command_sequences().at(command_hash);
         auto& worker_launch_message_buffer_state =
             this->cq_shared_state_->worker_launch_message_buffer_state[*sub_device_id];
-
-        // Dispatch metadata contains runtime information based on
-        // the kernel config ring buffer state
-        auto& dispatch_metadata = node.dispatch_metadata;
-        auto use_prefetcher_cache = cached_program_command_sequence.prefetcher_cache_used;
-        auto program_sizeB = cached_program_command_sequence.kernel_bins_sizeB;
-        if (dispatch_metadata.send_binary) {
-            if (use_prefetcher_cache) {
-                bool& is_cached = dispatch_metadata.prefetcher_cache_info.is_cached;
-                uint32_t& cache_offset = dispatch_metadata.prefetcher_cache_info.offset;
-                std::tie(is_cached, cache_offset) = this->query_prefetcher_cache(program.get_id(), program_sizeB);
-                TT_ASSERT(
-                    cache_offset + program_sizeB <= this->prefetcher_cache_sizeB_,
-                    "Prefetcher cache overflow: offset: {}, program size: {}, cache size: {}",
-                    cache_offset,
-                    program_sizeB,
-                    this->prefetcher_cache_sizeB_);
-                dispatch_metadata.prefetcher_cache_info.mesh_max_program_kernels_sizeB = program_sizeB;
-            } else {
-                // prefetcher cache will be overwritten, reset for next program
-                this->reset_prefetcher_cache_manager();
-            }
-        }
         // Update the generated dispatch commands based on the state of the CQ and the ring buffer
         program_dispatch::update_traced_program_dispatch_commands(
             node,
@@ -906,11 +838,6 @@ void HWCommandQueue::record_end() {
         this->expected_num_workers_completed_reset_,
         this->config_buffer_mgr_reset_);
     this->manager_.set_bypass_mode(false, true);  // stop trace capture
-
-    // Trace has modified the prefetcher cache manager so reset it first and then swap to restore the state as before
-    // the recording
-    this->reset_prefetcher_cache_manager();
-    swap(this->dummy_prefetcher_cache_manager_, this->prefetcher_cache_manager_);
 }
 
 void HWCommandQueue::terminate() {
@@ -923,18 +850,4 @@ void HWCommandQueue::terminate() {
 
 WorkerConfigBufferMgr& HWCommandQueue::get_config_buffer_mgr(uint32_t index) { return config_buffer_mgr_[index]; }
 
-std::pair<bool, size_t> HWCommandQueue::query_prefetcher_cache(uint64_t pgm_id, uint32_t lengthB) {
-    auto result = prefetcher_cache_manager_->get_cache_offset(pgm_id, lengthB);
-    TT_FATAL(
-        result.has_value(),
-        "Prefetcher cache query failed. Cache size: {}, requested: {}",
-        this->prefetcher_cache_manager_->get_cache_sizeB(),
-        lengthB);
-    return std::make_pair(result.value().is_cached, result.value().offset * this->prefetcher_dram_aligned_block_size_);
-}
-
-void HWCommandQueue::reset_prefetcher_cache_manager() { prefetcher_cache_manager_->reset(); }
-
-int HWCommandQueue::get_prefetcher_cache_sizeB() const { return this->prefetcher_cache_manager_->get_cache_sizeB(); }
-
 }  // namespace tt::tt_metal
diff --git a/tt_metal/impl/dispatch/hardware_command_queue.hpp b/tt_metal/impl/dispatch/hardware_command_queue.hpp
index d175773d5c..abbd2b2a97 100644
--- a/tt_metal/impl/dispatch/hardware_command_queue.hpp
+++ b/tt_metal/impl/dispatch/hardware_command_queue.hpp
@@ -33,7 +33,6 @@
 #include "trace/trace_node.hpp"
 #include "tt_metal/impl/buffers/dispatch.hpp"
 #include "tt_metal/common/multi_producer_single_consumer_queue.hpp"
-#include "ringbuffer_cache.hpp"
 
 namespace tt {
 namespace tt_metal {
@@ -167,20 +166,6 @@ private:
     CoreCoord completion_queue_writer_core_;
     NOC noc_index_;
 
-    const uint32_t prefetcher_dram_aligned_block_size_;
-    const uint64_t prefetcher_cache_sizeB_;
-    const uint32_t prefetcher_dram_aligned_num_blocks_;
-    const uint32_t prefetcher_cache_manager_size_;
-    // The prefetcher cache manager is used to track the state of the prefetcher cache.
-    std::unique_ptr<RingbufferCacheManager> prefetcher_cache_manager_;
-
-    // The backup prefetcher cache manager is used to stash away the prefetcher cache state during trace recording.
-    // Trace recording will change the state of the host side cache manager, without actually enqueueing the
-    // corresponding commands, which would cause the bookkeeping to go out of sync from the prefetcher cache. Hence we
-    // will use the following variable to swap out the cache manager into a backup variable before starting trace
-    // recording. At the end of the recording, we will reset the cache manager, and swap it with the backup.
-    std::unique_ptr<RingbufferCacheManager> dummy_prefetcher_cache_manager_;
-
     void allocate_trace_programs();
     void read_completion_queue();
 
@@ -190,12 +175,6 @@ private:
 
     void increment_num_entries_in_completion_q();
     void set_exit_condition();
-
-    std::pair<bool, size_t> query_prefetcher_cache(uint64_t pgm_id, uint32_t lengthB);
-
-    void reset_prefetcher_cache_manager();
-
-    int get_prefetcher_cache_sizeB() const;
 };
 
 }  // namespace tt::tt_metal
diff --git a/tt_metal/impl/dispatch/host_runtime_commands.cpp b/tt_metal/impl/dispatch/host_runtime_commands.cpp
index 38843010a5..0dfa10bf4a 100644
--- a/tt_metal/impl/dispatch/host_runtime_commands.cpp
+++ b/tt_metal/impl/dispatch/host_runtime_commands.cpp
@@ -98,8 +98,7 @@ EnqueueProgramCommand::EnqueueProgramCommand(
     uint32_t expected_num_workers_completed,
     uint32_t multicast_cores_launch_message_wptr,
     uint32_t unicast_cores_launch_message_wptr,
-    SubDeviceId sub_device_id,
-    program_dispatch::ProgramDispatchMetadata& dispatch_md) :
+    SubDeviceId sub_device_id) :
     command_queue_id(command_queue_id),
     noc_index(noc_index),
     manager(manager),
@@ -109,14 +108,17 @@ EnqueueProgramCommand::EnqueueProgramCommand(
     dispatch_core(dispatch_core),
     multicast_cores_launch_message_wptr(multicast_cores_launch_message_wptr),
     unicast_cores_launch_message_wptr(unicast_cores_launch_message_wptr),
-    sub_device_id(sub_device_id),
-    dispatch_metadata(dispatch_md) {
+    sub_device_id(sub_device_id) {
     this->device = device;
     this->dispatch_core_type = MetalContext::instance().get_dispatch_core_manager().get_dispatch_core_type();
     this->packed_write_max_unicast_sub_cmds = get_packed_write_max_unicast_sub_cmds(this->device);
 }
 
 void EnqueueProgramCommand::process() {
+    // Dispatch metadata contains runtime information based on
+    // the kernel config ring buffer state
+    program_dispatch::ProgramDispatchMetadata dispatch_metadata;
+
     // Compute the total number of workers this program uses
     uint32_t num_workers = 0;
     if (program.runs_on_noc_multicast_only_cores()) {
diff --git a/tt_metal/impl/dispatch/host_runtime_commands.hpp b/tt_metal/impl/dispatch/host_runtime_commands.hpp
index 799af2df67..3a0b095ccb 100644
--- a/tt_metal/impl/dispatch/host_runtime_commands.hpp
+++ b/tt_metal/impl/dispatch/host_runtime_commands.hpp
@@ -26,7 +26,6 @@
 #include "trace/trace_buffer.hpp"
 #include "tt_metal/impl/program/program_command_sequence.hpp"
 #include "worker_config_buffer.hpp"
-#include "program/dispatch.hpp"
 
 enum class CoreType;
 namespace tt {
@@ -85,7 +84,6 @@ private:
     uint32_t unicast_cores_launch_message_wptr = 0;
     // TODO: There will be multiple ids once programs support spanning multiple sub_devices
     SubDeviceId sub_device_id = SubDeviceId{0};
-    program_dispatch::ProgramDispatchMetadata& dispatch_metadata;
 
 public:
     EnqueueProgramCommand(
@@ -99,12 +97,11 @@ public:
         uint32_t expected_num_workers_completed,
         uint32_t multicast_cores_launch_message_wptr,
         uint32_t unicast_cores_launch_message_wptr,
-        SubDeviceId sub_device_id,
-        program_dispatch::ProgramDispatchMetadata& dispatch_md);
+        SubDeviceId sub_device_id);
 
-    void process() override;
+    void process();
 
-    EnqueueCommandType type() override { return EnqueueCommandType::ENQUEUE_PROGRAM; }
+    EnqueueCommandType type() { return EnqueueCommandType::ENQUEUE_PROGRAM; }
 
     constexpr bool has_side_effects() { return true; }
 };
@@ -118,9 +115,9 @@ private:
 public:
     EnqueueTerminateCommand(uint32_t command_queue_id, IDevice* device, SystemMemoryManager& manager);
 
-    void process() override;
+    void process();
 
-    EnqueueCommandType type() override { return EnqueueCommandType::TERMINATE; }
+    EnqueueCommandType type() { return EnqueueCommandType::TERMINATE; }
 
     constexpr bool has_side_effects() { return false; }
 };
diff --git a/tt_metal/impl/dispatch/kernel_config/prefetch.cpp b/tt_metal/impl/dispatch/kernel_config/prefetch.cpp
index f922ce4deb..8a3bcc6333 100644
--- a/tt_metal/impl/dispatch/kernel_config/prefetch.cpp
+++ b/tt_metal/impl/dispatch/kernel_config/prefetch.cpp
@@ -36,7 +36,7 @@ void PrefetchKernel::GenerateStaticConfigs() {
         tt::tt_metal::MetalContext::instance().get_cluster().get_assigned_channel_for_device(device_->id());
     uint8_t cq_id_ = this->cq_id_;
     auto& my_dispatch_constants = MetalContext::instance().dispatch_mem_map(GetCoreType());
-    auto l1_size = my_dispatch_constants.get_prefetcher_l1_size();
+
     // May be zero if not using dispatch on fabric
     static_config_.fabric_header_rb_base =
         my_dispatch_constants.get_device_command_queue_addr(CommandQueueDeviceAddrType::FABRIC_HEADER_RB);
@@ -208,14 +208,6 @@ void PrefetchKernel::GenerateStaticConfigs() {
     } else {
         TT_FATAL(false, "PrefetchKernel must be one of (or both) H and D variants");
     }
-    auto scratch_db_base = static_config_.scratch_db_base.value_or(0);
-    auto ringbuffer_size = static_config_.ringbuffer_size.value_or(0);
-    TT_ASSERT(
-        scratch_db_base + ringbuffer_size <= l1_size,
-        "Prefetcher allocations exceed L1 size: scratch_db_base: 0x{:X}, ringbuffer_size: 0x{:X} B, L1 size: 0x{:X} B",
-        scratch_db_base,
-        ringbuffer_size,
-        l1_size);
 
     if ((static_config_.is_h_variant.value() ^ static_config_.is_d_variant.value()) &&
         tt::tt_metal::MetalContext::instance().rtoptions().get_fd_fabric()) {
diff --git a/tt_metal/impl/dispatch/kernels/cq_commands.hpp b/tt_metal/impl/dispatch/kernels/cq_commands.hpp
index 1b4e49b330..ecf83e23e7 100644
--- a/tt_metal/impl/dispatch/kernels/cq_commands.hpp
+++ b/tt_metal/impl/dispatch/kernels/cq_commands.hpp
@@ -142,18 +142,15 @@ constexpr uint32_t CQ_PREFETCH_PAGED_TO_RING_BUFFER_FLAG_RESET_TO_START = 1;
 // Will always flush writes before reading.
 struct CQPrefetchPagedToRingbufferCmd {
     uint8_t flags;
+    uint8_t pad1;
     uint8_t log2_page_size;
-    uint8_t start_page;
-    uint32_t wp_offset_update;  // set final increment ringbuffer write pointer
+    uint32_t start_page;
     uint32_t base_addr;  // Base address of the interleaved buffer to read from.
     uint32_t length;     // multiple of DRAM alignment
 } __attribute__((packed));
 
 struct CQPrefetchSetRingbufferOffsetCmd {
     uint32_t offset;
-    uint16_t pad1;
-    uint8_t pad2;
-    uint8_t update_wp;  // if set, the ringbuffer write pointer will be updated to the offset
 } __attribute__((packed));
 
 // Current implementation limit is based on size of the l1_cache which stores the sub_cmds
diff --git a/tt_metal/impl/dispatch/kernels/cq_prefetch.cpp b/tt_metal/impl/dispatch/kernels/cq_prefetch.cpp
index 6e367eef06..af709f0d1b 100644
--- a/tt_metal/impl/dispatch/kernels/cq_prefetch.cpp
+++ b/tt_metal/impl/dispatch/kernels/cq_prefetch.cpp
@@ -1204,18 +1204,13 @@ uint32_t process_paged_to_ringbuffer_cmd(uint32_t cmd_ptr, uint32_t& downstream_
     uint32_t page_size = 1 << log2_page_size;
     uint32_t length = cmd->paged_to_ringbuffer.length;
     uint8_t flags = cmd->paged_to_ringbuffer.flags;
-    uint32_t wp_update_offset = cmd->paged_to_ringbuffer.wp_offset_update;
-
-    ASSERT(length <= wp_update_offset);
 
     if (flags & CQ_PREFETCH_PAGED_TO_RING_BUFFER_FLAG_RESET_TO_START) {
         ringbuffer_wp = scratch_db_base;
     }
 
     ASSERT(length % DRAM_ALIGNMENT == 0);
-    ASSERT(wp_update_offset + ringbuffer_wp <= ringbuffer_end);
-
-    ringbuffer_offset = ringbuffer_wp - scratch_db_base;
+    ASSERT(length + ringbuffer_wp <= ringbuffer_end);
 
     const bool is_dram = true;
     InterleavedPow2AddrGen<is_dram> addr_gen{.bank_base_address = base_addr, .log_base_2_of_page_size = log2_page_size};
@@ -1235,9 +1230,9 @@ uint32_t process_paged_to_ringbuffer_cmd(uint32_t cmd_ptr, uint32_t& downstream_
         scratch_read_addr += length;
     }
 
-    ringbuffer_wp += wp_update_offset;
+    ringbuffer_wp = scratch_read_addr;
 
-    // The consumer will perform a read barrier.
+    // The consumer will perforam a read barrier.
 
     return CQ_PREFETCH_CMD_BARE_MIN_SIZE;
 }
@@ -1246,12 +1241,7 @@ uint32_t process_set_ringbuffer_offset(uint32_t cmd_ptr) {
     volatile CQPrefetchCmd tt_l1_ptr* cmd = (volatile CQPrefetchCmd tt_l1_ptr*)cmd_ptr;
     uint32_t offset = cmd->set_ringbuffer_offset.offset;
 
-    if (cmd->set_ringbuffer_offset.update_wp) {
-        ringbuffer_wp = scratch_db_base + offset;
-        ASSERT(ringbuffer_wp <= ringbuffer_end);
-    } else {
-        ringbuffer_offset = offset;
-    }
+    ringbuffer_offset = offset;
 
     return CQ_PREFETCH_CMD_BARE_MIN_SIZE;
 }
@@ -1450,7 +1440,7 @@ bool process_cmd(
             break;
 
         case CQ_PREFETCH_CMD_RELAY_RINGBUFFER:
-            // DPRINT << "relay ringbuffer" << ENDL();
+            // DPRINT << "relay paged packed" << ENDL();
             if (exec_buf) {
                 stride = process_exec_buf_relay_ringbuffer_cmd(cmd_ptr, downstream_data_ptr, l1_cache, exec_buf_state);
             } else {
diff --git a/tt_metal/impl/dispatch/ringbuffer_cache.cpp b/tt_metal/impl/dispatch/ringbuffer_cache.cpp
deleted file mode 100644
index 01517c000d..0000000000
--- a/tt_metal/impl/dispatch/ringbuffer_cache.cpp
+++ /dev/null
@@ -1,237 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent Inc.
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include "tt_metal/impl/dispatch/ringbuffer_cache.hpp"
-#include "assert.hpp"
-// #include "tt_metal/hw/inc/dataflow_api.h"
-
-namespace tt::tt_metal {
-
-RingbufferCacheManager::RingbufferCacheManager(
-    int cache_block_sizeB, int cache_size_blocks, int manager_entry_initial_size) :
-    cache_block_sizeB_(cache_block_sizeB),
-    cache_size_blocks_(cache_size_blocks),
-    cache_manager_initial_entry_size_{manager_entry_initial_size} {
-    TT_ASSERT(cache_size_blocks > 0, "Ringbuffer cache size must be greater than 0");
-    TT_ASSERT(cache_block_sizeB > 0, "Ringbuffer cache block size must be greater than 0");
-    TT_ASSERT(
-        cache_manager_initial_entry_size_ > 0,
-        "Ringbuffer cache manager initial entry size ({}) must be greater than 0 ({}, {})",
-        cache_manager_initial_entry_size_,
-        cache_block_sizeB,
-        cache_size_blocks_);
-    TT_ASSERT(
-        (cache_manager_initial_entry_size_ & (cache_manager_initial_entry_size_ - 1)) == 0,
-        "Ringbuffer cache manager initial entry size ({}) is not a power of 2 ({}, {})",
-        cache_manager_initial_entry_size_,
-        cache_block_sizeB,
-        cache_size_blocks_);
-}
-
-// this function encapsulates the common code for adding a new entry to the ringbuffer manager
-// also, this function should be called when the cache is empty
-void RingbufferCacheManager::add_manager_entry_no_evict(uint64_t pgm_id, uint32_t offset, uint32_t length) {
-    TT_ASSERT(
-        offset + length <= cache_size_blocks_, "RingbufferCacheManager new allocation: offset + length > cache size");
-
-    auto idx = manager_.next_idx;
-    manager_.entry[idx] = {offset, length, pgm_id};
-
-    TT_ASSERT(pgm_id <= UINT32_MAX, "RingbufferCacheManager new allocation: pgm_id > UINT32_MAX");
-    valid_[pgm_id] = idx;
-
-    manager_.next_block_offset = offset + length == cache_size_blocks_ ? 0 : offset + length;
-    manager_.update_next_idx();
-}
-
-// this function should be called to allocate a new cache entry if the cache is not empty
-void RingbufferCacheManager::add_manager_entry(uint64_t pgm_id, uint32_t offset, uint32_t length) {
-    auto idx = manager_.next_idx;
-    if (idx == manager_.oldest_idx) {
-        // if the next index is the same as the oldest index, then we need to invalidate the oldest entry
-        invalidate_manager_entry();
-    }
-    add_manager_entry_no_evict(pgm_id, offset, length);
-}
-
-// this function will invalidate the oldest entry in the ringbuffer manager
-// caution: oldest entry must be valid
-void RingbufferCacheManager::invalidate_manager_entry() {
-    auto& entry = manager_.entry[manager_.oldest_idx];
-    auto valid_idx = entry.valid_idx;
-    TT_ASSERT(valid_idx < valid_.size(), "RingbufferCacheManager invalidation: pgm_id out of range");
-    TT_ASSERT(
-        valid_[valid_idx] != RingbufferCacheManager::invalid_cache_entry_,
-        "RingbufferCacheManager invalidation: entry not valid (mgr: oldest_idx:{}, next_idx:{}, offset:{}, length:{}, "
-        "valid idx (pgm id):{}, valid[pgm id]:{})",
-        manager_.oldest_idx,
-        manager_.next_idx,
-        entry.offset,
-        entry.length,
-        valid_idx,
-        valid_[valid_idx]);
-    // invalidate the entry
-    valid_[valid_idx] = RingbufferCacheManager::invalid_cache_entry_;
-    manager_.update_oldest_idx();
-}
-
-// this function will invalidate all oldest entries until the cache wraps around
-// caution: should not be called if the cache is empty
-bool RingbufferCacheManager::invalidate_oldest_until_wraparound() {
-    if (manager_.oldest_idx == manager_.next_idx) {
-        invalidate_manager_entry();
-    }
-    while ((manager_.entry[manager_.oldest_idx].offset >= manager_.next_block_offset) and
-           (manager_.oldest_idx != manager_.next_idx)) {
-        invalidate_manager_entry();
-    }
-    // true ==> cache is empty and caller should call add_manager_entry_no_evict
-    return manager_.oldest_idx == manager_.next_idx;
-}
-// this function is used to invalidate the oldest entry until there is sufficient space
-// caution: should not be called if the cache is empty
-bool RingbufferCacheManager::invalidate_sufficient_blocks(int required_space, int offset) {
-    if (manager_.oldest_idx == manager_.next_idx) {
-        invalidate_manager_entry();
-    }
-    int oldest_block_offset = manager_.entry[manager_.oldest_idx].offset;
-    int available_space = oldest_block_offset - offset;
-    while (available_space < required_space and manager_.oldest_idx != manager_.next_idx) {
-        available_space += manager_.entry[manager_.oldest_idx].length;
-        invalidate_manager_entry();  // invalidate oldest entry
-    }
-
-    // true ==> cache is empty and caller should call add_manager_entry_no_evict
-    return manager_.oldest_idx == manager_.next_idx;
-}
-
-std::optional<typename RingbufferCacheManager::CacheOffset> RingbufferCacheManager::get_cache_offset(
-    uint64_t pgm_id, uint32_t lengthB) {
-    uint32_t cache_offset;
-    CacheOffset query_result;
-
-    const int required_space = (lengthB + cache_block_sizeB_ - 1) / cache_block_sizeB_;
-    if (required_space > cache_size_blocks_) [[unlikely]] {
-        return std::nullopt;  // cannot fit in cache
-    } else if (manager_.entry.size() == 0) [[unlikely]] {
-        // first entry, so we can just add it
-        valid_.resize(pgm_id + 1, RingbufferCacheManager::invalid_cache_entry_);
-        manager_.entry.resize(std::min(cache_manager_initial_entry_size_, cache_size_blocks_));
-        add_manager_entry_no_evict(pgm_id, 0, required_space);
-        query_result.is_cached = false;
-        query_result.offset = 0;
-        return query_result;
-    } else if (pgm_id >= valid_.size()) {
-        valid_.resize(
-            std::max((uint64_t)cache_manager_initial_entry_size_, pgm_id + 1),
-            RingbufferCacheManager::invalid_cache_entry_);
-    } else if (valid_[pgm_id] != invalid_cache_entry_) {
-        unsigned mgr_idx = valid_[pgm_id];
-        TT_ASSERT(
-            mgr_idx < manager_.entry.size(),
-            "RingbufferCacheManager invalid cache hit: manager index out of bounds. pgm_id:{}, valid idx:{}, entry "
-            "size:{}",
-            pgm_id,
-            mgr_idx,
-            manager_.entry.size());
-        auto mgr_entry = manager_.entry[mgr_idx];
-        TT_ASSERT(
-            mgr_entry.length * cache_block_sizeB_ >= lengthB,
-            "RingbufferCacheManager invalid cache hit: requested length:{} > entry length:{}, manager idx:{}, cache "
-            "offset:{}, valid_idx/pgm_id requested:{}/{}",
-            lengthB,
-            mgr_entry.length * cache_block_sizeB_,
-            mgr_idx,
-            mgr_entry.offset,
-            mgr_entry.valid_idx,
-            pgm_id);
-        query_result.is_cached = true;
-        query_result.offset = mgr_entry.offset;
-        return query_result;
-    }
-
-    query_result.is_cached = false;
-
-    // find space in RB for new entry, or if full, then invalidate oldest entry(ies)
-    int next_block_offset = manager_.next_block_offset;
-    int oldest_block_offset = manager_.entry[manager_.oldest_idx].offset;
-    int free_space_to_end = cache_size_blocks_ - next_block_offset;
-    bool cache_emptied = false;
-    if (next_block_offset > oldest_block_offset) {
-        if (free_space_to_end < required_space) [[unlikely]] {
-            cache_emptied = invalidate_sufficient_blocks(required_space);  // free up space from beginning
-            cache_offset = 0;
-        } else {
-            cache_offset = next_block_offset;  // cache has space
-        }
-    } else {
-        if (free_space_to_end >= required_space) {
-            cache_emptied = invalidate_sufficient_blocks(required_space, next_block_offset);
-            cache_offset = next_block_offset;
-        } else [[unlikely]] {
-            // cache is not full, but must wraparound for sufficient space
-            cache_emptied = invalidate_oldest_until_wraparound();
-            if (not cache_emptied) {
-                cache_emptied = invalidate_sufficient_blocks(required_space);  // free up space from beginning
-            }
-            cache_offset = 0;
-        }
-    }
-    // made room, now allocate the new entry
-    if (cache_emptied) {
-        add_manager_entry_no_evict(pgm_id, cache_offset, required_space);
-    } else {
-        add_manager_entry(pgm_id, cache_offset, required_space);
-    }
-
-    query_result.offset = cache_offset;
-    return query_result;
-}
-
-void RingbufferCacheManager::reset() {
-    this->manager_.clear();
-    std::vector<RingbufferCacheManagerEntry> temp_entry;
-    this->manager_.entry.swap(temp_entry);
-
-    std::vector<int32_t> temp_valid;
-    this->valid_.swap(temp_valid);
-}
-
-void swap(RingbufferCacheManager& a, RingbufferCacheManager& b) {
-    TT_ASSERT(
-        a.cache_block_sizeB_ == b.cache_block_sizeB_,
-        "Ringbuffer cache block size mismatch: {} != {}",
-        a.cache_block_sizeB_,
-        b.cache_block_sizeB_);
-    TT_ASSERT(
-        a.cache_size_blocks_ == b.cache_size_blocks_,
-        "Ringbuffer cache size mismatch: {} != {}",
-        a.cache_size_blocks_,
-        b.cache_size_blocks_);
-    TT_ASSERT(
-        a.cache_manager_initial_entry_size_ == b.cache_manager_initial_entry_size_,
-        "Ringbuffer cache manager initial entry size mismatch: {} != {}",
-        a.cache_manager_initial_entry_size_,
-        b.cache_manager_initial_entry_size_);
-    using std::swap;
-    a.valid_.swap(b.valid_);
-    a.manager_.swap(b.manager_);
-}
-
-void RingbufferCacheManager::InternalManager::clear() {
-    this->oldest_idx = 0;
-    this->next_idx = 0;
-    this->next_block_offset = 0;
-}
-
-RingbufferCacheManager::InternalManager& RingbufferCacheManager::InternalManager::swap(InternalManager& b) {
-    using std::swap;
-    swap(this->entry, b.entry);
-    swap(this->oldest_idx, b.oldest_idx);
-    swap(this->next_idx, b.next_idx);
-    swap(this->next_block_offset, b.next_block_offset);
-    return *this;
-}
-
-}  // namespace tt::tt_metal
diff --git a/tt_metal/impl/dispatch/ringbuffer_cache.hpp b/tt_metal/impl/dispatch/ringbuffer_cache.hpp
deleted file mode 100644
index a621a89e47..0000000000
--- a/tt_metal/impl/dispatch/ringbuffer_cache.hpp
+++ /dev/null
@@ -1,112 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent Inc.
-//
-// SPDX-License-Identifier: Apache-2.0
-
-// Ringbuffer cache implementation
-
-#pragma once
-
-#include <cstdint>
-#include <array>
-#include <vector>
-#include <optional>
-#include <climits>
-#include "assert.hpp"
-#include <memory>
-namespace tt::tt_metal {
-
-/*! @brief Ringbuffer cache manager
- *  This class does recordkeeping for the prefetcher cache, which is implemented as a ringbuffer.
- *  It keeps track of the entries in the ringbuffer and their offsets. The ringbuffer is divided into blocks of size
- * cache_block_sizeB. Prefetcher cache justification: The ring buffer prefetcher cache stores data (eg, kernel bins)
- * read from DRAM in the L1 of the prefetcher. The current implementation of a ring buffer works well for traces where
- * all the data fits into the cache and gets reuse from locality.
- */
-class RingbufferCacheManager {
-    friend class RingbufferCacheRandomizedTestsFixture;  // for unit testing purposes
-
-public:
-    RingbufferCacheManager(int cache_block_sizeB, int cache_size_blocks, int manager_entry_initial_size);
-    RingbufferCacheManager() = delete;
-    RingbufferCacheManager(const RingbufferCacheManager&) = delete;
-    RingbufferCacheManager(RingbufferCacheManager&&) = delete;
-    RingbufferCacheManager& operator=(const RingbufferCacheManager&) = delete;
-    RingbufferCacheManager& operator=(RingbufferCacheManager&&) = delete;
-    ~RingbufferCacheManager() = default;
-
-    /*! @brief Reset the cache state */
-    void reset();
-
-    /*! @brief Swap the ringbuffer cache manager.
-     *  We provide this functionality to stash away the cache state for the duration of recording a trace.
-     */
-    friend void swap(RingbufferCacheManager& a, RingbufferCacheManager& b);
-
-    struct CacheOffset {
-        bool is_cached{false};  // true if the program is already cached
-        uint32_t offset{0};     // offset of the program in the ringbuffer
-    };
-    /*! @brief Check if the program is present in the ringbuffer
-     *  Add a new entry to the ringbuffer manager if not present
-     *  @param pgm_id program id
-     *  @param lengthB size of program in bytes
-     *  @return struct {bool, offset}, where bool::false indicates that the program was added
-     */
-    std::optional<CacheOffset> get_cache_offset(uint64_t pgm_id, uint32_t lengthB);
-
-    int get_cache_sizeB() const { return this->cache_size_blocks_ * this->cache_block_sizeB_; }
-
-private:
-    const uint32_t cache_block_sizeB_;
-    const uint32_t cache_size_blocks_;
-
-    /*! @brief cache manager entry */
-    const uint32_t cache_manager_initial_entry_size_;
-    struct RingbufferCacheManagerEntry {
-        uint32_t offset{0};     // offset in ringbuffer
-        uint32_t length{0};     // length of the program in blocks
-        uint32_t valid_idx{0};  // index into the valid_ array
-
-        RingbufferCacheManagerEntry() = default;
-        RingbufferCacheManagerEntry(uint32_t off, uint32_t len, uint32_t val) :
-            offset(off), length(len), valid_idx(val) {}
-    };
-
-    /*! @brief cache manager  */
-    struct InternalManager {
-        std::vector<RingbufferCacheManagerEntry> entry;  // ringbuffer manager entries
-        // the following indexes are for the ringbuffer manager, not the ringbuffer
-        uint32_t oldest_idx{0};
-        uint32_t next_idx{0};
-        // the following is saved for convenience
-        uint32_t next_block_offset{0};  // offset of the next block to allocate in ringbuffer
-
-        void update_oldest_idx() { this->oldest_idx = (this->oldest_idx + 1) & (entry.size() - 1); }
-        void update_next_idx() {
-            auto local_next_idx = this->next_idx + 1;
-            // wraparound next_idx if it is at the end and if cache is full
-            if (local_next_idx >= entry.size() and this->oldest_idx == 0) [[unlikely]] {
-                if (this->next_block_offset != 0) [[likely]] {
-                    // if cache offset of oldest index is 0 and next_block_offset is not 0, then cache is partially
-                    // filled
-                    this->entry.resize(entry.size() * 2);
-                }
-            }
-            this->next_idx = local_next_idx & (entry.size() - 1);
-        }
-        void clear();
-        InternalManager& swap(InternalManager& b);
-    } manager_;
-
-    /*! @brief indexed by program id. Contains index to entry in cache manager if cache hit */
-    std::vector<int32_t> valid_;
-    constexpr static int32_t invalid_cache_entry_ = -1;
-
-    void add_manager_entry(uint64_t pgm_id, uint32_t offset, uint32_t length);
-    void add_manager_entry_no_evict(uint64_t pgm_id, uint32_t offset, uint32_t length);
-    void invalidate_manager_entry();
-    bool invalidate_oldest_until_wraparound();
-    bool invalidate_sufficient_blocks(int required_space, int offset = 0);
-};
-
-}  // namespace tt::tt_metal
diff --git a/tt_metal/impl/dispatch/topology.cpp b/tt_metal/impl/dispatch/topology.cpp
index c08e8b577c..a1e18201d1 100644
--- a/tt_metal/impl/dispatch/topology.cpp
+++ b/tt_metal/impl/dispatch/topology.cpp
@@ -1236,84 +1236,38 @@ void configure_dispatch_cores(IDevice* device) {
     }
 }
 
-tt::tt_fabric::FabricEriscDatamoverType get_fabric_edm_type(
-    const tt::tt_fabric::ControlPlane& control_plane,
+bool check_dateline(
+    const tt_fabric::ControlPlane& control_plane,
     tt_fabric::Topology topology,
-    tt::tt_fabric::MeshId mesh_id,
+    tt_fabric::MeshId mesh_id,
     chip_id_t chip0,
     chip_id_t chip1,
     bool wrap_around_mesh) {
     if (topology != tt_fabric::Topology::Ring) {
-        return tt::tt_fabric::FabricEriscDatamoverType::Default;
+        return false;
+    }
+    if (chip1 < chip0) {
+        std::swap(chip0, chip1);
     }
 
     auto physical_mesh_shape = control_plane.get_physical_mesh_shape(mesh_id);
     TT_FATAL(physical_mesh_shape.dims() == 2, "Dateline routing only supported for 2D mesh");
 
-    auto mesh_num_rows = physical_mesh_shape[0];
-    auto mesh_num_columns = physical_mesh_shape[1];
-    auto fabric_edm_type = tt::tt_fabric::FabricEriscDatamoverType::Default;
-
-    auto smaller_chip_id = std::min(chip0, chip1);
-    auto larger_chip_id = std::max(chip0, chip1);
-
     // Refactor this once mesh_id has row/col control
-    // wrap_around_mesh is used to fold the edm connections on the corner chips of a 2D mesh to form an outer ring of
-    // devices on the mesh.
     if (wrap_around_mesh) {
         // Wrap around dateline
-        if (smaller_chip_id == 0 && larger_chip_id == mesh_num_columns) {
-            fabric_edm_type = tt::tt_fabric::FabricEriscDatamoverType::Dateline;
-        } else if ((chip0 == 0 || chip0 == mesh_num_columns) && chip1 == chip0 + 1) {
-            fabric_edm_type = tt::tt_fabric::FabricEriscDatamoverType::DatelineUpstream;
-        } else if ((chip1 == 0 || chip1 == mesh_num_columns) && chip0 == chip1 + 1) {
-            fabric_edm_type = tt::tt_fabric::FabricEriscDatamoverType::DatelineUpstreamAdjacentDevice;
-        }
+        return chip0 == 0 && chip1 == physical_mesh_shape[1];
     } else {
-        bool is_dateline_edm_along_column =
-            smaller_chip_id % mesh_num_columns == 0 && larger_chip_id == (smaller_chip_id + mesh_num_columns - 1);
-        bool is_dateline_edm_along_row = smaller_chip_id < mesh_num_columns &&
-                                         larger_chip_id >= (mesh_num_columns * (mesh_num_rows - 1)) &&
-                                         smaller_chip_id == larger_chip_id % mesh_num_columns;
-        bool is_dateline_upstream_edm_along_column =
-            (chip0 % mesh_num_columns == 0 && chip1 == chip0 + 1) ||
-            (chip0 % mesh_num_columns == mesh_num_columns - 1 && chip1 == chip0 - 1);
-        bool is_dateline_upstream_edm_along_row =
-            (chip0 < mesh_num_columns && chip1 == chip0 + mesh_num_columns) ||
-            (chip0 >= (mesh_num_columns * (mesh_num_rows - 1)) && chip1 == chip0 - mesh_num_columns);
-        bool is_dateline_upstream_adjacent_edm_along_column =
-            (chip1 % mesh_num_columns == 0 && chip0 == chip1 + 1) ||
-            (chip1 % mesh_num_columns == mesh_num_columns - 1 && chip0 == chip1 - 1);
-        bool is_dateline_upstream_adjacent_edm_along_row =
-            (chip1 < mesh_num_columns && chip0 == chip1 + mesh_num_columns) ||
-            (chip1 >= (mesh_num_columns * (mesh_num_rows - 1)) && chip0 == chip1 - mesh_num_columns);
-        // Column dateline
-        if (is_dateline_edm_along_column) {
-            fabric_edm_type = tt::tt_fabric::FabricEriscDatamoverType::Dateline;
-        }
-        // Row dateline
-        else if (is_dateline_edm_along_row) {
-            fabric_edm_type = tt::tt_fabric::FabricEriscDatamoverType::Dateline;
-        }
-        // Column dateline upstream
-        else if (is_dateline_upstream_edm_along_column) {
-            fabric_edm_type = tt::tt_fabric::FabricEriscDatamoverType::DatelineUpstream;
-        }
-        // Row dateline upstream
-        else if (is_dateline_upstream_edm_along_row) {
-            fabric_edm_type = tt::tt_fabric::FabricEriscDatamoverType::DatelineUpstream;
-        }
-        // Column dateline upstream adjacent
-        else if (is_dateline_upstream_adjacent_edm_along_column) {
-            fabric_edm_type = tt::tt_fabric::FabricEriscDatamoverType::DatelineUpstreamAdjacentDevice;
-        }
-        // Row dateline upstream adjacent
-        else if (is_dateline_upstream_adjacent_edm_along_row) {
-            fabric_edm_type = tt::tt_fabric::FabricEriscDatamoverType::DatelineUpstreamAdjacentDevice;
-        }
+        return
+            // Column dateline
+            // chip0 is the first col, chip1 is the last col on the same row
+            (chip0 % physical_mesh_shape[1] == 0 && (chip0 + physical_mesh_shape[1] - 1) == chip1) ||
+            // Row dateline
+            // chip0 is the first row, chip1 is the last row on the same column
+            (chip0 < physical_mesh_shape[1] && chip1 >= (physical_mesh_shape[1] * (physical_mesh_shape[0] - 1)) &&
+             chip1 % physical_mesh_shape[1] == chip0);
     }
-
-    return fabric_edm_type;
+    return false;
 }
 
 void build_tt_fabric_program(
@@ -1403,18 +1357,15 @@ void build_tt_fabric_program(
 
     for (const auto& [direction, remote_physical_chip_id] : chip_neighbors) {
         auto remote_fabric_node_id = control_plane.get_fabric_node_id_from_physical_chip_id(remote_physical_chip_id);
-        const auto& fabric_edm_type = get_fabric_edm_type(
-            control_plane,
-            topology,
-            fabric_node_id.mesh_id,
-            fabric_node_id.chip_id,
-            remote_fabric_node_id.chip_id,
-            wrap_around_mesh);
-
-        bool is_dateline = remote_fabric_node_id.mesh_id == fabric_node_id.mesh_id &&
-                           fabric_edm_type == tt::tt_fabric::FabricEriscDatamoverType::Dateline;
-
-        const auto& curr_edm_config = fabric_context.get_fabric_router_config(fabric_edm_type);
+        bool is_dateline = remote_fabric_node_id.mesh_id == fabric_node_id.mesh_id && check_dateline(
+                                                                                          control_plane,
+                                                                                          topology,
+                                                                                          fabric_node_id.mesh_id,
+                                                                                          fabric_node_id.chip_id,
+                                                                                          remote_fabric_node_id.chip_id,
+                                                                                          wrap_around_mesh);
+
+        const auto& curr_edm_config = fabric_context.get_fabric_router_config(is_dateline);
 
         for (const auto& eth_chan : active_fabric_eth_channels[direction]) {
             auto eth_logical_core = soc_desc.get_eth_core_for_channel(eth_chan, CoordSystem::LOGICAL);
diff --git a/tt_metal/impl/dispatch/util/dispatch_settings.cpp b/tt_metal/impl/dispatch/util/dispatch_settings.cpp
index cd17e5b963..b2642facfe 100644
--- a/tt_metal/impl/dispatch/util/dispatch_settings.cpp
+++ b/tt_metal/impl/dispatch/util/dispatch_settings.cpp
@@ -97,7 +97,7 @@ DispatchSettings DispatchSettings::eth_defaults(const tt::Cluster& /*cluster*/,
         .prefetch_max_cmd_size(32_KB)
         .prefetch_cmddat_q_size(64_KB)
         .prefetch_scratch_db_size(19_KB)
-        .prefetch_ringbuffer_size(70_KB)
+        .prefetch_ringbuffer_size(90_KB)
         .prefetch_d_buffer_size(128_KB)
 
         .dispatch_size(128_KB)
diff --git a/tt_metal/impl/kernels/kernel_impl.hpp b/tt_metal/impl/kernels/kernel_impl.hpp
index 3e255aaa20..d1965db4fb 100644
--- a/tt_metal/impl/kernels/kernel_impl.hpp
+++ b/tt_metal/impl/kernels/kernel_impl.hpp
@@ -65,7 +65,7 @@ public:
             magic_enum::enum_integer(HalProcessorClassType::DM) + magic_enum::enum_integer(config.processor);
     }
 
-    ~DataMovementKernel() override {}
+    ~DataMovementKernel() {}
 
     RISCV processor() const override;
 
@@ -102,7 +102,7 @@ public:
             magic_enum::enum_integer(HalProcessorClassType::DM) + magic_enum::enum_integer(config.processor);
     }
 
-    ~EthernetKernel() override {}
+    ~EthernetKernel() {}
 
     RISCV processor() const override;
 
@@ -137,7 +137,7 @@ public:
         this->dispatch_class_ = magic_enum::enum_integer(HalProcessorClassType::COMPUTE);
     }
 
-    ~ComputeKernel() override {}
+    ~ComputeKernel() {}
 
     RISCV processor() const override;
 
diff --git a/tt_metal/impl/program/dispatch.cpp b/tt_metal/impl/program/dispatch.cpp
index 4c3a967d05..426b012e0e 100644
--- a/tt_metal/impl/program/dispatch.cpp
+++ b/tt_metal/impl/program/dispatch.cpp
@@ -63,7 +63,6 @@
 #include "dispatch/worker_config_buffer.hpp"
 #include "tt_metal/distributed/mesh_workload_impl.hpp"
 #include "kernels/kernel_impl.hpp"
-#include "tt_metal/impl/dispatch/hardware_command_queue.hpp"
 
 namespace tt {
 namespace tt_metal {
@@ -1088,8 +1087,7 @@ public:
         const ProgramTransferInfo& program_transfer_info,
         const std::shared_ptr<Buffer>& kernels_buffer,
         const CommandConstants& constants,
-        DeviceCommandCalculator& calculator,
-        bool using_prefetcher_cache) {
+        DeviceCommandCalculator& calculator) {
         const auto& hal = MetalContext::instance().hal();
         const uint32_t max_length_per_sub_cmd =
             MetalContext::instance().dispatch_mem_map(constants.dispatch_core_type).scratch_db_size() / 2;
@@ -1126,43 +1124,31 @@ public:
                         kg_transfer_info.lengths[kernel_idx],
                         kg_transfer_info.riscvs[kernel_idx]);
                     // Difference between prefetch total relayed pages and dispatch write linear
-                    if (not using_prefetcher_cache) {
-                        uint32_t relayed_bytes =
-                            tt::align(kg_transfer_info.lengths[kernel_idx], HostMemDeviceCommand::PROGRAM_PAGE_SIZE);
-                        uint16_t length_adjust = uint16_t(relayed_bytes - kg_transfer_info.lengths[kernel_idx]);
-                        uint32_t base_address, page_offset;
-                        if (kg_transfer_info.page_offsets[kernel_idx] > CQ_PREFETCH_RELAY_PAGED_START_PAGE_MASK) {
-                            const uint32_t num_banks =
-                                device->allocator()->get_num_banks(kernels_buffer->buffer_type());
-                            page_offset = kg_transfer_info.page_offsets[kernel_idx] % num_banks;
-                            uint32_t num_full_pages_written_per_bank =
-                                kg_transfer_info.page_offsets[kernel_idx] / num_banks;
-                            base_address = kernels_buffer->address() +
-                                           num_full_pages_written_per_bank * kernels_buffer->page_size();
-                        } else {
-                            base_address = kernels_buffer->address();
-                            page_offset = kg_transfer_info.page_offsets[kernel_idx];
-                        }
-
-                        calculator.add_prefetch_relay_paged();
-                        kernel_bins_unicast_cmds.back().add_prefetch_relay_paged(
-                            true,  // is_dram
-                            page_offset,
-                            base_address,
-                            kernels_buffer->page_size(),
-                            relayed_bytes / kernels_buffer->page_size(),
-                            length_adjust);
+                    uint32_t relayed_bytes =
+                        tt::align(kg_transfer_info.lengths[kernel_idx], HostMemDeviceCommand::PROGRAM_PAGE_SIZE);
+                    uint16_t length_adjust = uint16_t(relayed_bytes - kg_transfer_info.lengths[kernel_idx]);
+
+                    uint32_t base_address, page_offset;
+                    if (kg_transfer_info.page_offsets[kernel_idx] > CQ_PREFETCH_RELAY_PAGED_START_PAGE_MASK) {
+                        const uint32_t num_banks = device->allocator()->get_num_banks(kernels_buffer->buffer_type());
+                        page_offset = kg_transfer_info.page_offsets[kernel_idx] % num_banks;
+                        uint32_t num_full_pages_written_per_bank =
+                            kg_transfer_info.page_offsets[kernel_idx] / num_banks;
+                        base_address =
+                            kernels_buffer->address() + num_full_pages_written_per_bank * kernels_buffer->page_size();
                     } else {
-                        calculator.add_prefetch_relay_ringbuffer(1);
-                        kernel_bins_unicast_cmds.back().add_prefetch_relay_ringbuffer(
-                            1,
-                            std::vector<CQPrefetchRelayRingbufferSubCmd>(
-                                1,
-                                CQPrefetchRelayRingbufferSubCmd(
-                                    {.start = kg_transfer_info.page_offsets[kernel_idx] *
-                                              HostMemDeviceCommand::PROGRAM_PAGE_SIZE,
-                                     .length = kg_transfer_info.lengths[kernel_idx]})));
+                        base_address = kernels_buffer->address();
+                        page_offset = kg_transfer_info.page_offsets[kernel_idx];
                     }
+
+                    calculator.add_prefetch_relay_paged();
+                    kernel_bins_unicast_cmds.back().add_prefetch_relay_paged(
+                        true,  // is_dram
+                        page_offset,
+                        base_address,
+                        kernels_buffer->page_size(),
+                        relayed_bytes / kernels_buffer->page_size(),
+                        length_adjust);
                 } else {
                     uint32_t base_address = kernels_buffer->address();
                     uint32_t page_offset = kg_transfer_info.page_offsets[kernel_idx];
@@ -1202,22 +1188,11 @@ public:
                             program.get_id(), DISPATCH_DATA_BINARY, write_length, kg_transfer_info.riscvs[kernel_idx]);
                         kernel_config_buffer_offset += write_length;
 
-                        if (not using_prefetcher_cache) {
-                            auto& prefetch_subcmds =
-                                kernel_bins_cmd.get_prefetch_subcmds<CQPrefetchRelayPagedPackedSubCmd>();
-                            prefetch_subcmds.emplace_back(CQPrefetchRelayPagedPackedSubCmd{
-                                .start_page = (uint16_t)page_offset,
-                                .log_page_size = (uint16_t)HostMemDeviceCommand::LOG2_PROGRAM_PAGE_SIZE,
-                                .base_addr = base_address,
-                                .length = read_length});
-                        } else {
-                            auto& prefetch_subcmds =
-                                kernel_bins_cmd.get_prefetch_subcmds<CQPrefetchRelayRingbufferSubCmd>();
-                            // start address for kernel bin is aligned with page boundary, consistent with the
-                            // non-cached case
-                            prefetch_subcmds.emplace_back(CQPrefetchRelayRingbufferSubCmd{
-                                .start = page_offset * HostMemDeviceCommand::PROGRAM_PAGE_SIZE, .length = read_length});
-                        }
+                        kernel_bins_cmd.prefetch_subcmds.emplace_back(CQPrefetchRelayPagedPackedSubCmd{
+                            .start_page = (uint16_t)page_offset,
+                            .log_page_size = (uint16_t)HostMemDeviceCommand::LOG2_PROGRAM_PAGE_SIZE,
+                            .base_addr = base_address,
+                            .length = read_length});
                         page_offset += read_length / HostMemDeviceCommand::PROGRAM_PAGE_SIZE;
                         aligned_length -= read_length;
                         kernel_bins_cmd.data_aligned_sizeB += read_length;
@@ -1225,24 +1200,14 @@ public:
                 }
             }
         }
-
-        if (using_prefetcher_cache) {
-            for (auto& kernel_bins_cmd : kernel_bins_cmds) {
-                calculator.add_dispatch_write_packed_large(kernel_bins_cmd.dispatch_subcmds.size());
-                auto& prefetch_subcmds = kernel_bins_cmd.get_prefetch_subcmds<CQPrefetchRelayRingbufferSubCmd>();
-                calculator.add_prefetch_relay_ringbuffer(prefetch_subcmds.size());
-            }
-        } else {
-            for (auto& kernel_bins_cmd : kernel_bins_cmds) {
-                calculator.add_dispatch_write_packed_large(kernel_bins_cmd.dispatch_subcmds.size());
-                auto& prefetch_subcmds = kernel_bins_cmd.get_prefetch_subcmds<CQPrefetchRelayPagedPackedSubCmd>();
-                calculator.add_prefetch_relay_paged_packed(prefetch_subcmds.size());
-            }
+        for (auto& kernel_bins_cmd : kernel_bins_cmds) {
+            calculator.add_dispatch_write_packed_large(kernel_bins_cmd.dispatch_subcmds.size());
+            calculator.add_prefetch_relay_paged_packed(kernel_bins_cmd.prefetch_subcmds.size());
         }
     }
 
     // Assemble the program binary commands into the device command sequence.
-    void assemble_commands(HostMemDeviceCommand& device_command_sequence, bool using_prefetcher_cache) const {
+    void assemble_commands(HostMemDeviceCommand& device_command_sequence) const {
         const auto& hal = MetalContext::instance().hal();
         for (const auto& kernel_bins_unicast_cmd : kernel_bins_unicast_cmds) {
             device_command_sequence.add_data(
@@ -1259,34 +1224,19 @@ public:
                 kernel_bins_cmd.dispatch_subcmds,
                 0,
                 DISPATCH_WRITE_OFFSET_TENSIX_BINARY_L1_CONFIG_BASE);
-            if (using_prefetcher_cache) {
-                auto& prefetch_subcmds = kernel_bins_cmd.get_prefetch_subcmds<CQPrefetchRelayRingbufferSubCmd>();
-                device_command_sequence.add_prefetch_relay_ringbuffer(prefetch_subcmds.size(), prefetch_subcmds);
-            } else {
-                auto& prefetch_subcmds = kernel_bins_cmd.get_prefetch_subcmds<CQPrefetchRelayPagedPackedSubCmd>();
-                device_command_sequence.add_prefetch_relay_paged_packed(
-                    kernel_bins_cmd.data_aligned_sizeB, prefetch_subcmds, prefetch_subcmds.size());
-            }
+            device_command_sequence.add_prefetch_relay_paged_packed(
+                kernel_bins_cmd.data_aligned_sizeB,
+                kernel_bins_cmd.prefetch_subcmds,
+                kernel_bins_cmd.prefetch_subcmds.size());
         }
     }
 
 private:
     struct KernelBinsCmds {
-        std::pair<std::vector<CQPrefetchRelayPagedPackedSubCmd>, std::vector<CQPrefetchRelayRingbufferSubCmd>>
-            prefetch_subcmds;
+        std::vector<CQPrefetchRelayPagedPackedSubCmd> prefetch_subcmds;
         std::vector<CQDispatchWritePackedLargeSubCmd> dispatch_subcmds;
         uint32_t data_aligned_sizeB{0};
-
-        template <class T>
-        std::vector<T>& get_prefetch_subcmds() {
-            return std::get<std::vector<T>>(prefetch_subcmds);
-        }
-        template <class T>
-        const std::vector<T>& get_prefetch_subcmds() const {
-            return std::get<std::vector<T>>(prefetch_subcmds);
-        }
     };
-
     std::vector<KernelBinsCmds> kernel_bins_cmds;
     std::vector<HostMemDeviceCommand> kernel_bins_unicast_cmds;
 };
@@ -1689,8 +1639,7 @@ void assemble_device_commands(
     ProgramCommandSequence& program_command_sequence,
     ProgramImpl& program,
     IDevice* device,
-    SubDeviceId sub_device_id,
-    bool use_prefetcher_cache) {
+    SubDeviceId sub_device_id) {
     CommandConstants constants;
     auto dispatch_core_config = MetalContext::instance().get_dispatch_core_manager().get_dispatch_core_config();
     constants.dispatch_core_type = dispatch_core_config.get_core_type();
@@ -1739,18 +1688,13 @@ void assemble_device_commands(
         program_transfer_info,
         program.get_kernels_buffer(device),
         constants,
-        program_binary_calculator,
-        use_prefetcher_cache);
+        program_binary_calculator);
     program_command_sequence.program_binary_command_sequence =
         HostMemDeviceCommand(program_binary_calculator.write_offset_bytes());
-    program_binary_command_generator.assemble_commands(
-        program_command_sequence.program_binary_command_sequence, use_prefetcher_cache);
+    program_binary_command_generator.assemble_commands(program_command_sequence.program_binary_command_sequence);
     TT_ASSERT(
         program_command_sequence.program_binary_command_sequence.size_bytes() ==
         program_command_sequence.program_binary_command_sequence.write_offset_bytes());
-    if (use_prefetcher_cache) {
-        program_command_sequence.kernel_bins_base_addr = program.get_kernels_buffer(device)->address();
-    }
 
     // Assemble launch message
     LaunchMessageGenerator launch_message_generator;
@@ -1964,42 +1908,6 @@ void update_program_dispatch_commands(
         memcpy(rta_update.dst, rta_update.src, rta_update.size);
     }
 
-    // Update prefetcher cache initialization
-    if (cached_program_command_sequence.prefetcher_cache_used) {
-        // reserve space for cache command
-        uint32_t pcie_alignment =
-            tt::tt_metal::MetalContext::instance().hal().get_alignment(tt::tt_metal::HalMemType::HOST);
-        cached_program_command_sequence.program_binary_setup_prefetcher_cache_command =
-            HostMemDeviceCommand(tt::align(sizeof(CQPrefetchCmd), pcie_alignment));
-
-        auto is_cached = dispatch_md.prefetcher_cache_info.is_cached;
-        auto cache_offset = dispatch_md.prefetcher_cache_info.offset;
-        auto program_sizeB = cached_program_command_sequence.kernel_bins_sizeB;
-        auto max_program_sizeB = dispatch_md.prefetcher_cache_info.mesh_max_program_kernels_sizeB;
-        if (is_cached) {
-            cached_program_command_sequence.program_binary_setup_prefetcher_cache_command
-                .add_prefetch_set_ringbuffer_offset(cache_offset);
-        } else {
-            TT_FATAL(
-                program_sizeB <= max_program_sizeB,
-                "Kernel binary size exceeds prefetcher cache size ({}, {})",
-                program_sizeB,
-                max_program_sizeB);
-            bool wraparound_flag = cache_offset != 0 ? 0 : CQ_PREFETCH_PAGED_TO_RING_BUFFER_FLAG_RESET_TO_START;
-            cached_program_command_sequence.program_binary_setup_prefetcher_cache_command
-                .add_prefetch_paged_to_ringbuffer(CQPrefetchPagedToRingbufferCmd{
-                    .flags = uint8_t(wraparound_flag),
-                    .log2_page_size = uint16_t(HostMemDeviceCommand::LOG2_PROGRAM_PAGE_SIZE),
-                    .start_page = 0,
-                    .wp_offset_update = max_program_sizeB,
-                    .base_addr = cached_program_command_sequence.kernel_bins_base_addr,
-                    .length = program_sizeB});
-        }
-        TT_ASSERT(
-            cached_program_command_sequence.program_binary_setup_prefetcher_cache_command.size_bytes() ==
-            cached_program_command_sequence.program_binary_setup_prefetcher_cache_command.write_offset_bytes());
-    }
-
     // Update launch messages
     for (auto& [is_multicast, original_launch_msg, launch_msg] : cached_program_command_sequence.launch_messages) {
         for (uint32_t i = 0; i < dispatch_md.kernel_config_addrs.size(); i++) {
@@ -2134,42 +2042,6 @@ void update_traced_program_dispatch_commands(
         std::memcpy(rta_update.dst, trace_node.rta_data[i].data(), rta_update.size);
     }
 
-    // Update prefetcher cache initialization
-    if (dispatch_md.send_binary && cached_program_command_sequence.prefetcher_cache_used) {
-        // reserve space for cache command
-        uint32_t pcie_alignment =
-            tt::tt_metal::MetalContext::instance().hal().get_alignment(tt::tt_metal::HalMemType::HOST);
-        cached_program_command_sequence.program_binary_setup_prefetcher_cache_command =
-            HostMemDeviceCommand(tt::align(sizeof(CQPrefetchCmd), pcie_alignment));
-
-        auto is_cached = dispatch_md.prefetcher_cache_info.is_cached;
-        auto cache_offset = dispatch_md.prefetcher_cache_info.offset;
-        auto program_sizeB = cached_program_command_sequence.kernel_bins_sizeB;
-        auto max_program_sizeB = dispatch_md.prefetcher_cache_info.mesh_max_program_kernels_sizeB;
-        if (is_cached) {
-            cached_program_command_sequence.program_binary_setup_prefetcher_cache_command
-                .add_prefetch_set_ringbuffer_offset(cache_offset);
-        } else {
-            TT_FATAL(
-                program_sizeB <= max_program_sizeB,
-                "Kernel binary size exceeds prefetcher cache size ({}, {})",
-                program_sizeB,
-                max_program_sizeB);
-            bool wraparound_flag = cache_offset != 0 ? 0 : CQ_PREFETCH_PAGED_TO_RING_BUFFER_FLAG_RESET_TO_START;
-            cached_program_command_sequence.program_binary_setup_prefetcher_cache_command
-                .add_prefetch_paged_to_ringbuffer(CQPrefetchPagedToRingbufferCmd{
-                    .flags = uint8_t(wraparound_flag),
-                    .log2_page_size = uint16_t(HostMemDeviceCommand::LOG2_PROGRAM_PAGE_SIZE),
-                    .start_page = 0,
-                    .wp_offset_update = max_program_sizeB,
-                    .base_addr = cached_program_command_sequence.kernel_bins_base_addr,
-                    .length = program_sizeB});
-        }
-        TT_ASSERT(
-            cached_program_command_sequence.program_binary_setup_prefetcher_cache_command.size_bytes() ==
-            cached_program_command_sequence.program_binary_setup_prefetcher_cache_command.write_offset_bytes());
-    }
-
     // Update launch messages
     for (auto& [is_multicast, original_launch_msg, launch_msg] : cached_program_command_sequence.launch_messages) {
         for (uint32_t i = 0; i < dispatch_md.nonbinary_kernel_config_addrs.size(); i++) {
@@ -2293,11 +2165,6 @@ void write_program_command_sequence(
 
     if (send_binary) {
         // Write the program binary
-        if (program_command_sequence.prefetcher_cache_used) {
-            write_data_to_cq(
-                program_command_sequence.program_binary_setup_prefetcher_cache_command.data(),
-                program_command_sequence.program_binary_setup_prefetcher_cache_command.size_bytes());
-        }
         write_data_to_cq(
             program_command_sequence.program_binary_command_sequence.data(),
             program_command_sequence.program_binary_command_sequence.size_bytes());
@@ -2320,9 +2187,9 @@ void write_program_command_sequence(
     }
 }
 
-TraceNode create_trace_node(ProgramImpl& program, IDevice* device, bool use_prefetcher_cache) {
+TraceNode create_trace_node(ProgramImpl& program, IDevice* device) {
     std::vector<SubDeviceId> sub_device_ids{program.determine_sub_device_ids(device)};
-    program.generate_trace_dispatch_commands(device, use_prefetcher_cache);
+    program.generate_trace_dispatch_commands(device);
     uint64_t command_hash = *device->get_active_sub_device_manager_id();
 
     // By using the traced command sequence, we know the RTA data source-of-truth isn't this command sequence (it's in a
diff --git a/tt_metal/impl/program/dispatch.hpp b/tt_metal/impl/program/dispatch.hpp
index e46f7a7417..7d64bcf78c 100644
--- a/tt_metal/impl/program/dispatch.hpp
+++ b/tt_metal/impl/program/dispatch.hpp
@@ -45,12 +45,6 @@ struct ProgramDispatchMetadata {
     uint32_t sync_count;
     uint32_t stall_first;
     uint32_t stall_before_program;
-
-    struct {
-        uint32_t mesh_max_program_kernels_sizeB;
-        bool is_cached;
-        uint32_t offset;
-    } prefetcher_cache_info;
 };
 
 uint32_t configure_rta_offsets_for_kernel_groups(
@@ -139,7 +133,7 @@ void update_traced_program_dispatch_commands(
     ProgramBinaryStatus program_binary_status,
     std::pair<bool, int> unicast_go_signal_update = {false, -1});
 
-TraceNode create_trace_node(detail::ProgramImpl& program, IDevice* device, bool use_prefetcher_cache);
+TraceNode create_trace_node(detail::ProgramImpl& program, IDevice* device);
 
 void write_program_command_sequence(
     const ProgramCommandSequence& program_command_sequence,
diff --git a/tt_metal/impl/program/program.cpp b/tt_metal/impl/program/program.cpp
index 6348949b5a..bf4700bd6f 100644
--- a/tt_metal/impl/program/program.cpp
+++ b/tt_metal/impl/program/program.cpp
@@ -1273,7 +1273,7 @@ void detail::ProgramImpl::allocate_kernel_bin_buf_on_device(IDevice* device) {
     }
 }
 
-void Program::generate_dispatch_commands(IDevice* device, bool use_prefetcher_cache) {
+void Program::generate_dispatch_commands(IDevice* device) {
     uint64_t command_hash = *device->get_active_sub_device_manager_id();
 
     uint64_t device_hash = BuildEnvManager::get_instance().get_device_build_env(device->build_id()).build_key;
@@ -1297,25 +1297,14 @@ void Program::generate_dispatch_commands(IDevice* device, bool use_prefetcher_ca
         ProgramCommandSequence program_command_sequence;
         program_dispatch::insert_empty_program_dispatch_preamble_cmd(program_command_sequence);
         program_dispatch::insert_stall_cmds(program_command_sequence, sub_device_id, device);
-        program_dispatch::assemble_device_commands(
-            program_command_sequence, impl(), device, sub_device_id, use_prefetcher_cache);
-
-        program_command_sequence.kernel_bins_sizeB = this->impl().kernel_bins_sizeB;
-        program_command_sequence.prefetcher_cache_used = use_prefetcher_cache;
-
+        program_dispatch::assemble_device_commands(program_command_sequence, impl(), device, sub_device_id);
         // TODO: We currently do not have a mechanism of removing entries in the cache when a manager is removed
         // This means programs will contain stale entries in the cache until the program is deleted
         cached_program_command_sequences.insert({command_hash, std::move(program_command_sequence)});
-    } else {
-        TT_ASSERT(
-            cached_program_command_sequences.at(command_hash).prefetcher_cache_used == use_prefetcher_cache,
-            "Prefetcher cache used mismatch for program {} on device {}",
-            this->get_id(),
-            device->id());
     }
 }
 
-void ProgramImpl::generate_trace_dispatch_commands(IDevice* device, bool use_prefetcher_cache) {
+void ProgramImpl::generate_trace_dispatch_commands(IDevice* device) {
     uint64_t command_hash = *device->get_active_sub_device_manager_id();
 
     uint64_t device_hash = BuildEnvManager::get_instance().get_device_build_env(device->build_id()).build_key;
@@ -1339,19 +1328,10 @@ void ProgramImpl::generate_trace_dispatch_commands(IDevice* device, bool use_pre
         ProgramCommandSequence program_command_sequence;
         program_dispatch::insert_empty_program_dispatch_preamble_cmd(program_command_sequence);
         program_dispatch::insert_stall_cmds(program_command_sequence, sub_device_id, device);
-        program_dispatch::assemble_device_commands(
-            program_command_sequence, *this, device, sub_device_id, use_prefetcher_cache);
-        program_command_sequence.prefetcher_cache_used = use_prefetcher_cache;
-        program_command_sequence.kernel_bins_sizeB = this->kernel_bins_sizeB;
+        program_dispatch::assemble_device_commands(program_command_sequence, *this, device, sub_device_id);
         // TODO: We currently do not have a mechanism of removing entries in the cache when a manager is removed
         // This means programs will contain stale entries in the cache until the program is deleted
         trace_cached_program_command_sequences.insert({command_hash, std::move(program_command_sequence)});
-    } else {
-        TT_ASSERT(
-            trace_cached_program_command_sequences.at(command_hash).prefetcher_cache_used == use_prefetcher_cache,
-            "Prefetcher cache used mismatch for program {} on device {}",
-            this->get_id(),
-            device->id());
     }
 }
 
@@ -1731,15 +1711,14 @@ void detail::ProgramImpl::finalize_offsets(IDevice* device) {
     std::array<ProgramImpl*, 1> programs_array = {this};
     tt::stl::Span<ProgramImpl*> programs(programs_array);
 
-    (void)ProgramImpl::finalize_program_offsets(
-        device, kernels_getter, kernel_groups_getter, semaphores_getter, programs);
+    ProgramImpl::finalize_program_offsets(device, kernels_getter, kernel_groups_getter, semaphores_getter, programs);
 
     set_finalized();
 }
 
 // Compute relative offsets (wrt the start of the kernel config ring buffer) and sizes of all
 // program data structures in L1. Will be used when assembling dispatch commands for this program
-uint32_t detail::ProgramImpl::finalize_program_offsets(
+void detail::ProgramImpl::finalize_program_offsets(
     IDevice* device,
     const KernelsGetter& kernels_getter,
     const KernelGroupsGetter& kernel_groups_getter,
@@ -1801,14 +1780,6 @@ uint32_t detail::ProgramImpl::finalize_program_offsets(
     for (auto& program : programs) {
         program->set_program_attrs_across_core_types(device);
     }
-
-    // determine max program size across all programs
-    uint32_t max_program_sizeB = 0;
-    for (auto& program : programs) {
-        program->kernel_bins_sizeB = state.kernel_text_size;
-        max_program_sizeB = std::max(max_program_sizeB, state.kernel_text_size);
-    }
-    return max_program_sizeB;
 }
 
 std::unordered_map<uint64_t, ProgramCommandSequence>&
diff --git a/tt_metal/impl/program/program_command_sequence.hpp b/tt_metal/impl/program/program_command_sequence.hpp
index 3c98c582f7..c71c531a02 100644
--- a/tt_metal/impl/program/program_command_sequence.hpp
+++ b/tt_metal/impl/program/program_command_sequence.hpp
@@ -38,7 +38,6 @@ struct ProgramCommandSequence {
     HostMemDeviceCommand stall_command_sequences[2];
     std::vector<HostMemDeviceCommand> runtime_args_command_sequences;
     HostMemDeviceCommand program_config_buffer_command_sequence;
-    HostMemDeviceCommand program_binary_setup_prefetcher_cache_command;
     HostMemDeviceCommand program_binary_command_sequence;
     HostMemDeviceCommand launch_msg_command_sequence;
     HostMemDeviceCommand go_msg_command_sequence;
@@ -52,10 +51,6 @@ struct ProgramCommandSequence {
     std::vector<CQDispatchWritePackedCmd*> unicast_launch_msg_write_packed_cmd_ptrs;
     CQDispatchGoSignalMcastCmd* mcast_go_signal_cmd_ptr;
 
-    bool prefetcher_cache_used = false;
-    uint32_t kernel_bins_sizeB = 0;
-    uint32_t kernel_bins_base_addr = 0;
-
     uint32_t get_rt_args_size() const {
         return std::accumulate(
             runtime_args_command_sequences.begin(),
@@ -68,10 +63,7 @@ struct ProgramCommandSequence {
         uint32_t one_shot_fetch_size =
             ((stall_before_program || stall_first) ? stall_command_sequences[current_stall_seq_idx].size_bytes() : 0) +
             preamble_command_sequence.size_bytes() + program_config_buffer_command_sequence.size_bytes() +
-            get_rt_args_size() +
-            (send_binary ? program_binary_command_sequence.size_bytes() +
-                               program_binary_setup_prefetcher_cache_command.size_bytes()
-                         : 0) +
+            get_rt_args_size() + (send_binary ? program_binary_command_sequence.size_bytes() : 0) +
             launch_msg_command_sequence.size_bytes() + go_msg_command_sequence.size_bytes();
         return one_shot_fetch_size;
     }
diff --git a/tt_metal/impl/program/program_impl.hpp b/tt_metal/impl/program/program_impl.hpp
index c412d4c892..c86290a636 100644
--- a/tt_metal/impl/program/program_impl.hpp
+++ b/tt_metal/impl/program/program_impl.hpp
@@ -54,8 +54,8 @@ void assemble_device_commands(
     ProgramCommandSequence& program_command_sequence,
     detail::ProgramImpl& program,
     IDevice* device,
-    SubDeviceId sub_device_id,
-    bool use_prefetcher_cache);
+    SubDeviceId sub_device_id);
+
 }
 
 using kernel_id_array_t = std::array<std::optional<KernelHandle>, DISPATCH_CLASS_MAX>;
@@ -186,7 +186,7 @@ public:
     const ProgramConfig& get_program_config(uint32_t programmable_core_type_index) const;
     const std::vector<SubDeviceId>& determine_sub_device_ids(const IDevice* device);
 
-    void generate_trace_dispatch_commands(IDevice* device, bool use_prefetcher_cache);
+    void generate_trace_dispatch_commands(IDevice* device);
     std::unordered_map<uint64_t, ProgramCommandSequence>& get_trace_cached_program_command_sequences() noexcept;
 
     // debug/test
@@ -198,10 +198,8 @@ public:
 
     void finalize_offsets(IDevice* device);
 
-    // Helper function to finalize program offsets with custom getters. Returns the maximum kernel binaries size among
-    // all the programs, to determine whether the mesh workload can fit in the prefetcher cache all of the programs in
-    // it.
-    static uint32_t finalize_program_offsets(
+    // Helper function to finalize program offsets with custom getters
+    static void finalize_program_offsets(
         IDevice* device,
         const KernelsGetter& kernels_getter,
         const KernelGroupsGetter& kernel_groups_getter,
@@ -284,8 +282,6 @@ private:
     // Counts how much space is needed for each core + each launch buffer msg queue.
     std::vector<uint32_t> program_config_sizes_;
 
-    uint32_t kernel_bins_sizeB = 0;
-
     // The rta_updates from one cached command sequence may reference data in another cached command sequence.
     std::unordered_map<uint64_t, ProgramCommandSequence> cached_program_command_sequences_;
     std::unordered_map<uint64_t, ProgramCommandSequence> trace_cached_program_command_sequences_;
@@ -336,8 +332,7 @@ private:
         ProgramCommandSequence& program_command_sequence,
         ProgramImpl& program,
         IDevice* device,
-        SubDeviceId sub_device_id,
-        bool use_prefetcher_cache);
+        SubDeviceId sub_device_id);
 
     friend HWCommandQueue;
     friend EnqueueProgramCommand;
diff --git a/tt_metal/impl/trace/trace_node.hpp b/tt_metal/impl/trace/trace_node.hpp
index d96ae7aceb..b72359981e 100644
--- a/tt_metal/impl/trace/trace_node.hpp
+++ b/tt_metal/impl/trace/trace_node.hpp
@@ -19,12 +19,6 @@ struct TraceDispatchMetadata {
     uint32_t sync_count = 0;
     uint32_t stall_first = false;
     uint32_t stall_before_program = false;
-
-    struct {
-        uint32_t mesh_max_program_kernels_sizeB;  // TBD: max program size across all programs in a mesh
-        bool is_cached;
-        uint32_t offset;
-    } prefetcher_cache_info;
 };
 
 // This struct contains all the information needed to execute a program on a device.
diff --git a/tt_metal/include/compute_kernel_api/eltwise_unary/logical_not_noti.h b/tt_metal/include/compute_kernel_api/eltwise_unary/logical_not_noti.h
index 743bd54b5e..c3c1ab6705 100644
--- a/tt_metal/include/compute_kernel_api/eltwise_unary/logical_not_noti.h
+++ b/tt_metal/include/compute_kernel_api/eltwise_unary/logical_not_noti.h
@@ -32,27 +32,8 @@ ALWI void logical_not_unary_tile(uint32_t idst) {
     MATH((llk_math_eltwise_unary_sfpu_logical_not_unary_op<APPROX>(idst)));
 }
 
-// clang-format off
-/**
- * Performs element-wise computation of the logical not unary operation for int32 dtype on each element of a tile
- * in DST register at index tile_index. The DST register buffer must be in
- * acquired state via *acquire_dst* call. This call is blocking and is only
- * available on the compute engine.
- *
- * Return value: None
- *
- * | Argument       | Description                                                                | Type     | Valid Range                                           | Required |
- * |----------------|----------------------------------------------------------------------------|----------|-------------------------------------------------------|----------|
- * | tile_index     | The index of the tile in DST register buffer to perform the computation on | uint32_t | Must be less than the size of the DST register buffer | True     |
- */
-// clang-format on
-ALWI void logical_not_unary_tile_int32(uint32_t idst) {
-    MATH((llk_math_eltwise_unary_sfpu_logical_not_unary_op_int32<APPROX>(idst)));
-}
-
 /**
  * Please refer to documentation for any_init.
  */
 ALWI void logical_not_unary_tile_init() { MATH((llk_math_eltwise_unary_sfpu_logical_not_unary_init<APPROX>())); }
-
 }  // namespace ckernel
diff --git a/tt_metal/llrt/tt_cluster.cpp b/tt_metal/llrt/tt_cluster.cpp
index c9196ecf83..4a7f785d77 100644
--- a/tt_metal/llrt/tt_cluster.cpp
+++ b/tt_metal/llrt/tt_cluster.cpp
@@ -1139,79 +1139,24 @@ std::unordered_set<CoreCoord> Cluster::get_active_ethernet_cores(
     return active_ethernet_cores;
 }
 
-void Cluster::configure_ethernet_cores_for_fabric_routers(
-    tt_metal::FabricConfig fabric_config, std::optional<uint8_t> num_routing_planes) {
+void Cluster::configure_ethernet_cores_for_fabric_routers(tt_metal::FabricConfig fabric_config) {
     if (fabric_config != tt_metal::FabricConfig::DISABLED) {
-        TT_FATAL(num_routing_planes.has_value(), "num_routing_planes should be set for reserving cores for fabric");
-        TT_FATAL(num_routing_planes.value() > 0, "Expected non-zero num_routing_planes for reserving cores for fabric");
-        this->reserve_ethernet_cores_for_fabric_routers(num_routing_planes.value());
+        this->reserve_ethernet_cores_for_fabric_routers();
     } else {
-        if (num_routing_planes.has_value()) {
-            log_warning(
-                tt::LogMetal,
-                "Got num_routing_planes while releasing fabric cores, ignoring it and releasing all reserved cores");
-        }
         this->release_ethernet_cores_for_fabric_routers();
     }
 }
 
-void Cluster::reserve_ethernet_cores_for_fabric_routers(uint8_t num_routing_planes) {
-    if (num_routing_planes == std::numeric_limits<uint8_t>::max()) {
-        // default behavior, reserve whatever cores are available
-        for (const auto& [chip_id, eth_cores] : this->device_eth_routing_info_) {
-            for (const auto& [eth_core, mode] : eth_cores) {
-                if (mode == EthRouterMode::IDLE) {
-                    this->device_eth_routing_info_[chip_id][eth_core] = EthRouterMode::FABRIC_ROUTER;
-                }
-            }
-        }
-
-        // Update sockets to reflect fabric routing
-        this->ethernet_sockets_.clear();
-        return;
-    }
-
-    // to reserve specified number of cores, ensure that the same are avaialble on connected chip id as well
-    for (const auto& chip_id : this->driver_->get_target_device_ids()) {
-        const auto& connected_chips_and_cores = this->get_ethernet_cores_grouped_by_connected_chips(chip_id);
-        for (const auto& [connnected_chip_id, cores] : connected_chips_and_cores) {
-            const uint8_t num_cores_to_reserve = std::min(num_routing_planes, static_cast<uint8_t>(cores.size()));
-            uint8_t num_reserved_cores = 0;
-            for (auto i = 0; i < cores.size(); i++) {
-                if (num_reserved_cores == num_cores_to_reserve) {
-                    break;
-                }
-
-                const auto eth_core = cores[i];
-                const auto connected_core =
-                    std::get<1>(this->get_connected_ethernet_core(std::make_tuple(chip_id, eth_core)));
-                if (this->device_eth_routing_info_.at(chip_id).at(eth_core) == EthRouterMode::FABRIC_ROUTER) {
-                    // already reserved for fabric, potenially by the connected chip id
-                    num_reserved_cores++;
-                    continue;
-                }
-
-                if (this->device_eth_routing_info_[chip_id][eth_core] == EthRouterMode::IDLE &&
-                    this->device_eth_routing_info_.at(connnected_chip_id).at(connected_core) == EthRouterMode::IDLE) {
-                    this->device_eth_routing_info_[chip_id][eth_core] = EthRouterMode::FABRIC_ROUTER;
-                    this->device_eth_routing_info_[connnected_chip_id][connected_core] = EthRouterMode::FABRIC_ROUTER;
-                    num_reserved_cores++;
-                }
+void Cluster::reserve_ethernet_cores_for_fabric_routers() {
+    for (const auto& [chip_id, eth_cores] : this->device_eth_routing_info_) {
+        for (const auto& [eth_core, mode] : eth_cores) {
+            if (mode == EthRouterMode::IDLE) {
+                this->device_eth_routing_info_[chip_id][eth_core] = EthRouterMode::FABRIC_ROUTER;
             }
-
-            TT_FATAL(
-                num_reserved_cores == num_cores_to_reserve,
-                "Unable to reserve {} routing planes b/w chip {} and {} for fabric, reserved only {}",
-                num_cores_to_reserve,
-                chip_id,
-                connnected_chip_id,
-                num_reserved_cores);
         }
     }
-
-    // re-init sockets to reflect fabric routing
+    // Update sockets to reflect fabric routing
     this->ethernet_sockets_.clear();
-    this->initialize_ethernet_sockets();
 }
 
 void Cluster::release_ethernet_cores_for_fabric_routers() {
diff --git a/tt_metal/llrt/tt_cluster.hpp b/tt_metal/llrt/tt_cluster.hpp
index 4dcbf0bb91..92d9487469 100644
--- a/tt_metal/llrt/tt_cluster.hpp
+++ b/tt_metal/llrt/tt_cluster.hpp
@@ -303,8 +303,7 @@ public:
     }
 
     // Configures ethernet cores for fabric routers depending on whether fabric is enabled
-    void configure_ethernet_cores_for_fabric_routers(
-        tt_metal::FabricConfig fabric_config, std::optional<uint8_t> num_routing_planes = std::nullopt);
+    void configure_ethernet_cores_for_fabric_routers(tt_metal::FabricConfig fabric_config);
 
     // Returns whether we are running on Galaxy.
     bool is_galaxy_cluster() const;
@@ -396,8 +395,8 @@ private:
     // If any device has to board type of GALAXY, we are on a TG cluster.
     ClusterType cluster_type_ = ClusterType::INVALID;
 
-    // Reserves specified number of ethernet cores for fabric routers
-    void reserve_ethernet_cores_for_fabric_routers(uint8_t num_routing_planes);
+    // Reserves all free ethernet cores for fabric routers
+    void reserve_ethernet_cores_for_fabric_routers();
 
     // Releases all reserved ethernet cores for fabric routers
     void release_ethernet_cores_for_fabric_routers();
diff --git a/tt_metal/sfpi-version.sh b/tt_metal/sfpi-version.sh
index fb791131db..53d0bb1f59 100644
--- a/tt_metal/sfpi-version.sh
+++ b/tt_metal/sfpi-version.sh
@@ -1,9 +1,7 @@
 # set SFPI release version information
-sfpi_version=v6.12.0
+sfpi_version=v6.11.1
 sfpi_url=https://github.com/tenstorrent/sfpi/releases/download
-sfpi_aarch64_Linux_txz_md5=b12a5c2e3467cc486e9d37201c822e75
-sfpi_aarch64_Linux_deb_md5=36fc0dab154d65bbb5c152b4bddd41f8
-sfpi_aarch64_Linux_rpm_md5=3983b6cbbaa216ceeaf087b0ac80fd0f
- sfpi_x86_64_Linux_txz_md5=056a58245dddec11c1e89a7bbeb4022f
- sfpi_x86_64_Linux_deb_md5=b6a826aa4ed8333788bdcccbb281afd8
- sfpi_x86_64_Linux_rpm_md5=a76fa7d91e439a8197f76a6acf512976
+sfpi_x86_64_Linux_txz_md5=14ade50b3fdf3fff5078195332edc15a
+sfpi_x86_64_Linux_deb_md5=93803c538ba87357df22b87b0d7c62ed
+sfpi_aarch64_Linux_tar_md5=517a64489eb87653f554f94e46a2bf17
+sfpi_aarch64_Linux_deb_md5=7bef7babb7c19f241adc6c3998aebe4d
diff --git a/tt_metal/tt_metal.cpp b/tt_metal/tt_metal.cpp
index 3727999591..b43d8f841e 100644
--- a/tt_metal/tt_metal.cpp
+++ b/tt_metal/tt_metal.cpp
@@ -375,8 +375,8 @@ bool ReadRegFromDevice(IDevice* device, const CoreCoord& logical_core, uint32_t
     return true;
 }
 
-void SetFabricConfig(FabricConfig fabric_config, std::optional<uint8_t> num_routing_planes) {
-    tt::tt_metal::MetalContext::instance().set_fabric_config(fabric_config, num_routing_planes);
+void InitializeFabricConfig(FabricConfig fabric_config) {
+    tt::tt_metal::MetalContext::instance().initialize_fabric_config(fabric_config);
 }
 
 std::map<chip_id_t, IDevice*> CreateDevices(
@@ -1175,6 +1175,8 @@ uint32_t CreateSemaphore(
             std::optional<uint32_t> semaphore_id;
             TT_FATAL(crs.ranges().size() > 0, "Expecting a non-empty CoreRangeSet!");
             for (const auto& core_range : crs.ranges()) {
+                CoreCoord start_core = core_range.start_coord;
+                CoreCoord end_core = core_range.end_coord;
                 std::optional<uint32_t> semaphore_id_candidate = get_semaphore_id(program, core_range, core_type);
                 if (!semaphore_id.has_value()) {
                     semaphore_id = semaphore_id_candidate;
diff --git a/ttnn/CMakeLists.txt b/ttnn/CMakeLists.txt
index c79e49158f..b9adf3e5a2 100644
--- a/ttnn/CMakeLists.txt
+++ b/ttnn/CMakeLists.txt
@@ -20,7 +20,7 @@ if(ENABLE_TTNN_SHARED_SUBLIBS)
 endif()
 
 add_library(ttnn_core ${LIB_TYPE})
-add_library(TTNN::Core ALIAS ttnn_core)
+add_library(TT::NN::Core ALIAS ttnn_core)
 
 target_compile_definitions(ttnn_core PUBLIC "$<$<CXX_COMPILER_ID:GNU>:DISABLE_NAMESPACE_STATIC_ASSERT>")
 target_precompile_headers(ttnn_core REUSE_FROM TT::CommonPCH)
@@ -286,7 +286,6 @@ set(CCL_EXPERIMENTAL_TTNN_SRCS_PYBIND
     ${CMAKE_CURRENT_SOURCE_DIR}/cpp/ttnn/operations/experimental/ccl/all_broadcast_async/all_broadcast_async_${PY_BINDING}.cpp
     ${CMAKE_CURRENT_SOURCE_DIR}/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/all_gather_concat_${PY_BINDING}.cpp
     ${CMAKE_CURRENT_SOURCE_DIR}/cpp/ttnn/operations/experimental/ccl/all_gather_matmul/all_gather_matmul_${PY_BINDING}.cpp
-    ${CMAKE_CURRENT_SOURCE_DIR}/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul_${PY_BINDING}.cpp
     ${CMAKE_CURRENT_SOURCE_DIR}/cpp/ttnn/operations/experimental/ccl/all_reduce/all_reduce_${PY_BINDING}.cpp
     ${CMAKE_CURRENT_SOURCE_DIR}/cpp/ttnn/operations/experimental/ccl/all_reduce_async/all_reduce_async_${PY_BINDING}.cpp
     ${CMAKE_CURRENT_SOURCE_DIR}/cpp/ttnn/operations/experimental/ccl/ccl_experimental_${PY_BINDING}.cpp
@@ -358,7 +357,7 @@ endif()
 # for those who need this second use case, now it is officially supported
 
 add_library(ttnncpp ${TTNN_CPP_BUILD_TYPE})
-add_library(TTNN::CPP ALIAS ttnncpp)
+add_library(TT::NN::CPP ALIAS ttnncpp)
 add_library(Metalium::TTNNCPP ALIAS ttnncpp) # backwards compatibility. FIXME: remove
 
 target_compile_definitions(ttnncpp PUBLIC "$<$<CXX_COMPILER_ID:GNU>:DISABLE_NAMESPACE_STATIC_ASSERT>")
@@ -430,63 +429,62 @@ target_link_libraries(
     ttnncpp
     PRIVATE
         TT::Metalium
-        TTNN::Core
-        TTNN::Ops::Bernoulli
-        TTNN::Ops::CCL
-        TTNN::Ops::Conv
-        TTNN::Ops::Core
-        TTNN::Ops::DataMovement
-        TTNN::Ops::Eltwise::Binary
-        TTNN::Ops::Eltwise::Binary::Backward
-        TTNN::Ops::Eltwise::Binary::NG
-        TTNN::Ops::Eltwise::Complex
-        TTNN::Ops::Eltwise::Complex::Binary
-        TTNN::Ops::Eltwise::Complex::Unary
-        TTNN::Ops::Eltwise::Complex::Unary::Backward
-        TTNN::Ops::Eltwise::Quantization
-        TTNN::Ops::Eltwise::Ternary
-        TTNN::Ops::Eltwise::Ternary::Backward
-        TTNN::Ops::Eltwise::Unary
-        TTNN::Ops::Eltwise::Unary::Backward
-        TTNN::Ops::Embedding
-        TTNN::Ops::Embedding::Backward
-        TTNN::Ops::Examples
-        TTNN::Ops::Experimental::AutoFormat
-        TTNN::Ops::Experimental::BcastTo
-        TTNN::Ops::Experimental::CCL
-        TTNN::Ops::Experimental::CNN
-        TTNN::Ops::Experimental::Conv3d
-        TTNN::Ops::Experimental::Copy
-        TTNN::Ops::Experimental::Dropout
-        TTNN::Ops::Experimental::Matmul
-        TTNN::Ops::Experimental::PagedCache
-        TTNN::Ops::Experimental::PlusOne
-        TTNN::Ops::Experimental::Reduction
-        TTNN::Ops::Experimental::Reshape
-        TTNN::Ops::Experimental::Scatter
-        TTNN::Ops::Experimental::SSM
-        TTNN::Ops::Experimental::Transformer
-        TTNN::Ops::Experimental::UnaryBackward
-        TTNN::Ops::Full
-        TTNN::Ops::FullLike
-        TTNN::Ops::IndexFill
-        TTNN::Ops::KvCache
-        TTNN::Ops::Loss
-        TTNN::Ops::Matmul
-        TTNN::Ops::Moreh
-        TTNN::Ops::Prefetcher
-        TTNN::Ops::Reduction
-        TTNN::Ops::SlidingWindow
-        TTNN::Ops::Transformer
-        TTNN::Ops::Uniform
-        TTNN::Ops::Pool
-        TTNN::Ops::Normalization
+        TT::NN::Core
+        TT::NN::Ops::Bernoulli
+        TT::NN::Ops::CCL
+        TT::NN::Ops::Conv
+        TT::NN::Ops::Core
+        TT::NN::Ops::DataMovement
+        TT::NN::Ops::Eltwise::Binary
+        TT::NN::Ops::Eltwise::Binary::Backward
+        TT::NN::Ops::Eltwise::Binary::NG
+        TT::NN::Ops::Eltwise::Complex
+        TT::NN::Ops::Eltwise::Complex::Binary
+        TT::NN::Ops::Eltwise::Complex::Unary
+        TT::NN::Ops::Eltwise::Complex::Unary::Backward
+        TT::NN::Ops::Eltwise::Quantization
+        TT::NN::Ops::Eltwise::Ternary
+        TT::NN::Ops::Eltwise::Ternary::Backward
+        TT::NN::Ops::Eltwise::Unary
+        TT::NN::Ops::Eltwise::Unary::Backward
+        TT::NN::Ops::Embedding
+        TT::NN::Ops::Embedding::Backward
+        TT::NN::Ops::Examples
+        TT::NN::Ops::Experimental::AutoFormat
+        TT::NN::Ops::Experimental::BcastTo
+        TT::NN::Ops::Experimental::CCL
+        TT::NN::Ops::Experimental::CNN
+        TT::NN::Ops::Experimental::Conv3d
+        TT::NN::Ops::Experimental::Copy
+        TT::NN::Ops::Experimental::Dropout
+        TT::NN::Ops::Experimental::Matmul
+        TT::NN::Ops::Experimental::PagedCache
+        TT::NN::Ops::Experimental::PlusOne
+        TT::NN::Ops::Experimental::Reduction
+        TT::NN::Ops::Experimental::Reshape
+        TT::NN::Ops::Experimental::Scatter
+        TT::NN::Ops::Experimental::SSM
+        TT::NN::Ops::Experimental::Transformer
+        TT::NN::Ops::Experimental::UnaryBackward
+        TT::NN::Ops::Full
+        TT::NN::Ops::FullLike
+        TT::NN::Ops::IndexFill
+        TT::NN::Ops::KvCache
+        TT::NN::Ops::Loss
+        TT::NN::Ops::Matmul
+        TT::NN::Ops::Moreh
+        TT::NN::Ops::Prefetcher
+        TT::NN::Ops::Reduction
+        TT::NN::Ops::SlidingWindow
+        TT::NN::Ops::Transformer
+        TT::NN::Ops::Uniform
+        TT::NN::Ops::Pool
+        TT::NN::Ops::Normalization
 )
-install(TARGETS ttnncpp LIBRARY COMPONENT ttnn-runtime)
 
 if(WITH_PYTHON_BINDINGS)
     add_library(ttnn SHARED)
-    add_library(TTNN::PYTHON ALIAS ttnn)
+    add_library(TT::NN::PYTHON ALIAS ttnn)
     add_library(Metalium::TTNN ALIAS ttnn) # backwards compatibility. FIXME: remove
     target_compile_definitions(ttnn PUBLIC "$<$<CXX_COMPILER_ID:GNU>:DISABLE_NAMESPACE_STATIC_ASSERT>")
     TT_ENABLE_UNITY_BUILD(ttnn)
@@ -524,13 +522,12 @@ if(WITH_PYTHON_BINDINGS)
     target_link_libraries(
         ttnn
         PUBLIC
-            TTNN::CPP
+            TT::NN::CPP
         PRIVATE
             pybind11::module
             Boost::algorithm
             TT::Metalium
     )
-    install(TARGETS ttnn LIBRARY COMPONENT ttnn-runtime)
 endif()
 
 if(WITH_PYTHON_BINDINGS)
@@ -566,10 +563,6 @@ else()
     )
 endif()
 
-if(TTNN_BUILD_TESTS)
-    add_subdirectory(test)
-endif()
-
 set(FixmeOpIncDirs
     ${CMAKE_CURRENT_SOURCE_DIR}
     ${CMAKE_CURRENT_SOURCE_DIR}/cpp
@@ -624,5 +617,3 @@ add_subdirectory(cpp/ttnn/operations/transformer)
 add_subdirectory(cpp/ttnn/operations/uniform)
 add_subdirectory(cpp/ttnn/operations/pool)
 add_subdirectory(cpp/ttnn/operations/normalization)
-
-add_subdirectory(cpp/ttnn/deprecated)
diff --git a/ttnn/api/ttnn/device_operation.hpp b/ttnn/api/ttnn/device_operation.hpp
index 29424ce526..49100b1ad3 100644
--- a/ttnn/api/ttnn/device_operation.hpp
+++ b/ttnn/api/ttnn/device_operation.hpp
@@ -199,8 +199,7 @@ auto get_operation_name(const typename device_operation_t::operation_attributes_
     }
 }
 
-// GCC 12 has a bug that causes a segfault when using reflection to log tensors.
-#if !defined(NDEBUG) && !defined(__GNUC__)
+#ifdef DEBUG
 
 template <typename device_operation_t>
 inline void log_operation(
diff --git a/ttnn/api/ttnn/graph/graph_processor.hpp b/ttnn/api/ttnn/graph/graph_processor.hpp
index e4ebf3eacf..3c8f93457d 100644
--- a/ttnn/api/ttnn/graph/graph_processor.hpp
+++ b/ttnn/api/ttnn/graph/graph_processor.hpp
@@ -28,7 +28,7 @@ public:
 
     bool hook_program(tt::tt_metal::Program* program) override;
 
-    ~ProcessorHooks() override = default;
+    virtual ~ProcessorHooks() = default;
 
     void set_block(bool block);
 
diff --git a/ttnn/api/ttnn/tensor/storage.hpp b/ttnn/api/ttnn/tensor/storage.hpp
index a38faef85f..4c5272624d 100644
--- a/ttnn/api/ttnn/tensor/storage.hpp
+++ b/ttnn/api/ttnn/tensor/storage.hpp
@@ -55,15 +55,17 @@ public:
     // TODO: #22169 - Remove this once there are no more usages of this constructor.
     explicit MultiDeviceHostStorage(std::vector<HostBuffer> buffers);
 
-    explicit MultiDeviceHostStorage(DistributedHostBuffer buffer);
-
-    const DistributedHostBuffer& distributed_buffer() const;
-
     static constexpr auto attribute_names = std::forward_as_tuple();
     auto attribute_values() const { return std::forward_as_tuple(); }
 
+    // Returns `HostBuffer` at position `buffer_index`;
+    HostBuffer get_buffer(int buffer_index) const;
+
+    // Returns the number of `HostBuffer`s in the storage;
+    size_t num_buffers() const;
+
 private:
-    DistributedHostBuffer distributed_buffer_;
+    std::vector<HostBuffer> buffers_;
 };
 
 using Storage = std::variant<HostStorage, DeviceStorage, MultiDeviceHostStorage>;
diff --git a/ttnn/api/ttnn/tensor/tensor_attributes.hpp b/ttnn/api/ttnn/tensor/tensor_attributes.hpp
index e431980caa..cc9b08ea81 100644
--- a/ttnn/api/ttnn/tensor/tensor_attributes.hpp
+++ b/ttnn/api/ttnn/tensor/tensor_attributes.hpp
@@ -21,6 +21,9 @@ public:
     const TensorSpec& get_tensor_spec() const;
     const DistributedTensorConfig& get_distributed_tensor_config() const;
 
+    // Determines mesh coordinates for the tensor based on the strategy and the mesh shape.
+    std::vector<distributed::MeshCoordinate> determine_distribution(const distributed::MeshShape& mesh_shape) const;
+
 private:
     Storage storage_;
     TensorSpec tensor_spec_;
diff --git a/ttnn/api/ttnn/tensor/tensor_ops.hpp b/ttnn/api/ttnn/tensor/tensor_ops.hpp
index eec3ac9a97..1f486ee7b1 100644
--- a/ttnn/api/ttnn/tensor/tensor_ops.hpp
+++ b/ttnn/api/ttnn/tensor/tensor_ops.hpp
@@ -27,6 +27,8 @@ Tensor tensor_to_device(
 
 Tensor tensor_to_layout(const Tensor& input_tensor, Layout target_layout, IDevice* worker);
 
+Tensor tensor_to_layout(const Tensor& input_tensor, Layout target_layout, distributed::MeshDevice* mesh_device);
+
 Tensor tensor_cpu(const Tensor& input_tensor, bool blocking, QueueId cq_id);
 
 void tensor_print(const Tensor& input_tensor);
diff --git a/ttnn/core/distributed/api.cpp b/ttnn/core/distributed/api.cpp
index 10b0cf56d7..c668abc400 100644
--- a/ttnn/core/distributed/api.cpp
+++ b/ttnn/core/distributed/api.cpp
@@ -8,7 +8,6 @@
 
 #include <tt_stl/overloaded.hpp>
 #include "tt-metalium/assert.hpp"
-#include "tt-metalium/distributed_host_buffer.hpp"
 #include "tt-metalium/mesh_coord.hpp"
 #include "ttnn/tensor/storage.hpp"
 #include "ttnn/tensor/tensor.hpp"
@@ -75,10 +74,10 @@ void close_mesh_device(const std::shared_ptr<MeshDevice>& mesh_device) { mesh_de
 std::vector<Tensor> get_device_tensors(const Tensor& tensor) {
     if (std::holds_alternative<tt::tt_metal::MultiDeviceHostStorage>(tensor.storage())) {
         std::vector<ttnn::Tensor> tensors;
-        auto& host_storage = std::get<tt::tt_metal::MultiDeviceHostStorage>(tensor.get_storage());
-        const auto& distributed_buffer = host_storage.distributed_buffer();
-        distributed_buffer.apply(
-            [&](const HostBuffer& buffer) { tensors.push_back(Tensor{buffer, tensor.get_tensor_spec()}); });
+        auto& host_storage = std::get<tt::tt_metal::MultiDeviceHostStorage>(tensor.storage());
+        for (int i = 0; i < host_storage.num_buffers(); ++i) {
+            tensors.push_back(Tensor{host_storage.get_buffer(i), tensor.tensor_spec()});
+        }
         return tensors;
     } else if (std::holds_alternative<tt::tt_metal::DeviceStorage>(tensor.storage())) {
         auto& device_storage = std::get<tt::tt_metal::DeviceStorage>(tensor.storage());
@@ -113,16 +112,28 @@ Tensor aggregate_as_tensor(
     // Based whether the first tensor shard has Host or Device buffer,
     // we want to use MultiDeviceHostStorage or MultiDeviceStorage
     StorageType storage_type = reference_shard.storage_type();
+    Tile tile = reference_shard.tensor_spec().tile();
     if (storage_type == StorageType::HOST) {
-        std::vector<HostBuffer> buffers;
+        std::vector<ttnn::TensorSpec> specs;
+        std::vector<HostBuffer> host_owned_buffers;
         for (const auto& shard : tensor_shards) {
-            buffers.push_back(std::get<HostStorage>(shard.get_storage()).buffer);
-            TT_FATAL(
-                shard.get_tensor_spec() == reference_shard.get_tensor_spec(),
-                "Error aggregating multichip tensors: Attempting to aggregate tensors with different tensor specs.");
+            host_owned_buffers.push_back(std::get<HostStorage>(shard.storage()).buffer);
+            specs.push_back(shard.tensor_spec());
+            Tile shard_tile = shard.tensor_spec().tile();
+            if (shard_tile != tile) {
+                TT_THROW(
+                    "Error aggregating multichip tensors: Attempting to aggregate tensors with different tiling "
+                    "configurations. Device {} has tiling ({}x{}) while device {} has tiling {}x{}.",
+                    reference_shard.device()->id(),
+                    tile.get_height(),
+                    tile.get_width(),
+                    shard.device()->id(),
+                    shard_tile.get_height(),
+                    shard_tile.get_width());
+            }
         }
-        auto storage = MultiDeviceHostStorage{std::move(buffers)};
-        return Tensor(std::move(storage), reference_shard.get_tensor_spec(), config);
+        auto storage = MultiDeviceHostStorage{std::move(host_owned_buffers)};
+        return Tensor(std::move(storage), reference_shard.tensor_spec(), config);
     } else if (storage_type == StorageType::DEVICE) {
         return combine_device_tensors_impl(tensor_shards, reference_shard);
     } else {
diff --git a/ttnn/core/distributed/distributed_tensor.cpp b/ttnn/core/distributed/distributed_tensor.cpp
index 90c6e633bf..1892075860 100644
--- a/ttnn/core/distributed/distributed_tensor.cpp
+++ b/ttnn/core/distributed/distributed_tensor.cpp
@@ -3,7 +3,6 @@
 // SPDX-License-Identifier: Apache-2.0
 
 #include "tensor/host_buffer/functions.hpp"
-#include "tensor/storage.hpp"
 #include "tt-metalium/shape.hpp"
 #include "tt-metalium/mesh_coord.hpp"
 #include <tt_stl/small_vector.hpp>
@@ -42,7 +41,7 @@ bool increment_indices(const tt::stl::SmallVector<int>& limits, tt::stl::SmallVe
 // Note the shapes of all shards must be the same; resulting in a uniform tensor spec.
 TensorSpec compute_tensor_spec_for_shards(
     const auto& xtensor_shards_views, const tt::tt_metal::TensorLayout& global_layout) {
-    std::optional<Shape> shard_shape;
+    std::optional<ttnn::Shape> shard_shape;
     for (const auto& [_, xtensor_view] : xtensor_shards_views) {
         if (!xtensor_view.has_value()) {
             continue;
@@ -64,30 +63,11 @@ TensorSpec compute_tensor_spec_for_shards(
 
 class NdTensorToMesh : public TensorToMesh {
 public:
-    // Specifies how a tensor sharded over a specific shape will be distributed to a mesh device, which potentially
-    // has a different shape.
-    enum class DistributionMode {
-        // Tensor shards will be distributed in row-major order over a mesh device.
-        ROW_MAJOR,
-
-        // Shards will be mapped to a mesh device as is, preserving coordinates.
-        // This requires a submesh to fit within the mesh device.
-        SUBMESH,
-    };
-
     NdTensorToMesh(
-        const MeshDevice& mesh_device,
-        DistributionMode distribution_mode,
-        const MeshShape& distribution_shape,
+        const ttnn::MeshShape& shape,
         const MeshMapperConfig& config,
         const tt::tt_metal::DistributedTensorConfig& distributed_tensor_config) :
-        global_shape_(mesh_device.shape()),
-        local_shape_(mesh_device.shape()),
-        local_offset_(MeshCoordinate::zero_coordinate(mesh_device.shape().dims())),
-        distribution_mode_(distribution_mode),
-        distribution_shape_(distribution_shape),
-        config_(config),
-        distributed_tensor_config_(distributed_tensor_config) {}
+        shape_(shape), config_(config), distributed_tensor_config_(distributed_tensor_config) {}
 
     Tensor operator()(const Tensor& tensor) const override {
         switch (tensor.tensor_spec().data_type()) {
@@ -131,13 +111,13 @@ private:
         tt::stl::SmallVector<int> tensor_dims;
         tt::stl::SmallVector<size_t> replicate_dims;
         size_t sharded_mesh_size = 1;
-        for (size_t mesh_dim_idx = 0; mesh_dim_idx < distribution_shape_.dims(); ++mesh_dim_idx) {
+        for (size_t mesh_dim_idx = 0; mesh_dim_idx < shape_.dims(); ++mesh_dim_idx) {
             const auto& placement = config_.placements[mesh_dim_idx];
             if (const auto* shard_placement = std::get_if<MeshMapperConfig::Shard>(&placement)) {
                 shard_dims.push_back(mesh_dim_idx);
-                num_chunks_per_dim.push_back(distribution_shape_[mesh_dim_idx]);
+                num_chunks_per_dim.push_back(shape_[mesh_dim_idx]);
                 tensor_dims.push_back(shard_placement->dim);
-                sharded_mesh_size *= distribution_shape_[mesh_dim_idx];
+                sharded_mesh_size *= shape_[mesh_dim_idx];
             } else {
                 replicate_dims.push_back(mesh_dim_idx);
             }
@@ -146,19 +126,19 @@ private:
         auto chunks = experimental::xtensor::chunk_ndim(input_xtensor, num_chunks_per_dim, tensor_dims);
         TT_FATAL(chunks.size() >= 1, "No chunks were produced");
         TT_FATAL(
-            distribution_shape_.dims() == 1 || chunks.size() == sharded_mesh_size,
+            shape_.dims() == 1 || chunks.size() == sharded_mesh_size,
             "ND sharding requires the number of chunks {} to match the mesh dimension size {}",
             chunks.size(),
             sharded_mesh_size);
 
         using StridedViewRef = std::reference_wrapper<experimental::xtensor::StridedView<decltype(input_xtensor)>>;
-        MeshContainer<std::optional<StridedViewRef>> sharded_xtensor_views(distribution_shape_, std::nullopt);
+        MeshContainer<std::optional<StridedViewRef>> sharded_xtensor_views(shape_, std::nullopt);
 
         // Distribute chunks to appropriate mesh coordinates.
         size_t chunk_idx = 0;
         tt::stl::SmallVector<int> shard_indices(shard_dims.size(), 0);
         do {
-            tt::stl::SmallVector<uint32_t> mesh_coords(distribution_shape_.dims(), 0);
+            tt::stl::SmallVector<uint32_t> mesh_coords(shape_.dims(), 0);
             for (size_t i = 0; i < shard_dims.size(); ++i) {
                 mesh_coords[shard_dims[i]] = shard_indices[i];
             }
@@ -171,7 +151,7 @@ private:
 
         tt::stl::SmallVector<int> replicate_sizes;
         for (size_t replicate_mesh_dim : replicate_dims) {
-            replicate_sizes.push_back(distribution_shape_[replicate_mesh_dim]);
+            replicate_sizes.push_back(shape_[replicate_mesh_dim]);
         }
 
         // Fill in gaps along replicated dimensions:
@@ -196,28 +176,15 @@ private:
             }
         }
 
-        return create_tensor<T>(sharded_xtensor_views, tensor.tensor_spec().tensor_layout());
-    }
-
-    template <typename T>
-    Tensor create_tensor(const auto& sharded_xtensor_views, const tt::tt_metal::TensorLayout& global_layout) const {
-        const TensorSpec shard_spec = compute_tensor_spec_for_shards(sharded_xtensor_views, global_layout);
+        const TensorSpec shard_spec =
+            compute_tensor_spec_for_shards(sharded_xtensor_views, tensor.tensor_spec().tensor_layout());
 
+        // TODO: #22169 - For multi-host, supply mesh device global/local shape, along with local offset.
         auto distributed_buffer =
-            tt::tt_metal::DistributedHostBuffer::create(global_shape_, local_shape_, local_offset_);
-        const auto global_range = MeshCoordinateRange(global_shape_);
-
-        auto get_dst_coord = [this, row_major_dst = global_range.begin()](const MeshCoordinate& src_coord) mutable {
-            switch (distribution_mode_) {
-                case DistributionMode::ROW_MAJOR: return *(row_major_dst++);
-                case DistributionMode::SUBMESH: return src_coord;
-            }
-            TT_THROW("Unreachable");
-        };
-
+            tt::tt_metal::DistributedHostBuffer::create(shape_, shape_, MeshCoordinate::zero_coordinate(shape_.dims()));
         for (const auto& [coord, xtensor_view] : sharded_xtensor_views) {
             if (xtensor_view.has_value()) {
-                distributed_buffer.emplace_shard(get_dst_coord(coord), [&xtensor_view, &shard_spec, &coord]() {
+                distributed_buffer.emplace_shard(coord, [&xtensor_view, &shard_spec, &coord]() {
                     xt::xarray<T> data(xtensor_view->get());
                     auto shard_tensor = experimental::xtensor::from_xtensor<T>(data, shard_spec);
                     return tt::tt_metal::host_buffer::get_host_buffer(shard_tensor);
@@ -225,23 +192,26 @@ private:
             }
         }
 
-        return Tensor(tt::tt_metal::MultiDeviceHostStorage(std::move(distributed_buffer)), shard_spec, config());
-    }
+        // TODO: #22169 - Directly create a multi-host distributed tensor from the distributed host buffer.
+        std::vector<Tensor> tensors;
+        for (const auto& coord : MeshCoordinateRange(shape_)) {
+            auto shard = distributed_buffer.get_shard(coord);
+            if (shard.has_value() && !shard->view_bytes().empty()) {
+                tensors.push_back(Tensor(std::move(*shard), shard_spec));
+            }
+        }
 
-    // MeshDevice parameters.
-    MeshShape global_shape_;
-    MeshShape local_shape_;
-    MeshCoordinate local_offset_;
-    DistributionMode distribution_mode_ = DistributionMode::ROW_MAJOR;
+        return aggregate_as_tensor(tensors, config());
+    }
 
-    MeshShape distribution_shape_;
+    ttnn::MeshShape shape_;
     MeshMapperConfig config_;
     tt::tt_metal::DistributedTensorConfig distributed_tensor_config_;
 };
 
 class NdMeshToTensor : public MeshToTensor {
 public:
-    NdMeshToTensor(const MeshShape& shape, const MeshComposerConfig& config) : shape_(shape), config_(config) {}
+    NdMeshToTensor(const ttnn::MeshShape& shape, const MeshComposerConfig& config) : shape_(shape), config_(config) {}
 
     Tensor compose(const std::vector<Tensor>& tensors) const override {
         TT_FATAL(
@@ -281,7 +251,7 @@ public:
     }
 
 private:
-    MeshShape shape_;
+    ttnn::MeshShape shape_;
     MeshComposerConfig config_;
 };
 
@@ -289,8 +259,6 @@ private:
 
 std::unique_ptr<TensorToMesh> replicate_tensor_to_mesh_mapper(MeshDevice& mesh_device) {
     return std::make_unique<NdTensorToMesh>(
-        mesh_device,
-        NdTensorToMesh::DistributionMode::ROW_MAJOR,
         MeshShape(mesh_device.num_devices()),
         MeshMapperConfig{
             .placements =
@@ -302,8 +270,6 @@ std::unique_ptr<TensorToMesh> replicate_tensor_to_mesh_mapper(MeshDevice& mesh_d
 
 std::unique_ptr<TensorToMesh> shard_tensor_to_mesh_mapper(MeshDevice& mesh_device, int dim) {
     return std::make_unique<NdTensorToMesh>(
-        mesh_device,
-        NdTensorToMesh::DistributionMode::ROW_MAJOR,
         MeshShape(mesh_device.num_devices()),
         MeshMapperConfig{
             .placements =
@@ -322,7 +288,7 @@ std::unique_ptr<MeshToTensor> concat_mesh_to_tensor_composer(MeshDevice& mesh_de
 }
 
 std::unique_ptr<TensorToMesh> create_mesh_mapper(
-    MeshDevice& mesh_device, const MeshMapperConfig& config, const std::optional<MeshShape>& shape) {
+    MeshDevice& mesh_device, const MeshMapperConfig& config, const std::optional<ttnn::MeshShape>& shape) {
     const auto distributed_shape = shape.value_or(mesh_device.shape());
     TT_FATAL(
         distributed_shape.mesh_size() <= mesh_device.shape().mesh_size(),
@@ -336,26 +302,6 @@ std::unique_ptr<TensorToMesh> create_mesh_mapper(
         distributed_shape,
         config);
 
-    // Select distribution mode.
-    const auto distribution_mode = [&]() {
-        if (!shape.has_value()) {
-            // When no shape is supplied, row-major order is equivalent to submesh.
-            return NdTensorToMesh::DistributionMode::SUBMESH;
-        } else if (shape->dims() != mesh_device.shape().dims()) {
-            // Shapes have different dimensions, so a reshape will be required.
-            return NdTensorToMesh::DistributionMode::ROW_MAJOR;
-        } else {
-            // Check if `shape` fits within the mesh device. If it does, we can use submesh distribution. Otherwise,
-            // a reshape will be required, and shards will be distributed in row-major order over the mesh device.
-            for (size_t i = 0; i < shape->dims(); ++i) {
-                if ((*shape)[i] > mesh_device.shape()[i]) {
-                    return NdTensorToMesh::DistributionMode::ROW_MAJOR;
-                }
-            }
-            return NdTensorToMesh::DistributionMode::SUBMESH;
-        }
-    }();
-
     // TODO: #22258 - `DistributedTensorConfig` will be replaced by distributed host buffer, which can be used directly
     // in Tensor storage.
     tt::tt_metal::DistributedTensorConfig distributed_tensor_config;
@@ -366,12 +312,11 @@ std::unique_ptr<TensorToMesh> create_mesh_mapper(
         distributed_tensor_config = tt::tt_metal::DistributedTensorConfig{tt::tt_metal::AllGatherTensor{}};
     }
 
-    return std::make_unique<NdTensorToMesh>(
-        mesh_device, distribution_mode, distributed_shape, config, distributed_tensor_config);
+    return std::make_unique<NdTensorToMesh>(distributed_shape, config, distributed_tensor_config);
 }
 
 std::unique_ptr<MeshToTensor> create_mesh_composer(
-    MeshDevice& mesh_device, const MeshComposerConfig& config, const std::optional<MeshShape>& shape) {
+    MeshDevice& mesh_device, const MeshComposerConfig& config, const std::optional<ttnn::MeshShape>& shape) {
     const auto distributed_shape = shape.value_or(mesh_device.shape());
     TT_FATAL(
         distributed_shape.mesh_size() <= mesh_device.shape().mesh_size(),
diff --git a/ttnn/core/tensor/flatbuffer/tensor_flatbuffer.cpp b/ttnn/core/tensor/flatbuffer/tensor_flatbuffer.cpp
index c5a33aadd2..4a3a5aabd4 100644
--- a/ttnn/core/tensor/flatbuffer/tensor_flatbuffer.cpp
+++ b/ttnn/core/tensor/flatbuffer/tensor_flatbuffer.cpp
@@ -7,7 +7,6 @@
 
 #include <tt-metalium/mesh_coord.hpp>
 #include <tt-metalium/host_buffer.hpp>
-#include <tt-metalium/distributed_host_buffer.hpp>
 #include <flatbuffers/flatbuffers.h>
 
 #include "ttnn/tensor/types.hpp"
@@ -40,7 +39,7 @@ tt::tt_metal::distributed::MeshCoordinate from_flatbuffer(const flatbuffer::Mesh
 
 flatbuffers::Offset<flatbuffer::MeshShape> to_flatbuffer(
     const tt::tt_metal::distributed::MeshShape& shape, flatbuffers::FlatBufferBuilder& builder) {
-    auto dimensions_vector = builder.CreateVector(std::vector<uint32_t>(shape.cbegin(), shape.cend()));
+    auto dimensions_vector = builder.CreateVector(std::vector<uint32_t>(shape.view().begin(), shape.view().end()));
     return flatbuffer::CreateMeshShape(builder, dimensions_vector);
 }
 
@@ -121,28 +120,29 @@ flatbuffers::Offset<ttnn::flatbuffer::Tensor> to_flatbuffer(
 
         std::vector<flatbuffers::Offset<ttnn::flatbuffer::TensorShard>> shards_vector;
         uint64_t data_offset = 0;
-        for (const auto& coord : multi_device_storage.distributed_buffer().shard_coords()) {
-            if (const auto& buffer = multi_device_storage.distributed_buffer().get_shard(coord); buffer.has_value()) {
-                const std::size_t buffer_size = buffer->view_bytes().size();
+        for (std::size_t i = 0; i < multi_device_storage.num_buffers(); i++) {
+            const auto& buffer = multi_device_storage.get_buffer(i);
+            const std::size_t buffer_size = buffer.view_bytes().size();
 
-                auto inline_storage = ttnn::flatbuffer::InlineFileStorage(data_offset, buffer_size);
-                auto mesh_coord_offset = to_flatbuffer(coord, builder);
+            auto inline_storage = ttnn::flatbuffer::InlineFileStorage(data_offset, buffer_size);
 
-                auto shard_offset = ttnn::flatbuffer::CreateTensorShard(
-                    builder,
-                    ttnn::flatbuffer::TensorBuffer::InlineFileStorage,
-                    builder.CreateStruct(inline_storage).Union(),
-                    mesh_coord_offset);
+            auto shard_offset = ttnn::flatbuffer::CreateTensorShard(
+                builder,
+                ttnn::flatbuffer::TensorBuffer::InlineFileStorage,
+                builder.CreateStruct(inline_storage).Union());
 
-                shards_vector.push_back(shard_offset);
-                data_offset += buffer_size;
-            }
+            shards_vector.push_back(shard_offset);
+            data_offset += buffer_size;
         }
         auto shards = builder.CreateVector(shards_vector);
 
-        auto mesh_shape_offset = to_flatbuffer(multi_device_storage.distributed_buffer().shape(), builder);
-
+        flatbuffers::Offset<ttnn::flatbuffer::MeshShape> mesh_shape_offset;
+        if (const auto* shard_2d = std::get_if<tt::tt_metal::ShardTensor2D>(&strategy)) {
+            ttnn::MeshShape mesh_shape{shard_2d->shard_mesh.x, shard_2d->shard_mesh.y};
+            mesh_shape_offset = to_flatbuffer(mesh_shape, builder);
+        }
         auto sharded_tensor = ttnn::flatbuffer::CreateShardedTensor(builder, mesh_shape_offset, shards);
+
         auto tensor_offset = ttnn::flatbuffer::CreateTensor(
             builder, tensor_spec_offset, ttnn::flatbuffer::TensorType::ShardedTensor, sharded_tensor.Union());
 
@@ -171,25 +171,25 @@ Tensor from_flatbuffer(const ttnn::flatbuffer::Tensor* fb_tensor, tt::stl::Span<
             return Tensor(std::move(host_buffer), spec);
         }
         case ttnn::flatbuffer::TensorType::ShardedTensor: {
-            const auto* sharded = fb_tensor->tensor_type_as_ShardedTensor();
-
-            const auto* mesh_shape = sharded->mesh_shape();
-            TT_FATAL(mesh_shape != nullptr, "Mesh shape is required for sharded tensor");
-            const tt::tt_metal::distributed::MeshShape ttnn_mesh_shape = from_flatbuffer(mesh_shape);
+            auto sharded = fb_tensor->tensor_type_as_ShardedTensor();
 
             tt::tt_metal::DistributedTensorConfig strategy;
-            if (ttnn_mesh_shape.dims() == 2) {
+            auto* mesh_shape = sharded->mesh_shape();
+            if (mesh_shape != nullptr) {
+                const tt::tt_metal::distributed::MeshShape ttnn_mesh_shape = from_flatbuffer(mesh_shape);
                 strategy = tt::tt_metal::ShardTensor2D{
                     tt::tt_metal::ShardMesh{.y = ttnn_mesh_shape[0], .x = ttnn_mesh_shape[1]}};
             }
 
             const size_t num_shards = sharded->shards()->size();
 
-            auto distributed_buffer = tt::tt_metal::DistributedHostBuffer::create(ttnn_mesh_shape);
-            for (size_t i = 0; i < sharded->shards()->size(); ++i) {
-                const auto* shard = sharded->shards()->Get(i);
+            std::vector<tt::tt_metal::HostBuffer> buffers;
+            std::vector<TensorSpec> specs(num_shards, spec);
+            buffers.reserve(num_shards);
+            for (size_t i = 0; i < num_shards; i++) {
+                auto* shard = sharded->shards()->Get(i);
 
-                const auto* inline_storage = shard->buffer_as<ttnn::flatbuffer::InlineFileStorage>();
+                auto* inline_storage = shard->buffer_as<ttnn::flatbuffer::InlineFileStorage>();
                 TT_FATAL(
                     inline_storage != nullptr, "Only InlineFileStorage is supported in flatbuffer deserialization");
 
@@ -199,13 +199,10 @@ Tensor from_flatbuffer(const ttnn::flatbuffer::Tensor* fb_tensor, tt::stl::Span<
                 tt::tt_metal::HostBuffer host_buffer = create_host_buffer_from_bytes(size, spec);
                 TT_FATAL(offset + size <= tensor_data.size(), "Tensor data out of bounds for shard {}", i);
                 std::memcpy(static_cast<void*>(host_buffer.view_bytes().data()), tensor_data.data() + offset, size);
-
-                TT_FATAL(shard->mesh_coordinate() != nullptr, "Mesh coordinate is required for each shard");
-                const auto coord = from_flatbuffer(shard->mesh_coordinate());
-                distributed_buffer.emplace_shard(coord, [&host_buffer]() { return std::move(host_buffer); });
+                buffers.push_back(std::move(host_buffer));
             }
 
-            tt::tt_metal::MultiDeviceHostStorage multi_device_storage{std::move(distributed_buffer)};
+            tt::tt_metal::MultiDeviceHostStorage multi_device_storage{std::move(buffers)};
 
             return Tensor(std::move(multi_device_storage), spec, strategy);
         }
diff --git a/ttnn/core/tensor/serialization.cpp b/ttnn/core/tensor/serialization.cpp
index 471c0e7fc1..55b3d680d6 100644
--- a/ttnn/core/tensor/serialization.cpp
+++ b/ttnn/core/tensor/serialization.cpp
@@ -13,9 +13,6 @@
 
 #include <tt_stl/overloaded.hpp>
 
-#include "distributed/distributed_tensor_config.hpp"
-#include "tensor/tensor_spec.hpp"
-#include "tt-metalium/mesh_coord.hpp"
 #include "ttnn/tensor/host_buffer/functions.hpp"
 #include "ttnn/tensor/storage.hpp"
 #include "ttnn/tensor/tensor_utils.hpp"
@@ -120,21 +117,18 @@ void dump_multi_device_host_storage(
     const MultiDeviceHostStorage& storage,
     const DistributedTensorConfig& strategy,
     const TensorSpec& tensor_spec) {
-    std::vector<HostBuffer> buffers;
-    storage.distributed_buffer().apply([&](const HostBuffer& shard) { buffers.push_back(shard); });
-
-    uint64_t num_buffers = buffers.size();
+    uint64_t num_buffers = storage.num_buffers();
     safe_fwrite(&num_buffers, sizeof(num_buffers), 1, output_file);
 
     // Use the user-specified strategy which defines how it gets distributed when mapped onto multi-device
     safe_fwrite(&strategy, sizeof(strategy), 1, output_file);
 
     if (std::holds_alternative<ReplicateTensor>(strategy)) {
-        dump_host_storage(output_file, buffers.front(), tensor_spec.data_type());
+        dump_host_storage(output_file, storage.get_buffer(0), tensor_spec.data_type());
         dump_tensor_spec(tensor_spec, output_file);
     } else {
         for (int i = 0; i < num_buffers; i++) {
-            dump_host_storage(output_file, buffers[i], tensor_spec.data_type());
+            dump_host_storage(output_file, storage.get_buffer(i), tensor_spec.data_type());
         }
         for (int i = 0; i < num_buffers; i++) {
             dump_tensor_spec(tensor_spec, output_file);
@@ -167,9 +161,7 @@ DistributedStorage load_multi_device_host_storage(
     safe_fread(&strategy, sizeof(strategy), 1, input_file);
 
     std::vector<HostBuffer> buffers;
-    // Tensor spec was serialized, but now TTNN enforces uniform tensor specs.
-    // Load the spec without using it, to correctly read the file.
-    auto ignore_spec = [](const TensorSpec&) {};
+    std::vector<ttnn::TensorSpec> specs;
     if (std::holds_alternative<ReplicateTensor>(strategy)) {
         uint64_t size = 0;
         safe_fread(&size, sizeof(size), 1, input_file);
@@ -177,11 +169,13 @@ DistributedStorage load_multi_device_host_storage(
         safe_fread(data.data(), sizeof(T) * size, 1, input_file);
         HostBuffer buffer = HostBuffer(std::move(data));
         buffers.push_back(std::move(buffer));
-        ignore_spec(load_tensor_spec(input_file));
+        auto spec = load_tensor_spec(input_file);
+        specs.push_back(spec);
 
         auto num_devices = mesh_device ? mesh_device->num_devices() : 1;
         for (std::size_t i = 1; i < num_devices; ++i) {
             buffers.push_back(buffers[0]);
+            specs.push_back(spec);
         }
 
     } else {
@@ -194,7 +188,7 @@ DistributedStorage load_multi_device_host_storage(
             buffers.push_back(std::move(buffer));
         }
         for (std::size_t i = 0; i < num_buffers; ++i) {
-            ignore_spec(load_tensor_spec(input_file));
+            specs.push_back(load_tensor_spec(input_file));
         }
     }
 
@@ -392,11 +386,10 @@ void dump_tensor_flatbuffer(const std::string& file_name, const Tensor& tensor)
                 TT_THROW("Device storage isn't supported in flatbuffer serialization");
             },
             [&output_file](const MultiDeviceHostStorage& storage) {
-                for (const auto& shard : storage.distributed_buffer().shard_coords()) {
-                    if (auto buffer = storage.distributed_buffer().get_shard(shard); buffer.has_value()) {
-                        auto buffer_view = buffer->view_bytes();
-                        safe_fwrite(buffer_view.data(), buffer_view.size(), 1, output_file);
-                    }
+                for (std::size_t i = 0; i < storage.num_buffers(); i++) {
+                    const auto& buffer = storage.get_buffer(i);
+                    auto buffer_view = buffer.view_bytes();
+                    safe_fwrite(buffer_view.data(), buffer_view.size(), 1, output_file);
                 }
             }},
         cpu_tensor.storage());
diff --git a/ttnn/core/tensor/storage.cpp b/ttnn/core/tensor/storage.cpp
index 1c0cfb4532..20a21bd1de 100644
--- a/ttnn/core/tensor/storage.cpp
+++ b/ttnn/core/tensor/storage.cpp
@@ -3,7 +3,6 @@
 // SPDX-License-Identifier: Apache-2.0
 
 #include <algorithm>
-#include <vector>
 
 #include "tt-metalium/mesh_coord.hpp"
 
diff --git a/ttnn/core/tensor/tensor_attributes.cpp b/ttnn/core/tensor/tensor_attributes.cpp
index 61668ade3b..ad9292f2af 100644
--- a/ttnn/core/tensor/tensor_attributes.cpp
+++ b/ttnn/core/tensor/tensor_attributes.cpp
@@ -32,4 +32,38 @@ const DistributedTensorConfig& TensorAttributes::get_distributed_tensor_config()
     return distributed_tensor_config_;
 }
 
+std::vector<distributed::MeshCoordinate> TensorAttributes::determine_distribution(
+    const distributed::MeshShape& mesh_shape) const {
+    const auto coord_range = [this, &mesh_shape]() {
+        if (auto* shard2d_strategy = std::get_if<ShardTensor2D>(&distributed_tensor_config_)) {
+            distributed::MeshShape distribution_shape(shard2d_strategy->shard_mesh.y, shard2d_strategy->shard_mesh.x);
+            return distributed::MeshCoordinateRange(distribution_shape);
+        } else {
+            return distributed::MeshCoordinateRange(mesh_shape);
+        }
+    }();
+
+    const int num_shards = std::visit(
+        tt::stl::overloaded{
+            [&mesh_shape](const HostStorage&) { return mesh_shape.mesh_size(); },
+            [&mesh_shape](const DeviceStorage& s) { return s.coords.size(); },
+            [&mesh_shape](const MultiDeviceHostStorage& s) { return s.num_buffers(); },
+        },
+        storage_);
+
+    TT_FATAL(
+        num_shards <= mesh_shape.mesh_size(),
+        "Number of shards {} exceeds the mesh size {}",
+        num_shards,
+        mesh_shape.mesh_size());
+
+    std::vector<distributed::MeshCoordinate> coords;
+    coords.reserve(num_shards);
+    auto coord_it = coord_range.begin();
+    for (int i = 0; i < num_shards; ++coord_it, ++i) {
+        coords.push_back(*coord_it);
+    }
+    return coords;
+}
+
 }  // namespace tt::tt_metal
diff --git a/ttnn/core/tensor/tensor_impl.cpp b/ttnn/core/tensor/tensor_impl.cpp
index 28d2998fb1..7b4e9c1117 100644
--- a/ttnn/core/tensor/tensor_impl.cpp
+++ b/ttnn/core/tensor/tensor_impl.cpp
@@ -34,7 +34,7 @@ namespace {
 
 // Threshold for switch for mmap-based allocations to regular allocations.
 // Determined empirically using a microbenchmark; see https://github.com/tenstorrent/tt-metal/pull/22959 for details.
-constexpr size_t kMmapThresholdBytes = 1 << 20;
+constexpr size_t kMmapThresholdBytes = 1 << 20;  // 1MB
 
 // Allocates memory on the host in batch; using either mmap for large allocations or std::vector for small allocations.
 using SharedMemoryPtr = std::shared_ptr<void>;
@@ -43,7 +43,7 @@ SharedMemoryPtr allocate_host_data(size_t size_bytes) {
         ZoneScopedN("AllocateBufferMmap");
         void* ptr = mmap(nullptr, size_bytes, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
         TT_FATAL(ptr != MAP_FAILED, "Failed to allocate {} bytes of memory", size_bytes);
-        return SharedMemoryPtr(ptr, [size_bytes](void* p) { madvise(p, size_bytes, MADV_FREE); });
+        return SharedMemoryPtr(ptr, [size_bytes](void* p) { munmap(p, size_bytes); });
     } else {
         auto vec = std::make_shared<std::vector<std::byte>>(size_bytes);
         return SharedMemoryPtr(vec, vec->data());
@@ -724,17 +724,16 @@ Tensor to_device<bfloat8_b>(
     return to_device<uint32_t>(tensor, target_device, memory_config, cq_id);
 }
 
-namespace {
-
+template <typename T>
 DeviceStorage replicate_to_mesh_buffer(
     const HostStorage& storage,
     distributed::MeshDevice* mesh_device,
     const std::shared_ptr<distributed::MeshBuffer>& mesh_buffer,
     const TensorSpec& tensor_spec,
     ttnn::QueueId cq_id) {
-    auto data_to_write = storage.buffer.view_bytes();
+    auto data_to_write = host_buffer::get_as<T>(storage.buffer);
     const auto expected_packed_buffer_size_bytes = tensor_spec.compute_packed_buffer_size_bytes();
-    const auto input_size_bytes = data_to_write.size();
+    const auto input_size_bytes = data_to_write.size() * sizeof(T);
     TT_FATAL(
         input_size_bytes == expected_packed_buffer_size_bytes,
         "Host data with total size {}B does not match expected size {}B of device buffer!",
@@ -752,22 +751,45 @@ DeviceStorage replicate_to_mesh_buffer(
     return DeviceStorage(mesh_buffer, std::move(coords));
 }
 
+template <typename T>
 DeviceStorage shard_to_mesh_buffer(
-    const DistributedHostBuffer& distributed_host_buffer,
+    const MultiDeviceHostStorage& storage,
+    distributed::MeshDevice* mesh_device,
     const std::shared_ptr<distributed::MeshBuffer>& mesh_buffer,
+    const TensorSpec& tensor_spec,
+    const std::vector<distributed::MeshCoordinate>& coords,
     ttnn::QueueId cq_id) {
-    mesh_buffer->device()->mesh_command_queue(*cq_id).enqueue_write(
-        mesh_buffer, distributed_host_buffer, /*blocking=*/false);
-    std::vector<distributed::MeshCoordinate> coords;
-    coords.reserve(distributed_host_buffer.shard_coords().size());
-    std::copy(
-        distributed_host_buffer.shard_coords().begin(),
-        distributed_host_buffer.shard_coords().end(),
-        std::back_inserter(coords));
-    return DeviceStorage(mesh_buffer, std::move(coords));
-}
+    const auto& mesh_shape = mesh_device->shape();
+    TT_FATAL(
+        coords.size() == storage.num_buffers(),
+        "Number of shards {} does not match number of buffers {}",
+        coords.size(),
+        storage.num_buffers());
 
-}  // namespace
+    std::vector<distributed::MeshCommandQueue::ShardDataTransfer> shard_data_transfers;
+    shard_data_transfers.reserve(storage.num_buffers());
+
+    const auto expected_packed_buffer_size_bytes = tensor_spec.compute_packed_buffer_size_bytes();
+    for (int i = 0; i < storage.num_buffers(); ++i) {
+        const auto& shard_host_buffer = storage.get_buffer(i);
+
+        auto data_to_write = host_buffer::get_as<T>(shard_host_buffer);
+        const auto input_size_bytes = data_to_write.size() * sizeof(T);
+        TT_FATAL(
+            input_size_bytes == expected_packed_buffer_size_bytes,
+            "Host data with total size {}B does not match expected size {}B of device buffer!",
+            input_size_bytes,
+            expected_packed_buffer_size_bytes);
+        shard_data_transfers.push_back(distributed::MeshCommandQueue::ShardDataTransfer{
+            .shard_coord = coords[i],
+            .host_data = const_cast<void*>(reinterpret_cast<const void*>(data_to_write.data())),
+            .region = BufferRegion(0, input_size_bytes)});
+    }
+
+    mesh_device->mesh_command_queue(*cq_id).enqueue_write_shards(mesh_buffer, shard_data_transfers, /*blocking=*/false);
+
+    return DeviceStorage(std::move(mesh_buffer), coords);
+}
 
 template <typename T>
 DeviceStorage to_device_mesh_buffer(
@@ -780,46 +802,13 @@ DeviceStorage to_device_mesh_buffer(
         tt::stl::overloaded{
             [&mesh_buffer, &tensor_spec, cq_id](const HostStorage& storage) {
                 // Replicate data across devices in a mesh.
-                return replicate_to_mesh_buffer(storage, mesh_buffer->device(), mesh_buffer, tensor_spec, cq_id);
+                return replicate_to_mesh_buffer<T>(storage, mesh_buffer->device(), mesh_buffer, tensor_spec, cq_id);
             },
             [&mesh_buffer, &tensor_spec, cq_id, &host_tensor_attributes](const MultiDeviceHostStorage& storage) {
                 // Shard multi device host shards across devices in a mesh.
-                if (storage.distributed_buffer().shape() == mesh_buffer->device()->shape()) {
-                    return shard_to_mesh_buffer(storage.distributed_buffer(), mesh_buffer, cq_id);
-                } else {
-                    // Reshape distributed host buffer.
-                    // TODO: #22169 - there are 2 reasons for this code path - legacy serialization path that stored
-                    // multi device host tensors without the necessary metadata, and `aggregate_as_tensor` calls that
-                    // similarly lack the metadata to properly distribute the shards across the mesh.
-                    auto* mesh_device = mesh_buffer->device();
-
-                    TT_FATAL(
-                        storage.distributed_buffer().shape().mesh_size() <= mesh_device->shape().mesh_size(),
-                        "Distributed host buffer has more shards than the mesh device");
-
-                    auto dst_distributed_host_buffer = DistributedHostBuffer::create(mesh_device->shape());
-
-                    const auto dst_range = [mesh_device, &host_tensor_attributes]() {
-                        if (auto* shard2d_strategy =
-                                std::get_if<ShardTensor2D>(&host_tensor_attributes.get_distributed_tensor_config())) {
-                            distributed::MeshShape distribution_shape(
-                                shard2d_strategy->shard_mesh.y, shard2d_strategy->shard_mesh.x);
-                            return distributed::MeshCoordinateRange(distribution_shape);
-                        } else {
-                            return distributed::MeshCoordinateRange(mesh_device->shape());
-                        }
-                    }();
-
-                    std::vector<HostBuffer> shards;
-                    storage.distributed_buffer().apply([&](const HostBuffer& shard) { shards.push_back(shard); });
-
-                    auto dst_coord_it = dst_range.begin();
-                    for (int i = 0; i < shards.size(); ++i, ++dst_coord_it) {
-                        dst_distributed_host_buffer.emplace_shard(
-                            *dst_coord_it, [&shards, i]() { return std::move(shards[i]); });
-                    }
-                    return shard_to_mesh_buffer(dst_distributed_host_buffer, mesh_buffer, cq_id);
-                }
+                auto* mesh_device = mesh_buffer->device();
+                auto coords = host_tensor_attributes.determine_distribution(mesh_device->shape());
+                return shard_to_mesh_buffer<T>(storage, mesh_device, mesh_buffer, tensor_spec, coords, cq_id);
             },
             [](const auto& s) -> DeviceStorage { TT_THROW("Unexpected storage type {}", tt::stl::get_type_name(s)); }},
         host_storage);
diff --git a/ttnn/core/tensor/tensor_ops.cpp b/ttnn/core/tensor/tensor_ops.cpp
index f4fd3b9f43..ba10cca4c0 100644
--- a/ttnn/core/tensor/tensor_ops.cpp
+++ b/ttnn/core/tensor/tensor_ops.cpp
@@ -82,7 +82,7 @@ Tensor tensor_cpu(const Tensor& input_tensor, bool blocking, QueueId cq_id) {
 Tensor tensor_to_layout(const Tensor& input_tensor, Layout target_layout, IDevice* worker) {
     ZoneScoped;
     GraphTracker::instance().track_function_start("Tensor::to_layout", input_tensor, target_layout, worker);
-    TT_FATAL(
+    TT_ASSERT(
         input_tensor.storage_type() != StorageType::DEVICE, "Bring tensor to host before converting to target layout");
     Tensor output = tensor_impl::to_layout_wrapper(input_tensor, target_layout);
     output = tt::tt_metal::set_tensor_id(output);
@@ -90,6 +90,50 @@ Tensor tensor_to_layout(const Tensor& input_tensor, Layout target_layout, IDevic
     return output;
 }
 
+Tensor tensor_to_layout(const Tensor& input_tensor, Layout target_layout, distributed::MeshDevice* mesh_device) {
+    ZoneScoped;
+    TT_FATAL(
+        is_cpu_tensor(input_tensor) || is_multi_device_host_tensor(input_tensor),
+        "to(layout) must be called on host tensors with MULTI_DEVICE_HOST_STORAGE when multiple "
+        "workers "
+        "are specified");
+
+    GraphTracker::instance().track_function_start("Tensor::to_layout", input_tensor, target_layout, mesh_device);
+    if (mesh_device) {
+        // Mesh Device provided - have a handle to the thread-pool
+        Tensor tensor_modified_layout = std::visit(
+            tt::stl::overloaded{
+                [&](const HostStorage& s) { return tensor_impl::to_layout_wrapper(input_tensor, target_layout); },
+                [&](const MultiDeviceHostStorage& s) {
+                    // TODO: #22045 - Move to `transform` and use OMP parallel for.
+                    std::vector<Tensor> shards(s.num_buffers());
+                    for (std::size_t shard_idx = 0; shard_idx < s.num_buffers(); ++shard_idx) {
+                        // Multi-Thread Host tilization of shards.
+                        mesh_device->enqueue_to_thread_pool([shard_idx, &s, &shards, target_layout, &input_tensor]() {
+                            ZoneScopedN("HostTilize");
+                            Tensor shard(s.get_buffer(shard_idx), input_tensor.tensor_spec());
+                            shards[shard_idx] = tensor_impl::to_layout_wrapper(shard, target_layout);
+                        });
+                    }
+                    mesh_device->wait_for_thread_pool();
+                    return ttnn::distributed::aggregate_as_tensor(shards, input_tensor.distributed_tensor_config());
+                },
+                [&](const DeviceStorage& s) -> Tensor { TT_THROW("Unexpected storage type"); },
+            },
+            input_tensor.storage());
+
+        tensor_modified_layout = tt::tt_metal::set_tensor_id(tensor_modified_layout);
+        GraphTracker::instance().track_function_end(tensor_modified_layout);
+        return tensor_modified_layout;
+    }
+
+    // Running without worker threads (non-async)
+    auto output = tensor_impl::to_layout_wrapper(input_tensor, target_layout);
+    output = tt::tt_metal::set_tensor_id(output);
+    GraphTracker::instance().track_function_end(output);
+    return output;
+}
+
 void tensor_print(const Tensor& input_tensor) {
     GraphTracker::instance().track_function_start("Tensor::print", input_tensor);
     std::cout << input_tensor.write_to_string() << std::endl;
diff --git a/ttnn/core/tensor/tensor_utils.cpp b/ttnn/core/tensor/tensor_utils.cpp
index 5cc5b9e1fe..a141d2688e 100644
--- a/ttnn/core/tensor/tensor_utils.cpp
+++ b/ttnn/core/tensor/tensor_utils.cpp
@@ -6,7 +6,6 @@
 
 #include <tt_stl/overloaded.hpp>
 
-#include "tt-metalium/distributed_host_buffer.hpp"
 #include "ttnn/distributed/api.hpp"
 #include "ttnn/tensor/host_buffer/functions.hpp"
 #include "ttnn/tensor/storage.hpp"
@@ -103,42 +102,30 @@ bool is_device_tensor(const Tensor& tensor) { return tensor.storage_type() == St
 
 Tensor transform(const Tensor& tensor, const std::function<Tensor(const Tensor&)>& transform_func) {
     TT_FATAL(is_multi_device_host_tensor(tensor), "transform only supports multi-device host tensors");
-    // TODO: #15840 - Push this down to OPs, so that instead of transforming the multi-device shards as `Tensor`, we
-    // operate on buffers directly. OPs code should not differentiate between host and multi-device host storage.
-    std::optional<TensorSpec> transformed_spec;
-    std::mutex transformed_buffer_mutex;
-    DistributedHostBuffer transformed_buffer =
-        std::get<MultiDeviceHostStorage>(tensor.storage())
-            .distributed_buffer()
-            .transform(
-                [&](const HostBuffer& buffer) {
-                    auto transformed_tensor = transform_func(Tensor(buffer, tensor.get_tensor_spec()));
-                    auto* host_storage = std::get_if<HostStorage>(&transformed_tensor.get_storage());
-                    TT_FATAL(host_storage != nullptr, "transform function must return a host tensor");
-                    {
-                        std::lock_guard<std::mutex> lock(transformed_buffer_mutex);
-                        if (transformed_spec.has_value()) {
-                            TT_FATAL(
-                                *transformed_spec == transformed_tensor.get_tensor_spec(),
-                                "All shards must have the same spec");
-                        } else {
-                            transformed_spec = transformed_tensor.get_tensor_spec();
-                        }
-                    }
-                    return host_storage->buffer;
-                },
-                DistributedHostBuffer::ProcessShardExecutionPolicy::PARALLEL);
-    return Tensor(
-        MultiDeviceHostStorage(std::move(transformed_buffer)),
-        transformed_spec.value_or(tensor.get_tensor_spec()),
-        tensor.get_distributed_tensor_config());
+    const auto& storage = std::get<MultiDeviceHostStorage>(tensor.storage());
+
+    std::vector<TensorSpec> transformed_specs;
+    std::vector<HostBuffer> transformed_buffers;
+    transformed_buffers.reserve(storage.num_buffers());
+    transformed_specs.reserve(storage.num_buffers());
+    for (size_t i = 0; i < storage.num_buffers(); i++) {
+        Tensor transformed_tensor_shard = transform_func(Tensor(storage.get_buffer(i), tensor.tensor_spec()));
+        transformed_specs.push_back(transformed_tensor_shard.tensor_spec());
+        auto* host_storage = std::get_if<HostStorage>(&transformed_tensor_shard.storage());
+        TT_FATAL(host_storage != nullptr, "transform function must return a host tensor");
+        transformed_buffers.push_back(std::move(host_storage->buffer));
+    }
+    TensorSpec reference_spec = transformed_specs.front();
+    MultiDeviceHostStorage transformed_storage(std::move(transformed_buffers));
+    return Tensor(std::move(transformed_storage), reference_spec, tensor.distributed_tensor_config());
 }
 
 void apply(const Tensor& tensor, const std::function<void(const Tensor&)>& callable) {
     TT_FATAL(is_multi_device_host_tensor(tensor), "apply only supports multi-device host tensors");
-    std::get<MultiDeviceHostStorage>(tensor.storage()).distributed_buffer().apply([&](const HostBuffer& buffer) {
-        callable(Tensor(buffer, tensor.get_tensor_spec()));
-    });
+    const auto& storage = std::get<MultiDeviceHostStorage>(tensor.storage());
+    for (size_t i = 0; i < storage.num_buffers(); i++) {
+        callable(Tensor(storage.get_buffer(i), tensor.tensor_spec()));
+    }
 }
 
 ShardDivisionSpec compute_shard_division_spec(const Shape2D& shape, const Shape2D& shard_shape) {
diff --git a/ttnn/cpp/ttnn-pybind/fabric.cpp b/ttnn/cpp/ttnn-pybind/fabric.cpp
index ffc46342ce..1c11ac8c90 100644
--- a/ttnn/cpp/ttnn-pybind/fabric.cpp
+++ b/ttnn/cpp/ttnn-pybind/fabric.cpp
@@ -20,11 +20,7 @@ void py_bind_fabric_api(py::module& module) {
         .value("FABRIC_2D", tt::tt_metal::FabricConfig::FABRIC_2D)
         .value("CUSTOM", tt::tt_metal::FabricConfig::CUSTOM);  // DISABLED = 0, FABRIC_1D = 1, FABRIC_2D = 2, CUSTOM = 4
 
-    module.def(
-        "set_fabric_config",
-        &tt::tt_metal::detail::SetFabricConfig,
-        py::arg("config"),
-        py::arg("num_planes") = std::nullopt);
+    module.def("initialize_fabric_config", &tt::tt_metal::detail::InitializeFabricConfig, py::arg("config"));
 }
 
 }  // namespace ttnn::fabric
diff --git a/ttnn/cpp/ttnn/deprecated/CMakeLists.txt b/ttnn/cpp/ttnn/deprecated/CMakeLists.txt
deleted file mode 100644
index 6bf8b17de0..0000000000
--- a/ttnn/cpp/ttnn/deprecated/CMakeLists.txt
+++ /dev/null
@@ -1,27 +0,0 @@
-add_library(ttnn_deprecated INTERFACE)
-
-set_target_properties(
-    ttnn_deprecated
-    PROPERTIES
-        VERIFY_INTERFACE_HEADER_SETS
-            FALSE
-)
-
-file(GLOB_RECURSE kernels tt_dnn/kernels/*)
-target_sources(
-    ttnn_deprecated
-    INTERFACE
-        FILE_SET kernels
-        TYPE HEADERS
-        BASE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}
-        FILES ${kernels}
-)
-
-install(
-    TARGETS
-        ttnn_deprecated
-    FILE_SET
-    kernels
-        DESTINATION ${CMAKE_INSTALL_LIBEXECDIR}/tt-metalium/ttnn/cpp/ttnn/deprecated
-        COMPONENT ttnn-runtime
-)
diff --git a/ttnn/cpp/ttnn/operations/bernoulli/CMakeLists.txt b/ttnn/cpp/ttnn/operations/bernoulli/CMakeLists.txt
index bde593e686..020d9155ca 100644
--- a/ttnn/cpp/ttnn/operations/bernoulli/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/bernoulli/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_bernoulli ${LIB_TYPE})
-add_library(TTNN::Ops::Bernoulli ALIAS ttnn_op_bernoulli)
+add_library(TT::NN::Ops::Bernoulli ALIAS ttnn_op_bernoulli)
 
 target_precompile_headers(ttnn_op_bernoulli REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_bernoulli)
@@ -17,7 +17,7 @@ target_link_libraries(
     ttnn_op_bernoulli
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_bernoulli LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/ccl/CMakeLists.txt b/ttnn/cpp/ttnn/operations/ccl/CMakeLists.txt
index c5eec351a8..6c447e875b 100644
--- a/ttnn/cpp/ttnn/operations/ccl/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/ccl/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_ccl ${LIB_TYPE})
-add_library(TTNN::Ops::CCL ALIAS ttnn_op_ccl)
+add_library(TT::NN::Ops::CCL ALIAS ttnn_op_ccl)
 
 target_precompile_headers(ttnn_op_ccl REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_ccl)
@@ -38,7 +38,7 @@ target_link_libraries(
     ttnn_op_ccl
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_ccl LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/ccl/ccl_common.cpp b/ttnn/cpp/ttnn/operations/ccl/ccl_common.cpp
index c120dd79c0..35f1a78cdf 100644
--- a/ttnn/cpp/ttnn/operations/ccl/ccl_common.cpp
+++ b/ttnn/cpp/ttnn/operations/ccl/ccl_common.cpp
@@ -26,19 +26,19 @@ void SyncModeSpec::add_signal(uint32_t sem_id, uint32_t wait_count) {
 
 LineTopology::LineTopology(size_t line_size, size_t line_index) : _line_size(line_size), _line_index(line_index) {}
 
-bool LineTopology::is_first_device_in_line(ttnn::ccl::LineDirection direction) const {
-    if (direction == ttnn::ccl::LineDirection::FORWARD) {
+bool LineTopology::is_first_device_in_line(ttnn::ccl::EdmLineFabricOpInterface::Direction direction) const {
+    if (direction == ttnn::ccl::EdmLineFabricOpInterface::Direction::FORWARD) {
         return _line_index == 0;
     } else {
-        TT_ASSERT(direction == ttnn::ccl::LineDirection::BACKWARD);
+        TT_ASSERT(direction == ttnn::ccl::EdmLineFabricOpInterface::Direction::BACKWARD);
         return _line_index == _line_size - 1;
     }
 }
-bool LineTopology::is_last_device_in_line(ttnn::ccl::LineDirection direction) const {
-    if (direction == ttnn::ccl::LineDirection::BACKWARD) {
+bool LineTopology::is_last_device_in_line(ttnn::ccl::EdmLineFabricOpInterface::Direction direction) const {
+    if (direction == ttnn::ccl::EdmLineFabricOpInterface::Direction::BACKWARD) {
         return _line_index == 0;
     } else {
-        TT_ASSERT(direction == ttnn::ccl::LineDirection::FORWARD);
+        TT_ASSERT(direction == ttnn::ccl::EdmLineFabricOpInterface::Direction::FORWARD);
         return _line_index == _line_size - 1;
     }
 }
@@ -49,8 +49,8 @@ size_t LineTopology::line_size() const { return _line_size; }
 
 size_t LineTopology::line_index() const { return _line_index; }
 
-size_t LineTopology::get_distance_to_end_of_line(ttnn::ccl::LineDirection direction) const {
-    if (direction == ttnn::ccl::LineDirection::FORWARD) {
+size_t LineTopology::get_distance_to_end_of_line(ttnn::ccl::EdmLineFabricOpInterface::Direction direction) const {
+    if (direction == ttnn::ccl::EdmLineFabricOpInterface::Direction::FORWARD) {
         return (_line_size - _line_index) - 1;
     } else {
         return _line_index;
@@ -1496,8 +1496,10 @@ std::tuple<size_t, size_t, bool> get_forward_backward_configuration(
     size_t num_targets_backward = 0;
     if (topology == Topology::Linear) {
         LineTopology line_topology(ring_size, ring_index);
-        num_targets_forward = line_topology.get_distance_to_end_of_line(ttnn::ccl::LineDirection::FORWARD);
-        num_targets_backward = line_topology.get_distance_to_end_of_line(ttnn::ccl::LineDirection::BACKWARD);
+        num_targets_forward =
+            line_topology.get_distance_to_end_of_line(ttnn::ccl::EdmLineFabricOpInterface::Direction::FORWARD);
+        num_targets_backward =
+            line_topology.get_distance_to_end_of_line(ttnn::ccl::EdmLineFabricOpInterface::Direction::BACKWARD);
     } else if (topology == ccl::Topology::Ring) {
         // TODO: Commonize
         num_targets_forward = tt::div_up(ring_size - 1, 2);
diff --git a/ttnn/cpp/ttnn/operations/ccl/ccl_common.hpp b/ttnn/cpp/ttnn/operations/ccl/ccl_common.hpp
index fa2e7b3e1c..b7baaadde8 100644
--- a/ttnn/cpp/ttnn/operations/ccl/ccl_common.hpp
+++ b/ttnn/cpp/ttnn/operations/ccl/ccl_common.hpp
@@ -16,16 +16,12 @@
 #include <tt-metalium/program.hpp>
 #include "ttnn/tensor/types.hpp"
 #include <tt-metalium/erisc_datamover_builder.hpp>
+#include "erisc_datamover_builder_helper.hpp"
 #include "cpp/ttnn/operations/ccl/common/host/ccl_command_stream_builders.hpp"
 
 namespace ttnn {
 namespace ccl {
 
-enum class LineDirection: uint8_t {
-    FORWARD,
-    BACKWARD,
-};
-
 // Creates a mesh workload by calling the `create_program` function for each coordinate in the `tensor_coords` set.
 tt::tt_metal::operation::MeshWorkloadWithCallbacks create_mesh_workload_from_programs(
     const ttnn::MeshCoordinateRangeSet& tensor_coords,
@@ -74,8 +70,8 @@ class LineTopology {
         size_t line_size,
         size_t line_index);
 
-    bool is_first_device_in_line(ttnn::ccl::LineDirection direction) const;
-    bool is_last_device_in_line(ttnn::ccl::LineDirection direction) const;
+    bool is_first_device_in_line(ttnn::ccl::EdmLineFabricOpInterface::Direction direction) const;
+    bool is_last_device_in_line(ttnn::ccl::EdmLineFabricOpInterface::Direction direction) const;
 
     bool is_at_end_of_line() const;
 
@@ -83,7 +79,7 @@ class LineTopology {
 
     size_t line_index() const;
 
-    size_t get_distance_to_end_of_line(ttnn::ccl::LineDirection direction) const;
+    size_t get_distance_to_end_of_line(ttnn::ccl::EdmLineFabricOpInterface::Direction direction) const;
 
     ttnn::ccl::Topology topology() const;
 
@@ -387,8 +383,8 @@ class RingReduceScatterBaseTensorSlicer : public LegacyCclTensorSlicer {
             wrapped);
     }
 
-    [[deprecated("deprecated code path for reduce scatter. Use nerw get_worker_slice API instead")]] void increment(
-        uint32_t num_pages) override {
+    [[deprecated("deprecated code path for reduce scatter. Use nerw get_worker_slice API instead")]]
+    virtual void increment(uint32_t num_pages) override {
         TT_THROW("deprecated code path for ");
     }
 
@@ -541,7 +537,7 @@ class InterleavedRingAllGatherTensorSlicer : public LegacyCclTensorSlicer {
         this->output_start_addr_offset = slice_idx /*ring_index*/ * output_addr_offset;
     }
 
-    void increment(uint32_t num_pages) override {
+    virtual void increment(uint32_t num_pages) override {
         if (num_pages /*pages_per_worker*/ > 0) {
             if (row_major) {
                 uint32_t num_rows_shifted = row_idx + num_pages /*pages_per_worker*/;
diff --git a/ttnn/cpp/ttnn/operations/ccl/common/host/ccl_worker_builder.hpp b/ttnn/cpp/ttnn/operations/ccl/common/host/ccl_worker_builder.hpp
index e81108ef1c..46dc6b4909 100644
--- a/ttnn/cpp/ttnn/operations/ccl/common/host/ccl_worker_builder.hpp
+++ b/ttnn/cpp/ttnn/operations/ccl/common/host/ccl_worker_builder.hpp
@@ -9,7 +9,6 @@
 #include "ttnn/operations/ccl/common/uops/ccl_command.hpp"
 #include "ttnn/operations/ccl/common/uops/ccl_host_commands.hpp"
 #include "cpp/ttnn/operations/ccl/common/host/command_backend_runtime_args_overrider.hpp"
-#include "cpp/ttnn/operations/ccl/erisc_datamover_builder_helper.hpp"
 
 #include <cstdint>
 #include <optional>
diff --git a/ttnn/cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/kernel_writers.hpp b/ttnn/cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/kernel_writers.hpp
index 59ef7d8274..2bd66ae9e5 100644
--- a/ttnn/cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/kernel_writers.hpp
+++ b/ttnn/cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/kernel_writers.hpp
@@ -7,8 +7,7 @@
 // CCL Kernel common includes
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
 #include <tt-metalium/fabric_edm_packet_header.hpp>
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/command_interpreter_base.hpp"
 #include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/ccl_command_base.hpp"
 
diff --git a/tt_metal/fabric/hw/inc/noc_addr.h b/ttnn/cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp
similarity index 66%
rename from tt_metal/fabric/hw/inc/noc_addr.h
rename to ttnn/cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp
index 2d5d4e0d2b..e4988f9c97 100644
--- a/tt_metal/fabric/hw/inc/noc_addr.h
+++ b/ttnn/cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp
@@ -1,10 +1,10 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
+// SPDX-FileCopyrightText: Â© 2024 Tenstorrent Inc.
 //
 // SPDX-License-Identifier: Apache-2.0
 
 #pragma once
 
-#include "tt_metal/api/tt-metalium/fabric_edm_types.hpp"
+#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
 
 #include "dataflow_api.h"
 #include "noc_nonblocking_api.h"
@@ -14,13 +14,7 @@
 static constexpr size_t VIRTUAL_COORDS_START_X = 16;
 static constexpr size_t VIRTUAL_COORDS_START_Y = 16;
 FORCE_INLINE bool is_using_noc_coords(uint16_t noc_x, uint16_t noc_y) {
-#ifdef ARCH_WORMHOLE
     return noc_x < VIRTUAL_COORDS_START_X && noc_y < VIRTUAL_COORDS_START_Y;
-#elif defined(COORDINATE_VIRTUALIZATION_ENABLED) && COORDINATE_VIRTUALIZATION_ENABLED == 1
-    return false;
-#else
-    return true;
-#endif
 }
 
 FORCE_INLINE uint64_t
@@ -35,9 +29,10 @@ safe_get_noc_addr(uint8_t dest_noc_x, uint8_t dest_noc_y, uint32_t dest_bank_add
     return get_noc_addr(noc_x, noc_y, dest_bank_addr, noc_id);
 }
 // TODO: COMMONIZE WITH THE ONE IN `ccl_send_writer.cpp`
-FORCE_INLINE std::pair<tt::tt_fabric::WorkerXY, uint32_t> get_noc_address_components(uint64_t noc_addr) {
+FORCE_INLINE std::pair<ttnn::ccl::WorkerXY, uint32_t> get_noc_address_components(uint64_t noc_addr) {
     const size_t bank_addr = noc_addr & 0xFFFFFFFF;
-    const size_t noc_x = NOC_UNICAST_ADDR_X(noc_addr);
-    const size_t noc_y = NOC_UNICAST_ADDR_Y(noc_addr);
-    return {tt::tt_fabric::WorkerXY(noc_x, noc_y), bank_addr};
+    const size_t noc_x = (noc_addr >> NOC_ADDR_LOCAL_BITS) & ((1 << NOC_ADDR_NODE_ID_BITS) - 1);
+    const size_t noc_y =
+        (noc_addr >> (NOC_ADDR_LOCAL_BITS + NOC_ADDR_NODE_ID_BITS)) & ((1 << NOC_ADDR_NODE_ID_BITS) - 1);
+    return {ttnn::ccl::WorkerXY(noc_x, noc_y), bank_addr};
 }
diff --git a/ttnn/cpp/ttnn/operations/ccl/common/kernels/ccl_send_reader_two_input.cpp b/ttnn/cpp/ttnn/operations/ccl/common/kernels/ccl_send_reader_two_input.cpp
index e02fc041ee..fae8cf42d4 100644
--- a/ttnn/cpp/ttnn/operations/ccl/common/kernels/ccl_send_reader_two_input.cpp
+++ b/ttnn/cpp/ttnn/operations/ccl/common/kernels/ccl_send_reader_two_input.cpp
@@ -16,10 +16,10 @@
 
 #include "tt_metal/api/tt-metalium/fabric_edm_packet_header.hpp"
 #include "tt_metal/fabric/hw/inc/edm_fabric/edm_fabric_worker_adapters.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
 
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
 #include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/io_descriptors.hpp"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "api/ttnn/tensor/enum_types.hpp"
 #include <cstdint>
 #include <utility>
diff --git a/ttnn/cpp/ttnn/operations/ccl/common/types/sharding_common.hpp b/ttnn/cpp/ttnn/operations/ccl/common/types/sharding_common.hpp
index b53803d0be..76f292ec65 100644
--- a/ttnn/cpp/ttnn/operations/ccl/common/types/sharding_common.hpp
+++ b/ttnn/cpp/ttnn/operations/ccl/common/types/sharding_common.hpp
@@ -12,17 +12,12 @@ namespace shard_addr_gen_consts {
 
 enum class ContiguityType {
     // Indicates logical sharding placed padding between pages so no contiguous pages exist
-    L1_PADDING_BETWEEN_PAGES = 0,
+    PADDING_BETWEEN_PAGES = 0,
     // Indicates some padding exists in the rightmost shard since the pages did not divide evenly into shards
-    L1_PADDING_IN_RIGHTMOST_SHARD,
+    PADDING_IN_RIGHTMOST_SHARD,
     // Indicates no sharding based padding exists so all pages within the same shard are contiguous
     // This is useful for height sharded tensors as multiple rows of the tensor can be contiguous.
-    L1_NO_SHARD_PADDING,
-
-    // DRAM variants
-    DRAM_PADDING_BETWEEN_PAGES,
-    DRAM_PADDING_IN_RIGHTMOST_SHARD,
-    DRAM_NO_SHARD_PADDING,
+    NO_SHARD_PADDING,
 };
 
 }  // namespace shard_addr_gen_consts
diff --git a/ttnn/cpp/ttnn/operations/ccl/kernel_common/sharding_addrgen.hpp b/ttnn/cpp/ttnn/operations/ccl/kernel_common/sharding_addrgen.hpp
index ef780e729e..f630674b47 100644
--- a/ttnn/cpp/ttnn/operations/ccl/kernel_common/sharding_addrgen.hpp
+++ b/ttnn/cpp/ttnn/operations/ccl/kernel_common/sharding_addrgen.hpp
@@ -57,9 +57,7 @@ struct ShardCoordInfo get_width_sharded_coordinates(uint32_t page_num) {
     uint32_t w_offset = page_col - w_core_id * columns_per_shard;
     coord_info.core_num = w_core_id;
     coord_info.page_num = page_row * columns_per_shard + w_offset;
-    if constexpr (
-        contiguity != shard_addr_gen_consts::ContiguityType::L1_PADDING_BETWEEN_PAGES &&
-        contiguity != shard_addr_gen_consts::ContiguityType::DRAM_PADDING_BETWEEN_PAGES) {
+    if constexpr (contiguity != shard_addr_gen_consts::ContiguityType::PADDING_BETWEEN_PAGES) {
         uint32_t space_left_in_shard = columns_per_shard - w_offset;
         uint32_t space_left_in_tensor = total_pages_last_dim - page_col;
         coord_info.num_contiguous_pages =
@@ -77,13 +75,9 @@ struct ShardCoordInfo get_height_sharded_coordinates(uint32_t page_num) {
     constexpr uint32_t num_pages_per_core = total_pages_last_dim * rows_per_shard;
     coord_info.core_num = page_num / num_pages_per_core;
     coord_info.page_num = page_num - coord_info.core_num * num_pages_per_core;
-    if constexpr (
-        contiguity == shard_addr_gen_consts::ContiguityType::L1_PADDING_BETWEEN_PAGES ||
-        contiguity == shard_addr_gen_consts::ContiguityType::DRAM_PADDING_BETWEEN_PAGES) {
+    if constexpr (contiguity == shard_addr_gen_consts::ContiguityType::PADDING_BETWEEN_PAGES) {
         coord_info.num_contiguous_pages = 1;
-    } else if constexpr (
-        contiguity == shard_addr_gen_consts::ContiguityType::L1_PADDING_IN_RIGHTMOST_SHARD ||
-        contiguity == shard_addr_gen_consts::ContiguityType::DRAM_PADDING_IN_RIGHTMOST_SHARD) {
+    } else if constexpr (contiguity == shard_addr_gen_consts::ContiguityType::PADDING_IN_RIGHTMOST_SHARD) {
         coord_info.num_contiguous_pages = total_pages_last_dim - page_num % total_pages_last_dim;
     } else {
         coord_info.num_contiguous_pages = num_pages_per_core - coord_info.page_num;
@@ -113,9 +107,7 @@ experimental::shard_addr_gen_utils::ShardCoordInfo get_block_sharded_coordinates
     // Find the coord_info
     coord_info.core_num = w_core_id + h_core_id * cores_per_block_row;
     coord_info.page_num = w_offset + h_offset * columns_per_shard;
-    if constexpr (
-        contiguity != shard_addr_gen_consts::ContiguityType::L1_PADDING_BETWEEN_PAGES &&
-        contiguity != shard_addr_gen_consts::ContiguityType::DRAM_PADDING_BETWEEN_PAGES) {
+    if constexpr (contiguity != shard_addr_gen_consts::ContiguityType::PADDING_BETWEEN_PAGES) {
         uint32_t space_left_in_shard = columns_per_shard - w_offset;
         uint32_t space_left_in_tensor = total_pages_last_dim - page_col;
         coord_info.num_contiguous_pages =
@@ -194,24 +186,11 @@ struct ShardedAddrGen {
     FORCE_INLINE
     std::uint64_t get_sharded_addr(
         const uint32_t l1_addr, const uint32_t sharding_coordinates, const uint32_t noc = noc_index) const {
-        constexpr uint32_t SHIFT_BITS = 8;
-        constexpr uint32_t MASK = 0xFF;
-        constexpr shard_addr_gen_consts::ContiguityType contiguity = CONSTANT_ARGS.contiguity;
-        constexpr bool is_dram = contiguity == shard_addr_gen_consts::ContiguityType::DRAM_PADDING_BETWEEN_PAGES ||
-                                 contiguity == shard_addr_gen_consts::ContiguityType::DRAM_PADDING_IN_RIGHTMOST_SHARD ||
-                                 contiguity == shard_addr_gen_consts::ContiguityType::DRAM_NO_SHARD_PADDING;
-        if constexpr (is_dram) {
-            uint32_t bank_id = (sharding_coordinates >> SHIFT_BITS) & MASK;
-            uint32_t src_addr = l1_addr + bank_to_dram_offset[bank_id];
-            uint32_t src_noc_xy = dram_bank_to_noc_xy[noc][bank_id];
-            return ((uint64_t)(src_noc_xy) << NOC_ADDR_COORD_SHIFT) | src_addr;
-        } else {
-            // Extracts the X and Y value and using the l1 address gets the noc address
-            return NOC_XY_ADDR(
-                DYNAMIC_NOC_X(noc, ((sharding_coordinates >> SHIFT_BITS) & MASK)),
-                DYNAMIC_NOC_Y(noc, (sharding_coordinates & MASK)),
-                l1_addr);
-        }
+        // Extracts the X and Y value and using the l1 address gets the noc address
+        return NOC_XY_ADDR(
+            DYNAMIC_NOC_X(noc, ((sharding_coordinates >> 8) & 0xFF)),
+            DYNAMIC_NOC_Y(noc, (sharding_coordinates & 0xFF)),
+            l1_addr);
     }
 
     std::uint32_t get_sharded_l1_addr(const uint32_t core_page, const uint32_t offset = 0) const {
diff --git a/ttnn/cpp/ttnn/operations/ccl/sharding_addrgen_helper.cpp b/ttnn/cpp/ttnn/operations/ccl/sharding_addrgen_helper.cpp
index 8cd169b213..2c127d22ca 100644
--- a/ttnn/cpp/ttnn/operations/ccl/sharding_addrgen_helper.cpp
+++ b/ttnn/cpp/ttnn/operations/ccl/sharding_addrgen_helper.cpp
@@ -35,7 +35,6 @@ std::vector<CoreCoord> get_shard_cores(const tt::tt_metal::Tensor& t) {
          ((t.memory_config().memory_layout() == TensorMemoryLayout::WIDTH_SHARDED ||
            t.memory_config().memory_layout() == TensorMemoryLayout::BLOCK_SHARDED) &&
           shard_spec.orientation == ShardOrientation::COL_MAJOR));
-    bool is_dram = t.memory_config().is_dram();
     bool last = false;
     uint32_t held_value = 0;
     uint32_t concatenated_core = 0;
@@ -53,8 +52,7 @@ std::vector<CoreCoord> get_shard_cores(const tt::tt_metal::Tensor& t) {
                  x_index++) {
                 for (uint32_t y_index = core_ranges.at(cr).start_coord.y; y_index <= core_ranges.at(cr).end_coord.y;
                      y_index++) {
-                    CoreCoord noc_core = is_dram ? CoreCoord(x_index, y_index)
-                                                 : device->worker_core_from_logical_core(CoreCoord(x_index, y_index));
+                    CoreCoord noc_core = device->worker_core_from_logical_core(CoreCoord(x_index, y_index));
                     coordinates.push_back(noc_core);
                 }
             }
@@ -63,8 +61,7 @@ std::vector<CoreCoord> get_shard_cores(const tt::tt_metal::Tensor& t) {
                  y_index++) {
                 for (uint32_t x_index = core_ranges.at(cr).start_coord.x; x_index <= core_ranges.at(cr).end_coord.x;
                      x_index++) {
-                    CoreCoord noc_core = is_dram ? CoreCoord(x_index, y_index)
-                                                 : device->worker_core_from_logical_core(CoreCoord(x_index, y_index));
+                    CoreCoord noc_core = device->worker_core_from_logical_core(CoreCoord(x_index, y_index));
                     coordinates.push_back(noc_core);
                 }
             }
@@ -84,7 +81,6 @@ std::vector<uint32_t> generate_run_time_args(const tt::tt_metal::Tensor& t) {
          ((t.memory_config().memory_layout() == TensorMemoryLayout::WIDTH_SHARDED ||
            t.memory_config().memory_layout() == TensorMemoryLayout::BLOCK_SHARDED) &&
           shard_spec.orientation == ShardOrientation::COL_MAJOR));
-    bool is_dram = t.memory_config().is_dram();
     bool last = false;
     uint32_t held_value = 0;
     uint32_t concatenated_core = 0;
@@ -102,8 +98,7 @@ std::vector<uint32_t> generate_run_time_args(const tt::tt_metal::Tensor& t) {
                  x_index++) {
                 for (uint32_t y_index = core_ranges.at(cr).start_coord.y; y_index <= core_ranges.at(cr).end_coord.y;
                      y_index++) {
-                    CoreCoord noc_core = is_dram ? CoreCoord(x_index, y_index)
-                                                 : device->worker_core_from_logical_core(CoreCoord(x_index, y_index));
+                    CoreCoord noc_core = device->worker_core_from_logical_core(CoreCoord(x_index, y_index));
                     concatenated_core = (noc_core.x & 0xFF) << 8 | (noc_core.y & 0xFF);
                     if (last) {
                         args.push_back(concatenated_core | (held_value << 16));
@@ -118,8 +113,7 @@ std::vector<uint32_t> generate_run_time_args(const tt::tt_metal::Tensor& t) {
                  y_index++) {
                 for (uint32_t x_index = core_ranges.at(cr).start_coord.x; x_index <= core_ranges.at(cr).end_coord.x;
                      x_index++) {
-                    CoreCoord noc_core = is_dram ? CoreCoord(x_index, y_index)
-                                                 : device->worker_core_from_logical_core(CoreCoord(x_index, y_index));
+                    CoreCoord noc_core = device->worker_core_from_logical_core(CoreCoord(x_index, y_index));
                     concatenated_core = (noc_core.x & 0xFF) << 8 | (noc_core.y & 0xFF);
                     if (last) {
                         args.push_back(concatenated_core | (held_value << 16));
@@ -153,24 +147,17 @@ std::vector<uint32_t> generate_compile_time_args(const tt::tt_metal::Tensor& t)
         "ShardedAddrGenArgBuilder::emit_ct_args was invoked with a tensor containing an unsupported (Sharded) Tensor "
         "Memory Layout: {}",
         t.memory_config().memory_layout());
-    bool is_dram = t.memory_config().is_dram();
     ShardSpec shard_spec = t.shard_spec().value();
     ShardSpecBuffer buf_shard_spec = t.buffer()->shard_spec();
     const auto& [pages_per_shard_y, pages_per_shard_x] = buf_shard_spec.shape_in_pages();
-    // contiguity is 0(3) if there is padding between unaligned page and target is L1(DRAM),
-    // 1(4) if there is padding in the rightmost shard and target is L1(DRAM),
-    // and 2(5) otherwise for L1(DRAM)
-    shard_addr_gen_consts::ContiguityType contiguity;
-    if (t.buffer()->aligned_page_size() != t.buffer()->page_size()) {
-        contiguity = is_dram ? shard_addr_gen_consts::ContiguityType::DRAM_PADDING_BETWEEN_PAGES
-                             : shard_addr_gen_consts::ContiguityType::L1_PADDING_BETWEEN_PAGES;
-    } else if (buf_shard_spec.tensor2d_shape_in_pages[1] == (pages_per_shard_x * get_sharding_core_count(t))) {
-        contiguity = is_dram ? shard_addr_gen_consts::ContiguityType::DRAM_NO_SHARD_PADDING
-                             : shard_addr_gen_consts::ContiguityType::L1_NO_SHARD_PADDING;
-    } else {
-        contiguity = is_dram ? shard_addr_gen_consts::ContiguityType::DRAM_PADDING_IN_RIGHTMOST_SHARD
-                             : shard_addr_gen_consts::ContiguityType::L1_PADDING_IN_RIGHTMOST_SHARD;
-    }
+    // contiguity is 0 if there is padding between unaligned page, 1 if there is padding in the rightmost shard, and 2
+    // otherwise
+    shard_addr_gen_consts::ContiguityType contiguity =
+        (t.buffer()->aligned_page_size() != t.buffer()->page_size())
+            ? shard_addr_gen_consts::ContiguityType::PADDING_BETWEEN_PAGES
+        : (buf_shard_spec.tensor2d_shape_in_pages[1] == (pages_per_shard_x * get_sharding_core_count(t)))
+            ? shard_addr_gen_consts::ContiguityType::NO_SHARD_PADDING
+            : shard_addr_gen_consts::ContiguityType::PADDING_IN_RIGHTMOST_SHARD;
     args.push_back(static_cast<uint32_t>(t.memory_config().memory_layout()));  // Memory layout
     args.push_back(static_cast<uint32_t>(get_sharding_core_count(t)));       // The number of sharding cores
     args.push_back(static_cast<uint32_t>(t.buffer()->aligned_page_size()));  // The page size we offset each write to
diff --git a/ttnn/cpp/ttnn/operations/conv/CMakeLists.txt b/ttnn/cpp/ttnn/operations/conv/CMakeLists.txt
index e5213fbd23..1e5ecdb053 100644
--- a/ttnn/cpp/ttnn/operations/conv/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/conv/CMakeLists.txt
@@ -1,14 +1,8 @@
 add_library(ttnn_op_conv ${LIB_TYPE})
-add_library(TTNN::Ops::Conv ALIAS ttnn_op_conv)
+add_library(TT::NN::Ops::Conv ALIAS ttnn_op_conv)
 
 target_precompile_headers(ttnn_op_conv REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_conv)
-set_target_properties(
-    ttnn_op_conv
-    PROPERTIES
-        VERIFY_INTERFACE_HEADER_SETS
-            FALSE
-)
 
 target_sources(
     ttnn_op_conv
@@ -25,30 +19,12 @@ target_sources(
         conv1d/conv1d.cpp
 )
 
-file(GLOB_RECURSE kernels conv2d/device/kernels/*)
-target_sources(
-    ttnn_op_conv
-    PUBLIC
-        FILE_SET kernels
-        TYPE HEADERS
-        BASE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}
-        FILES ${kernels}
-)
-
 target_include_directories(ttnn_op_conv PRIVATE ${FixmeOpIncDirs})
 target_link_libraries(
     ttnn_op_conv
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
-install(
-    TARGETS
-        ttnn_op_conv
-    FILE_SET
-    kernels
-        DESTINATION ${CMAKE_INSTALL_LIBEXECDIR}/tt-metalium/ttnn/cpp/ttnn/operations/conv
-        COMPONENT ttnn-runtime
-)
 install(TARGETS ttnn_op_conv LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/core/CMakeLists.txt b/ttnn/cpp/ttnn/operations/core/CMakeLists.txt
index 40bce226b3..a482149875 100644
--- a/ttnn/cpp/ttnn/operations/core/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/core/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_core ${LIB_TYPE})
-add_library(TTNN::Ops::Core ALIAS ttnn_op_core)
+add_library(TT::NN::Ops::Core ALIAS ttnn_op_core)
 
 target_precompile_headers(ttnn_op_core REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_core)
@@ -17,7 +17,7 @@ target_link_libraries(
     ttnn_op_core
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_core LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/data_movement/CMakeLists.txt b/ttnn/cpp/ttnn/operations/data_movement/CMakeLists.txt
index eb68d92226..ee8304b139 100644
--- a/ttnn/cpp/ttnn/operations/data_movement/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/data_movement/CMakeLists.txt
@@ -1,14 +1,8 @@
 add_library(ttnn_op_data_movement ${LIB_TYPE})
-add_library(TTNN::Ops::DataMovement ALIAS ttnn_op_data_movement)
+add_library(TT::NN::Ops::DataMovement ALIAS ttnn_op_data_movement)
 
 target_precompile_headers(ttnn_op_data_movement REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_data_movement)
-set_target_properties(
-    ttnn_op_data_movement
-    PROPERTIES
-        VERIFY_INTERFACE_HEADER_SETS
-            FALSE
-)
 
 target_sources(
     ttnn_op_data_movement
@@ -112,43 +106,12 @@ target_sources(
         view/view.cpp
 )
 
-file(
-    GLOB_RECURSE kernels
-    common/kernels/*
-    move/device/kernels/*
-    permute/device/kernels/*
-    sharded/device/kernels/*
-    transpose/device/kernels/*
-)
-target_sources(
-    ttnn_op_data_movement
-    PUBLIC
-        FILE_SET kernels
-        TYPE HEADERS
-        BASE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}
-        FILES
-            ${kernels}
-            reshape_view/device/device/dataflow/reader_reshape_tiled.cpp
-            reshape_view/device/device/dataflow/writer_reshape_tiled.cpp
-            reshape_view/device/device/rm_reshape_interleaved.cpp
-            reshape_view/device/hostdevcommon/common.hpp
-)
-
 target_include_directories(ttnn_op_data_movement PRIVATE ${FixmeOpIncDirs})
 target_link_libraries(
     ttnn_op_data_movement
     PRIVATE
         TT::Metalium
-        TTNN::Core
-)
-
-install(
-    TARGETS
-        ttnn_op_data_movement
-    FILE_SET
-    kernels
-        DESTINATION ${CMAKE_INSTALL_LIBEXECDIR}/tt-metalium/ttnn/cpp/ttnn/operations/data_movement
-        COMPONENT ttnn-runtime
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_data_movement LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/data_movement/permute/device/kernels/dataflow/reader_permute_interleaved_rm_blocked_generic.cpp b/ttnn/cpp/ttnn/operations/data_movement/permute/device/kernels/dataflow/reader_permute_interleaved_rm_blocked_generic.cpp
index c9e67ca335..f63aaab6d0 100644
--- a/ttnn/cpp/ttnn/operations/data_movement/permute/device/kernels/dataflow/reader_permute_interleaved_rm_blocked_generic.cpp
+++ b/ttnn/cpp/ttnn/operations/data_movement/permute/device/kernels/dataflow/reader_permute_interleaved_rm_blocked_generic.cpp
@@ -3,7 +3,6 @@
 // SPDX-License-Identifier: Apache-2.0
 
 #include <stdint.h>
-#include <algorithm>
 #include "dataflow_api.h"
 
 void kernel_main() {
@@ -70,11 +69,11 @@ void kernel_main() {
 
         // Compute X block boundaries
         uint32_t x_start = x_block * x_block_size;
-        uint32_t x_end = std::min(x_start + x_block_size, X);
+        uint32_t x_end = min(x_start + x_block_size, X);
 
         // Compute W block boundaries
         uint32_t w_start = w_block * w_block_size;
-        uint32_t w_end = std::min(w_start + w_block_size, input_shape[N - 1]);
+        uint32_t w_end = min(w_start + w_block_size, input_shape[N - 1]);
         uint32_t w_offset = w_start * element_size;
 
         uint32_t w_read_size_bytes = (w_end - w_start) * element_size;
diff --git a/ttnn/cpp/ttnn/operations/data_movement/permute/device/kernels/dataflow/writer_permute_interleaved_rm_blocked_generic.cpp b/ttnn/cpp/ttnn/operations/data_movement/permute/device/kernels/dataflow/writer_permute_interleaved_rm_blocked_generic.cpp
index 8e380b2b42..d80300ab71 100644
--- a/ttnn/cpp/ttnn/operations/data_movement/permute/device/kernels/dataflow/writer_permute_interleaved_rm_blocked_generic.cpp
+++ b/ttnn/cpp/ttnn/operations/data_movement/permute/device/kernels/dataflow/writer_permute_interleaved_rm_blocked_generic.cpp
@@ -3,7 +3,6 @@
 // SPDX-License-Identifier: Apache-2.0
 
 #include <stdint.h>
-#include <algorithm>
 #include "dataflow_api.h"
 #include "cpp/ttnn/operations/data_movement/common/kernels/common.hpp"
 
@@ -99,10 +98,10 @@ void kernel_main() {
 
         // Compute start/end boundaries for the current X and W blocks
         const uint32_t x_start = x_block * x_block_size;
-        const uint32_t x_end = std::min(x_start + x_block_size, X);
+        const uint32_t x_end = min(x_start + x_block_size, X);
 
         const uint32_t w_start = w_block * w_block_size;
-        const uint32_t w_end = std::min(w_start + w_block_size, W);
+        const uint32_t w_end = min(w_start + w_block_size, W);
 
         // Compute the read size for the X dimension
         const uint32_t x_read_size_bytes = (x_end - x_start) * element_size;
diff --git a/ttnn/cpp/ttnn/operations/data_movement/sharded/device/kernels/dataflow/writer_unary_sharded_blocks_start_id.cpp b/ttnn/cpp/ttnn/operations/data_movement/sharded/device/kernels/dataflow/writer_unary_sharded_blocks_start_id.cpp
deleted file mode 100644
index 57fe2df0be..0000000000
--- a/ttnn/cpp/ttnn/operations/data_movement/sharded/device/kernels/dataflow/writer_unary_sharded_blocks_start_id.cpp
+++ /dev/null
@@ -1,55 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include <stdint.h>
-#include "dataflow_api.h"
-#include "ttnn/cpp/ttnn/operations/ccl/kernel_common/sharding_addrgen.hpp"
-
-void kernel_main() {
-    // run-time args
-    const uint32_t dst_addr = get_arg_val<uint32_t>(0);
-    const uint32_t block_height_tiles = get_arg_val<uint32_t>(1);
-    const uint32_t block_width_tiles = get_arg_val<uint32_t>(2);
-    const uint32_t padded_offset = get_arg_val<uint32_t>(3);
-    const uint32_t block_width_padded_num_tiles = get_arg_val<uint32_t>(4);
-    const uint32_t output_width_tiles = get_arg_val<uint32_t>(5);
-    const uint32_t start_id_offset = get_arg_val<uint32_t>(6);
-    const uint32_t start_id_base = get_arg_val<uint32_t>(7);
-
-    // compile-time args
-    constexpr uint32_t cb_id_out = get_compile_time_arg_val(0);
-
-    // single-tile ublocks
-    const uint32_t tile_bytes = get_tile_size(cb_id_out);
-
-    using tensor_shard_info = ShardedInfo<
-        get_compile_time_arg_val(1),   // Memory layout
-        get_compile_time_arg_val(2),   // The number of sharding cores
-        get_compile_time_arg_val(3),   // The page size we offset each write to
-        get_compile_time_arg_val(4),   // The number of pages in each sharding row not including padding pages
-        get_compile_time_arg_val(5),   // This defines times when contiguous pages can't be calculated
-        get_compile_time_arg_val(6),   // pages_per_shard_x
-        get_compile_time_arg_val(7)>;  // pages_per_shard_y
-
-    const auto [mapping_table, rt_increment] =
-        experimental::shard_addr_gen_utils::get_shard_map<tensor_shard_info>(get_arg_addr(8));
-    experimental::ShardedAddrGen<tensor_shard_info> s = {.bank_base_address = dst_addr, .shard_array = mapping_table};
-
-    uint32_t row_start_tile_id = start_id_base + start_id_offset;
-    cb_wait_front(cb_id_out, block_width_padded_num_tiles);
-    uint32_t l1_read_addr = get_read_ptr(cb_id_out);
-    for (uint32_t h = 0; h < block_height_tiles; h++) {
-        uint32_t tile_id = row_start_tile_id;
-        for (uint32_t w = 0; w < block_width_tiles; w++) {
-            uint64_t dst_noc_addr = get_noc_addr(tile_id, s);
-            noc_async_write(l1_read_addr, dst_noc_addr, tile_bytes);
-            tile_id++;
-            l1_read_addr += tile_bytes;
-        }
-        l1_read_addr += padded_offset;
-        row_start_tile_id += output_width_tiles;
-    }
-    noc_async_write_barrier();
-    cb_pop_front(cb_id_out, block_width_padded_num_tiles);
-}
diff --git a/ttnn/cpp/ttnn/operations/data_movement/sharded/device/kernels/dataflow/writer_unary_sharded_stick_layout_start_id.cpp b/ttnn/cpp/ttnn/operations/data_movement/sharded/device/kernels/dataflow/writer_unary_sharded_stick_layout_start_id.cpp
deleted file mode 100644
index ff3c7a289a..0000000000
--- a/ttnn/cpp/ttnn/operations/data_movement/sharded/device/kernels/dataflow/writer_unary_sharded_stick_layout_start_id.cpp
+++ /dev/null
@@ -1,45 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include <stdint.h>
-#include "dataflow_api.h"
-#include "ttnn/cpp/ttnn/operations/ccl/kernel_common/sharding_addrgen.hpp"
-
-void kernel_main() {
-    // run-time args
-    const uint32_t dst_addr = get_arg_val<uint32_t>(0);
-    const uint32_t block_height = get_arg_val<uint32_t>(1);
-    const uint32_t block_width_bytes = get_arg_val<uint32_t>(2);
-    const uint32_t padded_block_width_bytes = get_arg_val<uint32_t>(3);
-    const uint32_t start_id = get_arg_val<uint32_t>(4);
-    const uint32_t output_width_in_pages = get_arg_val<uint32_t>(5);
-
-    // compile-time args
-    constexpr uint32_t cb_id_out0 = get_compile_time_arg_val(0);
-
-    using tensor_shard_info = ShardedInfo<
-        get_compile_time_arg_val(1),   // Memory layout
-        get_compile_time_arg_val(2),   // The number of sharding cores
-        get_compile_time_arg_val(3),   // The page size we offset each write to
-        get_compile_time_arg_val(4),   // The number of pages in each sharding row not including padding pages
-        get_compile_time_arg_val(5),   // This defines times when contiguous pages can't be calculated
-        get_compile_time_arg_val(6),   // pages_per_shard_x
-        get_compile_time_arg_val(7)>;  // pages_per_shard_y
-
-    const auto [mapping_table, rt_increment] =
-        experimental::shard_addr_gen_utils::get_shard_map<tensor_shard_info>(get_arg_addr(6));
-    experimental::ShardedAddrGen<tensor_shard_info> s = {.bank_base_address = dst_addr, .shard_array = mapping_table};
-
-    uint32_t stick_id = start_id;
-    cb_wait_front(cb_id_out0, block_height);
-    uint32_t l1_read_addr = get_read_ptr(cb_id_out0);
-    for (uint32_t h = 0; h < block_height; ++h) {
-        uint64_t dst_noc_addr = get_noc_addr(stick_id, s);
-        noc_async_write(l1_read_addr, dst_noc_addr, block_width_bytes);
-        stick_id += output_width_in_pages;
-        l1_read_addr += padded_block_width_bytes;
-    }
-    noc_async_write_barrier();
-    cb_pop_front(cb_id_out0, block_height);
-}
diff --git a/ttnn/cpp/ttnn/operations/data_movement/sharded/interleaved_to_sharded/device/interleaved_to_sharded_op.cpp b/ttnn/cpp/ttnn/operations/data_movement/sharded/interleaved_to_sharded/device/interleaved_to_sharded_op.cpp
index ca4dcdbc31..0cd9a94c9b 100644
--- a/ttnn/cpp/ttnn/operations/data_movement/sharded/interleaved_to_sharded/device/interleaved_to_sharded_op.cpp
+++ b/ttnn/cpp/ttnn/operations/data_movement/sharded/interleaved_to_sharded/device/interleaved_to_sharded_op.cpp
@@ -18,10 +18,8 @@ void InterleavedToShardedDeviceOperation::validate(const std::vector<Tensor>& in
 
     TT_FATAL(input_tensor.memory_config().memory_layout() == TensorMemoryLayout::INTERLEAVED, "Error");
     TT_FATAL(this->output_mem_config.is_sharded(), "Error");
-    if (this->output_mem_config.memory_layout() == TensorMemoryLayout::BLOCK_SHARDED) {
-        TT_FATAL(this->output_mem_config.buffer_type() == BufferType::L1, "We don't support DRAM block sharding");
-    }
-    if (input_tensor.get_layout() == Layout::ROW_MAJOR) {
+    TT_FATAL(this->output_mem_config.buffer_type() == BufferType::L1, "Error");
+    if (input_tensor.layout() == Layout::ROW_MAJOR) {
         TT_FATAL((*this->output_mem_config.shard_spec()).shape[1] * input_tensor.element_size() % hal::get_l1_alignment() == 0, "Shard page size must currently have L1 aligned page size");
     }
     if (input_tensor.dtype() != this->output_dtype) {
diff --git a/ttnn/cpp/ttnn/operations/data_movement/sharded/interleaved_to_sharded/device/interleaved_to_sharded_program_factory.cpp b/ttnn/cpp/ttnn/operations/data_movement/sharded/interleaved_to_sharded/device/interleaved_to_sharded_program_factory.cpp
index 7c784417b8..ef14f3a24f 100644
--- a/ttnn/cpp/ttnn/operations/data_movement/sharded/interleaved_to_sharded/device/interleaved_to_sharded_program_factory.cpp
+++ b/ttnn/cpp/ttnn/operations/data_movement/sharded/interleaved_to_sharded/device/interleaved_to_sharded_program_factory.cpp
@@ -2,15 +2,12 @@
 //
 // SPDX-License-Identifier: Apache-2.0
 
-#include "interleaved_to_sharded_program_factory.hpp"
-
 #include <math.h>
 
 #include "ttnn/operations/math.hpp"
 #include <tt-metalium/work_split.hpp>
 #include <tt-metalium/host_api.hpp>
 #include <tt-metalium/constants.hpp>
-#include "cpp/ttnn/operations/ccl/sharding_addrgen_helper.hpp"
 #include "cpp/ttnn/operations/data_movement/sharded/sharded_common.hpp"
 #include "cpp/ttnn/operations/data_movement/sharded_partial/interleaved_to_sharded_partial/device/interleaved_to_sharded_partial_op.hpp"
 #include <tt-metalium/tt_align.hpp>
@@ -45,7 +42,6 @@ operation::ProgramWithCallbacks interleaved_to_sharded_multi_core(
     auto src_buffer = input.buffer();
     auto dst_buffer = output.buffer();
     bool src_is_dram = src_buffer->buffer_type() == tt::tt_metal::BufferType::DRAM;
-    bool dst_is_dram = dst_buffer->buffer_type() == tt::tt_metal::BufferType::DRAM;
     bool is_blackhole = (input.device()->arch() == tt::ARCH::BLACKHOLE);
 
     if (input.layout() == Layout::TILE) {
@@ -107,10 +103,8 @@ operation::ProgramWithCallbacks interleaved_to_sharded_multi_core(
     }
     tt::tt_metal::CircularBufferConfig output_cb_out_config =
         tt::tt_metal::CircularBufferConfig(num_input_units * output_page_size, {{out_cb_index, output_cb_data_format}})
-            .set_page_size(out_cb_index, output_page_size);
-    if (!dst_is_dram) {
-        output_cb_out_config = output_cb_out_config.set_globally_allocated_address(*output.buffer());
-    }
+            .set_page_size(out_cb_index, output_page_size)
+            .set_globally_allocated_address(*output.buffer());
     auto cb_output = tt::tt_metal::CreateCircularBuffer(program, all_cores, output_cb_out_config);
     uint32_t dram_alignment = hal::get_dram_alignment();
     if (src_is_dram && input_unit_size % dram_alignment != 0 or is_blackhole or keep_l1_aligned) {
@@ -156,21 +150,10 @@ operation::ProgramWithCallbacks interleaved_to_sharded_multi_core(
             tt::tt_metal::ReaderDataMovementConfig(reader_compile_time_args));
     }
 
-    std::string writer_kernel;
     std::vector<uint32_t> writer_compile_time_args = {out_cb_index};
-    if (dst_is_dram) {
-        if (input.get_layout() == Layout::TILE) {
-            writer_kernel = std::string("ttnn/cpp/ttnn/operations/data_movement/sharded/device/kernels/dataflow/writer_unary_sharded_blocks_start_id.cpp");
-        } else {
-            writer_kernel = std::string("ttnn/cpp/ttnn/operations/data_movement/sharded/device/kernels/dataflow/writer_unary_sharded_stick_layout_start_id.cpp");
-        }
-        shard_builder::extend_sharding_compile_time_args(output, writer_compile_time_args);
-    } else {
-        writer_kernel = std::string("ttnn/cpp/ttnn/operations/data_movement/sharded/device/kernels/dataflow/writer_unary_sharded.cpp");
-    }
     tt::tt_metal::KernelHandle unary_writer_kernel_id = tt::tt_metal::CreateKernel(
         program,
-        writer_kernel,
+        "ttnn/cpp/ttnn/operations/data_movement/sharded/device/kernels/dataflow/writer_unary_sharded.cpp",
         all_cores,
         tt::tt_metal::WriterDataMovementConfig(writer_compile_time_args));
 
@@ -223,39 +206,18 @@ operation::ProgramWithCallbacks interleaved_to_sharded_multi_core(
                 }
             }
             curr_num_units_per_shard = shard_height * num_units_per_shard_width;
-
-            // Reader run-time args
-            std::vector<uint32_t> reader_run_time_args = {
-                src_buffer->address(),
-                shard_height,
-                shard_width,
-                padded_offset,
-                num_units_offset,
-                curr_num_units_per_shard,
-                curr_idx_h + curr_idx_w,
-                starting_idx_h};
-            tt::tt_metal::SetRuntimeArgs(program, unary_reader_kernel_id, core, reader_run_time_args);
-
-            // Writer run-time args
-            uint32_t pad_offset = (num_units_per_shard_width - shard_width) * output_unit_size;
-            std::vector<uint32_t> writer_run_time_args;
-            if (dst_is_dram) {
-                writer_run_time_args = {
-                    dst_buffer->address(),
-                    shard_height,
-                    shard_width,
-                    pad_offset,
-                    curr_num_units_per_shard,
-                    num_units_offset,
-                    curr_idx_h + curr_idx_w,
-                    starting_idx_h};
-                shard_builder::extend_sharding_run_time_args(output, writer_run_time_args);
-            } else {
-                writer_run_time_args = {curr_num_units_per_shard};
-            }
-            tt::tt_metal::SetRuntimeArgs(program, unary_writer_kernel_id, core, writer_run_time_args);
-
-            // Update indexing
+            tt::tt_metal::SetRuntimeArgs(
+                program,
+                unary_reader_kernel_id,
+                core,
+                {src_buffer->address(),
+                 shard_height,
+                 shard_width,
+                 padded_offset,
+                 num_units_offset,
+                 curr_num_units_per_shard,
+                 curr_idx_h + curr_idx_w,
+                 starting_idx_h});
             curr_idx_w += num_units_per_shard_width;
             if (curr_idx_w >= num_units_per_row) {
                 curr_idx_w = 0;
@@ -320,47 +282,27 @@ operation::ProgramWithCallbacks interleaved_to_sharded_multi_core(
                 aligned_offset = 0;
             }
 
-            // Reader run-time args
-            std::vector<uint32_t> reader_run_time_args = {
-                src_buffer->address(),
-                num_units_per_row,
-                shard_height,
-                shard_width,
-                padded_offset_bytes,
-                static_cast<uint32_t>(aligned),
-                aligned_width_offset,
-                aligned_shard_width,
-                aligned_offset,
-                curr_idx_h};
-            tt::tt_metal::SetRuntimeArgs(program, unary_reader_kernel_id, core, reader_run_time_args);
-
-            // Writer run-time args
-            std::vector<uint32_t> writer_run_time_args;
-            if (dst_is_dram) {
-                uint32_t page_id_within_row = curr_idx_w / input_unit_size;
-                uint32_t output_width_in_pages = tt::div_up(num_units_per_row, input_unit_size);
-                uint32_t start_id = curr_idx_h * output_width_in_pages + page_id_within_row;
-                writer_run_time_args = {
-                    dst_buffer->address(),
-                    shard_height,
-                    shard_width,
-                    padded_offset_bytes,
-                    start_id,
-                    output_width_in_pages
-                };
-                shard_builder::extend_sharding_run_time_args(output, writer_run_time_args);
-            } else {
-                writer_run_time_args = {curr_num_units_per_shard};
-            }
-            tt::tt_metal::SetRuntimeArgs(program, unary_writer_kernel_id, core, writer_run_time_args);
-
-            // Update indexing
+            tt::tt_metal::SetRuntimeArgs(
+                program,
+                unary_reader_kernel_id,
+                core,
+                {src_buffer->address(),
+                 num_units_per_row,
+                 shard_height,
+                 shard_width,
+                 padded_offset_bytes,
+                 static_cast<uint32_t>(aligned),
+                 aligned_width_offset,
+                 aligned_shard_width,
+                 aligned_offset,
+                 curr_idx_h});
             curr_idx_w += input_unit_size;
             if (curr_idx_w >= num_units_per_row) {
                 curr_idx_w = 0;
                 curr_idx_h += num_units_per_shard_height;
             }
         }
+        tt::tt_metal::SetRuntimeArgs(program, unary_writer_kernel_id, core, {curr_num_units_per_shard});
         if (convert_df) {
             tt::tt_metal::SetRuntimeArgs(program, compute_kernel_id, core, {curr_num_units_per_shard});
         }
@@ -376,8 +318,6 @@ operation::ProgramWithCallbacks interleaved_to_sharded_multi_core(
             auto src_buffer = input_tensors.at(0).buffer();
             auto dst_buffer = output_tensors.at(0).buffer();
 
-            bool dst_is_dram = dst_buffer->buffer_type() == tt::tt_metal::BufferType::DRAM;
-
             bool partial_op = num_slices > 1;
             uint32_t starting_idx_h = 0;
             if (partial_op) {
@@ -393,16 +333,7 @@ operation::ProgramWithCallbacks interleaved_to_sharded_multi_core(
                     runtime_args[7] = starting_idx_h;
                 }
             }
-
-            if (dst_is_dram) {
-                auto& runtime_args_by_core = GetRuntimeArgs(program, unary_writer_kernel_id);
-                for (const auto& core : cores) {
-                    auto& runtime_args = runtime_args_by_core[core.x][core.y];
-                    runtime_args[0] = dst_buffer->address();
-                }
-            } else {
-                UpdateDynamicCircularBufferAddress(program, cb_output, *dst_buffer);
-            }
+            UpdateDynamicCircularBufferAddress(program, cb_output, *dst_buffer);
         };
 
     return {.program = std::move(program), .override_runtime_arguments_callback = override_runtime_arguments_callback};
diff --git a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/pack_untilize_variable_num_blocks.cpp b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/pack_untilize_variable_num_blocks.cpp
deleted file mode 100644
index d0642991bf..0000000000
--- a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/pack_untilize_variable_num_blocks.cpp
+++ /dev/null
@@ -1,32 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include <cstdint>
-
-#include "compute_kernel_api/untilize.h"
-#include "compute_kernel_api/pack_untilize.h"
-
-namespace NAMESPACE {
-void MAIN {
-    const uint32_t per_core_block_cnt = get_arg_val<uint32_t>(0);
-
-    constexpr uint32_t per_core_block_tile_cnt = get_compile_time_arg_val(0);
-    constexpr uint32_t src_cb_id = get_compile_time_arg_val(1);
-    constexpr uint32_t out_cb_id = get_compile_time_arg_val(2);
-
-    pack_untilize_init<per_core_block_tile_cnt>(src_cb_id, out_cb_id);
-
-    for (uint32_t b = 0; b < per_core_block_cnt; ++b) {
-        cb_wait_front(src_cb_id, per_core_block_tile_cnt);
-        cb_reserve_back(out_cb_id, per_core_block_tile_cnt);
-
-        pack_untilize_block<per_core_block_tile_cnt>(src_cb_id, 1, out_cb_id);
-
-        cb_push_back(out_cb_id, per_core_block_tile_cnt);
-        cb_pop_front(src_cb_id, per_core_block_tile_cnt);
-    }
-
-    pack_untilize_uninit(out_cb_id);
-}
-}  // namespace NAMESPACE
diff --git a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/untilize.cpp b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/untilize.cpp
index afc4ed5c96..78356e6ce7 100644
--- a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/untilize.cpp
+++ b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/untilize.cpp
@@ -5,16 +5,19 @@
 #include <cstdint>
 
 #include "compute_kernel_api/untilize.h"
+// #include "debug/dprint.h"
 
 namespace NAMESPACE {
 void MAIN {
-    constexpr uint32_t per_core_block_cnt = get_compile_time_arg_val(0);
-    constexpr uint32_t per_core_block_tile_cnt = get_compile_time_arg_val(1);
-    constexpr uint32_t src_cb_id = get_compile_time_arg_val(2);
-    constexpr uint32_t out_cb_id = get_compile_time_arg_val(3);
-
+    uint32_t per_core_block_cnt = get_compile_time_arg_val(0);
+    uint32_t per_core_block_tile_cnt = get_compile_time_arg_val(1);
+    uint32_t src_cb_id = get_compile_time_arg_val(2);
+    uint32_t out_cb_id = get_compile_time_arg_val(3);
     untilize_init(src_cb_id, out_cb_id);
 
+    // UNPACK(( DPRINT << "Block count=" << uint32_t(per_core_block_cnt) << " tile count=" << per_core_block_tile_cnt <<
+    // ENDL() ));
+
     for (uint32_t b = 0; b < per_core_block_cnt; ++b) {
         cb_wait_front(src_cb_id, per_core_block_tile_cnt);
         cb_reserve_back(out_cb_id, per_core_block_tile_cnt);
diff --git a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/untilize_variable_num_blocks.cpp b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/untilize_variable_num_blocks.cpp
deleted file mode 100644
index ff11c8cd74..0000000000
--- a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/untilize_variable_num_blocks.cpp
+++ /dev/null
@@ -1,29 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-// //
-// SPDX-License-Identifier: Apache-2.0
-
-#include <cstdint>
-
-#include "compute_kernel_api/untilize.h"
-
-namespace NAMESPACE {
-void MAIN {
-    const uint32_t per_core_block_cnt = get_arg_val<uint32_t>(0);
-
-    constexpr uint32_t per_core_block_tile_cnt = get_compile_time_arg_val(0);
-    constexpr uint32_t src_cb_id = get_compile_time_arg_val(1);
-    constexpr uint32_t out_cb_id = get_compile_time_arg_val(2);
-
-    untilize_init(src_cb_id, out_cb_id);
-
-    for (uint32_t b = 0; b < per_core_block_cnt; ++b) {
-        cb_wait_front(src_cb_id, per_core_block_tile_cnt);
-        cb_reserve_back(out_cb_id, per_core_block_tile_cnt);
-
-        untilize_block(src_cb_id, per_core_block_tile_cnt, out_cb_id);
-
-        cb_push_back(out_cb_id, per_core_block_tile_cnt);
-        cb_pop_front(src_cb_id, per_core_block_tile_cnt);
-    }
-}
-}  // namespace NAMESPACE
diff --git a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/reader_unary_start_id.cpp b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/reader_unary_start_id.cpp
index ae2a1d9e84..89bedf5fe6 100644
--- a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/reader_unary_start_id.cpp
+++ b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/reader_unary_start_id.cpp
@@ -8,28 +8,27 @@
 
 void kernel_main() {
     // run-time args
-    const uint32_t src_addr = get_arg_val<uint32_t>(0);
-    const uint32_t num_tiles = get_arg_val<uint32_t>(1);
-    const uint32_t start_page_id = get_arg_val<uint32_t>(2);
+    uint32_t src_addr = get_arg_val<uint32_t>(0);
 
     // compile-time args
     constexpr bool src_is_dram = get_compile_time_arg_val(0) == 1;
     constexpr uint32_t cb_id_in0 = get_compile_time_arg_val(1);
-
-    const uint32_t tile_bytes = get_tile_size(cb_id_in0);
+    constexpr uint32_t num_tiles = get_compile_time_arg_val(2);
+    constexpr uint32_t tile_bytes = get_compile_time_arg_val(3);
+    constexpr uint32_t start_page_id = get_compile_time_arg_val(4);
 
 #ifdef SHARDED
     using tensor_shard_info = ShardedInfo<
-        get_compile_time_arg_val(2),   // Memory layout
-        get_compile_time_arg_val(3),   // The number of sharding cores
-        get_compile_time_arg_val(4),   // The page size we offset each write to
-        get_compile_time_arg_val(5),   // The number of pages in each sharding row not including padding pages
-        get_compile_time_arg_val(6),   // This defines times when contiguous pages can't be calculated
-        get_compile_time_arg_val(7),   // pages_per_shard_x
-        get_compile_time_arg_val(8)>;  // pages_per_shard_y
+        get_compile_time_arg_val(5),    // Memory layout
+        get_compile_time_arg_val(6),    // The number of sharding cores
+        get_compile_time_arg_val(7),    // The page size we offset each write to
+        get_compile_time_arg_val(8),    // The number of pages in each sharding row not including padding pages
+        get_compile_time_arg_val(9),    // This defines times when contiguous pages can't be calculated
+        get_compile_time_arg_val(10),   // pages_per_shard_x
+        get_compile_time_arg_val(11)>;  // pages_per_shard_y
 
     const auto [mapping_table, rt_increment] =
-        experimental::shard_addr_gen_utils::get_shard_map<tensor_shard_info>(get_arg_addr(3));
+        experimental::shard_addr_gen_utils::get_shard_map<tensor_shard_info>(get_arg_addr(1));
     experimental::ShardedAddrGen<tensor_shard_info> s = {.bank_base_address = src_addr, .shard_array = mapping_table};
 #else
     const DataFormat data_format = get_dataformat(cb_id_in0);
diff --git a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/writer_unary_stick_layout_split_rows_single_core.cpp b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/writer_unary_stick_layout_split_rows.cpp
similarity index 63%
rename from ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/writer_unary_stick_layout_split_rows_single_core.cpp
rename to ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/writer_unary_stick_layout_split_rows.cpp
index 40eaa8d155..d07465f52d 100644
--- a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/writer_unary_stick_layout_split_rows_single_core.cpp
+++ b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/writer_unary_stick_layout_split_rows.cpp
@@ -13,15 +13,15 @@ void kernel_main() {
     // compile-time args
     constexpr bool dst_is_dram = get_compile_time_arg_val(0) == 1;
     constexpr uint32_t cb_id_out0 = get_compile_time_arg_val(1);
-    constexpr uint32_t output_stick_size = get_compile_time_arg_val(2);
-    constexpr bool output_stick_size_is_power_of_two = get_compile_time_arg_val(3) == 1;
-    constexpr uint32_t output_log_base_2_of_page_size = get_compile_time_arg_val(4);
-    constexpr uint32_t tile_height = get_compile_time_arg_val(5);
-    constexpr uint32_t num_blocks_across_height = get_compile_time_arg_val(6);
-    constexpr uint32_t num_output_columns_of_blocks = get_compile_time_arg_val(7);
-    constexpr uint32_t num_blocks_per_output_column_row = get_compile_time_arg_val(8);
-    constexpr uint32_t num_tiles_per_output_block = get_compile_time_arg_val(9);
-    constexpr uint32_t output_single_block_width_size = get_compile_time_arg_val(10);
+    constexpr bool stick_size_is_power_of_two = get_compile_time_arg_val(2) == 1;
+    constexpr uint32_t log_base_2_of_page_size = get_compile_time_arg_val(3);
+    constexpr uint32_t tile_height = get_compile_time_arg_val(4);
+    constexpr uint32_t num_blocks_across_height = get_compile_time_arg_val(5);
+    constexpr uint32_t num_columns_of_blocks = get_compile_time_arg_val(6);
+    constexpr uint32_t num_blocks_per_column_row = get_compile_time_arg_val(7);
+    constexpr uint32_t num_tiles_per_block = get_compile_time_arg_val(8);
+    constexpr uint32_t single_block_width_size = get_compile_time_arg_val(9);
+    constexpr uint32_t stick_size = get_compile_time_arg_val(10);
 
 #ifdef SHARDED
     using tensor_shard_info = ShardedInfo<
@@ -37,39 +37,39 @@ void kernel_main() {
         experimental::shard_addr_gen_utils::get_shard_map<tensor_shard_info>(get_arg_addr(1));
     experimental::ShardedAddrGen<tensor_shard_info> s = {.bank_base_address = dst_addr, .shard_array = mapping_table};
 #else
-    const auto s = get_interleaved_addr_gen<dst_is_dram, output_stick_size_is_power_of_two>(
-        dst_addr, output_stick_size, output_log_base_2_of_page_size);
+    const auto s = get_interleaved_addr_gen<dst_is_dram, stick_size_is_power_of_two>(
+        dst_addr, stick_size, log_base_2_of_page_size);
 #endif
 
     uint64_t base_dst_noc_addr[tile_height];
 
     auto write_tiles_in_current_block = [&]() {
-        cb_wait_front(cb_id_out0, num_tiles_per_output_block);
+        cb_wait_front(cb_id_out0, num_tiles_per_block);
 
         uint32_t l1_read_addr = get_read_ptr(cb_id_out0);
         for (uint32_t l = 0; l < tile_height; ++l) {
             uint64_t dst_noc_addr = base_dst_noc_addr[l];
-            noc_async_write(l1_read_addr, dst_noc_addr, output_single_block_width_size);
-            l1_read_addr += output_single_block_width_size;
-            base_dst_noc_addr[l] += output_single_block_width_size;
+            noc_async_write(l1_read_addr, dst_noc_addr, single_block_width_size);
+            l1_read_addr += single_block_width_size;
+            base_dst_noc_addr[l] += single_block_width_size;
         }
 
         noc_async_write_barrier();
-        cb_pop_front(cb_id_out0, num_tiles_per_output_block);
+        cb_pop_front(cb_id_out0, num_tiles_per_block);
     };
 
     // Each row of tiles processed separately
     for (uint32_t i = 0; i < num_blocks_across_height; ++i) {
         // If width or block sharded, we'll have multiple columns of rows/blocks
-        for (uint32_t j = 0; j < num_output_columns_of_blocks; ++j) {
+        for (uint32_t j = 0; j < num_columns_of_blocks; ++j) {
             // Determine the base addresses for the row of blocks in the current column
             for (uint32_t k = 0; k < tile_height; ++k) {
-                uint32_t num_complete_rows_already_processed = (i * tile_height + k) * num_output_columns_of_blocks;
+                uint32_t num_complete_rows_already_processed = (i * tile_height + k) * num_columns_of_blocks;
                 uint32_t stick_id = num_complete_rows_already_processed + j;
                 base_dst_noc_addr[k] = get_noc_addr(stick_id, s);
             }
 
-            for (uint32_t k = 0; k < num_blocks_per_output_column_row; ++k) {
+            for (uint32_t k = 0; k < num_blocks_per_column_row; ++k) {
                 write_tiles_in_current_block();
             }
         }
diff --git a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/writer_unary_stick_layout_split_rows_interleaved.cpp b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/writer_unary_stick_layout_split_rows_interleaved.cpp
new file mode 100644
index 0000000000..dca3ab8f25
--- /dev/null
+++ b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/writer_unary_stick_layout_split_rows_interleaved.cpp
@@ -0,0 +1,67 @@
+// SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.
+//
+// SPDX-License-Identifier: Apache-2.0
+
+#include <stdint.h>
+#include "dataflow_api.h"
+
+void kernel_main() {
+    // Constexpr
+    constexpr uint32_t cb_id_out0 = 16;
+    constexpr uint32_t tile_height = 32;
+
+    const uint32_t dst_addr = get_arg_val<uint32_t>(0);
+    const uint32_t num_sticks = get_arg_val<uint32_t>(1);
+    const uint32_t stick_size = get_arg_val<uint32_t>(2);
+    const uint32_t num_tiles_per_block = get_arg_val<uint32_t>(3);
+    const uint32_t block_width_size = get_arg_val<uint32_t>(4);
+    const uint32_t num_full_blocks_in_row = get_arg_val<uint32_t>(5);
+    // const uint32_t num_leftover_tiles_in_row  = get_arg_val<uint32_t>(6);
+    // const uint32_t leftover_width_in_row      = get_arg_val<uint32_t>(7);
+    const uint32_t start_stick_id = get_arg_val<uint32_t>(8);
+
+    constexpr bool dst_is_dram = get_compile_time_arg_val(0) == 1;
+    constexpr bool stick_size_is_power_of_two = get_compile_time_arg_val(1) == 1;
+
+#if (stick_size_is_power_of_two)
+    constexpr uint32_t log_base_2_of_page_size = get_compile_time_arg_val(2);
+    const InterleavedPow2AddrGen<dst_is_dram> s = {
+        .bank_base_address = dst_addr,
+        .log_base_2_of_page_size = log_base_2_of_page_size  // TODO(AP): refactor
+    };
+#else
+    const InterleavedAddrGen<dst_is_dram> s = {.bank_base_address = dst_addr, .page_size = stick_size};
+#endif
+
+    uint64_t base_dst_noc_addr[tile_height];
+
+    auto write_tiles = [&](const uint32_t& num_tiles, const uint32_t& width_size) {
+        cb_wait_front(cb_id_out0, num_tiles);
+        uint32_t l1_read_addr = get_read_ptr(cb_id_out0);
+        for (uint32_t k = 0; k < tile_height; k++) {
+            uint64_t dst_noc_addr = base_dst_noc_addr[k];
+            noc_async_write(l1_read_addr, dst_noc_addr, width_size);
+            l1_read_addr += width_size;
+            base_dst_noc_addr[k] += width_size;
+        }
+        noc_async_write_barrier();
+        cb_pop_front(cb_id_out0, num_tiles);
+    };
+
+    uint32_t stick_id = start_stick_id;
+    for (uint32_t i = 0; i < num_sticks / tile_height; i++) {
+        // Get Base Addresses
+        for (uint32_t j = 0; j < tile_height; j++) {
+            base_dst_noc_addr[j] = get_noc_addr(stick_id, s);
+            stick_id++;
+        }
+
+        for (uint32_t j = 0; j < num_full_blocks_in_row; j++) {
+            write_tiles(num_tiles_per_block, block_width_size);
+        }
+
+        // if (num_leftover_tiles_in_row > 0) {
+        //     write_tiles(num_leftover_tiles_in_row, leftover_width_in_row);
+        // }
+    }
+}
diff --git a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/writer_unary_stick_layout_split_rows_multi_core.cpp b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/writer_unary_stick_layout_split_rows_multi_core.cpp
deleted file mode 100644
index 45fb72ac77..0000000000
--- a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/writer_unary_stick_layout_split_rows_multi_core.cpp
+++ /dev/null
@@ -1,129 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include <stdint.h>
-#include "dataflow_api.h"
-#include "ttnn/cpp/ttnn/operations/ccl/kernel_common/sharding_addrgen.hpp"
-
-void kernel_main() {
-    // run-time args
-    const uint32_t dst_addr = get_arg_val<uint32_t>(0);
-    const uint32_t num_input_blocks_to_process = get_arg_val<uint32_t>(1);
-    const uint32_t height_wise_input_block_start_index = get_arg_val<uint32_t>(2);
-    const uint32_t num_unpadded_cols_per_input_block = get_arg_val<uint32_t>(3);
-    const uint32_t width_wise_output_block_start_index = get_arg_val<uint32_t>(4);
-    const uint32_t num_cols_already_processed_in_first_output_block = get_arg_val<uint32_t>(5);
-
-    // compile-time args
-    constexpr bool dst_is_dram = get_compile_time_arg_val(0) == 1;
-    constexpr uint32_t cb_id_out0 = get_compile_time_arg_val(1);
-    constexpr uint32_t output_stick_size = get_compile_time_arg_val(2);
-    constexpr bool output_stick_size_is_power_of_two = get_compile_time_arg_val(3) == 1;
-    constexpr uint32_t output_log_base_2_of_page_size = get_compile_time_arg_val(4);
-    constexpr uint32_t tile_height = get_compile_time_arg_val(5);
-    constexpr uint32_t num_tiles_per_input_block = get_compile_time_arg_val(6);
-    constexpr uint32_t num_output_blocks_across_width = get_compile_time_arg_val(7);
-    constexpr uint32_t output_element_size = get_compile_time_arg_val(8);
-    constexpr uint32_t num_cols_per_input_block = get_compile_time_arg_val(9);
-    constexpr uint32_t num_cols_per_output_block = get_compile_time_arg_val(10);
-
-#ifdef SHARDED
-    using tensor_shard_info = ShardedInfo<
-        get_compile_time_arg_val(11),   // Memory layout
-        get_compile_time_arg_val(12),   // The number of sharding cores
-        get_compile_time_arg_val(13),   // The page size we offset each write to
-        get_compile_time_arg_val(14),   // The number of pages in each sharding row not including padding pages
-        get_compile_time_arg_val(15),   // This defines times when contiguous pages can't be calculated
-        get_compile_time_arg_val(16),   // pages_per_shard_x
-        get_compile_time_arg_val(17)>;  // pages_per_shard_y
-
-    const auto [mapping_table, rt_increment] =
-        experimental::shard_addr_gen_utils::get_shard_map<tensor_shard_info>(get_arg_addr(6));
-    experimental::ShardedAddrGen<tensor_shard_info> s = {.bank_base_address = dst_addr, .shard_array = mapping_table};
-#else
-    const auto s = get_interleaved_addr_gen<dst_is_dram, output_stick_size_is_power_of_two>(
-        dst_addr, output_stick_size, output_log_base_2_of_page_size);
-#endif
-
-    auto write_tiles_in_current_block = [&](uint32_t block_height_index) {
-        cb_wait_front(cb_id_out0, num_tiles_per_input_block);
-
-        // Base address of the row of elements we are going to be writing.
-        // If the input is unevenly sharded width wise and we are processing a block in the
-        // last shard width wise, we will not be writing the entire row of elements as the
-        // last x elements will be garbage. Tracking the base address of the row allows us to
-        // easily increment the current_read_addr to the next row of elements.
-        uint32_t base_l1_read_addr = get_read_ptr(cb_id_out0);
-
-        // Process each row of elements in the input block
-        for (uint32_t j = 0; j < tile_height; ++j) {
-            uint32_t current_l1_read_addr = base_l1_read_addr + j * num_cols_per_input_block * output_element_size;
-
-            // Note: For width or block sharding (either input, output, or both), there may be more/less blocks width
-            // wise in the output tensor compared to the input tensor. As a result, for the first output block we write
-            // to, we may be writing to the middle of a page (i.e. some byte offset within the page). For all following
-            // writes we'll be writing to the beginning of a page and not require an offset.
-
-            // Output page_id that we are going to be writing to
-            uint32_t num_rows_already_processed = block_height_index * tile_height + j;
-            uint32_t num_pages_already_processed_in_previous_rows =
-                num_rows_already_processed * num_output_blocks_across_width;
-            uint32_t output_page_id =
-                num_pages_already_processed_in_previous_rows + width_wise_output_block_start_index;
-
-            // For the first output page that we write to in the current row, it's first x columns may have been
-            // written to from a different input block. So we need to calculate the offset within this first output page
-            // to write to (which may be 0). We also need to determine how many columns in this output page have not yet
-            // been written to in order to determine how many columns from the input block to writer (min of all
-            // remaining columns in the input block, and the remaining unprocessed columns in the current output block).
-            uint32_t num_cols_remaining_in_current_output_block =
-                num_cols_per_output_block - num_cols_already_processed_in_first_output_block;
-            uint32_t output_offset_within_page_in_bytes =
-                num_cols_already_processed_in_first_output_block * output_element_size;
-
-            // Iterate through all columns in the input block that this core is processing
-            uint32_t num_input_cols_processed = 0;
-            while (num_input_cols_processed < num_unpadded_cols_per_input_block) {
-                // How many elements to write from the input block to the output block.
-                // Min of the number of remaining unprocessed columns in the input block
-                // and the number of remaining unprocessed columns in the current output block.
-                uint32_t num_cols_to_write = std::min(
-                    num_unpadded_cols_per_input_block - num_input_cols_processed,
-                    num_cols_remaining_in_current_output_block);
-                uint32_t num_bytes_to_write = num_cols_to_write * output_element_size;
-
-                // Perform the write
-                uint64_t dst_noc_addr = get_noc_addr(output_page_id, s, output_offset_within_page_in_bytes);
-                noc_async_write(current_l1_read_addr, dst_noc_addr, num_bytes_to_write);
-
-                // Increment the number of cols we've processed in the input block
-                num_input_cols_processed += num_cols_to_write;
-
-                // Increment l1_read_addr past the bytes we just read
-                current_l1_read_addr += num_bytes_to_write;
-
-                // Increment the output_page_id we're writing to. If we wrote the entire input block
-                // then this has no effect as the while loop terminates. If we wrote to a subset of the
-                // input block, then that subset corresponds to an entire output block, so we increment
-                // the output_page_id to the following output block.
-                output_page_id++;
-
-                // Only the first output block we write to can have some of it's columns already processed/written-to
-                num_cols_remaining_in_current_output_block = num_cols_per_output_block;
-                output_offset_within_page_in_bytes = 0;
-            }
-        }
-
-        noc_async_write_barrier();
-        cb_pop_front(cb_id_out0, num_tiles_per_input_block);
-    };
-
-    // Each input block processed separately
-    uint32_t height_wise_input_block_index = height_wise_input_block_start_index;
-    for (uint32_t i = 0; i < num_input_blocks_to_process; ++i) {
-        // Process the current block
-        write_tiles_in_current_block(height_wise_input_block_index);
-        height_wise_input_block_index++;
-    }
-}
diff --git a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/untilize_op.cpp b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/untilize_op.cpp
index 9b93207876..abc3f456d1 100644
--- a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/untilize_op.cpp
+++ b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/untilize_op.cpp
@@ -12,128 +12,93 @@ using namespace tt::tt_metal;
 
 namespace ttnn::operations::data_movement {
 
+namespace untilize_helpers {
+uint32_t get_num_cores(CoreCoord grid_size, uint32_t nblocks) {
+    int32_t ncores_x = grid_size.x;
+    int32_t ncores_y = grid_size.y;
+    int32_t ncores = ncores_x * ncores_y;
+    if (nblocks <= ncores) {
+        ncores = nblocks;
+    } else {
+        uint32_t nblocks_per_core = std::ceil((float)nblocks / ncores);
+        ncores = std::ceil((float)nblocks / nblocks_per_core);
+    }
+    return ncores;
+}
+}  // namespace untilize_helpers
+
 void Untilize::validate(const std::vector<Tensor>& input_tensors) const {
     using namespace tt::constants;
     const auto& input_tensor_a = input_tensors.at(0);
-
-    uint32_t tensor_width = input_tensor_a.padded_shape()[-1];
-    uint32_t tensor_height = input_tensor_a.physical_volume() / tensor_width;
-
-    bool input_is_sharded = input_tensor_a.is_sharded();
-    bool output_is_sharded = this->output_mem_config.is_sharded();
-
-    BufferType input_buffer_type = input_tensor_a.memory_config().buffer_type();
-    BufferType output_buffer_type = this->output_mem_config.buffer_type();
-
-    TensorMemoryLayout input_memory_layout = input_tensor_a.memory_config().memory_layout();
-    TensorMemoryLayout output_memory_layout = this->output_mem_config.memory_layout();
-
     TT_FATAL(input_tensor_a.storage_type() == StorageType::DEVICE, "Operands to untilize need to be on device!");
     TT_FATAL(input_tensor_a.buffer() != nullptr, "Operands to untilize need to be allocated in buffers on device!");
     TT_FATAL(input_tensor_a.layout() == Layout::TILE, "Can only untilize tile major data");
 
-    // Input must be in valid tile layout
-    TT_FATAL(tensor_width % TILE_WIDTH == 0, "Width must be evenly divisible into tiles");
-    TT_FATAL(tensor_height % TILE_HEIGHT == 0, "Height must be evenly divisible into tiles");
-
-    // Only support interleaved or sharded memory layouts
+    TT_FATAL(input_tensor_a.padded_shape()[-1] % TILE_WIDTH == 0, "Width must be evenly divisible into tiles");
     TT_FATAL(
-        input_memory_layout != TensorMemoryLayout::SINGLE_BANK, "Input memory layout must be interleaved or sharded");
-    TT_FATAL(
-        output_memory_layout != TensorMemoryLayout::SINGLE_BANK, "Output memory layout must be interleaved or sharded");
+        (input_tensor_a.physical_volume() / input_tensor_a.padded_shape()[-1]) % TILE_HEIGHT == 0,
+        "Height must be evenly divisible into tiles");
 
-    // Special conditions for sub_core_grids special case
     if (this->sub_core_grids.has_value()) {
         TT_FATAL(
-            input_memory_layout == TensorMemoryLayout::INTERLEAVED,
+            input_tensor_a.memory_config().memory_layout() == TensorMemoryLayout::INTERLEAVED,
             "Input memory layout must be interleaved when sub_core_grid argument provided");
         TT_FATAL(
-            output_memory_layout == TensorMemoryLayout::INTERLEAVED,
+            this->output_mem_config.memory_layout() == TensorMemoryLayout::INTERLEAVED,
             "Output memory layout must be interleaved when sub_core_grid argument provided");
         TT_FATAL(
             this->use_multicore == true,
             "sub_core_grid implementation only supported when use_multicore flag argument is set to true");
     }
 
-    // If input is sharded, then the shard shape must be in multiples of tiles
-    if (input_is_sharded) {
-        std::array<uint32_t, 2> input_shard_shape = input_tensor_a.shard_spec().value().shape;
-        uint32_t input_shard_width = input_shard_shape[1];
-        uint32_t input_shard_height = input_shard_shape[0];
+    if (!this->use_multicore) {
         TT_FATAL(
-            input_shard_width % TILE_WIDTH == 0,
-            "Input shard width {} must be a multiple of tile width",
-            input_shard_width);
+            input_tensor_a.memory_config().memory_layout() != TensorMemoryLayout::SINGLE_BANK,
+            "Input memory layout must be interleaved or sharded");
         TT_FATAL(
-            input_shard_height % TILE_HEIGHT == 0,
-            "Input shard height {} must be a multiple of tile height",
-            input_shard_height);
-    }
+            this->output_mem_config.memory_layout() != TensorMemoryLayout::SINGLE_BANK,
+            "Output memory layout must be interleaved or sharded");
 
-    // We don't support input or output uneven sharding for the single core implementation
-    if (!this->use_multicore) {
-        // Check for input uneven sharding
-        if (input_is_sharded) {
+        if (input_tensor_a.is_sharded()) {
             std::array<uint32_t, 2> input_shard_shape = input_tensor_a.shard_spec().value().shape;
-            uint32_t input_shard_width = input_shard_shape[1];
-            uint32_t input_shard_height = input_shard_shape[0];
             TT_FATAL(
-                tensor_width % input_shard_width == 0,
-                "Uneven input shard width {} for tensor width {} not supported for single core implementation",
-                input_shard_width,
-                tensor_width);
+                input_tensor_a.padded_shape()[-1] % input_shard_shape[1] == 0,
+                "Uneven input shard shape not supported");
             TT_FATAL(
-                tensor_height % input_shard_height == 0,
-                "Uneven input shard height {} for tensor height {} not supported for single core implementation",
-                input_shard_height,
-                tensor_height);
+                (input_tensor_a.physical_volume() / input_tensor_a.padded_shape()[-1]) % input_shard_shape[0] == 0,
+                "Uneven input shard shape not supported");
         }
-        // Check for output uneven sharding
-        if (output_is_sharded) {
+        if (this->output_mem_config.is_sharded()) {
             std::array<uint32_t, 2> output_shard_shape = this->output_mem_config.shard_spec().value().shape;
-            uint32_t output_shard_width = output_shard_shape[1];
-            uint32_t output_shard_height = output_shard_shape[0];
             TT_FATAL(
-                tensor_width % output_shard_width == 0,
-                "Uneven output shard width {} for tensor width {} not supported for single core implementation",
-                output_shard_width,
-                tensor_width);
+                input_tensor_a.padded_shape()[-1] % output_shard_shape[1] == 0,
+                "Uneven output shard shape not supported");
             TT_FATAL(
-                tensor_height % output_shard_height == 0,
-                "Uneven output shard height {} for tensor height {} not supported for single core implementation",
-                output_shard_height,
-                tensor_height);
+                (input_tensor_a.physical_volume() / input_tensor_a.padded_shape()[-1]) % output_shard_shape[0] == 0,
+                "Uneven output shard shape not supported");
         }
-    }
-
-    // We always support uneven input sharding for the multicore implementation. Uneven output sharding is only
-    // supported if the input and output memory layouts are identical (i.e. height->height, width->width, block->block)
-    // and the input and output shard specs are identical. Otherwise uneven output sharding is not supported.
-    if (output_is_sharded) {
-        std::array<uint32_t, 2> output_shard_shape = this->output_mem_config.shard_spec().value().shape;
-        uint32_t output_shard_width = output_shard_shape[1];
-        uint32_t output_shard_height = output_shard_shape[0];
-
-        bool output_is_uneven_sharded_width_wise = tensor_width % output_shard_width != 0;
-        bool output_is_uneven_sharded_height_wise = tensor_height % output_shard_height != 0;
-        if (output_is_uneven_sharded_width_wise || output_is_uneven_sharded_height_wise) {
-            TT_FATAL(
-                input_memory_layout == output_memory_layout,
-                "Input and output memory layouts must be identical if output is uneven sharded");
+    } else if (input_tensor_a.memory_config().is_sharded()) {
+        if (this->output_mem_config.is_sharded()) {
             TT_FATAL(
-                input_tensor_a.shard_spec().value() == this->output_mem_config.shard_spec().value(),
-                "Input and output shard specs must be identical if output is uneven sharded");
+                this->output_mem_config.memory_layout() == input_tensor_a.memory_config().memory_layout(), "Error");
         }
-    }
-
-    // Multicore implementation doesn't support input DRAM sharding
-    if (this->use_multicore && input_is_sharded) {
-        TT_FATAL(input_buffer_type == BufferType::L1, "Multicore implementation doesn't support DRAM sharding");
-    }
-
-    // We don't support output DRAM block sharding
-    if (output_memory_layout == TensorMemoryLayout::BLOCK_SHARDED) {
-        TT_FATAL(output_buffer_type == BufferType::L1, "We don't support DRAM block sharding");
+        if (input_tensor_a.memory_config().memory_layout() == TensorMemoryLayout::BLOCK_SHARDED) {
+            TT_FATAL(input_tensor_a.shard_spec().value().grid.ranges().size() == 1, "Error");
+        }
+        TT_FATAL(this->use_multicore == true, "Error");
+    } else if (this->output_mem_config.is_sharded()) {
+        TT_FATAL(this->use_multicore == true, "Error");
+        TT_FATAL(this->output_mem_config.memory_layout() == TensorMemoryLayout::HEIGHT_SHARDED, "Error");
+        uint32_t ntiles = input_tensor_a.physical_volume() / TILE_HW;
+        uint32_t ntiles_per_block = input_tensor_a.padded_shape()[-1] / TILE_WIDTH;
+        uint32_t nblocks = std::ceil((float)ntiles / ntiles_per_block);
+        auto num_cores =
+            untilize_helpers::get_num_cores(input_tensor_a.device()->compute_with_storage_grid_size(), nblocks);
+        uint32_t fused_height = input_tensor_a.physical_volume() / input_tensor_a.padded_shape()[-1];
+        TT_FATAL(fused_height % num_cores == 0, "Error");
+    } else {
+        TT_FATAL(input_tensor_a.memory_config().memory_layout() == TensorMemoryLayout::INTERLEAVED, "Error");
+        TT_FATAL(this->output_mem_config.memory_layout() == TensorMemoryLayout::INTERLEAVED, "Error");
     }
 }
 
@@ -142,6 +107,51 @@ std::vector<ttnn::TensorSpec> Untilize::compute_output_specs(const std::vector<T
     const auto& input_tensor = input_tensors.at(0);
     DataType output_dtype = input_tensor.dtype() == DataType::BFLOAT8_B ? DataType::BFLOAT16 : input_tensor.dtype();
 
+    if (!this->use_multicore) {
+        return {TensorSpec(
+            input_tensor.logical_shape(),
+            TensorLayout::fromPaddedShape(
+                output_dtype,
+                PageConfig(Layout::ROW_MAJOR),
+                this->output_mem_config,
+                input_tensor.logical_shape(),
+                input_tensor.padded_shape()))};
+    }
+
+    if (this->output_mem_config.is_sharded()) {
+        if (input_tensor.memory_config().is_sharded()) {
+            auto mem_config = this->output_mem_config.with_shard_spec(input_tensor.memory_config().shard_spec());
+            return {TensorSpec(
+                input_tensor.logical_shape(),
+                TensorLayout::fromPaddedShape(
+                    output_dtype,
+                    PageConfig(Layout::ROW_MAJOR),
+                    mem_config,
+                    input_tensor.logical_shape(),
+                    input_tensor.padded_shape()))};
+        }
+
+        uint32_t ntiles = input_tensor.physical_volume() / TILE_HW;
+        uint32_t ntiles_per_block = input_tensor.padded_shape()[-1] / TILE_WIDTH;
+        uint32_t nblocks = std::ceil((float)ntiles / ntiles_per_block);
+        auto num_cores =
+            untilize_helpers::get_num_cores(input_tensor.device()->compute_with_storage_grid_size(), nblocks);
+        auto shard_grid = tt::tt_metal::num_cores_to_corerangeset(
+            num_cores, input_tensor.device()->compute_with_storage_grid_size(), true);
+        uint32_t fused_height = input_tensor.physical_volume() / input_tensor.padded_shape()[-1];
+        std::array<uint32_t, 2> shard_shape = {fused_height / num_cores, input_tensor.padded_shape()[-1]};
+        ShardSpec shard_spec{shard_grid, shard_shape, ShardOrientation::ROW_MAJOR};
+        auto mem_config = this->output_mem_config.with_shard_spec(shard_spec);
+        return {TensorSpec(
+            input_tensor.logical_shape(),
+            TensorLayout::fromPaddedShape(
+                output_dtype,
+                PageConfig(Layout::ROW_MAJOR),
+                mem_config,
+                input_tensor.logical_shape(),
+                input_tensor.padded_shape()))};
+    }
+
     return {TensorSpec(
         input_tensor.logical_shape(),
         TensorLayout::fromPaddedShape(
@@ -157,24 +167,13 @@ operation::ProgramWithCallbacks Untilize::create_program(
     const auto& input_tensor_a = input_tensors.at(0);
     auto& output_tensor = output_tensors.at(0);
 
-    bool input_is_sharded = input_tensor_a.is_sharded();
-    bool output_is_sharded = output_tensor.is_sharded();
-
-    BufferType input_buffer_type = input_tensor_a.memory_config().buffer_type();
-    BufferType output_buffer_type = output_tensor.memory_config().buffer_type();
-
-    TensorMemoryLayout input_memory_layout = input_tensor_a.memory_config().memory_layout();
-    TensorMemoryLayout output_memory_layout = output_tensor.memory_config().memory_layout();
-
     if (!this->use_multicore) {
-        // Single core implementation
         return detail::untilize_single_core(
             input_tensor_a, output_tensor, this->use_pack_untilize, this->fp32_dest_acc_en);
     }
     if (this->sub_core_grids.has_value()) {
         // If sub_core_grids parameter is provided, use custom sub_core_grid implementation instead
-        // of the standard multicore implementation or the block multicore implementation.
-        // Note that this implementation does not support sharding, which is enforced in validate().
+        // of the standard multicore implementation or the block multicore implementation
         return detail::untilize_multi_core_sub_core_grids(
             input_tensor_a,
             output_tensor,
@@ -182,22 +181,11 @@ operation::ProgramWithCallbacks Untilize::create_program(
             this->fp32_dest_acc_en,
             this->sub_core_grids.value());
     }
-    if (!this->enough_space_height && !input_is_sharded && !output_is_sharded) {
-        // Optimized special case implementation, only supported when neither input or output is sharded
+    if (!this->enough_space_height && !input_tensor_a.is_sharded() && !output_tensor.is_sharded()) {
         return detail::untilize_multi_core_block(
             input_tensor_a, output_tensor, this->use_pack_untilize, this->fp32_dest_acc_en);
     }
-    if (input_is_sharded && output_is_sharded && input_buffer_type == BufferType::L1 &&
-        output_buffer_type == BufferType::L1 && input_memory_layout == output_memory_layout &&
-        input_tensor_a.shard_spec() == output_tensor.shard_spec()) {
-        // Optimized special case implementation for when both input and output are sharded, both are located in L1,
-        // have identical memory layouts (i.e. height->height, width->width, block->block), and have identical shard
-        // specs
-        return detail::untilize_multi_core_input_and_output_shard_type_and_shard_spec_identical(
-            input_tensor_a, output_tensor, this->use_pack_untilize, this->fp32_dest_acc_en);
-    }
 
-    // Default multi core implementation
     return detail::untilize_multi_core(input_tensor_a, output_tensor, this->use_pack_untilize, this->fp32_dest_acc_en);
 }
 
diff --git a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/untilize_program_factory.cpp b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/untilize_program_factory.cpp
index 69cb46760d..dfcdf385dd 100644
--- a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/untilize_program_factory.cpp
+++ b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/untilize_program_factory.cpp
@@ -737,172 +737,35 @@ operation::ProgramWithCallbacks untilize_multi_core_block(
     return {std::move(program), override_runtime_args_callback};
 }
 
-operation::ProgramWithCallbacks untilize_multi_core_input_and_output_shard_type_and_shard_spec_identical(
-    const Tensor& a, Tensor& output, bool use_pack_untilize, bool fp32_dest_acc_en) {
-    tt::tt_metal::Program program{};
-
-    tt::DataFormat input_cb_data_format = tt::tt_metal::datatype_to_dataformat_converter(a.dtype());
-    uint32_t input_single_tile_size = tt::tt_metal::detail::TileSize(input_cb_data_format);
-    tt::DataFormat output_cb_data_format = tt::tt_metal::datatype_to_dataformat_converter(output.dtype());
-    uint32_t output_single_tile_size = tt::tt_metal::detail::TileSize(output_cb_data_format);
-
-    tt::tt_metal::IDevice* device = a.device();
-    tt::tt_metal::Buffer* src0_buffer = a.buffer();
-    tt::tt_metal::Buffer* dst_buffer = output.buffer();
-    TT_FATAL(dst_buffer != nullptr, "Output buffer should be allocated on device!");
-
-    const auto& tile_shape = a.tensor_spec().tile().get_tile_shape();
-    uint32_t tile_height = tile_shape[0];
-    uint32_t tile_width = tile_shape[1];
-
-    ShardSpec shard_spec = a.shard_spec().value();
-    uint32_t shard_height = shard_spec.shape[0];
-    uint32_t shard_width = shard_spec.shape[1];
-
-    uint32_t num_tiles_per_block = shard_width / tile_width;
-    uint32_t num_blocks_per_core = shard_height / tile_height;
-    uint32_t num_tiles_per_shard = num_tiles_per_block * num_blocks_per_core;
-
-    // Input CB
-    auto [src0_cb_index, cb_src0] = create_cb(
-        tt::CBIndex::c_0,
-        program,
-        shard_spec.grid,
-        input_single_tile_size,
-        num_tiles_per_shard,
-        input_cb_data_format,
-        src0_buffer);
-
-    // Output CB
-    auto [output_cb_index, cb_output] = create_cb(
-        tt::CBIndex::c_16,
-        program,
-        shard_spec.grid,
-        output_single_tile_size,
-        num_tiles_per_shard,
-        output_cb_data_format,
-        dst_buffer);
-
-    // Reader compile-time args
-    std::vector<uint32_t> reader_compile_time_args = {(uint32_t)src0_cb_index};
-
-    // Reader kernel
-    KernelHandle unary_reader_kernel_id = tt::tt_metal::CreateKernel(
-        program,
-        "ttnn/cpp/ttnn/operations/eltwise/unary/device/kernels/dataflow/reader_unary_sharded.cpp",
-        shard_spec.grid,
-        tt::tt_metal::ReaderDataMovementConfig(reader_compile_time_args));
-
-    // Writer compile-time args
-    std::vector<uint32_t> writer_compile_time_args = {(uint32_t)output_cb_index};
-
-    // Writer kernel
-    KernelHandle unary_writer_kernel_id = tt::tt_metal::CreateKernel(
-        program,
-        "ttnn/cpp/ttnn/operations/data_movement/sharded/device/kernels/dataflow/writer_unary_sharded.cpp",
-        shard_spec.grid,
-        tt::tt_metal::WriterDataMovementConfig(writer_compile_time_args));
-
-    // Compute compile-time args
-    std::vector<uint32_t> compute_compile_time_args = {
-        (uint32_t)num_blocks_per_core,
-        (uint32_t)num_tiles_per_block,
-        (uint32_t)src0_cb_index,
-        (uint32_t)output_cb_index};
-
-    // Compute kernel
-    std::string compute_kernel;
-    if (num_tiles_per_block > MAX_PACK_UNTILIZE_WIDTH || !use_pack_untilize || a.dtype() == DataType::UINT16) {
-        log_debug(tt::LogOp, "Using slow untilize.");
-        compute_kernel =
-            std::string("ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/untilize.cpp");
-    } else {
-        log_debug(tt::LogOp, "Using fast pack untilize.");
-        compute_kernel =
-            std::string("ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/pack_untilize.cpp");
-    }
-    KernelHandle untilize_kernel_id = CreateKernel(
-        program,
-        compute_kernel,
-        shard_spec.grid,
-        ComputeConfig{.fp32_dest_acc_en = fp32_dest_acc_en, .compile_args = compute_compile_time_args});
-
-    // Run-time args
-    auto cores =
-        corerange_to_cores(shard_spec.grid, std::nullopt, shard_spec.orientation == ShardOrientation::ROW_MAJOR);
-    for (uint32_t i = 0; i < cores.size(); ++i) {
-        CoreCoord core = cores[i];
-
-        // Reader run-time args
-        uint32_t num_tiles_to_read = num_tiles_per_block * num_blocks_per_core;
-        std::vector<uint32_t> reader_run_time_args = {num_tiles_to_read};
-
-        // Writer run-time args
-        uint32_t num_tiles_to_write = num_tiles_per_block * num_blocks_per_core;
-        std::vector<uint32_t> writer_run_time_args = {num_tiles_to_write};
-
-        tt::tt_metal::SetRuntimeArgs(program, unary_reader_kernel_id, core, reader_run_time_args);
-        tt::tt_metal::SetRuntimeArgs(program, unary_writer_kernel_id, core, writer_run_time_args);
-    }
-
-    auto override_runtime_args_callback = [reader_kernel_id = unary_reader_kernel_id,
-                                           writer_kernel_id = unary_writer_kernel_id,
-                                           cb_src0 = cb_src0,
-                                           cb_output = cb_output](
-                                              const void* operation,
-                                              Program& program,
-                                              const std::vector<Tensor>& input_tensors,
-                                              const std::vector<std::optional<const Tensor>>&,
-                                              const std::vector<Tensor>& output_tensors) {
-        auto src_buffer = input_tensors.at(0).buffer();
-        auto dst_buffer = output_tensors.at(0).buffer();
-
-        UpdateDynamicCircularBufferAddress(program, cb_src0, *src_buffer);
-        UpdateDynamicCircularBufferAddress(program, cb_output, *dst_buffer);
-    };
-
-    return {std::move(program), override_runtime_args_callback};
-}
-
 operation::ProgramWithCallbacks untilize_multi_core(
     const Tensor& a, Tensor& output, bool use_pack_untilize, bool fp32_dest_acc_en) {
     tt::tt_metal::Program program{};
 
+    bool src_sharded = a.memory_config().is_sharded();
+    bool out_sharded = output.memory_config().is_sharded();
+
     tt::DataFormat input_cb_data_format = tt::tt_metal::datatype_to_dataformat_converter(a.dtype());
     uint32_t input_single_tile_size = tt::tt_metal::detail::TileSize(input_cb_data_format);
     tt::DataFormat output_cb_data_format = tt::tt_metal::datatype_to_dataformat_converter(output.dtype());
     uint32_t output_single_tile_size = tt::tt_metal::detail::TileSize(output_cb_data_format);
 
-    tt::tt_metal::IDevice* device = a.device();
-    tt::tt_metal::Buffer* src0_buffer = a.buffer();
-    tt::tt_metal::Buffer* dst_buffer = output.buffer();
-    TT_FATAL(dst_buffer != nullptr, "Output buffer should be allocated on device!");
-
-    uint32_t tensor_width = a.padded_shape()[-1];
-    uint32_t tensor_height = a.physical_volume() / tensor_width;
-
-    const auto& tile_shape = a.tensor_spec().tile().get_tile_shape();
-    uint32_t tile_height = tile_shape[0];
-    uint32_t tile_width = tile_shape[1];
-    uint32_t tile_volume = tile_height * tile_width;
+    IDevice* device = a.device();
 
-    bool input_is_sharded = a.is_sharded();
-    bool output_is_sharded = output.is_sharded();
+    uint32_t num_tiles_per_row = a.padded_shape()[-1] / TILE_WIDTH;
 
-    uint32_t num_tiles_per_row = tensor_width / tile_width;
-    uint32_t num_tiles_per_col = tensor_height / tile_height;
+    uint32_t num_tiles_per_col = a.padded_shape()[-2] / TILE_HEIGHT;
 
+    uint32_t ntiles = a.physical_volume() / TILE_HW;
+    uint32_t stick_s = a.padded_shape()[-1];
+    uint32_t ntiles_per_block = a.padded_shape()[-1] / TILE_WIDTH;
+    uint32_t nblocks = std::ceil((float)ntiles / ntiles_per_block);
+    uint32_t block_size_nbytes = a.padded_shape()[-1] * output.element_size();
     auto grid_size = device->compute_with_storage_grid_size();
-    auto
-        [num_compute_cores,
-         compute_core_range,
-         full_compute_core_range,
-         cliff_compute_core_range,
-         num_rows_per_full_core,
-         num_rows_per_cliff_core] = ttnn::split_blocks_for_tilize(grid_size, num_tiles_per_col);
+    auto [ncores, all_cores, core_range, core_range_cliff, nblocks_per_core, nblocks_per_core_cliff] =
+        ttnn::split_blocks_for_tilize(grid_size, nblocks);
 
     constexpr uint32_t threshold_row_block = 32;
-    if (!input_is_sharded and !output_is_sharded) {
+    if (!src_sharded and !out_sharded) {
         if (num_tiles_per_row > threshold_row_block) {
             if (num_tiles_per_col > threshold_row_block || num_tiles_per_row > num_tiles_per_col) {
                 uint32_t num_blocks_block = (a.padded_shape()[-1] * a.padded_shape()[-2]) / (TILE_HEIGHT * TILE_WIDTH);
@@ -924,357 +787,380 @@ operation::ProgramWithCallbacks untilize_multi_core(
                      full_cores_per_col] =
                         ttnn::split_blocks_for_tilize_wh(
                             grid_size, num_blocks_block, num_tiles_per_row, num_tiles_per_col);
-                if (num_compute_cores < ncores_block) {
+                if (ncores < ncores_block) {
                     return untilize_multi_core_block(a, output, use_pack_untilize, fp32_dest_acc_en);
                 }
             }
         }
     }
 
-    // TODO (#23449): This memory calculation is a) outdated/inaccurate and needs to be fixed and b) needs
-    // to be moved up a few layers as the available memory may be different upon a program cache hit
-
-    // Determine how much L1 space we can use for input and output CBs,
-    // ensuring that we don't intrude into other L1 storage space
     uint32_t max_l1_size =
-        device->l1_size_per_core() / 2 - device->allocator()->get_base_allocator_addr(HalMemType::L1);
-
-    // Determine the max number of tiles that can be in any CB at a given time (1 input CB + 1 output CB = 2 total CBs)
-    uint32_t max_tiles_per_cb = max_l1_size / (input_single_tile_size + output_single_tile_size);
+        a.device()->l1_size_per_core() / 2 - a.device()->allocator()->get_base_allocator_addr(HalMemType::L1);
+    uint32_t max_tiles =
+        (max_l1_size / (input_single_tile_size + output_single_tile_size));  // 2 CBs, double buffering each
 
     // TODO : currently multi_core parallelization on column only works for single tile height tensors.
     // Need to debug this to work on wide tensors that are higher than a single tile
-
-    // If the input is interleaved and an entire row of tiles can't fit in a CB at once
-    if (!input_is_sharded && num_tiles_per_row > max_tiles_per_cb) {
-        // If the output is also interleaved and the tensor is only a single tile high, we can
-        // parellize the work column wise. Otherwise we have to resort to the single core implementation,
-        // as the current default multi core implementation processes an entire row of tiles at once.
-        if (!output_is_sharded && num_tiles_per_col == 1) {
-            return untilize_multi_core_parallelize_column(a, output, use_pack_untilize, fp32_dest_acc_en);
-        } else {
-            return untilize_single_core(a, output, use_pack_untilize, fp32_dest_acc_en);
+    if (ntiles_per_block > max_tiles) {
+        if (!src_sharded and !out_sharded) {
+            uint32_t ntiles_height = ntiles / ntiles_per_block;
+            if (ntiles_height == 1) {
+                return untilize_multi_core_parallelize_column(a, output, use_pack_untilize, fp32_dest_acc_en);
+
+            } else {
+                return untilize_single_core(a, output, use_pack_untilize, fp32_dest_acc_en);
+            }
         }
     }
 
-    // Default values are for interleaved input.
-    // Cliff core applicable interleaved input only, it is the only core not processing the
-    // same number of rows (blocks) as all other cores.
-    uint32_t num_input_blocks_across_width = 1;
-    uint32_t num_tiles_per_input_block = num_tiles_per_row;
-    uint32_t num_input_blocks_per_full_core = num_rows_per_full_core;
-    uint32_t num_input_blocks_per_cliff_core = num_rows_per_cliff_core;
-    if (input_is_sharded) {
-        ShardSpec input_shard_spec = a.shard_spec().value();
-        uint32_t input_shard_height = input_shard_spec.shape[0];
-        uint32_t input_shard_width = input_shard_spec.shape[1];
-
-        num_compute_cores = input_shard_spec.grid.num_cores();
-        compute_core_range = input_shard_spec.grid;
-        full_compute_core_range = input_shard_spec.grid;
-        cliff_compute_core_range = CoreRangeSet();
-
-        // Note: Accounting for uneven input shards
-        num_input_blocks_across_width = tt::div_up(tensor_width, input_shard_width);
-        num_tiles_per_input_block = input_shard_width / tile_width;
-        num_input_blocks_per_full_core = input_shard_height / tile_height;
-        num_input_blocks_per_cliff_core = 0;
-    }
+    uint32_t ncores_x = grid_size.x;
+    uint32_t ncores_y = std::ceil(static_cast<float>(ncores) / ncores_x);
 
-    // Input CB
-    uint32_t input_cb_num_tiles;
-    if (input_is_sharded) {
-        // Have compute core untilize the entire shard at once
-        input_cb_num_tiles = num_tiles_per_input_block * num_input_blocks_per_full_core;
-    } else {
-        if (num_input_blocks_per_full_core == 1) {
-            // No need to double buffer if the core is only processing a single block
-            input_cb_num_tiles = num_tiles_per_input_block;
-        } else {
-            // Double buffer if the core is processing 2+ blocks
-            input_cb_num_tiles = num_tiles_per_input_block * 2;
-        }
+    bool row_major = true;
+    bool src_block_sharded = false;
+    uint32_t num_rows_block = 0, block_row_size = 0, output_row_size = 0, last_block_row_size_unpadded = 0,
+             num_output_rows_unpadded = 0;
+    CoreCoord end_core;
+    std::vector<CoreCoord> cores_with_rtargs;
+
+    if (src_sharded) {
+        auto shard_spec = a.shard_spec().value();
+        src_block_sharded = a.memory_config().memory_layout() != TensorMemoryLayout::HEIGHT_SHARDED;
+        row_major = shard_spec.orientation == ShardOrientation::ROW_MAJOR;
+        ncores_y = device->compute_with_storage_grid_size().y;
+        all_cores = shard_spec.grid;
+        uint32_t num_cores = all_cores.num_cores();
+        ncores = num_cores;
+        core_range = all_cores;
+        core_range_cliff = CoreRangeSet();
+        ntiles_per_block = shard_spec.shape[1] / TILE_WIDTH;
+        nblocks_per_core = shard_spec.shape[0] / TILE_HEIGHT;
+        nblocks_per_core_cliff = 0;
+
+        num_rows_block = shard_spec.shape[0];
+        block_row_size = shard_spec.shape[1] * output.element_size();  // in0_block_w * TILE_WIDTH * dtype_nbytes
+        output_row_size = output.padded_shape()[-1] * output.element_size();  // output row size bytes
+        last_block_row_size_unpadded = block_row_size - (tt::round_up(output.padded_shape()[-1], shard_spec.shape[1]) -
+                                                         output.padded_shape()[-1]) *
+                                                            output.element_size();
+        uint32_t num_output_rows = output.physical_volume() / output.padded_shape()[-1];
+        num_output_rows_unpadded =
+            num_rows_block - (tt::round_up(num_output_rows, shard_spec.shape[0]) - num_output_rows);
+        end_core = (*shard_spec.grid.ranges().begin()).end_coord;
     }
+
+    uint32_t num_input_tiles = src_sharded ? ntiles_per_block * nblocks_per_core : ntiles_per_block * 2;
     auto [src0_cb_index, cb_src0] = create_cb(
         tt::CBIndex::c_0,
         program,
-        compute_core_range,
+        all_cores,
         input_single_tile_size,
-        input_cb_num_tiles,
+        num_input_tiles,
         input_cb_data_format,
-        input_is_sharded ? src0_buffer : nullptr);
+        src_sharded ? a.buffer() : nullptr);
 
-    // Output CB
-    uint32_t output_cb_num_tiles;
-    if (num_input_blocks_per_full_core == 1) {
-        // No need to double buffer if the core is only processing a single block
-        output_cb_num_tiles = num_tiles_per_input_block;
-    } else {
-        // Double buffer if the core is processing 2+ blocks
-        output_cb_num_tiles = num_tiles_per_input_block * 2;
-    }
+    uint32_t num_output_tiles = out_sharded ? ntiles_per_block * nblocks_per_core : ntiles_per_block * 2;
     auto [output_cb_index, cb_output] = create_cb(
         tt::CBIndex::c_16,
         program,
-        compute_core_range,
+        all_cores,
         output_single_tile_size,
-        output_cb_num_tiles,
-        output_cb_data_format);
+        num_output_tiles,
+        output_cb_data_format,
+        out_sharded ? output.buffer() : nullptr);
 
-    // Reader compile-time args and kernel
+    Buffer* src0_buffer = a.buffer();
+    Buffer* dst_buffer = output.buffer();
+    TT_ASSERT(dst_buffer != nullptr, "Output buffer should be allocated on device!");
+
+    /** reader
+     */
     KernelHandle unary_reader_kernel_id;
-    if (input_is_sharded) {
-        // Sharded input
-        std::vector<uint32_t> reader_compile_time_args = {(uint32_t)src0_cb_index};
+
+    if (src_sharded) {
+        std::vector<uint32_t> reader_ct_args = {(std::uint32_t)src0_cb_index};
+
         unary_reader_kernel_id = tt::tt_metal::CreateKernel(
             program,
             "ttnn/cpp/ttnn/operations/eltwise/unary/device/kernels/dataflow/reader_unary_sharded.cpp",
-            compute_core_range,
-            tt::tt_metal::ReaderDataMovementConfig(reader_compile_time_args));
+            all_cores,
+            tt::tt_metal::ReaderDataMovementConfig(reader_ct_args));
     } else {
-        // Interleaved input
         bool src0_is_dram = src0_buffer->buffer_type() == BufferType::DRAM;
-        std::vector<uint32_t> reader_compile_time_args = {
-            (uint32_t)src0_is_dram,
-            (uint32_t)src0_cb_index,
-        };
+        std::vector<uint32_t> reader_ct_args = {(uint32_t)src0_is_dram};
+
         unary_reader_kernel_id = CreateKernel(
             program,
-            "ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/"
-            "reader_unary_start_id.cpp",
-            compute_core_range,
-            ReaderDataMovementConfig(reader_compile_time_args));
+            "ttnn/cpp/ttnn/operations/eltwise/unary/device/kernels/dataflow/reader_unary_interleaved_start_id.cpp",
+            all_cores,
+            ReaderDataMovementConfig(reader_ct_args));
     }
 
-    // Writer compute defines
-    std::map<string, string> writer_compute_defines;
-    if (output_is_sharded) {
-        writer_compute_defines["SHARDED"] = "1";
-    }
+    /** writer
+     */
+    KernelHandle unary_writer_kernel_id;
+    if (out_sharded) {
+        std::vector<uint32_t> writer_ct_args = {(std::uint32_t)output_cb_index};
+        unary_writer_kernel_id = tt::tt_metal::CreateKernel(
+            program,
+            "ttnn/cpp/ttnn/operations/data_movement/sharded/device/kernels/dataflow/writer_unary_sharded.cpp",
+            all_cores,
+            tt::tt_metal::WriterDataMovementConfig(writer_ct_args));
+    } else {
+        bool out_is_dram = dst_buffer->buffer_type() == BufferType::DRAM;
+        if (src_block_sharded) {
+            std::vector<uint32_t> writer_ct_args = {
+                (uint32_t)out_is_dram, (uint32_t)(input_cb_data_format == tt::DataFormat::Float32)};
+            unary_writer_kernel_id = CreateKernel(
+                program,
+                "ttnn/cpp/ttnn/deprecated/tt_dnn/kernels/dataflow/writer_unary_stick_layout_interleaved_blocks.cpp",
+                all_cores,
+                WriterDataMovementConfig(writer_ct_args));
+        } else {
+            bool stick_size_is_power_of_two = is_power_of_two_at_least_32(block_size_nbytes);
+            uint32_t log2_stick_size = stick_size_is_power_of_two ? (std::uint32_t)std::log2(block_size_nbytes) : 0;
+            std::vector<uint32_t> writer_ct_args = {
+                (uint32_t)out_is_dram,
+                (uint32_t)stick_size_is_power_of_two,
+                (uint32_t)log2_stick_size,
+            };
 
-    // Writer compile-time args
-    bool output_is_dram = dst_buffer->buffer_type() == tt::tt_metal::BufferType::DRAM;
-    uint32_t output_num_blocks_across_width = 1;
-    if (output.memory_config().memory_layout() == TensorMemoryLayout::WIDTH_SHARDED ||
-        output.memory_config().memory_layout() == TensorMemoryLayout::BLOCK_SHARDED) {
-        uint32_t output_shard_width = output.shard_spec().value().shape[1];
-        output_num_blocks_across_width = tensor_width / output_shard_width;
-    }
-    uint32_t output_stick_size = tensor_width * output.element_size() / output_num_blocks_across_width;
-    bool output_stick_size_is_power_of_two = is_power_of_two_at_least_32(output_stick_size);
-    uint32_t output_log_base_2_of_page_size =
-        output_stick_size_is_power_of_two ? (std::bit_width(output_stick_size) - 1) : 0;
-    uint32_t output_element_size = output.element_size();
-    uint32_t num_cols_per_input_block = num_tiles_per_input_block * tile_width;
-    uint32_t num_cols_per_output_block = tensor_width / output_num_blocks_across_width;
-    std::vector<uint32_t> writer_compile_time_args = {
-        (uint32_t)output_is_dram,
-        (uint32_t)output_cb_index,
-        (uint32_t)output_stick_size,
-        (uint32_t)output_stick_size_is_power_of_two,
-        (uint32_t)output_log_base_2_of_page_size,
-        (uint32_t)tile_height,
-        (uint32_t)num_tiles_per_input_block,
-        (uint32_t)output_num_blocks_across_width,
-        (uint32_t)output_element_size,
-        (uint32_t)num_cols_per_input_block,
-        (uint32_t)num_cols_per_output_block,
-    };
-    if (output_is_sharded) {
-        shard_builder::extend_sharding_compile_time_args(output, writer_compile_time_args);
+            unary_writer_kernel_id = CreateKernel(
+                program,
+                "ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/"
+                "writer_unary_stick_layout_split_rows_interleaved.cpp",
+                all_cores,
+                WriterDataMovementConfig(writer_ct_args));
+        }
     }
 
-    // Writer kernel
-    KernelHandle unary_writer_kernel_id = tt::tt_metal::CreateKernel(
-        program,
-        "ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/"
-        "writer_unary_stick_layout_split_rows_multi_core.cpp",
-        compute_core_range,
-        tt::tt_metal::WriterDataMovementConfig(writer_compile_time_args, writer_compute_defines));
+    /** compute
+     */
+    std::vector<uint32_t> compute_args = {
+        (uint32_t)nblocks_per_core,  // per_core_block_cnt
+        (uint32_t)ntiles_per_block,  // per_block_ntiles
+        (uint32_t)src0_cb_index,
+        (uint32_t)output_cb_index};
+    std::vector<uint32_t> compute_args_cliff = {
+        (uint32_t)nblocks_per_core_cliff,
+        (uint32_t)ntiles_per_block,  // per_block_ntiles
+        (uint32_t)src0_cb_index,
+        (uint32_t)output_cb_index};
 
-    // Compute kernel file
-    std::string compute_kernel;
-    if (num_tiles_per_input_block > MAX_PACK_UNTILIZE_WIDTH || !use_pack_untilize || a.dtype() == DataType::UINT16) {
+    std::string compute_kernel(
+        "ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/pack_untilize.cpp");
+    if (ntiles_per_block > MAX_PACK_UNTILIZE_WIDTH || !use_pack_untilize || a.dtype() == DataType::UINT16) {
         log_debug(tt::LogOp, "Using slow untilize.");
-        compute_kernel = std::string(
-            "ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/untilize_variable_num_blocks.cpp");
+        compute_kernel =
+            std::string("ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/untilize.cpp");
     } else {
         log_debug(tt::LogOp, "Using fast pack untilize.");
-        compute_kernel = std::string(
-            "ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/compute/"
-            "pack_untilize_variable_num_blocks.cpp");
     }
 
-    // Compute compile-time args and kernel
-    // Note: This condition is always true for sharded input
-    KernelHandle untilize_kernel_id = 0;
-    if (full_compute_core_range.ranges().size() > 0) {
-        std::vector<uint32_t> compute_compile_time_args = {
-            (uint32_t)num_tiles_per_input_block, (uint32_t)src0_cb_index, (uint32_t)output_cb_index};
-        untilize_kernel_id = CreateKernel(
+    if (core_range.ranges().size() > 0) {
+        auto untilize_kernel_id = CreateKernel(
             program,
             compute_kernel,
-            full_compute_core_range,
-            ComputeConfig{.fp32_dest_acc_en = fp32_dest_acc_en, .compile_args = compute_compile_time_args});
+            core_range,
+            ComputeConfig{.fp32_dest_acc_en = fp32_dest_acc_en, .compile_args = compute_args});
     }
-
-    // Compute Cliff compile_time args and kernel
-    // Note: This condition is always false for sharded input (sharded input will never have a cliff core)
-    KernelHandle untilize_cliff_kernel_id = 0;
-    if (cliff_compute_core_range.ranges().size() > 0) {
-        std::vector<uint32_t> compute_compile_time_args_cliff = {
-            (uint32_t)num_tiles_per_input_block, (uint32_t)src0_cb_index, (uint32_t)output_cb_index};
-        untilize_cliff_kernel_id = CreateKernel(
+    if (core_range_cliff.ranges().size() > 0) {
+        auto untilize_cliff_kernel_id = CreateKernel(
             program,
             compute_kernel,
-            cliff_compute_core_range,
-            ComputeConfig{.fp32_dest_acc_en = fp32_dest_acc_en, .compile_args = compute_compile_time_args_cliff});
+            core_range_cliff,
+            ComputeConfig{.fp32_dest_acc_en = fp32_dest_acc_en, .compile_args = compute_args_cliff});
     }
 
-    // Run-time arg assignment
-    // Note: This variable is only applicable to interleaved input
-    uint32_t tile_start_index = 0;
-
-    // Run-time args (full cores)
-    // Note: For sharded input, these are the only cores used
-    bool is_row_major = input_is_sharded ? a.shard_spec().value().orientation == ShardOrientation::ROW_MAJOR : true;
-    std::vector<CoreCoord> full_cores = corerange_to_cores(full_compute_core_range, std::nullopt, is_row_major);
-    for (uint32_t i = 0; i < full_cores.size(); ++i) {
-        CoreCoord core = full_cores[i];
-        uint32_t height_wise_input_block_start_index =
-            (i / num_input_blocks_across_width) * num_input_blocks_per_full_core;
-        uint32_t width_wise_input_block_index = i % num_input_blocks_across_width;
-
-        // Handle uneven input sharding width wise (writer run-time arg)
-        uint32_t num_unpadded_cols_per_input_block = num_cols_per_input_block;
-        if (input_is_sharded) {
-            bool is_last_input_shard_in_row = width_wise_input_block_index == num_input_blocks_across_width - 1;
-            if (is_last_input_shard_in_row) {
-                uint32_t input_shard_width = a.shard_spec().value().shape[1];
-                num_unpadded_cols_per_input_block =
-                    num_cols_per_input_block - (tt::round_up(tensor_width, input_shard_width) - tensor_width);
-            }
-        }
-
-        // Handle uneven input sharding height wise (reader, compute, writer run-time arg)
-        uint32_t num_input_blocks_to_process = num_input_blocks_per_full_core;
-        if (input_is_sharded) {
-            uint32_t input_shard_height = a.shard_spec().value().shape[0];
-            uint32_t height_wise_shard_index = i / num_input_blocks_across_width;
-            uint32_t num_shards_height_wise = tt::div_up(tensor_height, input_shard_height);
-            bool is_last_input_shard_in_col = height_wise_shard_index == num_shards_height_wise - 1;
-            if (is_last_input_shard_in_col) {
-                num_input_blocks_to_process =
-                    num_input_blocks_per_full_core -
-                    (tt::round_up(tensor_height, input_shard_height) - tensor_height) / tile_height;
-            }
+    // 1D distribution of blocks across all cores
+    uint32_t ncores_full = ncores;
+    auto full_cores = all_cores;
+    if (nblocks_per_core_cliff > 0 && nblocks_per_core_cliff < nblocks_per_core) {
+        // unequal case with cliff
+        ncores_full -= 1;
+        full_cores = core_range;
+    }
+    uint32_t tile_start_id = 0;
+    uint32_t row_start_id = 0;
+    auto cores = grid_to_cores(ncores_x * ncores_y, ncores_x, ncores_y, row_major);
+    for (uint32_t i = 0; i < cores.size(); i++) {
+        CoreCoord core = cores[i];
+        if (!full_cores.contains(core)) {
+            continue;
         }
+        // reader runtime args
+        std::vector<uint32_t> reader_rt_args;
 
-        // Reader run-time args
-        uint32_t num_tiles_to_read = num_tiles_per_input_block * num_input_blocks_to_process;
-        std::vector<uint32_t> reader_run_time_args;
-        if (input_is_sharded) {
-            // Sharded input
-            reader_run_time_args = {num_tiles_to_read};
+        if (src_sharded) {
+            reader_rt_args = {
+                ntiles_per_block * nblocks_per_core  // ntiles
+            };
         } else {
-            // Interleaved input
-            reader_run_time_args = {
-                src0_buffer->address(),
-                num_tiles_to_read,
-                tile_start_index,
+            reader_rt_args = {
+                src0_buffer->address(),               // src_addr
+                ntiles_per_block * nblocks_per_core,  // ntiles
+                tile_start_id                         // start_id
             };
         }
+        // log_debug(tt::LogOp, "reader[{}]: {},{} = {} ({})", src0_buffer->address(), core.x, core.y, tile_start_id,
+        // ntiles_per_block * nblocks_per_core);
+
+        // writer runtime args
+        std::vector<uint32_t> writer_rt_args;
+        if (out_sharded) {
+            writer_rt_args = {
+                ntiles_per_block * nblocks_per_core  // ntiles
+            };
+        } else {
+            if (src_block_sharded) {
+                uint32_t block_start_row_offset;
+                uint32_t block_start_row_id_offset;
+                uint32_t row_size_unpadded = block_row_size;
+                uint32_t num_rows_unpadded = num_rows_block;
+                if (row_major) {
+                    block_start_row_offset = core.x * block_row_size;
+                    block_start_row_id_offset = core.y * num_rows_block;
+                    if (core.x == end_core.x) {
+                        row_size_unpadded = last_block_row_size_unpadded;
+                    }
+                    if (core.y == end_core.y) {
+                        num_rows_unpadded = num_output_rows_unpadded;
+                    }
+                } else {
+                    block_start_row_offset = core.y * block_row_size;
+                    block_start_row_id_offset = core.x * num_rows_block;
+                    if (core.y == end_core.y) {
+                        row_size_unpadded = last_block_row_size_unpadded;
+                    }
+                    if (core.x == end_core.x) {
+                        num_rows_unpadded = num_output_rows_unpadded;
+                    }
+                }
 
-        // Writer run-time args
-        uint32_t input_block_global_col_index = width_wise_input_block_index * num_cols_per_input_block;
-        uint32_t width_wise_output_block_start_index = input_block_global_col_index / num_cols_per_output_block;
-        uint32_t num_cols_already_processed_in_first_output_block =
-            input_block_global_col_index % num_cols_per_output_block;
-        std::vector<uint32_t> writer_run_time_args = {
-            dst_buffer->address(),
-            num_input_blocks_to_process,
-            height_wise_input_block_start_index,
-            num_unpadded_cols_per_input_block,
-            width_wise_output_block_start_index,
-            num_cols_already_processed_in_first_output_block};
-        if (output_is_sharded) {
-            shard_builder::extend_sharding_run_time_args(output, writer_run_time_args);
+                writer_rt_args = {
+                    dst_buffer->address(),  // dst_addr
+                    num_rows_block,
+                    block_row_size,
+                    1,
+                    1,
+                    1,
+                    output_row_size,
+                    row_size_unpadded,
+                    num_rows_unpadded,
+                    block_start_row_id_offset,
+                    block_start_row_offset};
+            } else {
+                writer_rt_args = {
+                    dst_buffer->address(),           // dst_addr
+                    nblocks_per_core * TILE_HEIGHT,  // nblocks per core
+                    block_size_nbytes,               // block_size_nbytes
+                    ntiles_per_block,                // ntiles_per_block
+                    block_size_nbytes,               // block_size_nbytes
+                    1,                               // full blocks in a row
+                    0,
+                    0,
+                    row_start_id};
+            }
         }
+        // log_debug(tt::LogOp, "writer[{}]: {},{} = {} {}", dst_buffer->address(), core.x, core.y, block_size_nbytes,
+        // row_start_id);
 
-        // Compute run-time args
-        std::vector<uint32_t> compute_run_time_args = {num_input_blocks_to_process};
-
-        // Set run-time arg
-        tt::tt_metal::SetRuntimeArgs(program, unary_reader_kernel_id, core, reader_run_time_args);
-        tt::tt_metal::SetRuntimeArgs(program, unary_writer_kernel_id, core, writer_run_time_args);
-        tt::tt_metal::SetRuntimeArgs(program, untilize_kernel_id, core, compute_run_time_args);
+        tt::tt_metal::SetRuntimeArgs(program, unary_reader_kernel_id, core, reader_rt_args);
 
-        // Update index of first tile to read
-        tile_start_index += num_tiles_per_input_block * num_input_blocks_per_full_core;
+        tt::tt_metal::SetRuntimeArgs(program, unary_writer_kernel_id, core, writer_rt_args);
+        cores_with_rtargs.push_back(core);
+        tile_start_id += ntiles_per_block * nblocks_per_core;
+        row_start_id += TILE_HEIGHT * nblocks_per_core;
     }
+    if (ncores_full < ncores) {
+        // last core is the cliff core with nblocks_per_core_cliff blocks
+        CoreCoord core = row_major ? CoreCoord{ncores_full % ncores_x, ncores_full / ncores_x}
+                                   : CoreCoord{ncores_full / ncores_y, ncores_full % ncores_y};
+        // reader runtime args
+        std::vector<uint32_t> reader_rt_args;
 
-    // Run-time args (cliff core)
-    // Note: Only applicable if input is interleaved (sharded input will never have a cliff core)
-    std::vector<CoreCoord> cliff_cores = corerange_to_cores(cliff_compute_core_range, std::nullopt, is_row_major);
-    if (cliff_cores.size() > 0) {
-        // There should only ever be 0 or 1 cliff cores
-        CoreCoord cliff_core = cliff_cores[0];
-        uint32_t height_wise_input_block_start_index = full_cores.size() * num_input_blocks_per_full_core;
-        uint32_t width_wise_input_block_index = 0;
-
-        // Handle uneven input sharding width wise (writer run-time arg)
-        // Note: Since cliff core is only applicable to interleaved input, this core
-        // will never process an uneven shard (or any shard for that matter)
-        uint32_t num_unpadded_cols_per_input_block = num_cols_per_input_block;
-
-        // Handle uneven input sharding height wise (reader, compute, writer run-time arg)
-        // Note: Since cliff core is only applicable to interleaved input, this core
-        // will never process an uneven shard (or any shard for that matter)
-        uint32_t num_input_blocks_to_process = num_input_blocks_per_cliff_core;
-
-        // Writer run-time args
-        uint32_t input_block_global_col_index = width_wise_input_block_index * num_cols_per_input_block;
-        uint32_t width_wise_output_block_start_index = input_block_global_col_index / num_cols_per_output_block;
-        uint32_t num_cols_already_processed_in_first_output_block =
-            input_block_global_col_index % num_cols_per_output_block;
-        std::vector<uint32_t> writer_run_time_args = {
-            dst_buffer->address(),
-            num_input_blocks_to_process,
-            height_wise_input_block_start_index,
-            num_unpadded_cols_per_input_block,
-            width_wise_output_block_start_index,
-            num_cols_already_processed_in_first_output_block};
-        if (output_is_sharded) {
-            shard_builder::extend_sharding_run_time_args(output, writer_run_time_args);
+        if (src_sharded) {
+            reader_rt_args = {
+                ntiles_per_block * nblocks_per_core_cliff  // ntiles
+            };
+        } else {
+            reader_rt_args = {
+                src0_buffer->address(),                               // src_addr
+                (uint32_t)ntiles_per_block * nblocks_per_core_cliff,  // ntiles
+                tile_start_id                                         // start_id
+            };
         }
+        // log_debug(tt::LogOp, "reader: {},{} = {} ({})", core.x, core.y, tile_start_id, ntiles_per_block *
+        // nblocks_per_core_cliff);
+
+        // writer runtime args
+        std::vector<uint32_t> writer_rt_args;
+        if (out_sharded) {
+            writer_rt_args = {
+                ntiles_per_block * nblocks_per_core_cliff  // ntiles
+            };
+        } else {
+            if (src_block_sharded) {
+                uint32_t block_start_row_offset;
+                uint32_t block_start_row_id_offset;
+                uint32_t row_size_unpadded = block_row_size;
+                uint32_t num_rows_unpadded = num_rows_block;
+                if (row_major) {
+                    block_start_row_offset = core.x * block_row_size;
+                    block_start_row_id_offset = core.y * num_rows_block;
+                    if (core.x == end_core.x) {
+                        row_size_unpadded = last_block_row_size_unpadded;
+                    }
+                    if (core.y == end_core.y) {
+                        num_rows_unpadded = num_output_rows_unpadded;
+                    }
+                } else {
+                    block_start_row_offset = core.y * block_row_size;
+                    block_start_row_id_offset = core.x * num_rows_block;
+                    if (core.y == end_core.y) {
+                        row_size_unpadded = last_block_row_size_unpadded;
+                    }
+                    if (core.x == end_core.x) {
+                        num_rows_unpadded = num_output_rows_unpadded;
+                    }
+                }
+                writer_rt_args = {
+                    dst_buffer->address(),  // dst_addr
+                    num_rows_block,
+                    block_row_size,
+                    1,
+                    1,
+                    1,
+                    output_row_size,
+                    row_size_unpadded,
+                    num_rows_unpadded,
+                    block_start_row_id_offset,
+                    block_start_row_offset};
+            } else {
+                writer_rt_args = {
+                    dst_buffer->address(),                 // dst_addr
+                    nblocks_per_core_cliff * TILE_HEIGHT,  // nsticks
+                    block_size_nbytes,                     // stick_size_nbytes
+                    ntiles_per_block,                      // ntiles_per_block
+                    block_size_nbytes,                     // block_width_nbytes
+                    1,                                     // full blocks in a row
+                    0,                                     // UNUSED
+                    0,                                     // UNUSED
+                    row_start_id};
+            }
+        }
+        // log_debug(tt::LogOp, "writer: {},{} = {} {}", core.x, core.y, block_size_nbytes, row_start_id);
 
-        // Reader run-time args (always reading interleaved input as cliff core does not exist for sharded input)
-        uint32_t num_tiles_to_read = num_tiles_per_input_block * num_input_blocks_to_process;
-        std::vector<uint32_t> reader_run_time_args = {
-            src0_buffer->address(),
-            num_tiles_to_read,
-            tile_start_index,
-        };
-
-        // Compute run-time args
-        std::vector<uint32_t> compute_run_time_args = {num_input_blocks_to_process};
+        tt::tt_metal::SetRuntimeArgs(program, unary_reader_kernel_id, core, reader_rt_args);
 
-        // Set run-time args
-        tt::tt_metal::SetRuntimeArgs(program, unary_reader_kernel_id, cliff_core, reader_run_time_args);
-        tt::tt_metal::SetRuntimeArgs(program, unary_writer_kernel_id, cliff_core, writer_run_time_args);
-        tt::tt_metal::SetRuntimeArgs(program, untilize_cliff_kernel_id, cliff_core, compute_run_time_args);
+        tt::tt_metal::SetRuntimeArgs(program, unary_writer_kernel_id, core, writer_rt_args);
+        cores_with_rtargs.push_back(core);
     }
-
-    std::vector<CoreCoord> cores_with_run_time_args;
-    cores_with_run_time_args.reserve(full_cores.size() + cliff_cores.size());
-    cores_with_run_time_args.insert(cores_with_run_time_args.end(), full_cores.begin(), full_cores.end());
-    cores_with_run_time_args.insert(cores_with_run_time_args.end(), cliff_cores.begin(), cliff_cores.end());
-
     auto override_runtime_arguments_callback = [reader_kernel_id = unary_reader_kernel_id,
                                                 writer_kernel_id = unary_writer_kernel_id,
                                                 cb_src0 = cb_src0,
                                                 cb_output = cb_output,
-                                                cores_with_run_time_args](
+                                                cores_with_rtargs](
                                                    const void* operation,
                                                    Program& program,
                                                    const std::vector<Tensor>& input_tensors,
@@ -1283,31 +1169,31 @@ operation::ProgramWithCallbacks untilize_multi_core(
         auto src_buffer = input_tensors.at(0).buffer();
         auto dst_buffer = output_tensors.at(0).buffer();
 
-        bool input_is_sharded = input_tensors.at(0).is_sharded();
-        bool output_is_sharded = output_tensors.at(0).is_sharded();
+        bool src_sharded = input_tensors.at(0).memory_config().is_sharded();
+        bool out_sharded = output_tensors.at(0).memory_config().is_sharded();
 
-        // Reader
-        if (input_is_sharded) {
-            // Sharded input
+        if (src_sharded) {
             UpdateDynamicCircularBufferAddress(program, cb_src0, *src_buffer);
         } else {
-            // Interleaved input
             auto& runtime_args_by_core = GetRuntimeArgs(program, reader_kernel_id);
-            for (const CoreCoord& core : cores_with_run_time_args) {
+            for (const CoreCoord& core : cores_with_rtargs) {
                 auto& runtime_args = runtime_args_by_core[core.x][core.y];
                 runtime_args[0] = src_buffer->address();
             }
         }
 
-        // Writer
-        auto& runtime_args_by_core = GetRuntimeArgs(program, writer_kernel_id);
-        for (const CoreCoord& core : cores_with_run_time_args) {
-            auto& runtime_args = runtime_args_by_core[core.x][core.y];
-            runtime_args[0] = dst_buffer->address();
+        if (out_sharded) {
+            UpdateDynamicCircularBufferAddress(program, cb_output, *dst_buffer);
+        } else {
+            auto& runtime_args_by_core = GetRuntimeArgs(program, writer_kernel_id);
+            for (const CoreCoord& core : cores_with_rtargs) {
+                auto& runtime_args = runtime_args_by_core[core.x][core.y];
+                runtime_args[0] = dst_buffer->address();
+            }
         }
     };
 
-    return {std::move(program), override_runtime_arguments_callback};
+    return {.program = std::move(program), .override_runtime_arguments_callback = override_runtime_arguments_callback};
 }
 
 operation::ProgramWithCallbacks untilize_single_core(
@@ -1321,26 +1207,22 @@ operation::ProgramWithCallbacks untilize_single_core(
     tt::DataFormat output_cb_data_format = tt::tt_metal::datatype_to_dataformat_converter(output.dtype());
     uint32_t output_single_tile_size = tt::tt_metal::detail::TileSize(output_cb_data_format);
 
-    tt::tt_metal::IDevice* device = a.device();
     tt::tt_metal::Buffer* src0_buffer = a.buffer();
-    tt::tt_metal::Buffer* dst_buffer = output.buffer();
-    TT_ASSERT(dst_buffer != nullptr, "Output buffer should be allocated on device!");
 
     const auto& tile_shape = a.tensor_spec().tile().get_tile_shape();
     uint32_t tile_height = tile_shape[0];
     uint32_t tile_width = tile_shape[1];
     uint32_t tile_volume = tile_height * tile_width;
 
-    bool input_is_sharded = a.memory_config().is_sharded();
-    bool output_is_sharded = output.memory_config().is_sharded();
-
     uint32_t num_tiles = a.physical_volume() / tile_volume;
+
     uint32_t num_blocks_across_height = a.physical_volume() / a.padded_shape()[-1] / tile_height;
     uint32_t num_columns_of_blocks = 1;
     if (output.memory_config().memory_layout() == TensorMemoryLayout::WIDTH_SHARDED ||
         output.memory_config().memory_layout() == TensorMemoryLayout::BLOCK_SHARDED) {
         num_columns_of_blocks = a.padded_shape()[-1] / output.shard_spec().value().shape[1];
     }
+
     uint32_t num_tiles_per_column_row = a.padded_shape()[-1] / num_columns_of_blocks / tile_width;
 
     // Determine how much L1 space we can use for input and output CBs,
@@ -1365,27 +1247,38 @@ operation::ProgramWithCallbacks untilize_single_core(
     }
 
     uint32_t num_blocks_per_column_row = num_tiles_per_column_row / num_tiles_per_block;
-    uint32_t output_single_block_width_size = num_tiles_per_block * TILE_WIDTH * output.element_size();
+    uint32_t single_block_width_size = num_tiles_per_block * TILE_WIDTH * output.element_size();
     uint32_t num_total_sticks = a.physical_volume() / a.padded_shape()[-1] * num_columns_of_blocks;
-    uint32_t output_stick_size = a.physical_volume() * output.element_size() / num_total_sticks;
+    uint32_t stick_size = a.physical_volume() * output.element_size() / num_total_sticks;
 
-    // Input CB
-    uint32_t input_cb_num_tiles = num_tiles_per_block;
-    auto [src0_cb_index, cb_src0] =
-        create_cb(tt::CBIndex::c_0, program, core, input_single_tile_size, input_cb_num_tiles, input_cb_data_format);
+    // This should allocate a DRAM buffer on the device
+    tt::tt_metal::IDevice* device = a.device();
 
-    // Output CB
-    uint32_t output_cb_num_tiles = num_tiles_per_block;
-    auto [output_cb_index, cb_output] = create_cb(
-        tt::CBIndex::c_16, program, core, output_single_tile_size, output_cb_num_tiles, output_cb_data_format);
+    tt::tt_metal::Buffer* dst_buffer = output.buffer();
+    TT_ASSERT(dst_buffer != nullptr, "Output buffer should be allocated on device!");
+
+    uint32_t src0_cb_index = 0;
+    uint32_t num_input_tiles = num_tiles_per_block;
+    auto cb_src0_config = tt::tt_metal::CircularBufferConfig(
+                              num_input_tiles * input_single_tile_size, {{src0_cb_index, input_cb_data_format}})
+                              .set_page_size(src0_cb_index, input_single_tile_size);
+    auto cb_src0 = tt::tt_metal::CreateCircularBuffer(program, core, cb_src0_config);
+
+    uint32_t output_cb_index = tt::CBIndex::c_16;
+    uint32_t num_output_tiles = num_tiles_per_block;
+    auto cb_output_config = tt::tt_metal::CircularBufferConfig(
+                                num_output_tiles * output_single_tile_size, {{output_cb_index, output_cb_data_format}})
+                                .set_page_size(output_cb_index, output_single_tile_size);
+    auto cb_output = tt::tt_metal::CreateCircularBuffer(program, core, cb_output_config);
+
+    bool input_is_sharded = a.memory_config().is_sharded();
+    bool output_is_sharded = output.memory_config().is_sharded();
 
-    // Reader compute defines
     std::map<string, string> reader_compute_defines;
     if (input_is_sharded) {
         reader_compute_defines["SHARDED"] = "1";
     }
 
-    // Writer compute defines
     std::map<string, string> writer_compute_defines;
     if (output_is_sharded) {
         writer_compute_defines["SHARDED"] = "1";
@@ -1393,30 +1286,26 @@ operation::ProgramWithCallbacks untilize_single_core(
 
     // Reader compile-time args
     bool src0_is_dram = src0_buffer->buffer_type() == tt::tt_metal::BufferType::DRAM;
+    uint32_t tile_bytes = tile_volume * output.element_size();
+    uint32_t start_page_id = 0;
     std::vector<uint32_t> reader_compile_time_args = {
         (uint32_t)src0_is_dram,
         (uint32_t)src0_cb_index,
+        (uint32_t)num_tiles,
+        (uint32_t)tile_bytes,
+        (uint32_t)start_page_id,
     };
     if (input_is_sharded) {
         shard_builder::extend_sharding_compile_time_args(a, reader_compile_time_args);
     }
 
-    // Tilized reader
-    tt::tt_metal::KernelHandle unary_reader_kernel_id = tt::tt_metal::CreateKernel(
-        program,
-        "ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/"
-        "reader_unary_start_id.cpp",
-        core,
-        tt::tt_metal::ReaderDataMovementConfig(reader_compile_time_args, reader_compute_defines));
-
     // Writer compile-time args
     bool output_is_dram = dst_buffer->buffer_type() == tt::tt_metal::BufferType::DRAM;
-    bool stick_size_is_power_of_two = is_power_of_two_at_least_32(output_stick_size);
-    uint32_t log2_stick_size = stick_size_is_power_of_two ? (std::bit_width(output_stick_size) - 1) : 0;
+    bool stick_size_is_power_of_two = is_power_of_two_at_least_32(stick_size);
+    uint32_t log2_stick_size = stick_size_is_power_of_two ? (std::bit_width(stick_size) - 1) : 0;
     std::vector<uint32_t> writer_compile_time_args = {
         (uint32_t)output_is_dram,
         (uint32_t)output_cb_index,
-        (uint32_t)output_stick_size,
         (uint32_t)stick_size_is_power_of_two,
         (uint32_t)log2_stick_size,
         (uint32_t)tile_height,
@@ -1424,17 +1313,26 @@ operation::ProgramWithCallbacks untilize_single_core(
         (uint32_t)num_columns_of_blocks,
         (uint32_t)num_blocks_per_column_row,
         (uint32_t)num_tiles_per_block,
-        (uint32_t)output_single_block_width_size,
+        (uint32_t)single_block_width_size,
+        (uint32_t)stick_size,
     };
     if (output_is_sharded) {
         shard_builder::extend_sharding_compile_time_args(output, writer_compile_time_args);
     }
 
+    // Tilized reader
+    tt::tt_metal::KernelHandle unary_reader_kernel_id = tt::tt_metal::CreateKernel(
+        program,
+        "ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/"
+        "reader_unary_start_id.cpp",
+        core,
+        tt::tt_metal::ReaderDataMovementConfig(reader_compile_time_args, reader_compute_defines));
+
     // Untilized writer
     tt::tt_metal::KernelHandle unary_writer_kernel_id = tt::tt_metal::CreateKernel(
         program,
         "ttnn/cpp/ttnn/operations/data_movement/untilize/device/kernels/dataflow/"
-        "writer_unary_stick_layout_split_rows_single_core.cpp",
+        "writer_unary_stick_layout_split_rows.cpp",
         core,
         tt::tt_metal::WriterDataMovementConfig(writer_compile_time_args, writer_compute_defines));
 
@@ -1452,7 +1350,10 @@ operation::ProgramWithCallbacks untilize_single_core(
     // Compute compile-time args
     uint32_t num_blocks = num_columns_of_blocks * num_blocks_per_column_row * num_blocks_across_height;
     std::vector<uint32_t> compute_compile_time_args = {
-        (uint32_t)num_blocks, (uint32_t)num_tiles_per_block, (uint32_t)src0_cb_index, (uint32_t)output_cb_index};
+        (uint32_t)num_blocks,           // per_core_block_cnt
+        (uint32_t)num_tiles_per_block,  // per_core_block_tile_cnt
+        (uint32_t)src0_cb_index,
+        (uint32_t)output_cb_index};
 
     // Compute kernel
     auto untilize_kernel_id = tt::tt_metal::CreateKernel(
@@ -1462,24 +1363,17 @@ operation::ProgramWithCallbacks untilize_single_core(
         tt::tt_metal::ComputeConfig{.fp32_dest_acc_en = fp32_dest_acc_en, .compile_args = compute_compile_time_args});
 
     // Reader run-time args
-    uint32_t start_page_id = 0;
-    std::vector<uint32_t> reader_run_time_args = {
-        src0_buffer->address(),
-        num_tiles,
-        start_page_id,
-    };
+    std::vector<uint32_t> reader_run_time_args = {src0_buffer->address()};
     if (input_is_sharded) {
         shard_builder::extend_sharding_run_time_args(a, reader_run_time_args);
     }
+    tt::tt_metal::SetRuntimeArgs(program, unary_reader_kernel_id, core, reader_run_time_args);
 
     // Writer run-time args
     std::vector<uint32_t> writer_run_time_args = {dst_buffer->address()};
     if (output_is_sharded) {
         shard_builder::extend_sharding_run_time_args(output, writer_run_time_args);
     }
-
-    // Set run-time args
-    tt::tt_metal::SetRuntimeArgs(program, unary_reader_kernel_id, core, reader_run_time_args);
     tt::tt_metal::SetRuntimeArgs(program, unary_writer_kernel_id, core, writer_run_time_args);
 
     auto override_runtime_args_callback = [reader_kernel_id = unary_reader_kernel_id,
diff --git a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/untilize_program_factory.hpp b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/untilize_program_factory.hpp
index abaffa4eb6..984a6477c2 100644
--- a/ttnn/cpp/ttnn/operations/data_movement/untilize/device/untilize_program_factory.hpp
+++ b/ttnn/cpp/ttnn/operations/data_movement/untilize/device/untilize_program_factory.hpp
@@ -14,9 +14,6 @@ tt::tt_metal::operation::ProgramWithCallbacks untilize_multi_core_sub_core_grids
 tt::tt_metal::operation::ProgramWithCallbacks untilize_multi_core_block(
     const Tensor& a, Tensor& output, bool use_pack_untilize, bool fp32_dest_acc_en);
 
-tt::tt_metal::operation::ProgramWithCallbacks untilize_multi_core_input_and_output_shard_type_and_shard_spec_identical(
-    const Tensor& a, Tensor& output, bool use_pack_untilize, bool fp32_dest_acc_en);
-
 tt::tt_metal::operation::ProgramWithCallbacks untilize_multi_core(
     const Tensor& a, Tensor& output, bool use_pack_untilize, bool fp32_dest_acc_en);
 
diff --git a/ttnn/cpp/ttnn/operations/eltwise/binary/CMakeLists.txt b/ttnn/cpp/ttnn/operations/eltwise/binary/CMakeLists.txt
index 9ef7feb43c..ae8095a1f4 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/binary/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/eltwise/binary/CMakeLists.txt
@@ -1,14 +1,8 @@
 add_library(ttnn_op_eltwise_binary ${LIB_TYPE})
-add_library(TTNN::Ops::Eltwise::Binary ALIAS ttnn_op_eltwise_binary)
+add_library(TT::NN::Ops::Eltwise::Binary ALIAS ttnn_op_eltwise_binary)
 
 target_precompile_headers(ttnn_op_eltwise_binary REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_eltwise_binary)
-set_target_properties(
-    ttnn_op_eltwise_binary
-    PROPERTIES
-        VERIFY_INTERFACE_HEADER_SETS
-            FALSE
-)
 
 target_sources(
     ttnn_op_eltwise_binary
@@ -26,31 +20,12 @@ target_sources(
         device/broadcast_height_multi_core_sharded_program_factory.cpp
 )
 
-file(GLOB_RECURSE kernels device/kernels/*)
-target_sources(
-    ttnn_op_eltwise_binary
-    PUBLIC
-        FILE_SET kernels
-        TYPE HEADERS
-        BASE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}
-        FILES ${kernels}
-)
-
 target_include_directories(ttnn_op_eltwise_binary PRIVATE ${FixmeOpIncDirs})
 target_link_libraries(
     ttnn_op_eltwise_binary
     PRIVATE
         TT::Metalium
-        TTNN::Core
-)
-
-install(
-    TARGETS
-        ttnn_op_eltwise_binary
-    FILE_SET
-    kernels
-        DESTINATION ${CMAKE_INSTALL_LIBEXECDIR}/tt-metalium/ttnn/cpp/ttnn/operations/eltwise/binary
-        COMPONENT ttnn-runtime
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_eltwise_binary LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/eltwise/binary_backward/CMakeLists.txt b/ttnn/cpp/ttnn/operations/eltwise/binary_backward/CMakeLists.txt
index 046b3210a5..fe85894b5b 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/binary_backward/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/eltwise/binary_backward/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_eltwise_binary_backward ${LIB_TYPE})
-add_library(TTNN::Ops::Eltwise::Binary::Backward ALIAS ttnn_op_eltwise_binary_backward)
+add_library(TT::NN::Ops::Eltwise::Binary::Backward ALIAS ttnn_op_eltwise_binary_backward)
 
 target_precompile_headers(ttnn_op_eltwise_binary_backward REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_eltwise_binary_backward)
@@ -11,7 +11,7 @@ target_link_libraries(
     ttnn_op_eltwise_binary_backward
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_eltwise_binary_backward LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/CMakeLists.txt b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/CMakeLists.txt
index 00b73f6b65..d2227d7c2e 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/CMakeLists.txt
@@ -1,14 +1,8 @@
 add_library(ttnn_op_eltwise_binary_ng ${LIB_TYPE})
-add_library(TTNN::Ops::Eltwise::Binary::NG ALIAS ttnn_op_eltwise_binary_ng)
+add_library(TT::NN::Ops::Eltwise::Binary::NG ALIAS ttnn_op_eltwise_binary_ng)
 
 target_precompile_headers(ttnn_op_eltwise_binary_ng REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_eltwise_binary_ng)
-set_target_properties(
-    ttnn_op_eltwise_binary_ng
-    PROPERTIES
-        VERIFY_INTERFACE_HEADER_SETS
-            FALSE
-)
 
 target_sources(
     ttnn_op_eltwise_binary_ng
@@ -17,30 +11,13 @@ target_sources(
         device/binary_ng_program_factory.cpp
         device/binary_ng_utils.cpp
 )
-file(GLOB_RECURSE kernels device/kernels/*)
-target_sources(
-    ttnn_op_eltwise_binary_ng
-    PUBLIC
-        FILE_SET kernels
-        TYPE HEADERS
-        BASE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}
-        FILES ${kernels}
-)
 
 target_include_directories(ttnn_op_eltwise_binary_ng PRIVATE ${FixmeOpIncDirs})
 target_link_libraries(
     ttnn_op_eltwise_binary_ng
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
-install(
-    TARGETS
-        ttnn_op_eltwise_binary_ng
-    FILE_SET
-    kernels
-        DESTINATION ${CMAKE_INSTALL_LIBEXECDIR}/tt-metalium/ttnn/cpp/ttnn/operations/eltwise/binary_ng
-        COMPONENT ttnn-runtime
-)
 install(TARGETS ttnn_op_eltwise_binary_ng LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/binary_ng_program_factory.cpp b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/binary_ng_program_factory.cpp
index c64b9821eb..b6495b955c 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/binary_ng_program_factory.cpp
+++ b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/binary_ng_program_factory.cpp
@@ -328,7 +328,8 @@ void set_or_update_runtime_arguments(
                 auto b_shard_shape = b_shard_shape_generator(core);
                 b_num_tiles = b_shard_shape[0] * b_shard_shape[1];
             }
-            // TODO: after transition, remove b from writer completely
+            // for the specific case of subtile no_bcast type, writer no longer needs b's information
+            // for other cases, it remains needing b's information for now.
             std::array writer_runtime_args = {
                 b->buffer()->address(),
                 c.buffer()->address(),
@@ -416,38 +417,6 @@ void set_or_update_runtime_arguments(
     }
 }
 
-KernelName get_reader_kernel_name_and_defines(
-    const SubtileBroadcastType subtile_broadcast_type, std::map<std::string, std::string>& reader_defines) {
-    if (subtile_broadcast_type == SubtileBroadcastType::NONE) {
-        return KernelName::ReaderNoBcastNg;
-    } else if (
-        subtile_broadcast_type == SubtileBroadcastType::ROW_A ||
-        subtile_broadcast_type == SubtileBroadcastType::ROW_B) {
-        reader_defines["SRC_BCAST"] = subtile_broadcast_type == SubtileBroadcastType::ROW_A ? "1" : "0";
-        reader_defines["SRC_BCAST_B"] = subtile_broadcast_type == SubtileBroadcastType::ROW_B ? "1" : "0";
-        return KernelName::ReaderRowBcastNg;
-    } else if (
-        subtile_broadcast_type == SubtileBroadcastType::COL_A ||
-        subtile_broadcast_type == SubtileBroadcastType::COL_B) {
-        reader_defines["SRC_BCAST"] = subtile_broadcast_type == SubtileBroadcastType::COL_A ? "1" : "0";
-        reader_defines["SRC_BCAST_B"] = subtile_broadcast_type == SubtileBroadcastType::COL_B ? "1" : "0";
-        return KernelName::ReaderColBcastNg;
-    } else if (
-        subtile_broadcast_type == SubtileBroadcastType::ROW_B_COL_A ||
-        subtile_broadcast_type == SubtileBroadcastType::ROW_A_COL_B) {
-        reader_defines["SRC_BCAST_COL"] = subtile_broadcast_type == SubtileBroadcastType::ROW_B_COL_A ? "1" : "0";
-        reader_defines["SRC_BCAST_ROW_B"] = subtile_broadcast_type == SubtileBroadcastType::ROW_B_COL_A ? "1" : "0";
-        return KernelName::ReaderRowBColABcastNg;
-    } else if (
-        subtile_broadcast_type == SubtileBroadcastType::SCALAR_A ||
-        subtile_broadcast_type == SubtileBroadcastType::SCALAR_B) {
-        reader_defines["SRC_BCAST"] = subtile_broadcast_type == SubtileBroadcastType::SCALAR_A ? "1" : "0";
-        reader_defines["SRC_BCAST_B"] = subtile_broadcast_type == SubtileBroadcastType::SCALAR_B ? "1" : "0";
-        return KernelName::ReaderScalarBcastNg;
-    } else {
-        TT_FATAL(false, "Unsupported subtile broadcast type {}", static_cast<int>(subtile_broadcast_type));
-    }
-}
 }  // namespace CMAKE_UNIQUE_NAMESPACE
 }  // namespace
 
@@ -618,25 +587,17 @@ BinaryNgDeviceOperation::ProgramFactory::cached_program_t BinaryNgDeviceOperatio
         writer_kernel = kernel_config.writer_kernel;
         compute_kernel = kernel_config.compute_kernel;
     }
-
-    // to maintain backward compatibility, old writer kernel only needs b_dtype
-    auto writer_defines = make_dataflow_defines(b_dtype, a_dtype);
+    auto writer_defines = make_dataflow_defines(b_dtype);
     writer_defines["SRC_SHARDED"] = b_sharded ? "1" : "0";
     writer_defines["DST_SHARDED"] = c_sharded ? "1" : "0";
 
-    auto reader_defines = make_dataflow_defines(a_dtype, b_dtype);
-    reader_defines["SRC_SHARDED"] = a_sharded ? "1" : "0";
-    reader_defines["SRC_SHARDED_B"] = b_sharded ? "1" : "0";
-
-    // overwrite reader and write kernel names so that reader reads both and b and
-    // writer does not read b. For the transition, it can choose the original kernels
-    // or overwrite with new kernel here. If going back to old kernels, we can just
-    // skip the if clause.
-    if (b.has_value()) {
-        kernel_config.reader_kernel = CMAKE_UNIQUE_NAMESPACE::get_reader_kernel_name_and_defines(
-            operation_attributes.subtile_broadcast_type, reader_defines);
-        writer_kernel = KernelName::WriterNoBcastNg;
+    // overwrite reader and write kernel names for the following specific case
+    // so that reader reads of both and b and writer does not read b
+    if (b.has_value() && operation_attributes.subtile_broadcast_type == SubtileBroadcastType::NONE) {
+        kernel_config.reader_kernel = KernelName::ReaderNoBcastSplit;
+        writer_kernel = KernelName::WriterNoBcastSplit;
     }
+
     auto writer_kernel_id = tt_metal::CreateKernel(
         program,
         get_kernel_file_path(writer_kernel, is_sfpu_op),
@@ -644,6 +605,9 @@ BinaryNgDeviceOperation::ProgramFactory::cached_program_t BinaryNgDeviceOperatio
         tt_metal::WriterDataMovementConfig({b_is_dram, c_is_dram, has_sharding}, std::move(writer_defines)));
 
     // READER KERNEL
+    auto reader_defines = make_dataflow_defines(a_dtype);
+    reader_defines["SRC_SHARDED"] = a_sharded ? "1" : "0";
+    reader_defines["SRC_SHARDED_B"] = b_sharded ? "1" : "0";
 
     auto reader_kernel_id = tt_metal::CreateKernel(
         program,
diff --git a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/binary_ng_utils.cpp b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/binary_ng_utils.cpp
index 57d9358de3..b2a4b57c77 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/binary_ng_utils.cpp
+++ b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/binary_ng_utils.cpp
@@ -107,19 +107,14 @@ std::string BinaryNgKernelConfig::bcast_input_str() const {
 
 std::string get_kernel_file_path(KernelName kernel_name, bool is_sfpu) {
     constexpr std::string_view root = "ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels";
-    constexpr std::string_view root_ng = "ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng";
     constexpr std::string_view dataflow = "{}/dataflow/{}";
     constexpr std::string_view compute = "{}/compute/{}";
 
     switch (kernel_name) {
-        case KernelName::ReaderNoBcastNg: return fmt::format(dataflow, root_ng, "reader_interleaved_no_bcast.cpp");
-        case KernelName::ReaderRowBcastNg: return fmt::format(dataflow, root_ng, "reader_interleaved_row_bcast.cpp");
-        case KernelName::ReaderColBcastNg: return fmt::format(dataflow, root_ng, "reader_interleaved_col_bcast.cpp");
-        case KernelName::ReaderRowBColABcastNg:
-            return fmt::format(dataflow, root_ng, "reader_interleaved_row_col_mixed_bcast.cpp");
-        case KernelName::ReaderScalarBcastNg:
-            return fmt::format(dataflow, root_ng, "reader_interleaved_scalar_bcast.cpp");
-        case KernelName::WriterNoBcastNg: return fmt::format(dataflow, root_ng, "writer_interleaved_no_bcast.cpp");
+        case KernelName::ReaderNoBcastSplit:
+            return fmt::format(dataflow, root, "reader_interleaved_no_bcast_split.cpp");
+        case KernelName::WriterNoBcastSplit:
+            return fmt::format(dataflow, root, "writer_interleaved_no_bcast_split.cpp");
         case KernelName::ReaderNoBcast: return fmt::format(dataflow, root, "reader_interleaved_no_bcast.cpp");
         case KernelName::ReaderRowBcast: return fmt::format(dataflow, root, "reader_interleaved_row_bcast.cpp");
         case KernelName::ReaderColBcast: return fmt::format(dataflow, root, "reader_interleaved_col_bcast.cpp");
@@ -414,9 +409,8 @@ void add_activation_defines(
         });
 }
 
-std::map<std::string, std::string> make_dataflow_defines(const DataType dtype, const DataType b_dtype) {
+std::map<std::string, std::string> make_dataflow_defines(const DataType dtype) {
     std::map<std::string, std::string> defines;
-    // to maintain backward compatibility, we need to support both dtype and b_dtype
     if (dtype == DataType::FLOAT32) {
         defines["FILL_TILE_WITH_FIRST_COLUMN"] = "fill_tile_with_first_column";
         defines["FILL_TILE_WITH_FIRST_ROW"] = "fill_tile_with_first_row";
@@ -438,28 +432,6 @@ std::map<std::string, std::string> make_dataflow_defines(const DataType dtype, c
         defines["FILL_TILE_WITH_FIRST_ELEMENT"] = "fill_tile_with_first_element_bfloat16";
         defines["FILL_WITH_VALUE"] = "fill_with_val_bfloat16";
     }
-
-    if (b_dtype == DataType::FLOAT32) {
-        defines["FILL_TILE_WITH_FIRST_COLUMN_B"] = "fill_tile_with_first_column";
-        defines["FILL_TILE_WITH_FIRST_ROW_B"] = "fill_tile_with_first_row";
-        defines["FILL_TILE_WITH_FIRST_ELEMENT_B"] = "fill_tile_with_first_element<float>";
-        defines["FILL_WITH_VALUE_FLOAT_B"] = "fill_with_val<1024, float>";
-    } else if (b_dtype == DataType::INT32) {
-        defines["FILL_TILE_WITH_FIRST_COLUMN_B"] = "fill_tile_with_first_column";
-        defines["FILL_TILE_WITH_FIRST_ROW_B"] = "fill_tile_with_first_row";
-        defines["FILL_TILE_WITH_FIRST_ELEMENT_B"] = "fill_tile_with_first_element<int32_t>";
-        defines["FILL_WITH_VALUE_B"] = "fill_with_val<1024, int32_t>";
-    } else if (b_dtype == DataType::UINT32) {
-        defines["FILL_TILE_WITH_FIRST_COLUMN_B"] = "fill_tile_with_first_column";
-        defines["FILL_TILE_WITH_FIRST_ROW_B"] = "fill_tile_with_first_row";
-        defines["FILL_TILE_WITH_FIRST_ELEMENT_B"] = "fill_tile_with_first_element<uint32_t>";
-        defines["FILL_WITH_VALUE_B"] = "fill_with_val<1024, uint32_t>";
-    } else {
-        defines["FILL_TILE_WITH_FIRST_COLUMN_B"] = "fill_tile_with_first_column_bfloat16";
-        defines["FILL_TILE_WITH_FIRST_ROW_B"] = "fill_tile_with_first_row_bfloat16";
-        defines["FILL_TILE_WITH_FIRST_ELEMENT_B"] = "fill_tile_with_first_element_bfloat16";
-        defines["FILL_WITH_VALUE_B"] = "fill_with_val_bfloat16";
-    }
     return defines;
 }
 
diff --git a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/binary_ng_utils.hpp b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/binary_ng_utils.hpp
index a2105a9a9e..224ec200ea 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/binary_ng_utils.hpp
+++ b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/binary_ng_utils.hpp
@@ -26,12 +26,8 @@ enum class KernelName {
     ComputeNoBcast,
     ComputeBcast,
     ComputeScalar,
-    ReaderNoBcastNg,
-    WriterNoBcastNg,
-    ReaderRowBcastNg,
-    ReaderColBcastNg,
-    ReaderRowBColABcastNg,
-    ReaderScalarBcastNg,
+    ReaderNoBcastSplit,
+    WriterNoBcastSplit,
 };
 
 struct BinaryNgKernelConfig {
@@ -90,6 +86,6 @@ void add_activation_defines(
 
 uint32_t pack_scalar_runtime_arg(const float scalar, const DataType dtype, const bool is_quant_op);
 
-std::map<std::string, std::string> make_dataflow_defines(const DataType dtype, const DataType b_dtype);
+std::map<std::string, std::string> make_dataflow_defines(const DataType dtype);
 
 }  // namespace ttnn::operations::binary_ng
diff --git a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels/dataflow/reader_interleaved_no_bcast.cpp b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels/dataflow/reader_interleaved_no_bcast.cpp
index 1bf51ff1ff..31b4bd9208 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels/dataflow/reader_interleaved_no_bcast.cpp
+++ b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels/dataflow/reader_interleaved_no_bcast.cpp
@@ -60,7 +60,7 @@ void kernel_main() {
     for (uint32_t nd = start_d; nd < cND && num_tiles_read < dst_num_tiles; ++nd, start_n = 0) {
         for (uint32_t n = start_n; n < N && num_tiles_read < dst_num_tiles; ++n, start_c = 0) {
             for (uint32_t c = start_c; c < C && num_tiles_read < dst_num_tiles; ++c, start_th = 0) {
-                for (uint32_t th = start_th; th < Ht && num_tiles_read < dst_num_tiles; ++th) {
+                for (uint32_t th = start_th; th < Ht && num_tiles_read < dst_num_tiles; ++th, tile_offset += Wt) {
                     for (uint32_t tw = start_tw; tw < end_tw && num_tiles_read < dst_num_tiles;
                          ++tw, ++num_tiles_read) {
                         cb_reserve_back(cb_id_src, onetile);
@@ -73,7 +73,6 @@ void kernel_main() {
                         // next row of tiles should start at the first column
                         start_tw = 0;
                     }
-                    tile_offset += Wt;
                 }
                 tile_offset += next_channel_shift;
             }
diff --git a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/reader_interleaved_no_bcast.cpp b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels/dataflow/reader_interleaved_no_bcast_split.cpp
similarity index 95%
rename from ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/reader_interleaved_no_bcast.cpp
rename to ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels/dataflow/reader_interleaved_no_bcast_split.cpp
index 3d2ed81cb2..62b2399935 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/reader_interleaved_no_bcast.cpp
+++ b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels/dataflow/reader_interleaved_no_bcast_split.cpp
@@ -69,14 +69,12 @@ void kernel_main() {
     uint32_t end_tw = has_sharding ? start_tw + dst_shard_width : Wt;
 
     // this is the INPUT tile offset
-    uint32_t tile_offset = start_d * nD_stride + start_n * n_stride + start_c * c_stride;
-    tile_offset += start_th * Wt;
+    uint32_t tile_offset = start_d * nD_stride + start_n * n_stride + start_c * c_stride + start_th * Wt;
     uint32_t next_channel_shift = c_stride - HtWt;
     uint32_t next_batch_shift = n_stride - c_stride * C;
     uint32_t next_depth_shift = nD_stride - (n_stride * N);
 
-    uint32_t tile_offset_b = start_d * nD_stride_b + start_n * n_stride_b + start_c * c_stride_b;
-    tile_offset_b += start_th * Wt;
+    uint32_t tile_offset_b = start_d * nD_stride_b + start_n * n_stride_b + start_c * c_stride_b + start_th * Wt;
     uint32_t next_channel_shift_b = c_stride_b - HtWt;
     uint32_t next_batch_shift_b = n_stride_b - c_stride_b * C;
     uint32_t next_depth_shift_b = nD_stride_b - (n_stride_b * N);
@@ -85,7 +83,8 @@ void kernel_main() {
     for (uint32_t nd = start_d; nd < cND && num_tiles_read < dst_num_tiles; ++nd, start_n = 0) {
         for (uint32_t n = start_n; n < N && num_tiles_read < dst_num_tiles; ++n, start_c = 0) {
             for (uint32_t c = start_c; c < C && num_tiles_read < dst_num_tiles; ++c, start_th = 0) {
-                for (uint32_t th = start_th; th < Ht && num_tiles_read < dst_num_tiles; ++th) {
+                for (uint32_t th = start_th; th < Ht && num_tiles_read < dst_num_tiles;
+                     ++th, tile_offset += Wt, tile_offset_b += Wt) {
                     for (uint32_t tw = start_tw; tw < end_tw && num_tiles_read < dst_num_tiles;
                          ++tw, ++num_tiles_read) {
 #if !SRC_SHARDED
@@ -113,8 +112,6 @@ void kernel_main() {
                         // next row of tiles should start at the first column
                         start_tw = 0;
                     }
-                    tile_offset += Wt;
-                    tile_offset_b += Wt;
                 }
                 tile_offset += next_channel_shift;
                 tile_offset_b += next_channel_shift_b;
diff --git a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/writer_interleaved_no_bcast.cpp b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels/dataflow/writer_interleaved_no_bcast_split.cpp
similarity index 100%
rename from ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/writer_interleaved_no_bcast.cpp
rename to ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels/dataflow/writer_interleaved_no_bcast_split.cpp
diff --git a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/reader_interleaved_col_bcast.cpp b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/reader_interleaved_col_bcast.cpp
deleted file mode 100644
index 81d2e8df69..0000000000
--- a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/reader_interleaved_col_bcast.cpp
+++ /dev/null
@@ -1,171 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2024 Tenstorrent Inc.
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include <stdint.h>
-
-#include "dataflow_api.h"
-#include "cpp/ttnn/operations/eltwise/binary_ng/device/kernels/dataflow/fill_tile_utils.hpp"
-
-void kernel_main() {
-    const uint32_t src_addr = get_arg_val<uint32_t>(0);
-    const uint32_t start_tile_id = get_arg_val<uint32_t>(1);
-    const uint32_t src_num_tiles = get_arg_val<uint32_t>(2);
-    const uint32_t dst_num_tiles = get_arg_val<uint32_t>(3);
-    const uint32_t dst_shard_width = get_arg_val<uint32_t>(4);
-    const uint32_t nD_stride = get_arg_val<uint32_t>(5);
-    const uint32_t n_stride = get_arg_val<uint32_t>(6);
-    const uint32_t c_stride = get_arg_val<uint32_t>(7);
-    const uint32_t N = get_arg_val<uint32_t>(8);
-    const uint32_t C = get_arg_val<uint32_t>(9);
-    const uint32_t Ht = get_arg_val<uint32_t>(10);
-    const uint32_t Wt = get_arg_val<uint32_t>(11);
-    const uint32_t cND = get_arg_val<uint32_t>(12);  // collapsed dims > 4
-    const uint32_t src_addr_b = get_arg_val<uint32_t>(13);
-    const uint32_t nD_stride_b = get_arg_val<uint32_t>(14);
-    const uint32_t n_stride_b = get_arg_val<uint32_t>(15);
-    const uint32_t c_stride_b = get_arg_val<uint32_t>(16);
-    const uint32_t src_num_tiles_b = get_arg_val<uint32_t>(17);
-
-    constexpr auto cb_id_src = tt::CBIndex::c_0;
-    constexpr auto cb_id_src_b = tt::CBIndex::c_1;
-#if !SRC_SHARDED
-    constexpr bool src_is_dram = get_compile_time_arg_val(0) == 1;
-    const uint32_t src_tile_bytes = get_tile_size(cb_id_src);
-    const DataFormat src_data_format = get_dataformat(cb_id_src);
-    const InterleavedAddrGenFast<src_is_dram> src = {
-        .bank_base_address = src_addr, .page_size = src_tile_bytes, .data_format = src_data_format};
-#endif
-#if !SRC_SHARDED_B
-    constexpr bool src_is_dram_b = get_compile_time_arg_val(2) == 1;
-    const uint32_t src_tile_bytes_b = get_tile_size(cb_id_src_b);
-    const DataFormat src_data_format_b = get_dataformat(cb_id_src_b);
-    const InterleavedAddrGenFast<src_is_dram_b> src_b = {
-        .bank_base_address = src_addr_b, .page_size = src_tile_bytes_b, .data_format = src_data_format_b};
-#endif
-    constexpr uint32_t onetile = 1;
-    constexpr bool has_sharding = get_compile_time_arg_val(1) == 1;
-    const uint32_t HtWt = Ht * Wt;
-
-    const uint32_t tiles_per_depth = N * C * HtWt;
-    uint32_t start_d = start_tile_id / tiles_per_depth;  // ND index
-    uint32_t start_remaining_1 = start_tile_id % tiles_per_depth;
-    uint32_t tiles_per_batch = HtWt * C;
-    uint32_t start_n = start_remaining_1 / tiles_per_batch;  // N index
-    uint32_t start_remaining_2 = start_remaining_1 % tiles_per_batch;
-    uint32_t tiles_per_channel = HtWt;
-    uint32_t start_c = start_remaining_2 / tiles_per_channel;  // C index
-    uint32_t start_t = start_remaining_2 % tiles_per_channel;  // tile index within HtWt
-    uint32_t start_th = start_t / Wt;                          // H index
-    uint32_t start_tw = start_t % Wt;                          // W index
-    uint32_t end_tw = has_sharding ? start_tw + dst_shard_width : Wt;
-
-    // this is the INPUT tile offset
-    uint32_t tile_offset = start_d * nD_stride + start_n * n_stride + start_c * c_stride;
-#if !SRC_BCAST
-    tile_offset += start_th * Wt;
-#endif
-    uint32_t next_channel_shift = c_stride - HtWt;
-    uint32_t next_batch_shift = n_stride - c_stride * C;
-    uint32_t next_depth_shift = nD_stride - (n_stride * N);
-
-    uint32_t tile_offset_b = start_d * nD_stride_b + start_n * n_stride_b + start_c * c_stride_b;
-#if !SRC_BCAST_B
-    tile_offset_b += start_th * Wt;
-#endif
-    uint32_t next_channel_shift_b = c_stride_b - HtWt;
-    uint32_t next_batch_shift_b = n_stride_b - c_stride_b * C;
-    uint32_t next_depth_shift_b = nD_stride_b - (n_stride_b * N);
-
-    uint32_t num_tiles_read = 0;
-    for (uint32_t nd = start_d; nd < cND && num_tiles_read < dst_num_tiles; ++nd, start_n = 0) {
-        for (uint32_t n = start_n; n < N && num_tiles_read < dst_num_tiles; ++n, start_c = 0) {
-            for (uint32_t c = start_c; c < C && num_tiles_read < dst_num_tiles; ++c, start_th = 0) {
-                for (uint32_t th = start_th; th < Ht && num_tiles_read < dst_num_tiles; ++th) {
-#if SRC_BCAST
-                    cb_reserve_back(cb_id_src, onetile);
-#if !SRC_SHARDED
-                    uint32_t l1_write_addr_src = get_write_ptr(cb_id_src);
-                    noc_async_read_tile(tile_offset + th, src, l1_write_addr_src);
-                    noc_async_read_barrier();
-#endif
-                    FILL_TILE_WITH_FIRST_COLUMN(cb_id_src);
-                    cb_push_back(cb_id_src, onetile);
-#endif
-#if SRC_BCAST_B
-                    cb_reserve_back(cb_id_src_b, onetile);
-#if !SRC_SHARDED_B
-                    uint32_t l1_write_addr_src_b = get_write_ptr(cb_id_src_b);
-                    noc_async_read_tile(tile_offset_b + th, src_b, l1_write_addr_src_b);
-                    noc_async_read_barrier();
-#endif
-                    FILL_TILE_WITH_FIRST_COLUMN_B(cb_id_src_b);
-                    cb_push_back(cb_id_src_b, onetile);
-#endif
-                    for (uint32_t tw = start_tw; tw < end_tw && num_tiles_read < dst_num_tiles;
-                         ++tw, ++num_tiles_read) {
-#if !SRC_BCAST
-                        cb_reserve_back(cb_id_src, onetile);
-#if !SRC_SHARDED
-                        uint32_t l1_write_addr = get_write_ptr(cb_id_src);
-                        noc_async_read_tile(tile_offset + tw, src, l1_write_addr);
-                        noc_async_read_barrier();
-#endif
-                        cb_push_back(cb_id_src, onetile);
-#endif
-#if !SRC_BCAST_B
-                        cb_reserve_back(cb_id_src_b, onetile);
-#if !SRC_SHARDED_B
-                        uint32_t l1_write_addr_b = get_write_ptr(cb_id_src_b);
-                        noc_async_read_tile(tile_offset_b + tw, src_b, l1_write_addr_b);
-                        noc_async_read_barrier();
-#endif
-#if !SRC_SHARDED_B
-                        cb_push_back(cb_id_src_b, onetile);
-#endif
-#endif
-                    }
-                    if constexpr (!has_sharding) {
-                        // next row of tiles should start at the first column
-                        start_tw = 0;
-                    }
-#if !SRC_BCAST && !SRC_SHARDED
-                    tile_offset += Wt;
-#endif
-#if !SRC_BCAST_B && !SRC_SHARDED_B
-                    tile_offset_b += Wt;
-#endif
-                }
-#if !SRC_SHARDED
-#if SRC_BCAST
-                // same as following logically
-                // tile_offset += HtWt;
-                // tile_offset += next_channel_shift;
-                tile_offset += c_stride;
-#else
-                tile_offset += next_channel_shift;
-#endif
-#endif
-#if !SRC_SHARDED_B
-#if SRC_BCAST_B
-                tile_offset_b += c_stride_b;
-#else
-                tile_offset_b += next_channel_shift_b;
-#endif
-#endif
-            }
-#if !SRC_SHARDED
-            tile_offset += next_batch_shift;
-#endif
-#if !SRC_SHARDED_B
-            tile_offset_b += next_batch_shift_b;
-#endif
-        }
-#if !SRC_SHARDED
-        tile_offset += next_depth_shift;
-#endif
-#if !SRC_SHARDED_B
-        tile_offset_b += next_depth_shift_b;
-#endif
-    }
-}
diff --git a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/reader_interleaved_row_bcast.cpp b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/reader_interleaved_row_bcast.cpp
deleted file mode 100644
index 2c0f84506d..0000000000
--- a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/reader_interleaved_row_bcast.cpp
+++ /dev/null
@@ -1,153 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2024 Tenstorrent Inc.
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include <stdint.h>
-
-#include "dataflow_api.h"
-#include "cpp/ttnn/operations/eltwise/binary_ng/device/kernels/dataflow/fill_tile_utils.hpp"
-
-void kernel_main() {
-    const uint32_t src_addr = get_arg_val<uint32_t>(0);
-    const uint32_t start_tile_id = get_arg_val<uint32_t>(1);
-    const uint32_t src_num_tiles = get_arg_val<uint32_t>(2);
-    const uint32_t dst_num_tiles = get_arg_val<uint32_t>(3);
-    const uint32_t dst_shard_width = get_arg_val<uint32_t>(4);
-    const uint32_t nD_stride = get_arg_val<uint32_t>(5);
-    const uint32_t n_stride = get_arg_val<uint32_t>(6);
-    const uint32_t c_stride = get_arg_val<uint32_t>(7);
-    const uint32_t N = get_arg_val<uint32_t>(8);
-    const uint32_t C = get_arg_val<uint32_t>(9);
-    const uint32_t Ht = get_arg_val<uint32_t>(10);
-    const uint32_t Wt = get_arg_val<uint32_t>(11);
-    const uint32_t cND = get_arg_val<uint32_t>(12);  // collapsed dims > 4
-    const uint32_t src_addr_b = get_arg_val<uint32_t>(13);
-    const uint32_t nD_stride_b = get_arg_val<uint32_t>(14);
-    const uint32_t n_stride_b = get_arg_val<uint32_t>(15);
-    const uint32_t c_stride_b = get_arg_val<uint32_t>(16);
-    const uint32_t src_num_tiles_b = get_arg_val<uint32_t>(17);
-
-    constexpr auto cb_id_src = tt::CBIndex::c_0;
-    constexpr auto cb_id_src_b = tt::CBIndex::c_1;
-#if SRC_SHARDED
-    cb_reserve_back(cb_id_src, src_num_tiles);
-    cb_push_back(cb_id_src, src_num_tiles);
-#else
-    constexpr bool src_is_dram = get_compile_time_arg_val(0) == 1;
-    const uint32_t src_tile_bytes = get_tile_size(cb_id_src);
-    const DataFormat src_data_format = get_dataformat(cb_id_src);
-    const InterleavedAddrGenFast<src_is_dram> src = {
-        .bank_base_address = src_addr, .page_size = src_tile_bytes, .data_format = src_data_format};
-#endif
-#if SRC_SHARDED_B
-    cb_reserve_back(cb_id_src_b, src_num_tiles_b);
-    cb_push_back(cb_id_src_b, src_num_tiles_b);
-#else
-    constexpr bool src_is_dram_b = get_compile_time_arg_val(2) == 1;
-    const uint32_t src_tile_bytes_b = get_tile_size(cb_id_src_b);
-    const DataFormat src_data_format_b = get_dataformat(cb_id_src_b);
-    const InterleavedAddrGenFast<src_is_dram_b> src_b = {
-        .bank_base_address = src_addr_b, .page_size = src_tile_bytes_b, .data_format = src_data_format_b};
-#endif
-#if !SRC_SHARDED || !SRC_SHARDED_B
-    constexpr uint32_t onetile = 1;
-    constexpr bool has_sharding = get_compile_time_arg_val(1) == 1;
-    const uint32_t HtWt = Ht * Wt;
-
-    const uint32_t tiles_per_depth = N * C * HtWt;
-    uint32_t start_d = start_tile_id / tiles_per_depth;  // ND index
-    uint32_t start_remaining_1 = start_tile_id % tiles_per_depth;
-    uint32_t tiles_per_batch = HtWt * C;
-    uint32_t start_n = start_remaining_1 / tiles_per_batch;  // N index
-    uint32_t start_remaining_2 = start_remaining_1 % tiles_per_batch;
-    uint32_t tiles_per_channel = HtWt;
-    uint32_t start_c = start_remaining_2 / tiles_per_channel;  // C index
-    uint32_t start_t = start_remaining_2 % tiles_per_channel;  // tile index within HtWt
-    uint32_t start_th = start_t / Wt;                          // H index
-    uint32_t start_tw = start_t % Wt;                          // W index
-    uint32_t end_tw = has_sharding ? start_tw + dst_shard_width : Wt;
-
-    // this is the INPUT tile offset
-    uint32_t tile_offset = start_d * nD_stride + start_n * n_stride + start_c * c_stride;
-#if !SRC_BCAST
-    tile_offset += start_th * Wt;
-#endif
-    uint32_t next_channel_shift = c_stride - HtWt;
-    uint32_t next_batch_shift = n_stride - c_stride * C;
-    uint32_t next_depth_shift = nD_stride - (n_stride * N);
-
-    uint32_t tile_offset_b = start_d * nD_stride_b + start_n * n_stride_b + start_c * c_stride_b;
-#if !SRC_BCAST_B
-    tile_offset_b += start_th * Wt;
-#endif
-    uint32_t next_channel_shift_b = c_stride_b - HtWt;
-    uint32_t next_batch_shift_b = n_stride_b - c_stride_b * C;
-    uint32_t next_depth_shift_b = nD_stride_b - (n_stride_b * N);
-
-    uint32_t num_tiles_read = 0;
-    for (uint32_t nd = start_d; nd < cND && num_tiles_read < dst_num_tiles; ++nd, start_n = 0) {
-        for (uint32_t n = start_n; n < N && num_tiles_read < dst_num_tiles; ++n, start_c = 0) {
-            for (uint32_t c = start_c; c < C && num_tiles_read < dst_num_tiles; ++c, start_th = 0) {
-                for (uint32_t th = start_th; th < Ht && num_tiles_read < dst_num_tiles; ++th) {
-                    for (uint32_t tw = start_tw; tw < end_tw && num_tiles_read < dst_num_tiles;
-                         ++tw, ++num_tiles_read) {
-#if !SRC_SHARDED
-                        cb_reserve_back(cb_id_src, onetile);
-                        uint32_t l1_write_addr_src = get_write_ptr(cb_id_src);
-                        noc_async_read_tile(tile_offset + tw, src, l1_write_addr_src);
-#endif
-#if !SRC_SHARDED_B
-                        // read a tile from src_b
-                        cb_reserve_back(cb_id_src_b, onetile);
-                        uint32_t l1_write_addr_b = get_write_ptr(cb_id_src_b);
-                        noc_async_read_tile(tile_offset_b + tw, src_b, l1_write_addr_b);
-#endif
-#if !SRC_SHARDED || !SRC_SHARDED_B
-                        noc_async_read_barrier();
-#endif
-#if SRC_BCAST  // no sharding support for row bcast yet
-                        FILL_TILE_WITH_FIRST_ROW(cb_id_src);
-#endif
-#if SRC_BCAST_B  // no sharding support for row bcast yet
-                        FILL_TILE_WITH_FIRST_ROW_B(cb_id_src_b);
-#endif
-#if !SRC_SHARDED
-                        cb_push_back(cb_id_src, onetile);
-#endif
-#if !SRC_SHARDED_B
-                        cb_push_back(cb_id_src_b, onetile);
-#endif
-                    }
-                    if constexpr (!has_sharding) {
-                        // next row of tiles should start at the first column
-                        start_tw = 0;
-                    }
-#if !SRC_BCAST
-                    tile_offset += Wt;
-#endif
-#if !SRC_BCAST_B
-                    tile_offset_b += Wt;
-#endif
-                }
-#if SRC_BCAST
-                // same as following logically
-                // tile_offset += HtWt;
-                // tile_offset += next_channel_shift;
-                tile_offset += c_stride;
-#else
-                tile_offset += next_channel_shift;
-#endif
-#if SRC_BCAST_B
-                tile_offset_b += c_stride_b;
-#else
-                tile_offset_b += next_channel_shift_b;
-#endif
-            }
-            tile_offset += next_batch_shift;
-            tile_offset_b += next_batch_shift_b;
-        }
-        tile_offset += next_depth_shift;
-        tile_offset_b += next_depth_shift_b;
-    }
-#endif
-}
diff --git a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/reader_interleaved_row_col_mixed_bcast.cpp b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/reader_interleaved_row_col_mixed_bcast.cpp
deleted file mode 100644
index 67ba199b86..0000000000
--- a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/reader_interleaved_row_col_mixed_bcast.cpp
+++ /dev/null
@@ -1,157 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2024 Tenstorrent Inc.
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include <stdint.h>
-
-#include "dataflow_api.h"
-#include "cpp/ttnn/operations/eltwise/binary_ng/device/kernels/dataflow/fill_tile_utils.hpp"
-
-void kernel_main() {
-    const uint32_t src_addr = get_arg_val<uint32_t>(0);
-    const uint32_t start_tile_id = get_arg_val<uint32_t>(1);
-    const uint32_t src_num_tiles = get_arg_val<uint32_t>(2);
-    const uint32_t dst_num_tiles = get_arg_val<uint32_t>(3);
-    const uint32_t dst_shard_width = get_arg_val<uint32_t>(4);
-    const uint32_t nD_stride = get_arg_val<uint32_t>(5);
-    const uint32_t n_stride = get_arg_val<uint32_t>(6);
-    const uint32_t c_stride = get_arg_val<uint32_t>(7);
-    const uint32_t N = get_arg_val<uint32_t>(8);
-    const uint32_t C = get_arg_val<uint32_t>(9);
-    const uint32_t Ht = get_arg_val<uint32_t>(10);
-    const uint32_t Wt = get_arg_val<uint32_t>(11);
-    const uint32_t cND = get_arg_val<uint32_t>(12);  // collapsed dims > 4
-    const uint32_t src_addr_b = get_arg_val<uint32_t>(13);
-    const uint32_t nD_stride_b = get_arg_val<uint32_t>(14);
-    const uint32_t n_stride_b = get_arg_val<uint32_t>(15);
-    const uint32_t c_stride_b = get_arg_val<uint32_t>(16);
-    const uint32_t src_num_tiles_b = get_arg_val<uint32_t>(17);
-
-    constexpr auto cb_id_src = tt::CBIndex::c_0;
-    constexpr auto cb_id_src_b = tt::CBIndex::c_1;
-#if !SRC_SHARDED
-    constexpr bool src_is_dram = get_compile_time_arg_val(0) == 1;
-    const uint32_t src_tile_bytes = get_tile_size(cb_id_src);
-    const DataFormat src_data_format = get_dataformat(cb_id_src);
-    const InterleavedAddrGenFast<src_is_dram> src = {
-        .bank_base_address = src_addr, .page_size = src_tile_bytes, .data_format = src_data_format};
-#endif
-#if !SRC_SHARDED_B
-    constexpr bool src_is_dram_b = get_compile_time_arg_val(2) == 1;
-    const uint32_t src_tile_bytes_b = get_tile_size(cb_id_src_b);
-    const DataFormat src_data_format_b = get_dataformat(cb_id_src_b);
-    const InterleavedAddrGenFast<src_is_dram_b> src_b = {
-        .bank_base_address = src_addr_b, .page_size = src_tile_bytes_b, .data_format = src_data_format_b};
-#endif
-    constexpr uint32_t onetile = 1;
-    constexpr bool has_sharding = get_compile_time_arg_val(1) == 1;
-    const uint32_t HtWt = Ht * Wt;
-
-    const uint32_t tiles_per_depth = N * C * HtWt;
-    uint32_t start_d = start_tile_id / tiles_per_depth;  // ND index
-    uint32_t start_remaining_1 = start_tile_id % tiles_per_depth;
-    uint32_t tiles_per_batch = HtWt * C;
-    uint32_t start_n = start_remaining_1 / tiles_per_batch;  // N index
-    uint32_t start_remaining_2 = start_remaining_1 % tiles_per_batch;
-    uint32_t tiles_per_channel = HtWt;
-    uint32_t start_c = start_remaining_2 / tiles_per_channel;  // C index
-    uint32_t start_t = start_remaining_2 % tiles_per_channel;  // tile index within HtWt
-    uint32_t start_th = start_t / Wt;                          // H index
-    uint32_t start_tw = start_t % Wt;                          // W index
-    uint32_t end_tw = has_sharding ? start_tw + dst_shard_width : Wt;
-
-    // this is the INPUT tile offset
-    uint32_t tile_offset = start_d * nD_stride + start_n * n_stride + start_c * c_stride;
-
-    uint32_t next_channel_shift = c_stride - HtWt;
-    uint32_t next_batch_shift = n_stride - c_stride * C;
-    uint32_t next_depth_shift = nD_stride - (n_stride * N);
-
-    uint32_t tile_offset_b = start_d * nD_stride_b + start_n * n_stride_b + start_c * c_stride_b;
-
-    uint32_t next_channel_shift_b = c_stride_b - HtWt;
-    uint32_t next_batch_shift_b = n_stride_b - c_stride_b * C;
-    uint32_t next_depth_shift_b = nD_stride_b - (n_stride_b * N);
-
-    uint32_t num_tiles_read = 0;
-    for (uint32_t nd = start_d; nd < cND && num_tiles_read < dst_num_tiles; ++nd, start_n = 0) {
-        for (uint32_t n = start_n; n < N && num_tiles_read < dst_num_tiles; ++n, start_c = 0) {
-            for (uint32_t c = start_c; c < C && num_tiles_read < dst_num_tiles; ++c, start_th = 0) {
-                for (uint32_t th = start_th; th < Ht && num_tiles_read < dst_num_tiles; ++th) {
-#if SRC_BCAST_COL
-                    cb_reserve_back(cb_id_src, onetile);
-#if !SRC_SHARDED
-                    uint32_t l1_write_addr_src = get_write_ptr(cb_id_src);
-                    noc_async_read_tile(tile_offset + th, src, l1_write_addr_src);
-                    noc_async_read_barrier();
-#endif
-                    FILL_TILE_WITH_FIRST_COLUMN(cb_id_src);
-                    cb_push_back(cb_id_src, onetile);
-#else
-                    cb_reserve_back(cb_id_src_b, onetile);
-#if !SRC_SHARDED_B
-                    uint32_t l1_write_addr_src_b = get_write_ptr(cb_id_src_b);
-                    noc_async_read_tile(tile_offset_b + th, src_b, l1_write_addr_src_b);
-                    noc_async_read_barrier();
-#endif
-                    FILL_TILE_WITH_FIRST_COLUMN_B(cb_id_src_b);
-                    cb_push_back(cb_id_src_b, onetile);
-#endif
-                    for (uint32_t tw = start_tw; tw < end_tw && num_tiles_read < dst_num_tiles;
-                         ++tw, ++num_tiles_read) {
-#if SRC_BCAST_ROW_B
-                        // read a tile from src_b
-                        cb_reserve_back(cb_id_src_b, onetile);
-#if !SRC_SHARDED_B
-                        uint32_t l1_write_addr_b = get_write_ptr(cb_id_src_b);
-                        noc_async_read_tile(tile_offset_b + tw, src_b, l1_write_addr_b);
-                        noc_async_read_barrier();
-#endif
-                        FILL_TILE_WITH_FIRST_ROW_B(cb_id_src_b);
-#if !SRC_SHARDED_B
-                        cb_push_back(cb_id_src_b, onetile);
-#endif
-#else
-                        // read a tile from src
-                        cb_reserve_back(cb_id_src, onetile);
-#if !SRC_SHARDED_B
-                        uint32_t l1_write_addr = get_write_ptr(cb_id_src);
-                        noc_async_read_tile(tile_offset + tw, src, l1_write_addr);
-                        noc_async_read_barrier();
-#endif
-                        FILL_TILE_WITH_FIRST_ROW(cb_id_src);
-#if !SRC_SHARDED_B
-                        cb_push_back(cb_id_src, onetile);
-#endif
-#endif
-                    }
-                    if constexpr (!has_sharding) {
-                        // next row of tiles should start at the first column
-                        start_tw = 0;
-                    }
-                }
-#if !SRC_SHARDED
-                // same as following logically
-                // tile_offset += HtWt;
-                // tile_offset += next_channel_shift;
-                tile_offset += c_stride;
-#endif
-#if !SRC_SHARDED_B
-                tile_offset_b += c_stride_b;
-#endif
-            }
-#if !SRC_SHARDED
-            tile_offset += next_batch_shift;
-#endif
-#if !SRC_SHARDED_B
-            tile_offset_b += next_batch_shift_b;
-#endif
-        }
-#if !SRC_SHARDED
-        tile_offset += next_depth_shift;
-#endif
-#if !SRC_SHARDED_B
-        tile_offset_b += next_depth_shift_b;
-#endif
-    }
-}
diff --git a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/reader_interleaved_scalar_bcast.cpp b/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/reader_interleaved_scalar_bcast.cpp
deleted file mode 100644
index 8737784bee..0000000000
--- a/ttnn/cpp/ttnn/operations/eltwise/binary_ng/device/kernels_ng/dataflow/reader_interleaved_scalar_bcast.cpp
+++ /dev/null
@@ -1,171 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2024 Tenstorrent Inc.
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include <stdint.h>
-
-#include "dataflow_api.h"
-#include "cpp/ttnn/operations/eltwise/binary_ng/device/kernels/dataflow/fill_tile_utils.hpp"
-
-void kernel_main() {
-    const uint32_t src_addr = get_arg_val<uint32_t>(0);
-    const uint32_t start_tile_id = get_arg_val<uint32_t>(1);
-    const uint32_t src_num_tiles = get_arg_val<uint32_t>(2);
-    const uint32_t dst_num_tiles = get_arg_val<uint32_t>(3);
-    const uint32_t dst_shard_width = get_arg_val<uint32_t>(4);
-    const uint32_t nD_stride = get_arg_val<uint32_t>(5);
-    const uint32_t n_stride = get_arg_val<uint32_t>(6);
-    const uint32_t c_stride = get_arg_val<uint32_t>(7);
-    const uint32_t N = get_arg_val<uint32_t>(8);
-    const uint32_t C = get_arg_val<uint32_t>(9);
-    const uint32_t Ht = get_arg_val<uint32_t>(10);
-    const uint32_t Wt = get_arg_val<uint32_t>(11);
-    const uint32_t cND = get_arg_val<uint32_t>(12);  // collapsed dims > 4
-    const uint32_t src_addr_b = get_arg_val<uint32_t>(13);
-    const uint32_t nD_stride_b = get_arg_val<uint32_t>(14);
-    const uint32_t n_stride_b = get_arg_val<uint32_t>(15);
-    const uint32_t c_stride_b = get_arg_val<uint32_t>(16);
-    const uint32_t src_num_tiles_b = get_arg_val<uint32_t>(17);
-
-    constexpr auto cb_id_src = tt::CBIndex::c_0;
-    constexpr auto cb_id_src_b = tt::CBIndex::c_1;
-#if !SRC_SHARDED
-    constexpr bool src_is_dram = get_compile_time_arg_val(0) == 1;
-    const uint32_t src_tile_bytes = get_tile_size(cb_id_src);
-    const DataFormat src_data_format = get_dataformat(cb_id_src);
-    const InterleavedAddrGenFast<src_is_dram> src = {
-        .bank_base_address = src_addr, .page_size = src_tile_bytes, .data_format = src_data_format};
-#endif
-#if !SRC_SHARDED_B
-    constexpr bool src_is_dram_b = get_compile_time_arg_val(2) == 1;
-    const uint32_t src_tile_bytes_b = get_tile_size(cb_id_src_b);
-    const DataFormat src_data_format_b = get_dataformat(cb_id_src_b);
-    const InterleavedAddrGenFast<src_is_dram_b> src_b = {
-        .bank_base_address = src_addr_b, .page_size = src_tile_bytes_b, .data_format = src_data_format_b};
-#endif
-    constexpr uint32_t onetile = 1;
-    constexpr bool has_sharding = get_compile_time_arg_val(1) == 1;
-    const uint32_t HtWt = Ht * Wt;
-
-    const uint32_t tiles_per_depth = N * C * HtWt;
-    uint32_t start_d = start_tile_id / tiles_per_depth;  // ND index
-    uint32_t start_remaining_1 = start_tile_id % tiles_per_depth;
-    uint32_t tiles_per_batch = HtWt * C;
-    uint32_t start_n = start_remaining_1 / tiles_per_batch;  // N index
-    uint32_t start_remaining_2 = start_remaining_1 % tiles_per_batch;
-    uint32_t tiles_per_channel = HtWt;
-    uint32_t start_c = start_remaining_2 / tiles_per_channel;  // C index
-    uint32_t start_t = start_remaining_2 % tiles_per_channel;  // tile index within HtWt
-    uint32_t start_th = start_t / Wt;                          // H index
-    uint32_t start_tw = start_t % Wt;                          // W index
-    uint32_t end_tw = has_sharding ? start_tw + dst_shard_width : Wt;
-
-    // this is the INPUT tile offset
-    uint32_t tile_offset = start_d * nD_stride + start_n * n_stride + start_c * c_stride;
-#if !SRC_BCAST
-    tile_offset += start_th * Wt;
-#endif
-    uint32_t next_channel_shift = c_stride - HtWt;
-    uint32_t next_batch_shift = n_stride - c_stride * C;
-    uint32_t next_depth_shift = nD_stride - (n_stride * N);
-
-    uint32_t tile_offset_b = start_d * nD_stride_b + start_n * n_stride_b + start_c * c_stride_b;
-#if !SRC_BCAST_B
-    tile_offset_b += start_th * Wt;
-#endif
-    uint32_t next_channel_shift_b = c_stride_b - HtWt;
-    uint32_t next_batch_shift_b = n_stride_b - c_stride_b * C;
-    uint32_t next_depth_shift_b = nD_stride_b - (n_stride_b * N);
-
-    uint32_t num_tiles_read = 0;
-    for (uint32_t nd = start_d; nd < cND && num_tiles_read < dst_num_tiles; ++nd, start_n = 0) {
-        for (uint32_t n = start_n; n < N && num_tiles_read < dst_num_tiles; ++n, start_c = 0) {
-            for (uint32_t c = start_c; c < C && num_tiles_read < dst_num_tiles; ++c, start_th = 0) {
-#if SRC_BCAST
-                cb_reserve_back(cb_id_src, onetile);
-#if !SRC_SHARDED
-                uint32_t l1_write_addr_src = get_write_ptr(cb_id_src);
-                noc_async_read_tile(tile_offset, src, l1_write_addr_src);
-                noc_async_read_barrier();
-#endif
-                FILL_TILE_WITH_FIRST_ELEMENT(cb_id_src);
-                cb_push_back(cb_id_src, onetile);
-#endif
-#if SRC_BCAST_B
-                cb_reserve_back(cb_id_src_b, onetile);
-#if !SRC_SHARDED_B
-                uint32_t l1_write_addr_src_b = get_write_ptr(cb_id_src_b);
-                noc_async_read_tile(tile_offset_b, src_b, l1_write_addr_src_b);
-                noc_async_read_barrier();
-#endif
-                FILL_TILE_WITH_FIRST_ELEMENT_B(cb_id_src_b);
-                cb_push_back(cb_id_src_b, onetile);
-#endif
-                for (uint32_t th = start_th; th < Ht && num_tiles_read < dst_num_tiles; ++th) {
-                    for (uint32_t tw = start_tw; tw < end_tw && num_tiles_read < dst_num_tiles;
-                         ++tw, ++num_tiles_read) {
-#if !SRC_BCAST
-                        cb_reserve_back(cb_id_src, onetile);
-#if !SRC_SHARDED
-                        uint32_t l1_write_addr = get_write_ptr(cb_id_src);
-                        noc_async_read_tile(tile_offset + tw, src, l1_write_addr);
-                        noc_async_read_barrier();
-#endif
-                        cb_push_back(cb_id_src, onetile);
-#endif
-#if !SRC_BCAST_B
-                        cb_reserve_back(cb_id_src_b, onetile);
-#if !SRC_SHARDED_B
-                        uint32_t l1_write_addr_b = get_write_ptr(cb_id_src_b);
-                        noc_async_read_tile(tile_offset_b + tw, src_b, l1_write_addr_b);
-                        noc_async_read_barrier();
-#endif
-#if !SRC_SHARDED_B
-                        cb_push_back(cb_id_src_b, onetile);
-#endif
-#endif
-                    }
-                    if constexpr (!has_sharding) {
-                        // next row of tiles should start at the first column
-                        start_tw = 0;
-                    }
-#if !SRC_BCAST && !SRC_SHARDED
-                    tile_offset += Wt;
-#endif
-#if !SRC_BCAST_B && !SRC_SHARDED_B
-                    tile_offset_b += Wt;
-#endif
-                }
-#if !SRC_SHARDED
-#if SRC_BCAST
-                // same as following logically
-                // tile_offset += HtWt;
-                // tile_offset += next_channel_shift;
-                tile_offset += c_stride;
-#else
-                tile_offset += next_channel_shift;
-#endif
-#endif
-#if !SRC_SHARDED_B
-#if SRC_BCAST_B
-                tile_offset_b += c_stride_b;
-#else
-                tile_offset_b += next_channel_shift_b;
-#endif
-#endif
-            }
-#if !SRC_SHARDED
-            tile_offset += next_batch_shift;
-#endif
-#if !SRC_SHARDED_B
-            tile_offset_b += next_batch_shift_b;
-#endif
-        }
-#if !SRC_SHARDED
-        tile_offset += next_depth_shift;
-#endif
-#if !SRC_SHARDED_B
-        tile_offset_b += next_depth_shift_b;
-#endif
-    }
-}
diff --git a/ttnn/cpp/ttnn/operations/eltwise/complex/CMakeLists.txt b/ttnn/cpp/ttnn/operations/eltwise/complex/CMakeLists.txt
index e6ece84040..8c82852762 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/complex/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/eltwise/complex/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_eltwise_complex ${LIB_TYPE})
-add_library(TTNN::Ops::Eltwise::Complex ALIAS ttnn_op_eltwise_complex)
+add_library(TT::NN::Ops::Eltwise::Complex ALIAS ttnn_op_eltwise_complex)
 
 target_precompile_headers(ttnn_op_eltwise_complex REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_eltwise_complex)
@@ -11,7 +11,7 @@ target_link_libraries(
     ttnn_op_eltwise_complex
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_eltwise_complex LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/eltwise/complex_binary/CMakeLists.txt b/ttnn/cpp/ttnn/operations/eltwise/complex_binary/CMakeLists.txt
index 19ab93ee0c..66c0c19be3 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/complex_binary/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/eltwise/complex_binary/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_eltwise_complex_binary ${LIB_TYPE})
-add_library(TTNN::Ops::Eltwise::Complex::Binary ALIAS ttnn_op_eltwise_complex_binary)
+add_library(TT::NN::Ops::Eltwise::Complex::Binary ALIAS ttnn_op_eltwise_complex_binary)
 
 target_precompile_headers(ttnn_op_eltwise_complex_binary REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_eltwise_complex_binary)
@@ -11,7 +11,7 @@ target_link_libraries(
     ttnn_op_eltwise_complex_binary
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_eltwise_complex_binary LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/eltwise/complex_unary/CMakeLists.txt b/ttnn/cpp/ttnn/operations/eltwise/complex_unary/CMakeLists.txt
index aabe469d86..6c989915c8 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/complex_unary/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/eltwise/complex_unary/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_eltwise_complex_unary ${LIB_TYPE})
-add_library(TTNN::Ops::Eltwise::Complex::Unary ALIAS ttnn_op_eltwise_complex_unary)
+add_library(TT::NN::Ops::Eltwise::Complex::Unary ALIAS ttnn_op_eltwise_complex_unary)
 
 target_precompile_headers(ttnn_op_eltwise_complex_unary REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_eltwise_complex_unary)
@@ -11,7 +11,7 @@ target_link_libraries(
     ttnn_op_eltwise_complex_unary
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_eltwise_complex_unary LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/eltwise/complex_unary_backward/CMakeLists.txt b/ttnn/cpp/ttnn/operations/eltwise/complex_unary_backward/CMakeLists.txt
index acf1c86a53..2725424cf4 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/complex_unary_backward/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/eltwise/complex_unary_backward/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_eltwise_complex_unary_backward ${LIB_TYPE})
-add_library(TTNN::Ops::Eltwise::Complex::Unary::Backward ALIAS ttnn_op_eltwise_complex_unary_backward)
+add_library(TT::NN::Ops::Eltwise::Complex::Unary::Backward ALIAS ttnn_op_eltwise_complex_unary_backward)
 
 target_precompile_headers(ttnn_op_eltwise_complex_unary_backward REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_eltwise_complex_unary_backward)
@@ -11,7 +11,7 @@ target_link_libraries(
     ttnn_op_eltwise_complex_unary_backward
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_eltwise_complex_unary_backward LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/eltwise/quantization/CMakeLists.txt b/ttnn/cpp/ttnn/operations/eltwise/quantization/CMakeLists.txt
index af9e57c566..b0c07aa97c 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/quantization/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/eltwise/quantization/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_eltwise_quantization ${LIB_TYPE})
-add_library(TTNN::Ops::Eltwise::Quantization ALIAS ttnn_op_eltwise_quantization)
+add_library(TT::NN::Ops::Eltwise::Quantization ALIAS ttnn_op_eltwise_quantization)
 
 target_precompile_headers(ttnn_op_eltwise_quantization REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_eltwise_quantization)
@@ -11,7 +11,7 @@ target_link_libraries(
     ttnn_op_eltwise_quantization
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_eltwise_quantization LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/eltwise/ternary/CMakeLists.txt b/ttnn/cpp/ttnn/operations/eltwise/ternary/CMakeLists.txt
index c112f64b6f..9bc07bd8c8 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/ternary/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/eltwise/ternary/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_eltwise_ternary ${LIB_TYPE})
-add_library(TTNN::Ops::Eltwise::Ternary ALIAS ttnn_op_eltwise_ternary)
+add_library(TT::NN::Ops::Eltwise::Ternary ALIAS ttnn_op_eltwise_ternary)
 
 target_precompile_headers(ttnn_op_eltwise_ternary REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_eltwise_ternary)
@@ -16,7 +16,7 @@ target_link_libraries(
     ttnn_op_eltwise_ternary
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_eltwise_ternary LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/eltwise/ternary_backward/CMakeLists.txt b/ttnn/cpp/ttnn/operations/eltwise/ternary_backward/CMakeLists.txt
index ed5b6eaa02..277c695d7e 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/ternary_backward/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/eltwise/ternary_backward/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_eltwise_ternary_backward ${LIB_TYPE})
-add_library(TTNN::Ops::Eltwise::Ternary::Backward ALIAS ttnn_op_eltwise_ternary_backward)
+add_library(TT::NN::Ops::Eltwise::Ternary::Backward ALIAS ttnn_op_eltwise_ternary_backward)
 
 target_precompile_headers(ttnn_op_eltwise_ternary_backward REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_eltwise_ternary_backward)
@@ -11,7 +11,7 @@ target_link_libraries(
     ttnn_op_eltwise_ternary_backward
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_eltwise_ternary_backward LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/eltwise/unary/CMakeLists.txt b/ttnn/cpp/ttnn/operations/eltwise/unary/CMakeLists.txt
index 3edd37fb7b..73339f0646 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/unary/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/eltwise/unary/CMakeLists.txt
@@ -1,13 +1,7 @@
 add_library(ttnn_op_eltwise_unary ${LIB_TYPE})
-add_library(TTNN::Ops::Eltwise::Unary ALIAS ttnn_op_eltwise_unary)
+add_library(TT::NN::Ops::Eltwise::Unary ALIAS ttnn_op_eltwise_unary)
 
 target_precompile_headers(ttnn_op_eltwise_unary REUSE_FROM TT::CommonPCH)
-set_target_properties(
-    ttnn_op_eltwise_unary
-    PROPERTIES
-        VERIFY_INTERFACE_HEADER_SETS
-            FALSE
-)
 
 target_sources(
     ttnn_op_eltwise_unary
@@ -24,31 +18,12 @@ target_sources(
         unary.cpp
 )
 
-file(GLOB_RECURSE kernels device/kernels/*)
-target_sources(
-    ttnn_op_eltwise_unary
-    PUBLIC
-        FILE_SET kernels
-        TYPE HEADERS
-        BASE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}
-        FILES ${kernels}
-)
-
 target_include_directories(ttnn_op_eltwise_unary PRIVATE ${FixmeOpIncDirs})
 target_link_libraries(
     ttnn_op_eltwise_unary
     PRIVATE
         TT::Metalium
-        TTNN::Core
-)
-
-install(
-    TARGETS
-        ttnn_op_eltwise_unary
-    FILE_SET
-    kernels
-        DESTINATION ${CMAKE_INSTALL_LIBEXECDIR}/tt-metalium/ttnn/cpp/ttnn/operations/eltwise/unary
-        COMPONENT ttnn-runtime
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_eltwise_unary LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/eltwise/unary/common/unary_op_utils.cpp b/ttnn/cpp/ttnn/operations/eltwise/unary/common/unary_op_utils.cpp
index 1f2c9ebd03..4aa7a2a72f 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/unary/common/unary_op_utils.cpp
+++ b/ttnn/cpp/ttnn/operations/eltwise/unary/common/unary_op_utils.cpp
@@ -336,12 +336,7 @@ std::pair<string, string> get_op_init_and_func_default(
             break;
         case UnaryOpType::ISNAN: op_init_and_name = {"isnan_tile_init();", fmt::format("isnan_tile({});", idst)}; break;
         case UnaryOpType::LOGICAL_NOT_UNARY:
-            if (input_dtype == DataType::INT32) {
-                op_init_and_name = {
-                    "logical_not_unary_tile_init();", fmt::format("logical_not_unary_tile_int32({});", idst)};
-            } else {
-                op_init_and_name = {"logical_not_unary_tile_init();", fmt::format("logical_not_unary_tile({});", idst)};
-            }
+            op_init_and_name = {"logical_not_unary_tile_init();", fmt::format("logical_not_unary_tile({});", idst)};
             break;
         case UnaryOpType::I0: op_init_and_name = {"i0_tile_init();", fmt::format("i0_tile({});", idst)}; break;
         case UnaryOpType::I1: op_init_and_name = {"i1_tile_init();", fmt::format("i1_tile({});", idst)}; break;
diff --git a/ttnn/cpp/ttnn/operations/eltwise/unary/unary_pybind.cpp b/ttnn/cpp/ttnn/operations/eltwise/unary/unary_pybind.cpp
index 4c4d8b1db2..0b16465a07 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/unary/unary_pybind.cpp
+++ b/ttnn/cpp/ttnn/operations/eltwise/unary/unary_pybind.cpp
@@ -1878,7 +1878,7 @@ void py_module(py::module& module) {
         ttnn::logical_not,
         R"doc(\mathrm{{output\_tensor}}_i = \mathrm{{!input\_tensor_i}})doc",
         "",
-        R"doc(BFLOAT16, BFLOAT8_B, INT32)doc");
+        R"doc(BFLOAT16, BFLOAT8_B)doc");
     bind_unary_operation(
         module,
         ttnn::ltz,
@@ -2198,7 +2198,7 @@ void py_module(py::module& module) {
         ttnn::logical_not_,
         R"doc(Performs logical_not inplace function on :attr:`input_tensor`.)doc",
         "",
-        R"doc(BFLOAT16, BFLOAT8_B, INT32)doc");
+        R"doc(BFLOAT16, BFLOAT8_B)doc");
     bind_unary_composite(
         module,
         ttnn::normalize_global,
diff --git a/ttnn/cpp/ttnn/operations/eltwise/unary_backward/CMakeLists.txt b/ttnn/cpp/ttnn/operations/eltwise/unary_backward/CMakeLists.txt
index 48c0bcd05c..fee7b1ed56 100644
--- a/ttnn/cpp/ttnn/operations/eltwise/unary_backward/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/eltwise/unary_backward/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_eltwise_unary_backward ${LIB_TYPE})
-add_library(TTNN::Ops::Eltwise::Unary::Backward ALIAS ttnn_op_eltwise_unary_backward)
+add_library(TT::NN::Ops::Eltwise::Unary::Backward ALIAS ttnn_op_eltwise_unary_backward)
 
 target_precompile_headers(ttnn_op_eltwise_unary_backward REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_eltwise_unary_backward)
@@ -11,7 +11,7 @@ target_link_libraries(
     ttnn_op_eltwise_unary_backward
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_eltwise_unary_backward LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/embedding/CMakeLists.txt b/ttnn/cpp/ttnn/operations/embedding/CMakeLists.txt
index f2ce799ac5..77b8d6883c 100644
--- a/ttnn/cpp/ttnn/operations/embedding/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/embedding/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_embedding ${LIB_TYPE})
-add_library(TTNN::Ops::Embedding ALIAS ttnn_op_embedding)
+add_library(TT::NN::Ops::Embedding ALIAS ttnn_op_embedding)
 
 target_precompile_headers(ttnn_op_embedding REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_embedding)
@@ -16,7 +16,7 @@ target_link_libraries(
     ttnn_op_embedding
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_embedding LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/embedding_backward/CMakeLists.txt b/ttnn/cpp/ttnn/operations/embedding_backward/CMakeLists.txt
index bdc8f59933..debb6189d2 100644
--- a/ttnn/cpp/ttnn/operations/embedding_backward/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/embedding_backward/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_embedding_backward ${LIB_TYPE})
-add_library(TTNN::Ops::Embedding::Backward ALIAS ttnn_op_embedding_backward)
+add_library(TT::NN::Ops::Embedding::Backward ALIAS ttnn_op_embedding_backward)
 
 target_precompile_headers(ttnn_op_embedding_backward REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_embedding_backward)
@@ -17,7 +17,7 @@ target_link_libraries(
     ttnn_op_embedding_backward
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_embedding_backward LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/examples/CMakeLists.txt b/ttnn/cpp/ttnn/operations/examples/CMakeLists.txt
index d09b886365..cbc252e3c8 100644
--- a/ttnn/cpp/ttnn/operations/examples/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/examples/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_examples ${LIB_TYPE})
-add_library(TTNN::Ops::Examples ALIAS ttnn_op_examples)
+add_library(TT::NN::Ops::Examples ALIAS ttnn_op_examples)
 
 target_precompile_headers(ttnn_op_examples REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_examples)
@@ -20,7 +20,7 @@ target_link_libraries(
     ttnn_op_examples
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_examples LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/auto_format/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/auto_format/CMakeLists.txt
index 68b9489e3f..6f5ff26ee8 100644
--- a/ttnn/cpp/ttnn/operations/experimental/auto_format/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/auto_format/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_experimental_auto_format ${LIB_TYPE})
-add_library(TTNN::Ops::Experimental::AutoFormat ALIAS ttnn_op_experimental_auto_format)
+add_library(TT::NN::Ops::Experimental::AutoFormat ALIAS ttnn_op_experimental_auto_format)
 
 target_precompile_headers(ttnn_op_experimental_auto_format REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_experimental_auto_format)
@@ -11,7 +11,7 @@ target_link_libraries(
     ttnn_op_experimental_auto_format
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_auto_format LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/bcast_to/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/bcast_to/CMakeLists.txt
index fb3a17f996..aee5ec41dc 100644
--- a/ttnn/cpp/ttnn/operations/experimental/bcast_to/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/bcast_to/CMakeLists.txt
@@ -1,14 +1,8 @@
 add_library(ttnn_op_experimental_bcast_to ${LIB_TYPE})
-add_library(TTNN::Ops::Experimental::BcastTo ALIAS ttnn_op_experimental_bcast_to)
+add_library(TT::NN::Ops::Experimental::BcastTo ALIAS ttnn_op_experimental_bcast_to)
 
 target_precompile_headers(ttnn_op_experimental_bcast_to REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_experimental_bcast_to)
-set_target_properties(
-    ttnn_op_experimental_bcast_to
-    PROPERTIES
-        VERIFY_INTERFACE_HEADER_SETS
-            FALSE
-)
 
 target_sources(
     ttnn_op_experimental_bcast_to
@@ -19,31 +13,12 @@ target_sources(
         device/bcast_to_utils.cpp
 )
 
-file(GLOB_RECURSE kernels device/kernels/*)
-target_sources(
-    ttnn_op_experimental_bcast_to
-    PUBLIC
-        FILE_SET kernels
-        TYPE HEADERS
-        BASE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}
-        FILES ${kernels}
-)
-
 target_include_directories(ttnn_op_experimental_bcast_to PRIVATE ${FixmeOpIncDirs})
 target_link_libraries(
     ttnn_op_experimental_bcast_to
     PRIVATE
         TT::Metalium
-        TTNN::Core
-)
-
-install(
-    TARGETS
-        ttnn_op_experimental_bcast_to
-    FILE_SET
-    kernels
-        DESTINATION ${CMAKE_INSTALL_LIBEXECDIR}/tt-metalium/ttnn/cpp/ttnn/operations/experimental/bcast_to
-        COMPONENT ttnn-runtime
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_bcast_to LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/ccl/CMakeLists.txt
index 518c08ed54..ccd276b75f 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_experimental_ccl ${LIB_TYPE})
-add_library(TTNN::Ops::Experimental::CCL ALIAS ttnn_op_experimental_ccl)
+add_library(TT::NN::Ops::Experimental::CCL ALIAS ttnn_op_experimental_ccl)
 
 target_precompile_headers(ttnn_op_experimental_ccl REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_experimental_ccl)
@@ -20,9 +20,6 @@ target_sources(
         llama_reduce_scatter/llama_reduce_scatter.cpp
         llama_reduce_scatter/device/llama_reduce_scatter_device_operation.cpp
         llama_reduce_scatter/device/llama_reduce_scatter_program_factory.cpp
-        llama_reduce_scatter_matmul/rs_matmul.cpp
-        llama_reduce_scatter_matmul/device/rs_matmul_op.cpp
-        llama_reduce_scatter_matmul/device/rs_matmul_program_factory.cpp
         llama_reduce_scatter_create_heads/llama_reduce_scatter_create_heads.cpp
         llama_reduce_scatter_create_heads/device/llama_reduce_scatter_create_heads_device_op.cpp
         llama_reduce_scatter_create_heads/device/llama_reduce_scatter_create_heads_program_factory.cpp
@@ -56,7 +53,7 @@ target_link_libraries(
     ttnn_op_experimental_ccl
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_ccl LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/all_broadcast_async/device/kernels/all_broadcast_rm_writer.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/all_broadcast_async/device/kernels/all_broadcast_rm_writer.cpp
index 821e7821f0..84695a6cf1 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/all_broadcast_async/device/kernels/all_broadcast_rm_writer.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/all_broadcast_async/device/kernels/all_broadcast_rm_writer.cpp
@@ -5,9 +5,8 @@
 #include "dataflow_api.h"
 #include <tt-metalium/buffer_types.hpp>
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/minimal_ccl_common.hpp"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
 #include <cstdint>
 #include <utility>
 #include "cpp/ttnn/operations/ccl/shared_with_host/sharded_tensor_addr_gen.hpp"
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/all_broadcast_async/device/kernels/all_broadcast_tile_writer.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/all_broadcast_async/device/kernels/all_broadcast_tile_writer.cpp
index 817508b22a..49b67ba063 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/all_broadcast_async/device/kernels/all_broadcast_tile_writer.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/all_broadcast_async/device/kernels/all_broadcast_tile_writer.cpp
@@ -5,8 +5,7 @@
 #include "dataflow_api.h"
 #include <tt-metalium/buffer_types.hpp>
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/minimal_ccl_common.hpp"
 #include <cstdint>
 #include <utility>
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/all_gather_async_program.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/all_gather_async_program.cpp
index dc8a42495c..683c77d4ca 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/all_gather_async_program.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/all_gather_async_program.cpp
@@ -219,8 +219,10 @@ tt::tt_metal::operation::ProgramWithCallbacks all_gather_async_multi_core_with_w
     size_t num_targets_backward = 0;
     if (topology == ccl::Topology::Linear) {
         LineTopology line_topology(ring_size, ring_index);
-        num_targets_forward = line_topology.get_distance_to_end_of_line(ttnn::ccl::LineDirection::FORWARD);
-        num_targets_backward = line_topology.get_distance_to_end_of_line(ttnn::ccl::LineDirection::BACKWARD);
+        num_targets_forward =
+            line_topology.get_distance_to_end_of_line(ttnn::ccl::EdmLineFabricOpInterface::Direction::FORWARD);
+        num_targets_backward =
+            line_topology.get_distance_to_end_of_line(ttnn::ccl::EdmLineFabricOpInterface::Direction::BACKWARD);
     } else if (topology == ccl::Topology::Ring) {
         // TODO: Commonize
         num_targets_forward = tt::div_up(ring_size - 1, 2);
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/interleaved_dim3_1_1_32_any_writer.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/interleaved_dim3_1_1_32_any_writer.cpp
index 2710f466c3..f5cce1c85a 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/interleaved_dim3_1_1_32_any_writer.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/interleaved_dim3_1_1_32_any_writer.cpp
@@ -5,8 +5,7 @@
 #include "dataflow_api.h"
 #include <tt-metalium/buffer_types.hpp>
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "minimal_ccl_common.hpp"
 #include <cstdint>
 #include <utility>
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/interleaved_dim3_1_1_any_any_receiver_writer.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/interleaved_dim3_1_1_any_any_receiver_writer.cpp
index 867d589ecf..486e9b72d0 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/interleaved_dim3_1_1_any_any_receiver_writer.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/interleaved_dim3_1_1_any_any_receiver_writer.cpp
@@ -5,8 +5,7 @@
 #include "dataflow_api.h"
 #include <tt-metalium/buffer_types.hpp>
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "cpp/ttnn/operations/ccl/kernel_common/worker_sync_utils.hpp"
 #include "cpp/ttnn/operations/ccl/ccl_host_types.hpp"
 #include "minimal_ccl_common.hpp"
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/interleaved_dim3_1_1_any_any_writer.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/interleaved_dim3_1_1_any_any_writer.cpp
index 15273060a0..a09571531b 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/interleaved_dim3_1_1_any_any_writer.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/interleaved_dim3_1_1_any_any_writer.cpp
@@ -5,7 +5,7 @@
 #include "dataflow_api.h"
 #include <tt-metalium/buffer_types.hpp>
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "cpp/ttnn/operations/ccl/kernel_common/worker_sync_utils.hpp"
 #include "cpp/ttnn/operations/ccl/ccl_host_types.hpp"
 #include "minimal_ccl_common.hpp"
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/llama_shapes_sharded_writer.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/llama_shapes_sharded_writer.cpp
index 492cc93a73..94193f1dd3 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/llama_shapes_sharded_writer.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/llama_shapes_sharded_writer.cpp
@@ -5,8 +5,7 @@
 #include "dataflow_api.h"
 #include <tt-metalium/buffer_types.hpp>
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "minimal_ccl_common.hpp"
 #include <cstdint>
 #include <utility>
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/minimal_ccl_common.hpp b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/minimal_ccl_common.hpp
index 75ab606cc4..e992f2b66d 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/minimal_ccl_common.hpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/minimal_ccl_common.hpp
@@ -5,8 +5,7 @@
 #include "dataflow_api.h"
 #include <tt-metalium/buffer_types.hpp>
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include <tt-metalium/fabric_edm_packet_header.hpp>
 #include <cstdint>
 #include <utility>
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/all_gather_concat_program.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/all_gather_concat_program.cpp
index eef40bee1b..7877bd18ac 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/all_gather_concat_program.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/all_gather_concat_program.cpp
@@ -103,8 +103,10 @@ tt::tt_metal::operation::ProgramWithCallbacks all_gather_concat_llama_sharded(
     //          - batch 1 (from batch_start_1 to batch end_1)
     //          - batch 2 (from batch_start_2 to batch end_2) if applicable
     LineTopology line_topology(ring_size, ring_index);
-    const size_t num_targets_right = line_topology.get_distance_to_end_of_line(ttnn::ccl::LineDirection::FORWARD);
-    const size_t num_targets_left = line_topology.get_distance_to_end_of_line(ttnn::ccl::LineDirection::BACKWARD);
+    const size_t num_targets_right =
+        line_topology.get_distance_to_end_of_line(ttnn::ccl::EdmLineFabricOpInterface::Direction::FORWARD);
+    const size_t num_targets_left =
+        line_topology.get_distance_to_end_of_line(ttnn::ccl::EdmLineFabricOpInterface::Direction::BACKWARD);
 
     uint32_t batch_size = 1;
     uint32_t batch_start_1 = 8;
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/kernels/llama_all_gather_concat_reader.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/kernels/llama_all_gather_concat_reader.cpp
index 446f6c4bc7..2ade1ed2a6 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/kernels/llama_all_gather_concat_reader.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/kernels/llama_all_gather_concat_reader.cpp
@@ -5,7 +5,7 @@
 #include "dataflow_api.h"
 #include <tt-metalium/buffer_types.hpp>
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/minimal_ccl_common.hpp"
 #include <cstdint>
 #include <utility>
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/kernels/llama_all_gather_concat_writer.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/kernels/llama_all_gather_concat_writer.cpp
index 3485add984..f8da48966a 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/kernels/llama_all_gather_concat_writer.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/kernels/llama_all_gather_concat_writer.cpp
@@ -5,7 +5,7 @@
 #include "dataflow_api.h"
 #include <tt-metalium/buffer_types.hpp>
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/minimal_ccl_common.hpp"
 #include <cstdint>
 #include <utility>
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/kernels/llama_concat_reader.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/kernels/llama_concat_reader.cpp
index 44a91d79de..4ff9a1b84f 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/kernels/llama_concat_reader.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/device/kernels/llama_concat_reader.cpp
@@ -5,7 +5,7 @@
 #include "dataflow_api.h"
 #include <tt-metalium/buffer_types.hpp>
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/minimal_ccl_common.hpp"
 #include <cstdint>
 #include <utility>
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/all_reduce_async/device/kernels/dataflow/worker_writer.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/all_reduce_async/device/kernels/dataflow/worker_writer.cpp
index 76428979e3..ed027c5d9b 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/all_reduce_async/device/kernels/dataflow/worker_writer.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/all_reduce_async/device/kernels/dataflow/worker_writer.cpp
@@ -5,7 +5,7 @@
 #include "dataflow_api.h"
 #include <tt-metalium/buffer_types.hpp>
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/minimal_ccl_common.hpp"
 #include <cstdint>
 #include <utility>
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/all_to_all_async/device/kernels/interleaved_all_to_all_writer.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/all_to_all_async/device/kernels/interleaved_all_to_all_writer.cpp
index 0a6706de18..a8d2d8cf5a 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/all_to_all_async/device/kernels/interleaved_all_to_all_writer.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/all_to_all_async/device/kernels/interleaved_all_to_all_writer.cpp
@@ -5,8 +5,7 @@
 #include "dataflow_api.h"
 #include <tt-metalium/buffer_types.hpp>
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "ttnn/cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/minimal_ccl_common.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include <cstdint>
 #include <utility>
 
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/ccl_experimental_pybind.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/ccl_experimental_pybind.cpp
index 3a6a82baf0..6a1b580d82 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/ccl_experimental_pybind.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/ccl_experimental_pybind.cpp
@@ -16,7 +16,6 @@
 #include "ttnn/operations/experimental/ccl/all_to_all_async/all_to_all_async_pybind.hpp"
 #include "ttnn/operations/experimental/ccl/all_gather_concat_heads_fused/all_gather_concat_pybind.hpp"
 #include "ttnn/operations/experimental/ccl/reduce_scatter_async/reduce_scatter_pybind.hpp"
-#include "ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul_pybind.hpp"
 #include "ttnn/operations/experimental/ccl/reduce_scatter_minimal_async/reduce_scatter_minimal_async_pybind.hpp"
 #include "ttnn/operations/experimental/ccl/all_reduce_async/all_reduce_async_pybind.hpp"
 #include "ttnn/operations/experimental/ccl/llama_reduce_scatter/llama_reduce_scatter_pybind.hpp"
@@ -35,7 +34,6 @@ void py_module(pybind11::module& module) {
     ccl::py_bind_all_gather_concat(module);
     ccl::py_bind_matmul_reduce_scatter_async(module);
     ccl::py_bind_reduce_scatter_async(module);
-    ccl::py_bind_rs_matmul(module);
     ccl::py_bind_reduce_scatter_minimal_async(module);
     ccl::py_bind_all_reduce_async(module);
     ccl::py_bind_llama_reduce_scatter(module);
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter/device/kernels/dataflow/writer_llama_reduce_scatter.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter/device/kernels/dataflow/writer_llama_reduce_scatter.cpp
index 151ffc4673..fe5305f969 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter/device/kernels/dataflow/writer_llama_reduce_scatter.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter/device/kernels/dataflow/writer_llama_reduce_scatter.cpp
@@ -9,9 +9,8 @@
 #include "tests/tt_metal/tt_metal/perf_microbenchmark/common/kernel_utils.hpp"
 #include "ttnn/cpp/ttnn/operations/data_movement/common/kernels/common.hpp"
 #include "ttnn/cpp/ttnn/operations/ccl/kernel_common/sharding_addrgen.hpp"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 
 constexpr bool flush = false;
 
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter/device/llama_reduce_scatter_device_operation.hpp b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter/device/llama_reduce_scatter_device_operation.hpp
index 49b280d6d8..af6a67b5ce 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter/device/llama_reduce_scatter_device_operation.hpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter/device/llama_reduce_scatter_device_operation.hpp
@@ -58,30 +58,12 @@ struct LlamaReduceScatterDeviceOperation {
             const tensor_args_t& tensor_args,
             tensor_return_value_t& tensor_return_value);
 
-        static ttnn::device_operation::CachedProgram<shared_variables_t> create_at_helper(
-            const operation_attributes_t& operation_attributes,
-            const ttnn::MeshCoordinate& mesh_coordinate,
-            const tensor_args_t& tensor_args,
-            tensor_return_value_t& tensor_return_value);
         static ttnn::device_operation::CachedProgram<shared_variables_t> create_at(
             const operation_attributes_t& operation_attributes,
             const ttnn::MeshCoordinate& mesh_coordinate,
             const tensor_args_t& tensor_args,
-            tensor_return_value_t& tensor_return_value,
-            tt::tt_metal::Program& program);
+            tensor_return_value_t& tensor_return_value);
 
-        static shared_variables_t create_at_program_processing(
-            const operation_attributes_t& operation_attributes,
-            const ttnn::MeshCoordinate& mesh_coordinate,
-            const tensor_args_t& tensor_args,
-            tensor_return_value_t& tensor_return_value,
-            tt::tt_metal::Program& program);
-        static void override_runtime_arguments_per_program(
-            const shared_variables_t& shared_variables,
-            tt::tt_metal::Program& program,
-            const operation_attributes_t& operation_attributes,
-            const tensor_args_t& tensor_args,
-            LlamaReduceScatterDeviceOperation::tensor_return_value_t& tensor_return_value);
         static void override_runtime_arguments(
             cached_mesh_workload_t& cached_program,
             const operation_attributes_t& operation_attributes,
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter/device/llama_reduce_scatter_program_factory.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter/device/llama_reduce_scatter_program_factory.cpp
index 7bc217f83d..ec1ec202e2 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter/device/llama_reduce_scatter_program_factory.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter/device/llama_reduce_scatter_program_factory.cpp
@@ -228,10 +228,6 @@ uint32_t max_shards_per_worker(const std::vector<std::vector<ReadRequest>>& sche
     }
     return max_shards_per_worker;
 }
-CoreRangeSet get_llama_ccl_cores() {
-    // We externally reserve this core range for CCL cores
-    return CoreRangeSet(CoreRange({1, 6}, {2, 7}));
-}
 
 CoreRangeSet get_worker_cores(const CoreRangeSet& available_cores, const uint32_t num_workers, bool row_wise) {
     CoreRangeSet worker_cores;
@@ -252,17 +248,6 @@ CoreRangeSet get_worker_cores(const CoreRangeSet& available_cores, const uint32_
 
 }  // namespace detail
 
-ttnn::device_operation::CachedProgram<LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::shared_variables_t>
-LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::create_at_helper(
-    const operation_attributes_t& operation_attributes,
-    const ttnn::MeshCoordinate& mesh_coordinate,
-    const tensor_args_t& tensor_args,
-    tensor_return_value_t& tensor_return_value) {
-    tt::tt_metal::Program program{};
-    return LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::create_at(
-        operation_attributes, mesh_coordinate, tensor_args, tensor_return_value, program);
-}
-
 LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::cached_mesh_workload_t
 LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::create_mesh_workload(
     const operation_attributes_t& operation_attributes,
@@ -272,7 +257,7 @@ LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::create_mesh_workload(
     tt::tt_metal::distributed::MeshWorkload workload;
     std::unordered_map<ttnn::MeshCoordinateRange, shared_variables_t> shared_variables;
     for (const auto& coord : tensor_coords.coords()) {
-        auto cached_program = create_at_helper(operation_attributes, coord, tensor_args, tensor_return_value);
+        auto cached_program = create_at(operation_attributes, coord, tensor_args, tensor_return_value);
         workload.add_program(ttnn::MeshCoordinateRange(coord), std::move(cached_program.program));
         shared_variables.emplace(coord, std::move(cached_program.shared_variables));
     }
@@ -284,20 +269,7 @@ LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::create_at(
     const operation_attributes_t& operation_attributes,
     const ttnn::MeshCoordinate& mesh_coordinate,
     const tensor_args_t& tensor_args,
-    tensor_return_value_t& tensor_return_value,
-    tt::tt_metal::Program& program) {
-    return {
-        std::move(program),
-        create_at_program_processing(operation_attributes, mesh_coordinate, tensor_args, tensor_return_value, program)};
-}
-
-LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::shared_variables_t
-LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::create_at_program_processing(
-    const operation_attributes_t& operation_attributes,
-    const ttnn::MeshCoordinate& mesh_coordinate,
-    const tensor_args_t& tensor_args,
-    tensor_return_value_t& tensor_return_value,
-    tt::tt_metal::Program& program) {
+    tensor_return_value_t& tensor_return_value) {
     using namespace tt;
     using namespace tt::tt_metal;
     using namespace tt::tt_fabric;
@@ -398,6 +370,7 @@ LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::create_at_program_proc
         tt::tt_metal::HalProgrammableCoreType::TENSIX,
         operation_attributes.subdevice_id.value_or(mesh_device->get_sub_device_ids().at(0)));
 
+    tt::tt_metal::Program program{};
 
     auto fabric_max_packet_size = tt::tt_fabric::get_tt_fabric_channel_buffer_size_bytes();
     size_t packet_size_bytes =
@@ -427,7 +400,7 @@ LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::create_at_program_proc
         num_packets_total_per_device,
         input_shard_spec.orientation == ShardOrientation::ROW_MAJOR);
 
-    auto available_cores = detail::get_llama_ccl_cores();
+    auto available_cores = sub_device_cores.subtract(packet_worker_cores_grid);
 
     auto sender_core_grid = detail::get_worker_cores(
         available_cores, num_workers_per_link * num_links, input_shard_spec.orientation == ShardOrientation::ROW_MAJOR);
@@ -800,44 +773,12 @@ LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::create_at_program_proc
     }
 
     return {
-        .unary_reader_kernel_id = unary_reader_kernel_id,
-        .unary_writer_kernel_id = unary_writer_kernel_id,
-        .compute_kernel_id = compute_kernel_id,
-        .cb_handles = {cb_input_tensor_handle, cb_output_tensor_handle, cb_fabric_receiver_handle},
-        .core_range = all_cores_grid};
-}
-
-void LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::override_runtime_arguments_per_program(
-    const LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::shared_variables_t& shared_variables,
-    tt::tt_metal::Program& program,
-    const LlamaReduceScatterDeviceOperation::operation_attributes_t& operation_attributes,
-    const LlamaReduceScatterDeviceOperation::tensor_args_t& tensor_args,
-    LlamaReduceScatterDeviceOperation::tensor_return_value_t& tensor_return_value) {
-    auto& unary_reader_kernel_id = shared_variables.unary_reader_kernel_id;
-    auto& unary_writer_kernel_id = shared_variables.unary_writer_kernel_id;
-
-    const auto& input_tensor = tensor_args.input_tensor;
-    const auto& intermediate_packet_buffer = tensor_args.intermediate_packet_buffer;
-    auto& output_tensor = tensor_return_value;
-
-    auto input_tensor_buffer = input_tensor.buffer();
-    auto output_tensor_buffer = output_tensor.buffer();
-    auto packet_buffer = intermediate_packet_buffer.buffer();
-
-    auto& all_cores_grid = shared_variables.core_range;
-
-    auto cores = corerange_to_cores(all_cores_grid, std::nullopt);
-
-    UpdateDynamicCircularBufferAddress(program, shared_variables.cb_handles[0], *input_tensor_buffer);
-    UpdateDynamicCircularBufferAddress(program, shared_variables.cb_handles[1], *output_tensor_buffer);
-    UpdateDynamicCircularBufferAddress(program, shared_variables.cb_handles[2], *packet_buffer);
-
-    for (const auto& core : cores) {
-        auto& writer_runtime_args = tt::tt_metal::GetRuntimeArgs(program, unary_writer_kernel_id, core);
-        writer_runtime_args[0] = (uint32_t)operation_attributes.cross_device_semaphore->address();
-        auto& reader_runtime_args = tt::tt_metal::GetRuntimeArgs(program, unary_reader_kernel_id, core);
-        reader_runtime_args[0] = (uint32_t)operation_attributes.cross_device_semaphore->address();
-    }
+        std::move(program),
+        {.unary_reader_kernel_id = unary_reader_kernel_id,
+         .unary_writer_kernel_id = unary_writer_kernel_id,
+         .compute_kernel_id = compute_kernel_id,
+         .cb_handles = {cb_input_tensor_handle, cb_output_tensor_handle, cb_fabric_receiver_handle},
+         .core_range = all_cores_grid}};
 }
 
 void LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::override_runtime_arguments(
@@ -847,8 +788,32 @@ void LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::override_runtime_
     tensor_return_value_t& tensor_return_value) {
     for (auto& [range, program] : cached_workload.workload.get_programs()) {
         const auto& shared_variables = cached_workload.shared_variables.at(range);
-        override_runtime_arguments_per_program(
-            shared_variables, program, operation_attributes, tensor_args, tensor_return_value);
+
+        auto& unary_reader_kernel_id = shared_variables.unary_reader_kernel_id;
+        auto& unary_writer_kernel_id = shared_variables.unary_writer_kernel_id;
+
+        const auto& input_tensor = tensor_args.input_tensor;
+        const auto& intermediate_packet_buffer = tensor_args.intermediate_packet_buffer;
+        auto& output_tensor = tensor_return_value;
+
+        auto input_tensor_buffer = input_tensor.buffer();
+        auto output_tensor_buffer = output_tensor.buffer();
+        auto packet_buffer = intermediate_packet_buffer.buffer();
+
+        auto& all_cores_grid = shared_variables.core_range;
+
+        auto cores = corerange_to_cores(all_cores_grid, std::nullopt);
+
+        UpdateDynamicCircularBufferAddress(program, shared_variables.cb_handles[0], *input_tensor_buffer);
+        UpdateDynamicCircularBufferAddress(program, shared_variables.cb_handles[1], *output_tensor_buffer);
+        UpdateDynamicCircularBufferAddress(program, shared_variables.cb_handles[2], *packet_buffer);
+
+        for (const auto& core : cores) {
+            auto& writer_runtime_args = tt::tt_metal::GetRuntimeArgs(program, unary_writer_kernel_id, core);
+            writer_runtime_args[0] = (uint32_t)operation_attributes.cross_device_semaphore->address();
+            auto& reader_runtime_args = tt::tt_metal::GetRuntimeArgs(program, unary_reader_kernel_id, core);
+            reader_runtime_args[0] = (uint32_t)operation_attributes.cross_device_semaphore->address();
+        }
     }
 }
 
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_create_heads/device/kernels/dataflow/writer_llama_reduce_scatter.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_create_heads/device/kernels/dataflow/writer_llama_reduce_scatter.cpp
index 1fe0a5af3c..8548cbefee 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_create_heads/device/kernels/dataflow/writer_llama_reduce_scatter.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_create_heads/device/kernels/dataflow/writer_llama_reduce_scatter.cpp
@@ -9,9 +9,8 @@
 #include "tests/tt_metal/tt_metal/perf_microbenchmark/common/kernel_utils.hpp"
 #include "ttnn/cpp/ttnn/operations/data_movement/common/kernels/common.hpp"
 #include "ttnn/cpp/ttnn/operations/ccl/kernel_common/sharding_addrgen.hpp"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 
 constexpr bool flush = false;
 void kernel_main() {
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_create_heads/device/llama_reduce_scatter_create_heads_program_factory.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_create_heads/device/llama_reduce_scatter_create_heads_program_factory.cpp
index 2e9f463eda..316ea8c1aa 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_create_heads/device/llama_reduce_scatter_create_heads_program_factory.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_create_heads/device/llama_reduce_scatter_create_heads_program_factory.cpp
@@ -756,8 +756,10 @@ LlamaReduceScatterCreateHeadsDeviceOperation::LlamaReduceScatterCreateHeads::cre
     bool forward_fabric_connection = false, backward_fabric_connection = false;
     if (operation_attributes.topology == ttnn::ccl::Topology::Linear) {
         LineTopology line_topology(ring_size, ring_index);
-        forward_fabric_connection = !(line_topology.is_first_device_in_line(ttnn::ccl::LineDirection::BACKWARD));
-        backward_fabric_connection = !(line_topology.is_last_device_in_line(ttnn::ccl::LineDirection::BACKWARD));
+        forward_fabric_connection =
+            !(line_topology.is_first_device_in_line(ttnn::ccl::EdmLineFabricOpInterface::Direction::BACKWARD));
+        backward_fabric_connection =
+            !(line_topology.is_last_device_in_line(ttnn::ccl::EdmLineFabricOpInterface::Direction::BACKWARD));
     } else if (operation_attributes.topology == ttnn::ccl::Topology::Ring) {
         forward_fabric_connection = true;
         backward_fabric_connection = true;
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/device/rs_matmul_op.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/device/rs_matmul_op.cpp
deleted file mode 100644
index 0581e128d6..0000000000
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/device/rs_matmul_op.cpp
+++ /dev/null
@@ -1,122 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include "ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/device/rs_matmul_op.hpp"
-
-#include <tt-metalium/core_coord.hpp>
-#include "ttnn/operations/experimental/ccl/llama_reduce_scatter/device/llama_reduce_scatter_device_operation.hpp"
-#include "ttnn/operations/math.hpp"
-#include "ttnn/tensor/tensor_utils.hpp"
-#include "ttnn/operations/ccl/ccl_common.hpp"
-#include "ttnn/operations/ccl/sharding_addrgen_helper.hpp"
-
-namespace ttnn::operations::experimental::ccl {
-
-Matmul_RS::program_factory_t Matmul_RS::select_program_factory(
-    const operation_attributes_t& operation_attributes, const tensor_args_t& tensor_args) {
-    return Matmul_RS_PF{};
-}
-
-void Matmul_RS::validate_on_program_cache_hit(
-    const operation_attributes_t& operation_attributes, const tensor_args_t& tensor_args) {
-    operation_attributes.matmul.validate(
-        {tensor_args.matmul.input_tensor, tensor_args.matmul.weight_tensor}, {std::nullopt}, {});
-    operation_attributes.rs.validate_on_program_cache_hit(operation_attributes.rs_op, tensor_args.rs);
-}
-
-void Matmul_RS::validate_on_program_cache_miss(
-    const operation_attributes_t& operation_attributes, const tensor_args_t& tensor_args) {
-    operation_attributes.matmul.validate(
-        {tensor_args.matmul.input_tensor, tensor_args.matmul.weight_tensor}, {std::nullopt}, {});
-    operation_attributes.rs.validate_on_program_cache_miss(operation_attributes.rs_op, tensor_args.rs);
-}
-
-Matmul_RS::spec_return_value_t Matmul_RS::compute_output_specs(
-    const operation_attributes_t& operation_attributes, const tensor_args_t& tensor_args) {
-    // Reduce Scatter shape
-    ttnn::TensorSpec reduce_scatter_output_spec =
-        operation_attributes.rs.compute_output_specs(operation_attributes.rs_op, tensor_args.rs);
-    // Matmul shape
-    ttnn::TensorSpec matmul_output_specs = operation_attributes.matmul.compute_output_specs(
-        {tensor_args.matmul.input_tensor, tensor_args.matmul.weight_tensor}, {})[0];
-
-    return {matmul_output_specs, reduce_scatter_output_spec};
-}
-
-Matmul_RS::tensor_return_value_t Matmul_RS::create_output_tensors(
-    const operation_attributes_t& operation_attributes, const tensor_args_t& tensor_args) {
-    // Matmul output tensor
-    Tensor matmul_output_tensor = operation_attributes.matmul.create_output_tensors(
-        {tensor_args.matmul.input_tensor, tensor_args.matmul.weight_tensor}, {})[0];
-    Tensor rs_output_tensor = operation_attributes.rs.create_output_tensors(operation_attributes.rs_op, tensor_args.rs);
-
-    return {matmul_output_tensor, rs_output_tensor};
-}
-
-std::tuple<Matmul_RS::operation_attributes_t, Matmul_RS::tensor_args_t> Matmul_RS::invoke(
-    const ttnn::Tensor& input_tensor,
-    const ttnn::Tensor& weight_tensor,  // mm1 used
-    const ttnn::Tensor& rs_tensor,      // rs1
-    ttnn::Tensor& intermediate_packet_buffer,
-    const int32_t dim,
-    const GlobalSemaphore& semaphore,
-    const uint32_t cluster_axis,
-    const uint32_t ring_devices,
-    const uint32_t num_links,
-    const tt::tt_metal::SubDeviceId& subdevice_id,
-    const std::optional<ttnn::MemoryConfig>& memory_config_rs,                           // default std::nullopt
-    const std::optional<ttnn::MemoryConfig>& memory_config_mm,                           // default std::nullopt
-    const std::optional<const ttnn::DeviceComputeKernelConfig> compute_kernel_config,    // default std::nullopt
-    const std::optional<const GlobalCircularBuffer>& global_cb,                          // default std::nullopt
-    const std::optional<const ttnn::CoreGrid> core_grid,                                 // default std::nullopt
-    const bool transpose_a,                                                              // degault false
-    const bool transpose_b,                                                              // default false
-    const std::optional<const DataType> dtype,                                           // default std::nullopt
-    const std::optional<const operations::matmul::MatmulProgramConfig>& program_config,  // default std::nullopt
-    const std::optional<const std::string>& activation,                                  // default std::nullopt
-    const std::optional<const tt::tt_metal::Tile>& output_tile,                          // default std::nullopt
-    const std::optional<Tensor>& optional_output_tensor,                                 // default std::nullopt
-    tt::tt_fabric::Topology topology) {
-    LlamaReduceScatterDeviceOperation rs_struct{};
-    std::optional<CoreCoord> user_core_coord;
-    if (core_grid.has_value()) {
-        user_core_coord = CoreCoord(core_grid->x, core_grid->y);
-    }
-    bool user_run_batched = ttnn::operations::matmul::detail::is_input_batched(weight_tensor.get_logical_shape());
-    return {
-        operation_attributes_t{
-            rs_struct,
-            LlamaReduceScatterDeviceOperation::operation_attributes_t{
-                .dim = (dim < 0 ? uint32_t(rs_tensor.get_logical_shape().rank() + dim) : (uint32_t)dim),
-                .cross_device_semaphore = semaphore,
-                .subdevice_id = subdevice_id,
-                .cluster_axis = cluster_axis,
-                .output_mem_config = memory_config_rs,
-                .ring_devices = ring_devices,
-                .num_links = num_links,
-                .topology = topology},
-            operations::matmul::create_matmul_struct(
-                input_tensor,
-                weight_tensor,
-                /*parameters=*/
-                operations::matmul::Matmul{
-                    program_config,
-                    /*bcast_batch=*/std::nullopt,
-                    memory_config_mm.value_or(input_tensor.memory_config()),
-                    dtype.value_or(input_tensor.get_dtype()),
-                    compute_kernel_config,
-                    /*untilize_out=*/false,
-                    user_core_coord,
-                    ttnn::operations::matmul::get_fused_activation(activation),
-                    user_run_batched,
-                    transpose_a,
-                    transpose_b,
-                    output_tile,
-                    global_cb})},
-        tensor_args_t{
-            LlamaReduceScatterDeviceOperation::tensor_args_t{rs_tensor, intermediate_packet_buffer},
-            matmul_tensor_args_t{input_tensor, weight_tensor}}};
-}
-
-}  // namespace ttnn::operations::experimental::ccl
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/device/rs_matmul_op.hpp b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/device/rs_matmul_op.hpp
deleted file mode 100644
index 39570bc6e2..0000000000
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/device/rs_matmul_op.hpp
+++ /dev/null
@@ -1,111 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#pragma once
-
-#include <cstdint>
-#include <tt-metalium/core_coord.hpp>
-#include <tt-metalium/buffer.hpp>
-#include "ttnn/distributed/types.hpp"
-#include "ttnn/tensor/tensor.hpp"
-#include "ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
-#include <tt-metalium/constants.hpp>
-#include "ttnn/operations/ccl/ccl_host_datastructures.hpp"
-#include "ttnn/operations/ccl/ccl_common.hpp"
-
-#include "ttnn/run_operation.hpp"
-
-#include <optional>
-#include <vector>
-#include <algorithm>
-
-/* Fusion includes */
-#include "cpp/ttnn/operations/matmul/device/matmul_op.hpp"
-#include "cpp/ttnn/operations/matmul/matmul.hpp"
-#include "ttnn/operations/ccl/ccl_op_fusion.hpp"
-#include "ttnn/operations/experimental/ccl/llama_reduce_scatter/device/llama_reduce_scatter_device_operation.hpp"
-
-namespace ttnn::operations::experimental::ccl {
-
-struct Matmul_RS {
-    using spec_return_value_t = std::vector<ttnn::TensorSpec>;
-    using tensor_return_value_t = std::vector<Tensor>;
-    struct matmul_tensor_args_t {
-        const Tensor input_tensor;
-        const Tensor weight_tensor;
-    };
-    struct tensor_args_t {
-        LlamaReduceScatterDeviceOperation::tensor_args_t rs;
-        matmul_tensor_args_t matmul;
-    };
-    struct operation_attributes_t {
-        LlamaReduceScatterDeviceOperation rs;
-        LlamaReduceScatterDeviceOperation::operation_attributes_t rs_op;
-        operations::matmul::Matmul matmul;
-    };
-    struct Matmul_RS_PF {
-        // Shared variables are the variables that are shared between the create and override_runtime_arguments methods
-        struct shared_variables_t {
-            LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::shared_variables_t rs_shared_vars;
-            ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t matmul_shared_vars;
-        };
-        using cached_mesh_workload_t = ttnn::device_operation::AdaptedCachedMeshWorkload<shared_variables_t>;
-
-        static cached_mesh_workload_t create_mesh_workload(
-            const operation_attributes_t& operation_attributes,
-            const ttnn::MeshCoordinateRangeSet& tensor_coords,
-            const tensor_args_t& tensor_args,
-            std::vector<Tensor>& tensor_return_value);
-
-        static ttnn::device_operation::CachedProgram<shared_variables_t> create_at(
-            const operation_attributes_t& operation_attributes,
-            const ttnn::MeshCoordinate& mesh_coordinate,
-            const tensor_args_t& tensor_args,
-            std::vector<Tensor>& tensor_return_value);
-
-        static void override_runtime_arguments(
-            cached_mesh_workload_t& cached_program,
-            const operation_attributes_t& operation_attributes,
-            const tensor_args_t& tensor_args,
-            std::vector<Tensor>& tensor_return_value);
-    };
-    using program_factory_t = std::variant<Matmul_RS_PF>;
-
-    static program_factory_t select_program_factory(
-        const operation_attributes_t& operation_attributes, const tensor_args_t& tensor_args);
-    static void validate_on_program_cache_hit(const operation_attributes_t&, const tensor_args_t&);
-    static void validate_on_program_cache_miss(const operation_attributes_t&, const tensor_args_t&);
-    static spec_return_value_t compute_output_specs(const operation_attributes_t&, const tensor_args_t&);
-    static tensor_return_value_t create_output_tensors(const operation_attributes_t&, const tensor_args_t&);
-    static std::tuple<operation_attributes_t, tensor_args_t> invoke(
-        const ttnn::Tensor& input_tensor,
-        const ttnn::Tensor& weight_tensor,
-        const ttnn::Tensor& rs_tensor,
-        ttnn::Tensor& intermediate_packet_buffer,
-        const int32_t dim,
-        const GlobalSemaphore& semaphore,
-        const uint32_t cluster_axis,
-        const uint32_t ring_devices,
-        const uint32_t num_links,
-        const tt::tt_metal::SubDeviceId& subdevice_id,
-        const std::optional<ttnn::MemoryConfig>& memory_config_rs = std::nullopt,
-        const std::optional<ttnn::MemoryConfig>& memory_config_mm = std::nullopt,
-        const std::optional<const ttnn::DeviceComputeKernelConfig> compute_kernel_config = std::nullopt,
-        const std::optional<const GlobalCircularBuffer>& global_cb = std::nullopt,
-        const std::optional<const ttnn::CoreGrid> core_grid = std::nullopt,
-        const bool transpose_a = false,
-        const bool transpose_b = false,
-        const std::optional<const DataType> dtype = std::nullopt,
-        const std::optional<const operations::matmul::MatmulProgramConfig>& program_config = std::nullopt,
-        const std::optional<const std::string>& activation = std::nullopt,
-        const std::optional<const tt::tt_metal::Tile>& output_tile = std::nullopt,
-        const std::optional<Tensor>& optional_output_tensor = std::nullopt,
-        tt::tt_fabric::Topology topology = tt::tt_fabric::Topology::Linear);
-};
-
-}  // namespace ttnn::operations::experimental::ccl
-namespace ttnn::prim {
-constexpr auto llama_rs_matmul =
-    ttnn::register_operation<"ttnn::prim::llama_rs_matmul", ttnn::operations::experimental::ccl::Matmul_RS>();
-}
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/device/rs_matmul_program_factory.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/device/rs_matmul_program_factory.cpp
deleted file mode 100644
index 9e9b6cfb2c..0000000000
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/device/rs_matmul_program_factory.cpp
+++ /dev/null
@@ -1,90 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include "rs_matmul_op.hpp"
-#include <tt-metalium/work_split.hpp>
-#include <vector>
-#include <tt-metalium/constants.hpp>
-#include <tt-metalium/device_pool.hpp>
-#include "ttnn/distributed/types.hpp"
-#include "ttnn/operations/ccl/ccl_common.hpp"
-#include "ttnn/operations/experimental/ccl/all_gather_async/device/all_gather_async_op.hpp"
-#include "cpp/ttnn/operations/ccl/shared_with_host/sharded_tensor_addr_gen.hpp"
-#include "cpp/ttnn/operations/ccl/sharding_addrgen_helper.hpp"
-#include <tt-metalium/core_coord.hpp>
-#include <tt-metalium/erisc_datamover_builder.hpp>
-#include "cpp/ttnn/operations/ccl/common/host/ccl_worker_builder.hpp"
-#include <tt-metalium/fabric.hpp>
-namespace ttnn::operations::experimental::ccl {
-Matmul_RS::Matmul_RS_PF::cached_mesh_workload_t Matmul_RS::Matmul_RS_PF::create_mesh_workload(
-    const operation_attributes_t& operation_attributes,
-    const ttnn::MeshCoordinateRangeSet& tensor_coords,
-    const tensor_args_t& tensor_args,
-    std::vector<Tensor>& tensor_return_value) {
-    tt::tt_metal::distributed::MeshWorkload workload;
-    std::unordered_map<ttnn::MeshCoordinateRange, shared_variables_t> shared_variables;
-    for (const auto& coord : tensor_coords.coords()) {
-        auto cached_program = create_at(operation_attributes, coord, tensor_args, tensor_return_value);
-        workload.add_program(ttnn::MeshCoordinateRange(coord), std::move(cached_program.program));
-        shared_variables.emplace(coord, std::move(cached_program.shared_variables));
-    }
-    return cached_mesh_workload_t(std::move(workload), std::move(shared_variables));
-}
-
-ttnn::device_operation::CachedProgram<Matmul_RS::Matmul_RS_PF::shared_variables_t> Matmul_RS::Matmul_RS_PF::create_at(
-    const operation_attributes_t& operation_attributes,
-    const ttnn::MeshCoordinate& mesh_coordinate,
-    const tensor_args_t& tensor_args,
-    std::vector<Tensor>& tensor_return_value) {
-    tt::tt_metal::Program program{};
-    std::optional<ttnn::experimental::ccl::MatmulFusedOpSignaler> empty_fused_op_signaler;
-    tt::tt_metal::SubDeviceId sub_device_id = operation_attributes.rs_op.subdevice_id.value();
-    CoreRangeSet rs_cores =
-        CoreRangeSet(std::set{::CoreRange{{1, 1}, {3, 2}}, ::CoreRange{{1, 3}, {2, 3}}, ::CoreRange{{1, 6}, {2, 7}}});
-    std::optional<CoreRangeSet> optional_core_range = rs_cores;
-    return {
-        std::move(program),
-        shared_variables_t{
-            LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::create_at_program_processing(
-                operation_attributes.rs_op, mesh_coordinate, tensor_args.rs, tensor_return_value.at(1), program),
-            matmul::matmul_multi_core_reuse_mcast_1d_optimized_helper(
-                program,
-                tensor_args.matmul.input_tensor,
-                {tensor_args.matmul.weight_tensor},
-                std::nullopt /*bias*/,
-                {tensor_return_value.at(0)},
-                operation_attributes.matmul.bcast_batch.value(),
-                operation_attributes.matmul.compute_kernel_config.value(),
-                operation_attributes.matmul.program_config.value(),
-                operation_attributes.matmul.untilize_out,
-                empty_fused_op_signaler,
-                operation_attributes.matmul.global_cb,
-                sub_device_id /*sub_device_id*/,
-                tt::CBIndex::c_6 /*start cb index*/,
-                optional_core_range)}};
-}
-
-void Matmul_RS::Matmul_RS_PF::override_runtime_arguments(
-    cached_mesh_workload_t& cached_workload,
-    const operation_attributes_t& operation_attributes,
-    const tensor_args_t& tensor_args,
-    std::vector<Tensor>& tensor_return_value) {
-    for (auto& [range, program] : cached_workload.workload.get_programs()) {
-        const auto& shared_variables = cached_workload.shared_variables.at(range);
-        LlamaReduceScatterDeviceOperation::LlamaReduceScatterAdd::override_runtime_arguments_per_program(
-            shared_variables.rs_shared_vars,
-            program,
-            operation_attributes.rs_op,
-            tensor_args.rs,
-            tensor_return_value.at(1));
-        reuse_mcast_1d_optimized_helpers::override_program_parameters(
-            shared_variables.matmul_shared_vars,
-            &operation_attributes.matmul,
-            program,
-            {tensor_args.matmul.input_tensor, tensor_args.matmul.weight_tensor},
-            {},
-            {tensor_return_value.at(0)});
-    }
-}
-}  // namespace ttnn::operations::experimental::ccl
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul.cpp
deleted file mode 100644
index 97e0d4c6d5..0000000000
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul.cpp
+++ /dev/null
@@ -1,66 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include "ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/device/rs_matmul_op.hpp"
-#include "ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul.hpp"
-
-namespace ttnn::operations::experimental::ccl {
-
-std::vector<ttnn::Tensor> ExecuteLlamaReduceScatterMatmul::invoke(
-    QueueId queue_id,
-    const ttnn::Tensor& input_tensor,               // mm0 used
-    const ttnn::Tensor& weight_tensor,              // mm1 used
-    const ttnn::Tensor& rs_tensor,                  // rs1
-    ttnn::Tensor& intermediate_packet_buffer,       // rs2
-    int32_t dim,                                    // rs3
-    const GlobalSemaphore& cross_device_semaphore,  // rs4
-    const uint32_t cluster_axis,                    // rs 5
-    const MeshDevice& mesh_device,                  // rs 6
-    const uint32_t num_links,                       // rs 7 default 1
-    const tt::tt_metal::SubDeviceId& subdevice_id,
-    tt::tt_fabric::Topology topology,
-    const std::optional<ttnn::MemoryConfig>& memory_config_rs,  // rs 8 default std::nullopt
-    const std::optional<ttnn::MemoryConfig>& memory_config_mm,  // mm4 used but default std::nullopt
-    const std::optional<const ttnn::DeviceComputeKernelConfig>
-        compute_kernel_config,                                   // mm8 used but default std::nullopt
-    const std::optional<const GlobalCircularBuffer>& global_cb,  // mm12 used but default std::nullopt
-    const std::optional<const ttnn::CoreGrid> core_grid,         // mm9 may use but default std::nullopt
-    const bool transpose_a,                                      // mm2 set false
-    const bool transpose_b,                                      // mm3 set false
-    const std::optional<const DataType> dtype,                   // mm5 set false
-    const std::optional<const operations::matmul::MatmulProgramConfig>& program_config,  // mm6 std::nullopt
-    const std::optional<const std::string>& activation,                                  // mm7 set false
-    const std::optional<const tt::tt_metal::Tile>& output_tile,                          // mm10 std::nullopt
-    const std::optional<Tensor>& optional_output_tensor                                  // mm11 std::nullopt
-) {
-    const auto& mesh_view = mesh_device.get_view();
-    const uint32_t ring_devices = (cluster_axis == 0) ? mesh_view.num_rows() : mesh_view.num_cols();
-    TT_FATAL(ring_devices > 1, "reduce_scatter async op will only work for ring_devices > 1, but has {}", ring_devices);
-    return ttnn::prim::llama_rs_matmul(
-        input_tensor,
-        weight_tensor,
-        rs_tensor,
-        intermediate_packet_buffer,
-        dim,
-        cross_device_semaphore,
-        cluster_axis,
-        ring_devices,
-        num_links,
-        subdevice_id,
-        memory_config_rs,
-        memory_config_mm,
-        compute_kernel_config,
-        global_cb,
-        core_grid,
-        transpose_a,
-        transpose_b,
-        dtype,
-        program_config,
-        activation,
-        output_tile,
-        optional_output_tensor,
-        topology);
-}
-
-}  // namespace ttnn::operations::experimental::ccl
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul.hpp b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul.hpp
deleted file mode 100644
index 87eb6faf8b..0000000000
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul.hpp
+++ /dev/null
@@ -1,53 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#pragma once
-
-#include "ttnn/decorators.hpp"
-#include <tt-metalium/core_coord.hpp>
-#include "ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/device/rs_matmul_op.hpp"
-#include "ttnn/distributed/api.hpp"
-
-namespace ttnn {
-namespace operations::experimental::ccl {
-
-struct ExecuteLlamaReduceScatterMatmul {
-    static std::vector<ttnn::Tensor> invoke(
-        QueueId queue_id,
-        const ttnn::Tensor& input_tensor,               // mm0 used
-        const ttnn::Tensor& weight_tensor,              // mm1 used
-        const ttnn::Tensor& rs_tensor,                  // rs1
-        ttnn::Tensor& intermediate_packet_buffer,       // rs2
-        int32_t dim,                                    // rs3
-        const GlobalSemaphore& cross_device_semaphore,  // rs4
-        const uint32_t cluster_axis,                    // rs 5
-        const MeshDevice& mesh_device,                  // rs 6
-        const uint32_t num_links,                       // rs 7 default 1
-        const tt::tt_metal::SubDeviceId& subdevice_id,
-        tt::tt_fabric::Topology topology = tt::tt_fabric::Topology::Linear,
-        const std::optional<ttnn::MemoryConfig>& memory_config_rs = std::nullopt,  // rs 8 default std::nullopt
-        const std::optional<ttnn::MemoryConfig>& memory_config_mm = std::nullopt,  // mm4 used but default std::nullopt
-        const std::optional<const ttnn::DeviceComputeKernelConfig> compute_kernel_config =
-            std::nullopt,  // mm8 used but default std::nullopt
-        const std::optional<const GlobalCircularBuffer>& global_cb =
-            std::nullopt,                                                    // mm12 used but default std::nullopt
-        const std::optional<const ttnn::CoreGrid> core_grid = std::nullopt,  // mm9 may use but default std::nullopt
-        const bool transpose_a = false,                                      // mm2 set false
-        const bool transpose_b = false,                                      // mm3 set false
-        const std::optional<const DataType> dtype = std::nullopt,            // mm5 set false
-        const std::optional<const operations::matmul::MatmulProgramConfig>& program_config =
-            std::nullopt,                                                           // mm6 std::nullopt
-        const std::optional<const std::string>& activation = std::nullopt,          // mm7 set false
-        const std::optional<const tt::tt_metal::Tile>& output_tile = std::nullopt,  // mm10 std::nullopt
-        const std::optional<Tensor>& optional_output_tensor = std::nullopt          // mm11 std::nullopt
-    );
-};
-
-}  // namespace operations::experimental::ccl
-namespace experimental {
-constexpr auto llama_rs_matmul = ttnn::register_operation<
-    "ttnn::experimental::llama_rs_matmul",
-    ttnn::operations::experimental::ccl::ExecuteLlamaReduceScatterMatmul>();
-}  // namespace experimental
-}  // Namespace ttnn
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul_pybind.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul_pybind.cpp
deleted file mode 100644
index cbaf66be98..0000000000
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul_pybind.cpp
+++ /dev/null
@@ -1,134 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#include "rs_matmul_pybind.hpp"
-
-#include <pybind11/pybind11.h>
-#include <pybind11/stl.h>
-
-#include "ttnn-pybind/decorators.hpp"
-#include "ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul.hpp"
-#include "ttnn/types.hpp"
-
-namespace ttnn::operations::experimental::ccl {
-
-// TODO: Update with all_gather_matmul docs
-void py_bind_rs_matmul(pybind11::module& module) {
-    namespace py = pybind11;
-
-    ttnn::bind_registered_operation(
-        module,
-        ttnn::experimental::llama_rs_matmul,
-        R"doc(all_gather_matmul(input_tensor: ttnn.Tensor, weight_tensor: ttnn.Tensor, dim: int, *, num_links: int = 1, memory_config: Optional[ttnn.MemoryConfig] = None) -> (ttnn.Tensor, ttnn.Tensor)
-
-        Performs an all-gather operation on multi-device :attr:`input_tensor` across all devices.
-
-        Args:
-            * :attr:`input_tensor` (ttnn.Tensor): multi-device tensor
-            * :attr:`weight_tensor` (ttnn.Tensor): multi-device tensor
-            * :attr:`dim` (int)
-            * :attr:`all_gather_core_grid_offset` (ttnn.CoreCoord): Core grid offset for the all-gather operation.
-
-        Keyword Args:
-            * :attr:`num_links` (int): Number of links to use for the all-gather operation.
-            * :attr:`memory_config_ag` (Optional[ttnn.MemoryConfig]): Memory configuration for the All Gather operation.
-            * :attr:`memory_config_mm` (Optional[ttnn.MemoryConfig]): Memory configuration for the Matmul operation.
-            * :attr:`transpose_a` (bool)
-            * :attr:`transpose_b` (bool)
-            * :attr:`dtype` (Optional[DataType])
-            * :attr:`program_config` (Optional[ttnn.MatmulProgramConfig])
-            * :attr:`activation` (Optional[str])
-            * :attr:`compute_kernel_config` (Optional[DeviceComputeKernelConfig])
-            * :attr:`core_grid` (Optional[ttnn.CoreGrid])
-
-        Example:
-
-            >>> tensor = ttnn.from_torch(torch.tensor((1, 2), dtype=torch.bfloat16), device=device)
-            >>> weight_tensor = ttnn.from_torch(torch.tensor((2, 1), dtype=torch.bfloat16), device=device)
-            >>> all_gathered_mm_in, mm_out = ttnn.all_gather_matmul(tensor, weight_tensor, dim=0, (0, 0))
-
-        )doc",
-        ttnn::pybind_overload_t{
-            [](const decltype(ttnn::experimental::llama_rs_matmul)& self,
-               const ttnn::Tensor& input_tensor,               // mm0 used
-               const ttnn::Tensor& weight_tensor,              // mm1 used
-               const ttnn::Tensor& rs_tensor,                  // rs1
-               ttnn::Tensor& intermediate_packet_buffer,       // rs2
-               int32_t dim,                                    // rs3
-               const GlobalSemaphore& cross_device_semaphore,  // rs4
-               const uint32_t cluster_axis,                    // rs 5
-               const MeshDevice& mesh_device,                  // rs 6
-               const uint32_t num_links,                       // rs 7 default 1
-               const tt::tt_metal::SubDeviceId& subdevice_id,
-               tt::tt_fabric::Topology topology,
-               const std::optional<ttnn::MemoryConfig>& memory_config_rs,  // rs 8 default std::nullopt
-               const std::optional<ttnn::MemoryConfig>& memory_config_mm,  // mm4 used but default std::nullopt
-               const std::optional<const ttnn::DeviceComputeKernelConfig>&
-                   compute_kernel_config,                                   // mm8 used but default std::nullopt
-               const std::optional<const GlobalCircularBuffer>& global_cb,  // mm12 used but default std::nullopt
-               std::optional<const ttnn::CoreGrid>& core_grid,              // mm9 may use but default std::nullopt
-               const bool transpose_a,                                      // mm2 set false
-               const bool transpose_b,                                      // mm3 set false
-               const std::optional<const DataType>& dtype,                  // mm5 set false
-               const std::optional<const operations::matmul::MatmulProgramConfig>& program_config,  // mm6 std::nullopt
-               const std::optional<const std::string>& activation,                                  // mm7 set false
-               const std::optional<const tt::tt_metal::Tile>& output_tile,                          // mm10 std::nullopt
-               std::optional<Tensor>& optional_output_tensor,                                       // mm11 std::nullopt
-               QueueId queue_id  // rs 9 default DefaultQueueId
-
-               ) -> std::vector<ttnn::Tensor> {
-                return self(
-                    queue_id,
-                    input_tensor,
-                    weight_tensor,
-                    rs_tensor,
-                    intermediate_packet_buffer,
-                    dim,
-                    cross_device_semaphore,
-                    cluster_axis,
-                    mesh_device,
-                    num_links,
-                    subdevice_id,
-                    topology,
-                    memory_config_rs,
-                    memory_config_mm,
-                    compute_kernel_config,
-                    global_cb,
-                    core_grid,
-                    transpose_a,
-                    transpose_b,
-                    dtype,
-                    program_config,
-                    activation,
-                    output_tile,
-                    optional_output_tensor);
-            },
-            py::arg("input_tensor"),
-            py::arg("weight_tensor"),
-            py::arg("rs_tensor"),
-            py::arg("intermediate_packet_buffer"),
-            py::arg("dim"),
-            py::arg("cross_device_semaphore"),
-            py::arg("cluster_axis"),
-            py::arg("mesh_device"),
-            py::arg("num_links"),
-            py::arg("subdevice_id"),
-            py::kw_only(),
-            py::arg("topology") = tt::tt_fabric::Topology::Linear,
-            py::arg("memory_config_rs") = std::nullopt,
-            py::arg("memory_config_mm") = std::nullopt,
-            py::arg("compute_kernel_config") = std::nullopt,
-            py::arg("global_cb") = std::nullopt,
-            py::arg("core_grid") = std::nullopt,
-            py::arg("transpose_a") = false,
-            py::arg("transpose_b") = false,
-            py::arg("dtype") = std::nullopt,
-            py::arg("program_config") = std::nullopt,
-            py::arg("activation") = std::nullopt,
-            py::arg("output_tile") = std::nullopt,
-            py::arg("optional_output_tensor") = std::nullopt,
-            py::arg("queue_id") = DefaultQueueId});
-}
-
-}  // namespace ttnn::operations::experimental::ccl
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul_pybind.hpp b/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul_pybind.hpp
deleted file mode 100644
index 5903b9728e..0000000000
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/llama_reduce_scatter_matmul/rs_matmul_pybind.hpp
+++ /dev/null
@@ -1,13 +0,0 @@
-// SPDX-FileCopyrightText: Â© 2025 Tenstorrent AI ULC
-//
-// SPDX-License-Identifier: Apache-2.0
-
-#pragma once
-
-#include "ttnn-pybind/pybind_fwd.hpp"
-
-namespace ttnn::operations::experimental::ccl {
-
-void py_bind_rs_matmul(pybind11::module& module);
-
-}  // namespace ttnn::operations::experimental::ccl
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/reduce_scatter_async/device/reduce_scatter_async_program.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/reduce_scatter_async/device/reduce_scatter_async_program.cpp
index af2c1ca44c..501fc09f9a 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/reduce_scatter_async/device/reduce_scatter_async_program.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/reduce_scatter_async/device/reduce_scatter_async_program.cpp
@@ -107,10 +107,10 @@ enum fabric_lifetime_mode {
 enum LineDirection { FORWARD, BACKWARD };
 static_assert(
     static_cast<size_t>(LineDirection::FORWARD) ==
-    static_cast<size_t>(ttnn::ccl::LineDirection::FORWARD));
+    static_cast<size_t>(ttnn::ccl::EdmLineFabricOpInterface::Direction::FORWARD));
 static_assert(
     static_cast<size_t>(LineDirection::BACKWARD) ==
-    static_cast<size_t>(ttnn::ccl::LineDirection::BACKWARD));
+    static_cast<size_t>(ttnn::ccl::EdmLineFabricOpInterface::Direction::BACKWARD));
 
 constexpr LineDirection relay_to_final_output_dir = LineDirection::FORWARD;
 // TODO: promote to header
@@ -1209,7 +1209,7 @@ static void populate_partial_reduce_rt_args(
     std::unordered_map<CoreCoord, ttnn::ccl::tensor_address_runtime_args_overrider>& reader_rt_args_overrider_map,
     std::unordered_map<CoreCoord, ttnn::ccl::tensor_address_runtime_args_overrider>& writer_rt_args_overrider_map) {
     using namespace ttnn::ccl::worker_detail;
-    using Direction = ttnn::ccl::LineDirection;
+    using Direction = ttnn::ccl::EdmLineFabricOpInterface::Direction;
 
     auto const& all_tensors = builder_config.all_tensors.get();
     auto const& kernel_ids = builder_config.kernel_ids.get();
@@ -1526,7 +1526,7 @@ static void create_end_of_line_worker_runtime_args(
     std::unordered_map<CoreCoord, ttnn::ccl::tensor_address_runtime_args_overrider>& writer_rt_args_overrider_map) {
     using namespace ttnn::ccl::worker_detail;
     using namespace ttnn::ccl::cmd;
-    using Direction = ttnn::ccl::LineDirection;
+    using Direction = ttnn::ccl::EdmLineFabricOpInterface::Direction;
     Program& program = builder_config.program.get();
     IDevice* device = builder_config.device;
     ProgramTensorsBundle const& all_tensors = builder_config.all_tensors.get();
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/reduce_scatter_minimal_async/device/kernels/reduce_scatter_minimal_async_writer.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/reduce_scatter_minimal_async/device/kernels/reduce_scatter_minimal_async_writer.cpp
index b5147317db..522b931b43 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/reduce_scatter_minimal_async/device/kernels/reduce_scatter_minimal_async_writer.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/reduce_scatter_minimal_async/device/kernels/reduce_scatter_minimal_async_writer.cpp
@@ -5,10 +5,9 @@
 #include "dataflow_api.h"
 #include <tt-metalium/buffer_types.hpp>
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "cpp/ttnn/operations/ccl/kernel_common/worker_sync_utils.hpp"
 #include "cpp/ttnn/operations/ccl/ccl_host_types.hpp"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
 #include <cstdint>
 #include <utility>
 
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/rms_allgather/device/kernels/dataflow/rms_writer.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/rms_allgather/device/kernels/dataflow/rms_writer.cpp
index 4bafe26f57..886f7bd928 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/rms_allgather/device/kernels/dataflow/rms_writer.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/rms_allgather/device/kernels/dataflow/rms_writer.cpp
@@ -7,8 +7,7 @@
 #include "hostdevcommon/common_values.hpp"
 #include <tt-metalium/buffer_types.hpp>
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/minimal_ccl_common.hpp"
 #include "cpp/ttnn/deprecated/tt_dnn/kernels/dataflow/generate_reduce_scaler.hpp"
 #include "cpp/ttnn/deprecated/tt_dnn/kernels/dataflow/generate_bcast_scalar.hpp"
diff --git a/ttnn/cpp/ttnn/operations/experimental/ccl/rms_allgather/device/multi_core/rms_allgather_pf.cpp b/ttnn/cpp/ttnn/operations/experimental/ccl/rms_allgather/device/multi_core/rms_allgather_pf.cpp
index 757133dbbf..fe1bb12f1b 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ccl/rms_allgather/device/multi_core/rms_allgather_pf.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/ccl/rms_allgather/device/multi_core/rms_allgather_pf.cpp
@@ -115,8 +115,10 @@ operation::ProgramWithCallbacks frmsnorm_multi_core_sharded(
     size_t num_targets_backward = 0;
     if (topology == ccl::Topology::Linear) {
         ccl::LineTopology line_topology(ring_size, ring_index);
-        num_targets_forward = line_topology.get_distance_to_end_of_line(ttnn::ccl::LineDirection::FORWARD);
-        num_targets_backward = line_topology.get_distance_to_end_of_line(ttnn::ccl::LineDirection::BACKWARD);
+        num_targets_forward =
+            line_topology.get_distance_to_end_of_line(ttnn::ccl::EdmLineFabricOpInterface::Direction::FORWARD);
+        num_targets_backward =
+            line_topology.get_distance_to_end_of_line(ttnn::ccl::EdmLineFabricOpInterface::Direction::BACKWARD);
     } else if (topology == ccl::Topology::Ring) {
         num_targets_forward = tt::div_up(ring_size - 1, 2);
         num_targets_backward = ring_size - 1 - num_targets_forward;
diff --git a/ttnn/cpp/ttnn/operations/experimental/cnn/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/cnn/CMakeLists.txt
index 50b861bbeb..a3c1184b28 100644
--- a/ttnn/cpp/ttnn/operations/experimental/cnn/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/cnn/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_experimental_cnn OBJECT)
-add_library(TTNN::Ops::Experimental::CNN ALIAS ttnn_op_experimental_cnn)
+add_library(TT::NN::Ops::Experimental::CNN ALIAS ttnn_op_experimental_cnn)
 
 target_precompile_headers(ttnn_op_experimental_cnn REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_experimental_cnn)
@@ -20,7 +20,7 @@ target_link_libraries(
     ttnn_op_experimental_cnn
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_cnn LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/conv3d/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/conv3d/CMakeLists.txt
index 633f5ff3e9..374e20d333 100644
--- a/ttnn/cpp/ttnn/operations/experimental/conv3d/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/conv3d/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_experimental_conv3d ${LIB_TYPE})
-add_library(TTNN::Ops::Experimental::Conv3d ALIAS ttnn_op_experimental_conv3d)
+add_library(TT::NN::Ops::Experimental::Conv3d ALIAS ttnn_op_experimental_conv3d)
 
 target_precompile_headers(ttnn_op_experimental_conv3d REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_experimental_conv3d)
@@ -17,7 +17,7 @@ target_link_libraries(
     ttnn_op_experimental_conv3d
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_conv3d LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/copy/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/copy/CMakeLists.txt
index 2af3dcca06..98ca61e436 100644
--- a/ttnn/cpp/ttnn/operations/experimental/copy/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/copy/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_experimental_copy ${LIB_TYPE})
-add_library(TTNN::Ops::Experimental::Copy ALIAS ttnn_op_experimental_copy)
+add_library(TT::NN::Ops::Experimental::Copy ALIAS ttnn_op_experimental_copy)
 
 target_precompile_headers(ttnn_op_experimental_copy REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_experimental_copy)
@@ -11,7 +11,7 @@ target_link_libraries(
     ttnn_op_experimental_copy
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_copy LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/dropout/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/dropout/CMakeLists.txt
index 3b13e66757..72c00ee718 100644
--- a/ttnn/cpp/ttnn/operations/experimental/dropout/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/dropout/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_experimental_dropout ${LIB_TYPE})
-add_library(TTNN::Ops::Experimental::Dropout ALIAS ttnn_op_experimental_dropout)
+add_library(TT::NN::Ops::Experimental::Dropout ALIAS ttnn_op_experimental_dropout)
 
 target_precompile_headers(ttnn_op_experimental_dropout REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_experimental_dropout)
@@ -17,7 +17,7 @@ target_link_libraries(
     ttnn_op_experimental_dropout
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_dropout LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/matmul/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/matmul/CMakeLists.txt
index 39604aea28..9133e7d801 100644
--- a/ttnn/cpp/ttnn/operations/experimental/matmul/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/matmul/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_experimental_matmul ${LIB_TYPE})
-add_library(TTNN::Ops::Experimental::Matmul ALIAS ttnn_op_experimental_matmul)
+add_library(TT::NN::Ops::Experimental::Matmul ALIAS ttnn_op_experimental_matmul)
 
 target_precompile_headers(ttnn_op_experimental_matmul REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_experimental_matmul)
@@ -20,7 +20,7 @@ target_link_libraries(
     ttnn_op_experimental_matmul
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_matmul LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/paged_cache/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/paged_cache/CMakeLists.txt
index 1bbc23a145..af209d9674 100644
--- a/ttnn/cpp/ttnn/operations/experimental/paged_cache/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/paged_cache/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_experimental_paged_cache ${LIB_TYPE})
-add_library(TTNN::Ops::Experimental::PagedCache ALIAS ttnn_op_experimental_paged_cache)
+add_library(TT::NN::Ops::Experimental::PagedCache ALIAS ttnn_op_experimental_paged_cache)
 
 target_precompile_headers(ttnn_op_experimental_paged_cache REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_experimental_paged_cache)
@@ -19,7 +19,7 @@ target_link_libraries(
     ttnn_op_experimental_paged_cache
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_paged_cache LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/plusone/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/plusone/CMakeLists.txt
index 154b431ae6..4327bc53e1 100644
--- a/ttnn/cpp/ttnn/operations/experimental/plusone/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/plusone/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_experimental_plusone ${LIB_TYPE})
-add_library(TTNN::Ops::Experimental::PlusOne ALIAS ttnn_op_experimental_plusone)
+add_library(TT::NN::Ops::Experimental::PlusOne ALIAS ttnn_op_experimental_plusone)
 
 target_precompile_headers(ttnn_op_experimental_plusone REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_experimental_plusone)
@@ -17,7 +17,7 @@ target_link_libraries(
     ttnn_op_experimental_plusone
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_plusone LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/reduction/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/reduction/CMakeLists.txt
index a8e9f116bd..79077d05f7 100644
--- a/ttnn/cpp/ttnn/operations/experimental/reduction/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/reduction/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_experimental_reduction ${LIB_TYPE})
-add_library(TTNN::Ops::Experimental::Reduction ALIAS ttnn_op_experimental_reduction)
+add_library(TT::NN::Ops::Experimental::Reduction ALIAS ttnn_op_experimental_reduction)
 
 target_precompile_headers(ttnn_op_experimental_reduction REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_experimental_reduction)
@@ -27,7 +27,7 @@ target_link_libraries(
     ttnn_op_experimental_reduction
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_reduction LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/reshape/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/reshape/CMakeLists.txt
index 0ee5168193..227513b0cb 100644
--- a/ttnn/cpp/ttnn/operations/experimental/reshape/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/reshape/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_experimental_reshape ${LIB_TYPE})
-add_library(TTNN::Ops::Experimental::Reshape ALIAS ttnn_op_experimental_reshape)
+add_library(TT::NN::Ops::Experimental::Reshape ALIAS ttnn_op_experimental_reshape)
 
 target_precompile_headers(ttnn_op_experimental_reshape REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_experimental_reshape)
@@ -11,7 +11,7 @@ target_link_libraries(
     ttnn_op_experimental_reshape
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_reshape LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/scatter/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/scatter/CMakeLists.txt
index 7ebc1c5743..d410e031a5 100644
--- a/ttnn/cpp/ttnn/operations/experimental/scatter/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/scatter/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_experimental_scatter ${LIB_TYPE})
-add_library(TTNN::Ops::Experimental::Scatter ALIAS ttnn_op_experimental_scatter)
+add_library(TT::NN::Ops::Experimental::Scatter ALIAS ttnn_op_experimental_scatter)
 
 target_precompile_headers(ttnn_op_experimental_scatter REUSE_FROM TT::CommonPCH)
 
@@ -16,7 +16,7 @@ target_link_libraries(
     ttnn_op_experimental_scatter
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_scatter LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/slice_write/device/kernels/dataflow/slice_write_writer_interleaved.cpp b/ttnn/cpp/ttnn/operations/experimental/slice_write/device/kernels/dataflow/slice_write_writer_interleaved.cpp
index 86572f17cd..0454e08dc7 100644
--- a/ttnn/cpp/ttnn/operations/experimental/slice_write/device/kernels/dataflow/slice_write_writer_interleaved.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/slice_write/device/kernels/dataflow/slice_write_writer_interleaved.cpp
@@ -3,7 +3,6 @@
 // SPDX-License-Identifier: Apache-2.0
 
 #include <stdint.h>
-#include <algorithm>
 #include "dataflow_api.h"
 
 void kernel_main() {
@@ -35,7 +34,7 @@ void kernel_main() {
     constexpr bool dst0_is_dram = get_compile_time_arg_val(1) == 1;
 
     const InterleavedAddrGen<dst0_is_dram> s0 = {.bank_base_address = dst_addr, .page_size = output_stick_size};
-    const uint32_t noc_write_size = std::min(output_stick_size, input_stick_size);
+    const uint32_t noc_write_size = min(output_stick_size, input_stick_size);
     uint32_t dst_stick_id = start_id;
     uint32_t sticks_read = 0;
     for (uint32_t iter = 0; iter < num_sticks_per_core_read and sticks_read < num_sticks_per_core; ++iter) {
diff --git a/ttnn/cpp/ttnn/operations/experimental/ssm/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/ssm/CMakeLists.txt
index 7c026333b4..2babad24d8 100644
--- a/ttnn/cpp/ttnn/operations/experimental/ssm/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/ssm/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_experimental_ssm ${LIB_TYPE})
-add_library(TTNN::Ops::Experimental::SSM ALIAS ttnn_op_experimental_ssm)
+add_library(TT::NN::Ops::Experimental::SSM ALIAS ttnn_op_experimental_ssm)
 
 target_precompile_headers(ttnn_op_experimental_ssm REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_experimental_ssm)
@@ -23,7 +23,7 @@ target_link_libraries(
     ttnn_op_experimental_ssm
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_ssm LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/transformer/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/transformer/CMakeLists.txt
index 3ce7d8a309..635e613600 100644
--- a/ttnn/cpp/ttnn/operations/experimental/transformer/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/transformer/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_experimental_transformer ${LIB_TYPE})
-add_library(TTNN::Ops::Experimental::Transformer ALIAS ttnn_op_experimental_transformer)
+add_library(TT::NN::Ops::Experimental::Transformer ALIAS ttnn_op_experimental_transformer)
 
 target_precompile_headers(ttnn_op_experimental_transformer REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_experimental_transformer)
@@ -61,7 +61,7 @@ target_link_libraries(
     ttnn_op_experimental_transformer
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_transformer LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/experimental/transformer/all_reduce_create_qkv_heads/device/all_reduce_create_qkv_heads_program_factory.cpp b/ttnn/cpp/ttnn/operations/experimental/transformer/all_reduce_create_qkv_heads/device/all_reduce_create_qkv_heads_program_factory.cpp
index ad5cacd5df..fc39998a3f 100644
--- a/ttnn/cpp/ttnn/operations/experimental/transformer/all_reduce_create_qkv_heads/device/all_reduce_create_qkv_heads_program_factory.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/transformer/all_reduce_create_qkv_heads/device/all_reduce_create_qkv_heads_program_factory.cpp
@@ -155,8 +155,10 @@ tt::tt_metal::operation::ProgramWithCallbacks all_reduce_create_qkv_heads_minima
     size_t num_targets_backward = 0;
     if (topology == ccl::Topology::Linear) {
         LineTopology line_topology(ring_size, ring_index);
-        num_targets_forward = line_topology.get_distance_to_end_of_line(ttnn::ccl::LineDirection::FORWARD);
-        num_targets_backward = line_topology.get_distance_to_end_of_line(ttnn::ccl::LineDirection::BACKWARD);
+        num_targets_forward =
+            line_topology.get_distance_to_end_of_line(ttnn::ccl::EdmLineFabricOpInterface::Direction::FORWARD);
+        num_targets_backward =
+            line_topology.get_distance_to_end_of_line(ttnn::ccl::EdmLineFabricOpInterface::Direction::BACKWARD);
     } else if (topology == ccl::Topology::Ring) {
         // TODO: Commonize
         num_targets_forward = tt::div_up(ring_size - 1, 2);
diff --git a/ttnn/cpp/ttnn/operations/experimental/transformer/all_reduce_create_qkv_heads/device/kernels/dataflow/worker_writer.cpp b/ttnn/cpp/ttnn/operations/experimental/transformer/all_reduce_create_qkv_heads/device/kernels/dataflow/worker_writer.cpp
index b75ac11a96..c37997a6c0 100644
--- a/ttnn/cpp/ttnn/operations/experimental/transformer/all_reduce_create_qkv_heads/device/kernels/dataflow/worker_writer.cpp
+++ b/ttnn/cpp/ttnn/operations/experimental/transformer/all_reduce_create_qkv_heads/device/kernels/dataflow/worker_writer.cpp
@@ -5,9 +5,8 @@
 #include "dataflow_api.h"
 #include <tt-metalium/buffer_types.hpp>
 #include "tt_metal/fabric/hw/inc/edm_fabric/fabric_connection_manager.hpp"
-#include "tt_metal/fabric/hw/inc/noc_addr.h"
+#include "cpp/ttnn/operations/ccl/common/interpreter_backends/kernel_common/noc_addr.hpp"
 #include "cpp/ttnn/operations/experimental/ccl/all_gather_async/device/kernels/minimal_ccl_common.hpp"
-#include "cpp/ttnn/operations/ccl/shared_with_host/hetergeneous_data_structs.hpp"
 #include <cstdint>
 #include <utility>
 
diff --git a/ttnn/cpp/ttnn/operations/experimental/unary_backward/CMakeLists.txt b/ttnn/cpp/ttnn/operations/experimental/unary_backward/CMakeLists.txt
index 07163edb68..b332bef4ce 100644
--- a/ttnn/cpp/ttnn/operations/experimental/unary_backward/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/experimental/unary_backward/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_experimental_unary_backward ${LIB_TYPE})
-add_library(TTNN::Ops::Experimental::UnaryBackward ALIAS ttnn_op_experimental_unary_backward)
+add_library(TT::NN::Ops::Experimental::UnaryBackward ALIAS ttnn_op_experimental_unary_backward)
 
 target_precompile_headers(ttnn_op_experimental_unary_backward REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_experimental_unary_backward)
@@ -17,7 +17,7 @@ target_link_libraries(
     ttnn_op_experimental_unary_backward
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_experimental_unary_backward LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/full/CMakeLists.txt b/ttnn/cpp/ttnn/operations/full/CMakeLists.txt
index 3cbf6ee61c..4577408efa 100644
--- a/ttnn/cpp/ttnn/operations/full/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/full/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_full ${LIB_TYPE})
-add_library(TTNN::Ops::Full ALIAS ttnn_op_full)
+add_library(TT::NN::Ops::Full ALIAS ttnn_op_full)
 
 target_precompile_headers(ttnn_op_full REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_full)
@@ -17,7 +17,7 @@ target_link_libraries(
     ttnn_op_full
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_full LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/full_like/CMakeLists.txt b/ttnn/cpp/ttnn/operations/full_like/CMakeLists.txt
index 4dd34834b7..b27f1f1ff6 100644
--- a/ttnn/cpp/ttnn/operations/full_like/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/full_like/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_full_like ${LIB_TYPE})
-add_library(TTNN::Ops::FullLike ALIAS ttnn_op_full_like)
+add_library(TT::NN::Ops::FullLike ALIAS ttnn_op_full_like)
 
 target_precompile_headers(ttnn_op_full_like REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_full_like)
@@ -17,7 +17,7 @@ target_link_libraries(
     ttnn_op_full_like
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_full_like LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/index_fill/CMakeLists.txt b/ttnn/cpp/ttnn/operations/index_fill/CMakeLists.txt
index e04c23c69c..4299be0d17 100644
--- a/ttnn/cpp/ttnn/operations/index_fill/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/index_fill/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_index_fill ${LIB_TYPE})
-add_library(TTNN::Ops::IndexFill ALIAS ttnn_op_index_fill)
+add_library(TT::NN::Ops::IndexFill ALIAS ttnn_op_index_fill)
 
 target_precompile_headers(ttnn_op_index_fill REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_index_fill)
@@ -17,7 +17,7 @@ target_link_libraries(
     ttnn_op_index_fill
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_index_fill LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/kv_cache/CMakeLists.txt b/ttnn/cpp/ttnn/operations/kv_cache/CMakeLists.txt
index f014416e7c..d62a4c1b2f 100644
--- a/ttnn/cpp/ttnn/operations/kv_cache/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/kv_cache/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_kv_cache ${LIB_TYPE})
-add_library(TTNN::Ops::KvCache ALIAS ttnn_op_kv_cache)
+add_library(TT::NN::Ops::KvCache ALIAS ttnn_op_kv_cache)
 
 target_precompile_headers(ttnn_op_kv_cache REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_kv_cache)
@@ -17,7 +17,7 @@ target_link_libraries(
     ttnn_op_kv_cache
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_kv_cache LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/loss/CMakeLists.txt b/ttnn/cpp/ttnn/operations/loss/CMakeLists.txt
index ee0aa99436..03b161cff3 100644
--- a/ttnn/cpp/ttnn/operations/loss/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/loss/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_loss ${LIB_TYPE})
-add_library(TTNN::Ops::Loss ALIAS ttnn_op_loss)
+add_library(TT::NN::Ops::Loss ALIAS ttnn_op_loss)
 
 target_precompile_headers(ttnn_op_loss REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_loss)
@@ -11,7 +11,7 @@ target_link_libraries(
     ttnn_op_loss
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_loss LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/matmul/CMakeLists.txt b/ttnn/cpp/ttnn/operations/matmul/CMakeLists.txt
index cd972a5383..09ce9f4916 100644
--- a/ttnn/cpp/ttnn/operations/matmul/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/matmul/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_matmul ${LIB_TYPE})
-add_library(TTNN::Ops::Matmul ALIAS ttnn_op_matmul)
+add_library(TT::NN::Ops::Matmul ALIAS ttnn_op_matmul)
 
 target_precompile_headers(ttnn_op_matmul REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_matmul)
@@ -22,7 +22,7 @@ target_link_libraries(
     ttnn_op_matmul
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_matmul LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/matmul/device/kernels/compute/bmm_large_block_zm_fused_bias_activation_gathered.cpp b/ttnn/cpp/ttnn/operations/matmul/device/kernels/compute/bmm_large_block_zm_fused_bias_activation_gathered.cpp
index 065e12f813..b99d9754f1 100644
--- a/ttnn/cpp/ttnn/operations/matmul/device/kernels/compute/bmm_large_block_zm_fused_bias_activation_gathered.cpp
+++ b/ttnn/cpp/ttnn/operations/matmul/device/kernels/compute/bmm_large_block_zm_fused_bias_activation_gathered.cpp
@@ -155,12 +155,7 @@ void MAIN {
     constexpr bool untilize_out = get_compile_time_arg_val(15);                // untilize output
     constexpr bool in1_is_dram_interleaved = get_compile_time_arg_val(16);     // in1 is in dram
     constexpr bool in1_is_dram_sharded = get_compile_time_arg_val(17);
-    constexpr uint32_t in0_cb_id = get_compile_time_arg_val(18);
-    constexpr uint32_t in1_cb_id = get_compile_time_arg_val(19);
-    constexpr uint32_t in2_cb_id = get_compile_time_arg_val(20);
-    constexpr uint32_t sync_cb = get_compile_time_arg_val(21);
-    constexpr uint32_t sync_cb2 = get_compile_time_arg_val(22);
-    constexpr uint32_t OUTPUT_CB_ARRAY_IDX = get_compile_time_arg_val(23);
+    constexpr uint32_t OUTPUT_CB_ARRAY_IDX = 18;
     constexpr std::array<uint32_t, batch> mm_out_cb_ids =
         fill_array_with_next_n_args<uint32_t, OUTPUT_CB_ARRAY_IDX, batch>();
     constexpr uint32_t INTERM_CB_ARRAY_IDX = OUTPUT_CB_ARRAY_IDX + batch;
@@ -182,6 +177,12 @@ void MAIN {
 
     constexpr uint32_t out_block_w = out_subblock_w * in1_num_subblocks;
 
+    constexpr uint32_t in0_cb_id = tt::CBIndex::c_0;
+    constexpr uint32_t in1_cb_id = tt::CBIndex::c_1;
+    constexpr uint32_t in2_cb_id = tt::CBIndex::c_2;
+    constexpr uint32_t sync_cb = tt::CBIndex::c_3;
+    constexpr uint32_t sync_cb2 = tt::CBIndex::c_4;
+
 #ifdef SFPU_OP_INIT_ACTIVATION
     SFPU_OP_INIT_ACTIVATION
 #endif
diff --git a/ttnn/cpp/ttnn/operations/matmul/device/kernels/dataflow/reader_bmm_tile_layout_in0_ring_all_gather.cpp b/ttnn/cpp/ttnn/operations/matmul/device/kernels/dataflow/reader_bmm_tile_layout_in0_ring_all_gather.cpp
index c3203b8c13..6c5b5bc1a0 100644
--- a/ttnn/cpp/ttnn/operations/matmul/device/kernels/dataflow/reader_bmm_tile_layout_in0_ring_all_gather.cpp
+++ b/ttnn/cpp/ttnn/operations/matmul/device/kernels/dataflow/reader_bmm_tile_layout_in0_ring_all_gather.cpp
@@ -40,8 +40,8 @@ void kernel_main() {
         reinterpret_cast<volatile tt_l1_ptr uint32_t*>(signal_semaphore_addr);
     uint64_t remote_signal_semaphore_addr = get_noc_addr(next_core_noc_x, next_core_noc_y, signal_semaphore_addr, noc);
 
-    constexpr uint32_t cb_id_in0 = get_compile_time_arg_val(5);
-    constexpr uint32_t cb_id_in2 = get_compile_time_arg_val(6);
+    constexpr uint32_t cb_id_in0 = tt::CBIndex::c_0;
+    constexpr uint32_t cb_id_in2 = tt::CBIndex::c_2;
 
     constexpr uint32_t in0_single_tile_size_bytes = get_tile_size(cb_id_in0);
     constexpr uint32_t shard_size_in_tiles = shard_width_in_tiles * shard_height_in_tiles;
diff --git a/ttnn/cpp/ttnn/operations/matmul/device/kernels/dataflow/reader_bmm_tile_layout_in1_ring_all_gather.cpp b/ttnn/cpp/ttnn/operations/matmul/device/kernels/dataflow/reader_bmm_tile_layout_in1_ring_all_gather.cpp
index 8efa9c53c6..8d232e1f8a 100644
--- a/ttnn/cpp/ttnn/operations/matmul/device/kernels/dataflow/reader_bmm_tile_layout_in1_ring_all_gather.cpp
+++ b/ttnn/cpp/ttnn/operations/matmul/device/kernels/dataflow/reader_bmm_tile_layout_in1_ring_all_gather.cpp
@@ -61,10 +61,10 @@ void kernel_main() {
     const uint32_t vc = get_arg_val<uint32_t>(rt_args_idx++);
     const uint32_t dram_read_offset = get_arg_val<uint32_t>(rt_args_idx++);
 
-    constexpr uint32_t cb_id_in1 = get_compile_time_arg_val(11);
-    constexpr uint32_t sync_cb = get_compile_time_arg_val(12);
-    constexpr uint32_t sync_cb2 = get_compile_time_arg_val(13);
-    constexpr uint32_t remote_cb_id = get_compile_time_arg_val(14);
+    constexpr uint32_t cb_id_in1 = tt::CBIndex::c_1;
+    constexpr uint32_t sync_cb = tt::CBIndex::c_3;
+    constexpr uint32_t sync_cb2 = tt::CBIndex::c_4;
+    constexpr uint32_t remote_cb_id = tt::CBIndex::c_31;
 
     const uint32_t in1_block_num_tiles = in1_block_height_in_tiles * in1_block_width_in_tiles;
 
diff --git a/ttnn/cpp/ttnn/operations/matmul/device/matmul_op.hpp b/ttnn/cpp/ttnn/operations/matmul/device/matmul_op.hpp
index c4d7534541..3645a5fee2 100644
--- a/ttnn/cpp/ttnn/operations/matmul/device/matmul_op.hpp
+++ b/ttnn/cpp/ttnn/operations/matmul/device/matmul_op.hpp
@@ -12,26 +12,13 @@
 #include "ttnn/tensor/tensor.hpp"
 #include "ttnn/tensor/tensor_utils.hpp"
 #include "ttnn/types.hpp"
+
 namespace ttnn {
 
 namespace operations {
 
 namespace matmul {
 
-// shared variables between override and program
-
-enum class Matmul1DType { MCAST_IN0, GATHER_IN0, MCAST_IN1 };
-
-struct matmul_mcast_1d_common_override_variables_t {
-    std::vector<tt::tt_metal::KernelHandle> kernels;
-    std::vector<tt::tt_metal::CBHandle> cbs;
-    bool extract_shard_sub_blocks;
-    CoreCoord start_core;
-    std::vector<CoreCoord> cores;
-    uint32_t num_cores_with_work;
-    Matmul1DType type;
-};
-
 // Define the buffering depth for input CBs (0 and 1) for mcast variants.
 // 2 = double buffer, 3 = triple buffer, etc.
 // Allows easily changing buffering strategy in one place for relevant factories.
@@ -241,22 +228,6 @@ Matmul create_matmul_struct(
     const struct Matmul& parameters,
     const std::vector<std::optional<Tensor>>& optional_output_tensors = {std::nullopt});
 
-matmul_mcast_1d_common_override_variables_t matmul_multi_core_reuse_mcast_1d_optimized_helper(
-    tt::tt_metal::Program& program,
-    const Tensor& input_tensor_a,
-    const std::vector<Tensor>& input_tensors_b,
-    const std::optional<const Tensor>& bias,
-    const std::vector<Tensor>& output_tensors,
-    bool bcast_batch,
-    DeviceComputeKernelConfig compute_kernel_config,
-    const MatmulProgramConfig& program_config,
-    bool untilize_out,
-    std::optional<ttnn::experimental::ccl::MatmulFusedOpSignaler>& fused_op_signaler,
-    const std::optional<const tt::tt_metal::experimental::GlobalCircularBuffer>& global_cb,
-    const std::optional<tt::tt_metal::SubDeviceId>& sub_device_id,
-    const uint32_t start_cb_index,
-    std::optional<CoreRangeSet> restricted_cores);
-
 tt::tt_metal::operation::ProgramWithCallbacks matmul_multi_core_reuse_mcast_1d_optimized_helper(
     tt::tt_metal::Program& program,
     const Tensor& input_tensor_a,
@@ -314,13 +285,3 @@ std::tuple<uint32_t, uint32_t> get_matmul_subblock_params(
     const bool fp32_dest_acc_en);
 
 }  // namespace bmm_op_utils
-
-namespace reuse_mcast_1d_optimized_helpers {
-void override_program_parameters(
-    const ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t& shared_variables,
-    const void* operation,
-    tt::tt_metal::Program& program,
-    const std::vector<tt::tt_metal::Tensor>& input_tensors,
-    const std::vector<std::optional<const tt::tt_metal::Tensor>>& optional_input_tensors,
-    const std::vector<tt::tt_metal::Tensor>& output_tensors);
-}  // namespace reuse_mcast_1d_optimized_helpers
diff --git a/ttnn/cpp/ttnn/operations/matmul/device/matmul_op_multi_core_reuse_mcast_1d_program_factory.cpp b/ttnn/cpp/ttnn/operations/matmul/device/matmul_op_multi_core_reuse_mcast_1d_program_factory.cpp
index 2d5e678817..419ad99fb0 100644
--- a/ttnn/cpp/ttnn/operations/matmul/device/matmul_op_multi_core_reuse_mcast_1d_program_factory.cpp
+++ b/ttnn/cpp/ttnn/operations/matmul/device/matmul_op_multi_core_reuse_mcast_1d_program_factory.cpp
@@ -59,8 +59,7 @@ uint32_t get_preferred_noc(
     return use_dedicated_noc ? 1 : noc;
 }
 
-ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t
-process_mcast_in0_program_and_create_override_variables(
+tt::tt_metal::operation::ProgramWithCallbacks create_program_mcast_in0(
     tt_metal::Program& program,
     const tt::tt_metal::Tensor& a,
     tt_metal::IDevice* device,
@@ -894,19 +893,87 @@ process_mcast_in0_program_and_create_override_variables(
                 program, mm_kernel_in1_sender_writer_id, core, mm_in1_sender_writer_args);  // RISCV_0_default
         }
     }
-    return ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t{
-        {mm_kernel_in0_mcast_cores_with_work_and_in_receiver_grid_id, mm_kernel_in1_sender_writer_id},
-        {cb_src1, cb_src2, cb_src3, cb_output},
-        false,
-        start_core,
-        cores,
-        num_cores_with_work,
-        ttnn::operations::matmul::Matmul1DType::MCAST_IN0};
+
+    auto override_runtime_arguments_callback =
+        [mm_kernel_in0_mcast_cores_with_work_and_in_receiver_grid_id,
+         mm_kernel_in1_sender_writer_id,
+         cb_src1,
+         cb_src2,
+         cb_src3,
+         cb_output,
+         start_core,
+         cores,
+         num_cores_with_work](
+            const void* operation,
+            tt::tt_metal::Program& program,
+            const std::vector<tt::tt_metal::Tensor>& input_tensors,
+            const std::vector<std::optional<const tt::tt_metal::Tensor>>& optional_input_tensors,
+            const std::vector<tt::tt_metal::Tensor>& output_tensors) {
+            TT_FATAL(
+                input_tensors.size() + optional_input_tensors.size() == 3,
+                "Total number of input tensors (required ({}) + optional ({})) must be 3",
+                input_tensors.size(),
+                optional_input_tensors.size());
+            TT_FATAL(output_tensors.size() == 1, "Number of output tensors ({}) must be 1", output_tensors.size());
+
+            auto src_buffer_a = input_tensors.at(0).buffer();
+            auto src_buffer_b = input_tensors.at(1).buffer();
+            auto bias_tensor = optional_input_tensors.at(0);
+
+            std::optional<tt::tt_metal::Buffer*> bias_buffer;
+            if (bias_tensor.has_value()) {
+                bias_buffer = bias_tensor.value().buffer();
+            }
+
+            auto dst_buffer = output_tensors.at(0).buffer();
+
+            bool src0_sharded = input_tensors[0].is_sharded();
+            bool src1_sharded = input_tensors[1].is_sharded();
+            bool out_sharded = output_tensors[0].is_sharded();
+
+            // Manually unroll sender core
+            if (src0_sharded) {
+                UpdateDynamicCircularBufferAddress(program, cb_src2, *src_buffer_a);
+            } else {
+                // in0 sender
+                auto& reader_sender_runtime_args =
+                    GetRuntimeArgs(program, mm_kernel_in0_mcast_cores_with_work_and_in_receiver_grid_id, start_core);
+                reader_sender_runtime_args[0] = src_buffer_a->address();
+            }
+
+            if (src1_sharded) {
+                UpdateDynamicCircularBufferAddress(program, cb_src1, *src_buffer_b);
+            }
+
+            if (bias_tensor.has_value() && bias_tensor.value().is_sharded()) {
+                UpdateDynamicCircularBufferAddress(program, cb_src3, *bias_buffer.value());
+            }
+
+            auto& writer_runtime_args_by_core = GetRuntimeArgs(program, mm_kernel_in1_sender_writer_id);
+
+            for (uint32_t i = 0; i < num_cores_with_work; ++i) {
+                const auto& core = cores[i];
+
+                auto& writer_runtime_args = writer_runtime_args_by_core[core.x][core.y];
+
+                // in1 sender
+                writer_runtime_args[0] = src_buffer_b->address();
+                writer_runtime_args[6] = dst_buffer->address();
+                if (bias_tensor.has_value()) {
+                    writer_runtime_args[17] = (*bias_buffer)->address();
+                }
+            }
+
+            if (out_sharded) {
+                UpdateDynamicCircularBufferAddress(program, cb_output, *dst_buffer);
+            }
+        };
+
+    return {.program = std::move(program), .override_runtime_arguments_callback = override_runtime_arguments_callback};
 }
 
-ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t
-process_mcast_in1_program_and_create_override_variables(
-    tt_metal::Program& program,
+tt::tt_metal::operation::ProgramWithCallbacks create_program_mcast_in1(
+    tt::tt_metal::Program& program,
     const tt::tt_metal::Tensor& a,
     tt_metal::IDevice* device,
     MathFidelity math_fidelity,
@@ -1583,20 +1650,93 @@ process_mcast_in1_program_and_create_override_variables(
         };
         tt_metal::SetRuntimeArgs(program, mm_kernel_in0_sender_id, core, mm_in0_sender_args);  // RISCV_1_default
     }
-    return ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t{
-        {mm_kernel_in0_sender_id, mm_kernel_in1_sender_writer_id, mm_kernel_in1_receiver_writer_id},
-        {cb_src0, cb_src2, cb_output},
-        extract_shard_sub_blocks,
-        start_core,
-        cores,
-        0,
-        ttnn::operations::matmul::Matmul1DType::MCAST_IN1};
+
+    auto override_runtime_arguments_callback =
+        [mm_kernel_in0_sender_id,
+         mm_kernel_in1_sender_writer_id,
+         mm_kernel_in1_receiver_writer_id,
+         extract_shard_sub_blocks,
+         cb_src0,
+         cb_src2,
+         cb_output,
+         start_core,
+         cores](
+            const void* operation,
+            tt::tt_metal::Program& program,
+            const std::vector<tt::tt_metal::Tensor>& input_tensors,
+            const std::vector<std::optional<const tt::tt_metal::Tensor>>& optional_input_tensors,
+            const std::vector<tt::tt_metal::Tensor>& output_tensors) {
+            TT_FATAL(
+                input_tensors.size() + optional_input_tensors.size() == 3,
+                "Total number of input tensors (required ({}) + optional ({})) must be 3",
+                input_tensors.size(),
+                optional_input_tensors.size());
+            TT_FATAL(output_tensors.size() == 1, "Number of output tensors ({}) must be 1", output_tensors.size());
+
+            auto src_buffer_a = input_tensors.at(0).buffer();
+            auto src_buffer_b = input_tensors.at(1).buffer();
+            auto bias_tensor = optional_input_tensors.at(0);
+
+            std::optional<tt::tt_metal::Buffer*> bias_buffer;
+            if (bias_tensor.has_value()) {
+                bias_buffer = bias_tensor.value().buffer();
+            }
+
+            auto dst_buffer = output_tensors.at(0).buffer();
+
+            bool src0_sharded = input_tensors[0].is_sharded();
+            bool out_sharded = output_tensors[0].is_sharded();
+
+            auto& reader_runtime_args_by_core = GetRuntimeArgs(program, mm_kernel_in0_sender_id);
+
+            // Manually unroll sender core
+            {
+                // in0 sender
+                auto& reader_runtime_args = reader_runtime_args_by_core[start_core.x][start_core.y];
+                reader_runtime_args[0] = src_buffer_a->address();
+
+                // in1 sender
+                auto& sender_writer_runtime_args = GetRuntimeArgs(program, mm_kernel_in1_sender_writer_id, start_core);
+                sender_writer_runtime_args[0] = src_buffer_b->address();
+                sender_writer_runtime_args[6] = dst_buffer->address();
+                if (bias_tensor.has_value()) {
+                    sender_writer_runtime_args[17] = (*bias_buffer)->address();
+                }
+            }
+
+            auto& receiver_writer_runtime_args_by_core = GetRuntimeArgs(program, mm_kernel_in1_receiver_writer_id);
+
+            for (uint32_t i = 1; i < cores.size(); ++i) {
+                const CoreCoord& core = cores[i];
+
+                auto& reader_runtime_args = reader_runtime_args_by_core[core.x][core.y];
+
+                auto& writer_runtime_args = receiver_writer_runtime_args_by_core[core.x][core.y];
+
+                // in0 sender
+                reader_runtime_args[0] = src_buffer_a->address();
+                // in1 receiver
+                writer_runtime_args[2] = dst_buffer->address();
+            }
+
+            if (src0_sharded) {
+                if (extract_shard_sub_blocks) {
+                    UpdateDynamicCircularBufferAddress(program, cb_src2, *src_buffer_a);
+                } else {
+                    UpdateDynamicCircularBufferAddress(program, cb_src0, *src_buffer_a);
+                }
+            }
+
+            if (out_sharded) {
+                UpdateDynamicCircularBufferAddress(program, cb_output, *dst_buffer);
+            }
+        };
+    return {.program = std::move(program), .override_runtime_arguments_callback = override_runtime_arguments_callback};
 }
 
 enum class CORE_TYPE : uint32_t { IDLE_CORE = 0, WORKER_CORE = 1, HOP_CORE = 2 };
 
-ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t
-process_gather_in0_program_and_create_override_variables(
+tt::tt_metal::operation::ProgramWithCallbacks create_program_gather_in0(
     tt_metal::Program& program,
     const tt::tt_metal::Tensor& a,
     const std::vector<tt::tt_metal::Tensor>& b_tensors,
@@ -1607,7 +1747,6 @@ process_gather_in0_program_and_create_override_variables(
     bool packer_l1_acc,
     bool dst_full_sync_en,
     CoreCoord compute_with_storage_grid_size,
-    uint32_t base_cb_index,
     uint32_t B,
     uint32_t M,
     uint32_t N,
@@ -1632,8 +1771,7 @@ process_gather_in0_program_and_create_override_variables(
     bool untilize_out,
     const std::optional<const tt::tt_metal::experimental::GlobalCircularBuffer>& global_cb,
     uint32_t num_global_cb_receivers,
-    const std::optional<tt::tt_metal::SubDeviceId>& sub_device_id,
-    std::optional<CoreRangeSet> restricted_cores) {
+    const std::optional<tt::tt_metal::SubDeviceId>& sub_device_id) {
     const auto b = b_tensors[0];
     const auto num_output_cb = out_buffers.size();
     const auto batch = b_tensors.size();
@@ -1645,21 +1783,17 @@ process_gather_in0_program_and_create_override_variables(
     constexpr bool row_major = true;
     CoreRangeSet all_worker_cores = a.shard_spec().value().grid;
     CoreRangeSet non_idle_cores = all_worker_cores.merge(hop_cores);
-    CoreRangeSet all_cores = non_idle_cores;
-    std::vector<CoreRange> non_idle_cores_vec;
     auto subdevice_cores = device->worker_cores(
         tt::tt_metal::HalProgrammableCoreType::TENSIX,
         sub_device_id.has_value() ? *sub_device_id : device->get_sub_device_ids().at(0));
-    if (restricted_cores.has_value()) {
-        subdevice_cores = subdevice_cores.subtract(restricted_cores.value());
-    }
+    std::vector<CoreRange> non_idle_cores_vec;
     for (auto& cr : subdevice_cores.ranges()) {
         auto intersection = non_idle_cores.intersection(cr);
         if (intersection.size() > 0) {
             non_idle_cores_vec.push_back(intersection.bounding_box());
         }
     }
-    all_cores = CoreRangeSet(non_idle_cores_vec);
+    CoreRangeSet all_cores = CoreRangeSet(non_idle_cores_vec);
     std::vector<CoreRange> ring_list = all_worker_cores.ranges();
     std::vector<CoreRange> hop_list = hop_cores.ranges();
     ring_list.insert(ring_list.end(), hop_list.begin(), hop_list.end());
@@ -1758,7 +1892,7 @@ process_gather_in0_program_and_create_override_variables(
     uint32_t out_subblock_num_tiles = out_subblock_h * out_subblock_w;
 
     /* Create circular buffers */
-    uint32_t src0_cb_index = base_cb_index;
+    uint32_t src0_cb_index = tt::CBIndex::c_0;
     tt_metal::CircularBufferConfig src0_cb_config =
         tt_metal::CircularBufferConfig(in0_CB_size, {{src0_cb_index, in0_data_format}})
             .set_page_size(src0_cb_index, in0_single_tile_size)
@@ -1766,11 +1900,11 @@ process_gather_in0_program_and_create_override_variables(
             .set_globally_allocated_address(*in0_buffer);
     auto cb_src0 = tt_metal::CreateCircularBuffer(program, all_cores, src0_cb_config);
 
-    uint32_t src1_cb_index = base_cb_index + 1;
+    uint32_t src1_cb_index = tt::CBIndex::c_1;
     tt::tt_metal::CBHandle cb_src1;
-    uint32_t remote_cb_index = tt::CBIndex::c_31;
     if (use_global_cb) {
         uint32_t in1_block_size_bytes = in1_single_tile_size * in1_block_num_tiles;
+        uint32_t remote_cb_index = tt::CBIndex::c_31;
         tt_metal::CircularBufferConfig remote_cb_config =
             tt_metal::CircularBufferConfig((global_cb->size() / in1_block_size_bytes) * in1_block_size_bytes);
         remote_cb_config.remote_index(remote_cb_index)
@@ -1789,29 +1923,29 @@ process_gather_in0_program_and_create_override_variables(
         cb_src1 = tt_metal::CreateCircularBuffer(program, all_cores, src1_cb_config);
     }
 
-    uint32_t src2_cb_index = base_cb_index + 2;
+    uint32_t src2_cb_index = tt::CBIndex::c_2;
     tt_metal::CircularBufferConfig src2_cb_config =
         tt_metal::CircularBufferConfig(in2_CB_size, {{src2_cb_index, in0_data_format}})
             .set_page_size(src2_cb_index, in2_single_tile_size)
             .set_tile_dims(src2_cb_index, in0_tile);
     auto cb_src2 = tt_metal::CreateCircularBuffer(program, all_cores, src2_cb_config);
 
-    uint32_t sync_cb_index = base_cb_index + 3;
+    uint32_t sync_cb_index = tt::CBIndex::c_3;
     uint32_t sync_cb_size_bytes = 16;
     tt_metal::CircularBufferConfig sync_cb_config =
         tt_metal::CircularBufferConfig(sync_cb_size_bytes, {{sync_cb_index, DataFormat::UInt16}})
             .set_page_size(sync_cb_index, sync_cb_size_bytes);
     auto cb_sync = tt_metal::CreateCircularBuffer(program, all_cores, sync_cb_config);
 
-    uint32_t sync_cb2_index = base_cb_index + 4;
+    uint32_t sync_cb2_index = tt::CBIndex::c_4;
     uint32_t sync_cb2_size_bytes = 16;
     tt_metal::CircularBufferConfig sync_cb2_config =
         tt_metal::CircularBufferConfig(sync_cb2_size_bytes, {{sync_cb2_index, DataFormat::UInt16}})
             .set_page_size(sync_cb2_index, sync_cb2_size_bytes);
     auto cb2_sync = tt_metal::CreateCircularBuffer(program, all_cores, sync_cb2_config);
 
-    uint32_t output_cb_index = base_cb_index + 5;  // output operands start at index 16
-    uint32_t interm0_cb_index = base_cb_index + 6;
+    uint32_t output_cb_index = tt::CBIndex::c_5;  // output operands start at index 16
+    uint32_t interm0_cb_index = tt::CBIndex::c_6;
     tt_metal::CircularBufferConfig interm0_cb_config =
         tt_metal::CircularBufferConfig(0, {{interm0_cb_index, interm0_data_format}});
     tt_metal::CircularBufferConfig output_cb_config =
@@ -1890,8 +2024,6 @@ process_gather_in0_program_and_create_override_variables(
         (std::uint32_t)batch,       // batch
         (std::uint32_t)ring_size,   // ring_size
         (std::uint32_t)in0_signal_semaphore_id,
-        (std::uint32_t)src0_cb_index,
-        (std::uint32_t)src2_cb_index,
     };
 
     std::vector<uint32_t> in1_sender_writer_compile_time_args = {
@@ -1906,10 +2038,6 @@ process_gather_in0_program_and_create_override_variables(
         (std::uint32_t)in1_block_page_size_last,
         (std::uint32_t)in1_block_width_num_pages,
         (std::uint32_t)in1_shard_width_in_dram,
-        (std::uint32_t)src1_cb_index,
-        (std::uint32_t)sync_cb_index,
-        (std::uint32_t)sync_cb2_index,
-        (std::uint32_t)remote_cb_index,
     };
 
     /* compute kernel args */
@@ -1940,13 +2068,7 @@ process_gather_in0_program_and_create_override_variables(
         untilize_out,             // untilize_out
         in1_is_dram_interleaved,  // in1_is_dram_interleaved
         in1_is_dram_sharded,      // in1_is_dram_sharded
-        src0_cb_index,
-        src1_cb_index,
-        src2_cb_index,
-        sync_cb_index,
-        sync_cb2_index,
     };
-    compute_kernel_args.push_back(compute_kernel_args.size() + 1);  // The CT index of the output_cbs
     for (uint32_t i = 0; i < num_output_cb; ++i) {
         compute_kernel_args.push_back(output_cb_indices[i]);
     }
@@ -2192,234 +2314,63 @@ process_gather_in0_program_and_create_override_variables(
         mm_kernel_args.push_back((std::uint32_t)core_type);
         tt_metal::SetRuntimeArgs(program, mm_kernel, core, mm_kernel_args);
     }
-    std::vector<tt::tt_metal::CBHandle> shared_cbs = {cb_src0, cb_src1};
-    shared_cbs.insert(shared_cbs.end(), cb_outputs.begin(), cb_outputs.end());
-
-    return ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t{
-        {mm_kernel_in1_sender_writer_id},
-        shared_cbs,
-        false,
-        CoreCoord{0, 0},
-        all_cores_vec,
-        0,
-        ttnn::operations::matmul::Matmul1DType::GATHER_IN0};
-}
-
-inline void override_mcast_in1_program_parameters(
-    const ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t& override_variables,
-    const void* operation,
-    tt_metal::Program& program,
-    const std::vector<tt::tt_metal::Tensor>& input_tensors,
-    const std::vector<std::optional<const tt::tt_metal::Tensor>>& optional_input_tensors,
-    const std::vector<tt::tt_metal::Tensor>& output_tensors) {
-    TT_FATAL(
-        input_tensors.size() + optional_input_tensors.size() == 3,
-        "mcast in1 requires 3 input tensors, {} + {} = {} provided",
-        input_tensors.size(),
-        optional_input_tensors.size(),
-        optional_input_tensors.size() + input_tensors.size());
-    TT_FATAL(
-        output_tensors.size() == 1, "matmul mcast in1 requires 1 output tensor, {} provided", output_tensors.size());
-
-    auto src_buffer_a = input_tensors.at(0).buffer();
-    auto src_buffer_b = input_tensors.at(1).buffer();
-    auto bias_tensor = optional_input_tensors.at(0);
-
-    std::optional<tt::tt_metal::Buffer*> bias_buffer;
-    if (bias_tensor.has_value()) {
-        bias_buffer = bias_tensor.value().buffer();
-    }
-
-    auto dst_buffer = output_tensors.at(0).buffer();
-
-    bool src0_sharded = input_tensors[0].is_sharded();
-    bool out_sharded = output_tensors[0].is_sharded();
-
-    auto& reader_runtime_args_by_core = GetRuntimeArgs(program, override_variables.kernels.at(0));
-
-    // Manually unroll sender core
-    {
-        // in0 sender
-        auto& reader_runtime_args =
-            reader_runtime_args_by_core[override_variables.start_core.x][override_variables.start_core.y];
-        reader_runtime_args[0] = src_buffer_a->address();
-
-        // in1 sender
-        auto& sender_writer_runtime_args =
-            GetRuntimeArgs(program, override_variables.kernels.at(1), override_variables.start_core);
-        sender_writer_runtime_args[0] = src_buffer_b->address();
-        sender_writer_runtime_args[6] = dst_buffer->address();
-        if (bias_tensor.has_value()) {
-            sender_writer_runtime_args[17] = (*bias_buffer)->address();
-        }
-    }
-
-    auto& receiver_writer_runtime_args_by_core = GetRuntimeArgs(program, override_variables.kernels.at(2));
-
-    for (uint32_t i = 1; i < override_variables.cores.size(); ++i) {
-        const CoreCoord& core = override_variables.cores[i];
-
-        auto& reader_runtime_args = reader_runtime_args_by_core[core.x][core.y];
-
-        auto& writer_runtime_args = receiver_writer_runtime_args_by_core[core.x][core.y];
-
-        // in0 sender
-        reader_runtime_args[0] = src_buffer_a->address();
-        // in1 receiver
-        writer_runtime_args[2] = dst_buffer->address();
-    }
-
-    if (src0_sharded) {
-        if (override_variables.extract_shard_sub_blocks) {
-            UpdateDynamicCircularBufferAddress(program, override_variables.cbs.at(1), *src_buffer_a);
-        } else {
-            UpdateDynamicCircularBufferAddress(program, override_variables.cbs.at(0), *src_buffer_a);
-        }
-    }
-
-    if (out_sharded) {
-        UpdateDynamicCircularBufferAddress(program, override_variables.cbs.at(2), *dst_buffer);
-    }
-}
-
-inline void override_mcast_in0_program_parameters(
-    const ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t& override_variables,
-    const void* operation,
-    tt_metal::Program& program,
-    const std::vector<tt::tt_metal::Tensor>& input_tensors,
-    const std::vector<std::optional<const tt::tt_metal::Tensor>>& optional_input_tensors,
-    const std::vector<tt::tt_metal::Tensor>& output_tensors) {
-    TT_FATAL(
-        input_tensors.size() + optional_input_tensors.size() == 3,
-        "mcast in0 requires 3 input tensors, {} + {} = {} provided",
-        input_tensors.size(),
-        optional_input_tensors.size(),
-        optional_input_tensors.size() + input_tensors.size());
-    TT_FATAL(
-        output_tensors.size() == 1, "matmul mcast in0 requires 1 output tensor, {} provided", output_tensors.size());
-
-    auto src_buffer_a = input_tensors.at(0).buffer();
-    auto src_buffer_b = input_tensors.at(1).buffer();
-    auto bias_tensor = optional_input_tensors.at(0);
-
-    std::optional<tt::tt_metal::Buffer*> bias_buffer;
-    if (bias_tensor.has_value()) {
-        bias_buffer = bias_tensor.value().buffer();
-    }
-
-    auto dst_buffer = output_tensors.at(0).buffer();
-
-    bool src0_sharded = input_tensors[0].is_sharded();
-    bool src1_sharded = input_tensors[1].is_sharded();
-    bool out_sharded = output_tensors[0].is_sharded();
-
-    // Manually unroll sender core
-    if (src0_sharded) {
-        UpdateDynamicCircularBufferAddress(program, override_variables.cbs.at(1), *src_buffer_a);
-    } else {
-        // in0 sender
-        auto& reader_sender_runtime_args =
-            GetRuntimeArgs(program, override_variables.kernels.at(0), override_variables.start_core);
-        reader_sender_runtime_args[0] = src_buffer_a->address();
-    }
-
-    if (src1_sharded) {
-        UpdateDynamicCircularBufferAddress(program, override_variables.cbs.at(0), *src_buffer_b);
-    }
-
-    if (bias_tensor.has_value() && bias_tensor.value().is_sharded()) {
-        UpdateDynamicCircularBufferAddress(program, override_variables.cbs.at(2), *bias_buffer.value());
-    }
 
-    auto& writer_runtime_args_by_core = GetRuntimeArgs(program, override_variables.kernels.at(1));
-
-    for (uint32_t i = 0; i < override_variables.num_cores_with_work; ++i) {
-        const auto& core = override_variables.cores[i];
-
-        auto& writer_runtime_args = writer_runtime_args_by_core[core.x][core.y];
+    auto override_runtime_arguments_callback =
+        [mm_kernel_in0_id, mm_kernel_in1_sender_writer_id, cb_src0, cb_src1, cb_outputs, num_cores, all_cores_vec](
+            const void* operation,
+            tt::tt_metal::Program& program,
+            const std::vector<tt::tt_metal::Tensor>& input_tensors,
+            const std::vector<std::optional<const tt::tt_metal::Tensor>>& optional_input_tensors,
+            const std::vector<tt::tt_metal::Tensor>& output_tensors) {
+            auto& global_cb = static_cast<const ttnn::operations::matmul::Matmul*>(operation)->global_cb;
+
+            if (!global_cb.has_value()) {
+                TT_FATAL(
+                    input_tensors.size() + optional_input_tensors.size() == 3,
+                    "Total number of input tensors (required ({}) + optional ({})) must be 3",
+                    input_tensors.size(),
+                    optional_input_tensors.size());
+                TT_FATAL(output_tensors.size() == 1, "Number of output tensors ({}) must be 1", output_tensors.size());
+            }
 
-        // in1 sender
-        writer_runtime_args[0] = src_buffer_b->address();
-        writer_runtime_args[6] = dst_buffer->address();
-        if (bias_tensor.has_value()) {
-            writer_runtime_args[17] = (*bias_buffer)->address();
-        }
-    }
+            auto src_buffer_a = input_tensors[0].buffer();
+            auto src_buffer_b = input_tensors[1].buffer();
+            auto dst_buffer = output_tensors[0].buffer();
 
-    if (out_sharded) {
-        UpdateDynamicCircularBufferAddress(program, override_variables.cbs.at(3), *dst_buffer);
-    }
-}
+            bool src0_sharded = input_tensors[0].is_sharded();
+            bool src1_sharded = input_tensors[1].is_sharded();
+            bool out_sharded = output_tensors[0].is_sharded();
 
-inline void override_gather_in0_program_parameters(
-    const ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t& override_variables,
-    const void* operation,
-    tt_metal::Program& program,
-    const std::vector<tt::tt_metal::Tensor>& input_tensors,
-    const std::vector<std::optional<const tt::tt_metal::Tensor>>& optional_input_tensors,
-    const std::vector<tt::tt_metal::Tensor>& output_tensors) {
-    auto& global_cb = static_cast<const ttnn::operations::matmul::Matmul*>(operation)->global_cb;
-
-    auto src_buffer_a = input_tensors[0].buffer();
-    auto src_buffer_b = input_tensors[1].buffer();
-    auto dst_buffer = output_tensors[0].buffer();
-
-    bool src0_sharded = input_tensors[0].is_sharded();
-    bool src1_sharded = input_tensors[1].is_sharded();
-    bool out_sharded = output_tensors[0].is_sharded();
-
-    // Manually unroll sender core
-    if (src0_sharded) {
-        UpdateDynamicCircularBufferAddress(program, override_variables.cbs[0], *src_buffer_a);
-    }
-    if (src1_sharded) {
-        if (!global_cb.has_value() && !src_buffer_b->is_dram()) {
-            UpdateDynamicCircularBufferAddress(program, override_variables.cbs[1], *src_buffer_b);
-        }
-    }
-    if (out_sharded) {
-        for (uint32_t i = 0; i < override_variables.cbs.size() - 2; ++i) {
-            // cbs 0 and 1 contain cb_src0 and cb_src1
-            // the rest contains the actual output cbs
-            const auto& cb_output = override_variables.cbs[i + 2];
-            const auto& out_buffer = output_tensors[i].buffer();
-            UpdateDynamicCircularBufferAddress(program, cb_output, *out_buffer);
-        }
-    }
+            // Manually unroll sender core
+            if (src0_sharded) {
+                UpdateDynamicCircularBufferAddress(program, cb_src0, *src_buffer_a);
+            }
+            if (src1_sharded) {
+                if (!global_cb.has_value() && !src_buffer_b->is_dram()) {
+                    UpdateDynamicCircularBufferAddress(program, cb_src1, *src_buffer_b);
+                }
+            }
+            if (out_sharded) {
+                for (uint32_t i = 0; i < cb_outputs.size(); ++i) {
+                    const auto& cb_output = cb_outputs[i];
+                    const auto& out_buffer = output_tensors[i].buffer();
+                    UpdateDynamicCircularBufferAddress(program, cb_output, *out_buffer);
+                }
+            }
 
-    if (not src1_sharded) {
-        auto& writer_runtime_args_by_core = GetRuntimeArgs(program, override_variables.kernels.at(0));
-        for (uint32_t i = 0; i < override_variables.cores.size(); ++i) {
-            const auto& core = override_variables.cores[i];
-            auto& writer_runtime_args = writer_runtime_args_by_core[core.x][core.y];
+            if (not src1_sharded) {
+                auto& writer_runtime_args_by_core = GetRuntimeArgs(program, mm_kernel_in1_sender_writer_id);
+                for (uint32_t i = 0; i < all_cores_vec.size(); ++i) {
+                    const auto& core = all_cores_vec[i];
+                    auto& writer_runtime_args = writer_runtime_args_by_core[core.x][core.y];
 
-            /* in1 */
-            writer_runtime_args[1] = src_buffer_b->address();
-        }
-    }
-}
+                    /* in1 */
+                    writer_runtime_args[1] = src_buffer_b->address();
+                }
+            }
+        };
 
-void override_program_parameters(
-    const ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t& override_variables,
-    const void* operation,
-    tt_metal::Program& program,
-    const std::vector<tt::tt_metal::Tensor>& input_tensors,
-    const std::vector<std::optional<const tt::tt_metal::Tensor>>& optional_input_tensors,
-    const std::vector<tt::tt_metal::Tensor>& output_tensors) {
-    switch (override_variables.type) {
-        case ttnn::operations::matmul::Matmul1DType::MCAST_IN0:
-            override_mcast_in0_program_parameters(
-                override_variables, operation, program, input_tensors, optional_input_tensors, output_tensors);
-            break;
-        case ttnn::operations::matmul::Matmul1DType::GATHER_IN0:
-            override_gather_in0_program_parameters(
-                override_variables, operation, program, input_tensors, optional_input_tensors, output_tensors);
-            break;
-        case ttnn::operations::matmul::Matmul1DType::MCAST_IN1:
-            override_mcast_in1_program_parameters(
-                override_variables, operation, program, input_tensors, optional_input_tensors, output_tensors);
-            break;
-    }
+    return {.program = std::move(program), .override_runtime_arguments_callback = override_runtime_arguments_callback};
 }
 
 }  // namespace reuse_mcast_1d_optimized_helpers
@@ -2430,8 +2381,8 @@ namespace operations {
 
 namespace matmul {
 
-ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t matmul_multi_core_reuse_mcast_1d_optimized_(
-    tt_metal::Program& program,
+tt::tt_metal::operation::ProgramWithCallbacks matmul_multi_core_reuse_mcast_1d_optimized_(
+    tt::tt_metal::Program& program,
     const Tensor& a,
     const std::vector<Tensor>& b_tensors,
     const std::optional<const Tensor>& bias,
@@ -2455,9 +2406,7 @@ ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t matmul_mul
     std::optional<ttnn::experimental::ccl::MatmulFusedOpSignaler>& fused_op_signaler,
     const std::optional<const tt::tt_metal::experimental::GlobalCircularBuffer>& global_cb,
     uint32_t num_global_cb_receivers,
-    const std::optional<tt::tt_metal::SubDeviceId>& sub_device_id,
-    uint32_t start_cb_index,
-    std::optional<CoreRangeSet> restricted_cores) {
+    const std::optional<tt::tt_metal::SubDeviceId>& sub_device_id) {
     const auto b = b_tensors[0];
     const auto output = output_tensors[0];
 
@@ -2562,7 +2511,7 @@ ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t matmul_mul
         for (const auto& output_tensor : output_tensors) {
             out_buffers.push_back(output_tensor.buffer());
         }
-        return reuse_mcast_1d_optimized_helpers::process_gather_in0_program_and_create_override_variables(
+        return reuse_mcast_1d_optimized_helpers::create_program_gather_in0(
             program,
             a,
             b_tensors,
@@ -2573,7 +2522,6 @@ ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t matmul_mul
             packer_l1_acc,
             dst_full_sync_en,
             compute_with_storage_grid_size,
-            start_cb_index,
             B,
             Mt,
             Nt,
@@ -2598,12 +2546,11 @@ ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t matmul_mul
             untilize_out,
             global_cb,
             num_global_cb_receivers,
-            sub_device_id,
-            restricted_cores);
+            sub_device_id);
     }
-    TT_FATAL(start_cb_index == tt::CBIndex::c_0, "mcast does not support a non-zero start cb index");
+
     if (mcast_in0) {
-        return reuse_mcast_1d_optimized_helpers::process_mcast_in0_program_and_create_override_variables(
+        return reuse_mcast_1d_optimized_helpers::create_program_mcast_in0(
             program,
             a,
             device,
@@ -2644,7 +2591,7 @@ ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t matmul_mul
             untilize_out,
             fused_op_signaler);
     } else {
-        return reuse_mcast_1d_optimized_helpers::process_mcast_in1_program_and_create_override_variables(
+        return reuse_mcast_1d_optimized_helpers::create_program_mcast_in1(
             program,
             a,
             device,
@@ -2711,51 +2658,36 @@ tt::tt_metal::operation::ProgramWithCallbacks matmul_multi_core_reuse_mcast_1d_o
     tt_metal::Program program{}; /* Create a program */
     std::optional<ttnn::experimental::ccl::MatmulFusedOpSignaler> empty_fused_op_signaler;
 
-    ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t shared_vars =
-        matmul_multi_core_reuse_mcast_1d_optimized_(
-            program,
-            a,
-            b_tensors,
-            bias,
-            output_tensors,
-            broadcast_batch,
-            compute_with_storage_grid_size,
-            compute_kernel_config,
-            in0_block_w,
-            out_subblock_h,
-            out_subblock_w,
-            out_block_h,
-            out_block_w,
-            per_core_M,
-            per_core_N,
-            fuse_batch,
-            std::move(fused_activation),
-            mcast_in0,
-            gather_in0,
-            std::move(hop_cores),
-            untilize_out,
-            empty_fused_op_signaler,
-            global_cb,
-            num_global_cb_receivers,
-            sub_device_id,
-            tt::CBIndex::c_0,
-            std::nullopt);
-    auto override_runtime_arguments_callback =
-        [shared_vars](
-            const void* operation,
-            tt_metal::Program& program,
-            const std::vector<tt::tt_metal::Tensor>& input_tensors,
-            const std::vector<std::optional<const tt::tt_metal::Tensor>>& optional_input_tensors,
-            const std::vector<tt::tt_metal::Tensor>& output_tensors) {
-            reuse_mcast_1d_optimized_helpers::override_program_parameters(
-                shared_vars, operation, program, input_tensors, optional_input_tensors, output_tensors);
-        };
-
-    return {.program = std::move(program), .override_runtime_arguments_callback = override_runtime_arguments_callback};
+    return matmul_multi_core_reuse_mcast_1d_optimized_(
+        program,
+        a,
+        b_tensors,
+        bias,
+        output_tensors,
+        broadcast_batch,
+        compute_with_storage_grid_size,
+        compute_kernel_config,
+        in0_block_w,
+        out_subblock_h,
+        out_subblock_w,
+        out_block_h,
+        out_block_w,
+        per_core_M,
+        per_core_N,
+        fuse_batch,
+        std::move(fused_activation),
+        mcast_in0,
+        gather_in0,
+        std::move(hop_cores),
+        untilize_out,
+        empty_fused_op_signaler,
+        global_cb,
+        num_global_cb_receivers,
+        sub_device_id);
 }
 
-ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t matmul_multi_core_reuse_mcast_1d_optimized_helper(
-    tt_metal::Program& program,
+tt::tt_metal::operation::ProgramWithCallbacks matmul_multi_core_reuse_mcast_1d_optimized_helper(
+    tt::tt_metal::Program& program,
     const Tensor& a,
     const std::vector<Tensor>& b_tensors,
     const std::optional<const Tensor>& bias,
@@ -2766,9 +2698,7 @@ ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t matmul_mul
     bool untilize_out,
     std::optional<ttnn::experimental::ccl::MatmulFusedOpSignaler>& fused_op_signaler,
     const std::optional<const tt::tt_metal::experimental::GlobalCircularBuffer>& global_cb,
-    const std::optional<tt::tt_metal::SubDeviceId>& sub_device_id,
-    uint32_t start_cb_index,
-    std::optional<CoreRangeSet> restricted_cores) {
+    const std::optional<tt::tt_metal::SubDeviceId>& sub_device_id) {
     MatmulMultiCoreReuseMultiCast1DProgramConfig config =
         std::get<MatmulMultiCoreReuseMultiCast1DProgramConfig>(program_config);
 
@@ -2797,52 +2727,7 @@ ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t matmul_mul
         fused_op_signaler,
         global_cb,
         config.num_global_cb_receivers,
-        sub_device_id,
-        start_cb_index,
-        restricted_cores);
-}
-
-tt::tt_metal::operation::ProgramWithCallbacks matmul_multi_core_reuse_mcast_1d_optimized_helper(
-    tt_metal::Program& program,
-    const Tensor& a,
-    const std::vector<Tensor>& b_tensors,
-    const std::optional<const Tensor>& bias,
-    const std::vector<Tensor>& output_tensors,
-    bool broadcast_batch,
-    DeviceComputeKernelConfig compute_kernel_config,
-    const MatmulProgramConfig& program_config,
-    bool untilize_out,
-    std::optional<ttnn::experimental::ccl::MatmulFusedOpSignaler>& fused_op_signaler,
-    const std::optional<const tt::tt_metal::experimental::GlobalCircularBuffer>& global_cb,
-    const std::optional<tt::tt_metal::SubDeviceId>& sub_device_id) {
-    ttnn::operations::matmul::matmul_mcast_1d_common_override_variables_t shared_vars =
-        matmul_multi_core_reuse_mcast_1d_optimized_helper(
-            program,
-            a,
-            b_tensors,
-            bias,
-            output_tensors,
-            broadcast_batch,
-            compute_kernel_config,
-            program_config,
-            untilize_out,
-            fused_op_signaler,
-            global_cb,
-            sub_device_id,
-            tt::CBIndex::c_0,
-            std::nullopt);
-    auto override_runtime_arguments_callback =
-        [shared_vars](
-            const void* operation,
-            tt_metal::Program& program,
-            const std::vector<tt::tt_metal::Tensor>& input_tensors,
-            const std::vector<std::optional<const tt::tt_metal::Tensor>>& optional_input_tensors,
-            const std::vector<tt::tt_metal::Tensor>& output_tensors) {
-            reuse_mcast_1d_optimized_helpers::override_program_parameters(
-                shared_vars, operation, program, input_tensors, optional_input_tensors, output_tensors);
-        };
-
-    return {.program = std::move(program), .override_runtime_arguments_callback = override_runtime_arguments_callback};
+        sub_device_id);
 }
 
 }  // namespace matmul
diff --git a/ttnn/cpp/ttnn/operations/moreh/CMakeLists.txt b/ttnn/cpp/ttnn/operations/moreh/CMakeLists.txt
index 264bb20313..8958f7f803 100644
--- a/ttnn/cpp/ttnn/operations/moreh/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/moreh/CMakeLists.txt
@@ -1,14 +1,8 @@
 add_library(ttnn_op_moreh ${LIB_TYPE})
-add_library(TTNN::Ops::Moreh ALIAS ttnn_op_moreh)
+add_library(TT::NN::Ops::Moreh ALIAS ttnn_op_moreh)
 
 target_precompile_headers(ttnn_op_moreh REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_moreh)
-set_target_properties(
-    ttnn_op_moreh
-    PROPERTIES
-        VERIFY_INTERFACE_HEADER_SETS
-            FALSE
-)
 
 target_sources(
     ttnn_op_moreh
@@ -142,35 +136,13 @@ target_sources(
         moreh_clip_grad_norm/moreh_clip_grad_norm_step3/device/moreh_clip_grad_norm_step3_program_factory.cpp
         moreh_clip_grad_norm/moreh_clip_grad_norm.cpp
 )
-file(GLOB_RECURSE kernels moreh_dot/device/kernels/*)
-target_sources(
-    ttnn_op_moreh
-    PUBLIC
-        FILE_SET kernels
-        TYPE HEADERS
-        BASE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}
-        FILES
-            ${kernels}
-            moreh_sum/device/moreh_sum_w_impl_kernels/moreh_sum_w.cpp
-            moreh_sum/device/moreh_sum_w_impl_kernels/reader_moreh_sum_w.cpp
-            moreh_sum/device/moreh_sum_w_impl_kernels/writer_moreh_sum_w.cpp
-)
 
 target_include_directories(ttnn_op_moreh PRIVATE ${FixmeOpIncDirs})
 target_link_libraries(
     ttnn_op_moreh
     PRIVATE
         TT::Metalium
-        TTNN::Core
-)
-
-install(
-    TARGETS
-        ttnn_op_moreh
-    FILE_SET
-    kernels
-        DESTINATION ${CMAKE_INSTALL_LIBEXECDIR}/tt-metalium/ttnn/cpp/ttnn/operations/moreh
-        COMPONENT ttnn-runtime
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_moreh LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/moreh/moreh_getitem/device/moreh_getitem_tilized_kernels/reader_moreh_getitem_tilize_w.cpp b/ttnn/cpp/ttnn/operations/moreh/moreh_getitem/device/moreh_getitem_tilized_kernels/reader_moreh_getitem_tilize_w.cpp
index a2299e3987..ade9f74f98 100644
--- a/ttnn/cpp/ttnn/operations/moreh/moreh_getitem/device/moreh_getitem_tilized_kernels/reader_moreh_getitem_tilize_w.cpp
+++ b/ttnn/cpp/ttnn/operations/moreh/moreh_getitem/device/moreh_getitem_tilized_kernels/reader_moreh_getitem_tilize_w.cpp
@@ -2,7 +2,6 @@
 //
 // SPDX-License-Identifier: Apache-2.0
 
-#include <algorithm>
 #include "dataflow_api.h"
 #include "cpp/ttnn/operations/moreh/moreh_getitem/device/moreh_getitem_tilized_kernels/common.hpp"
 
@@ -141,7 +140,7 @@ void kernel_main() {
         uint32_t index_w_index = i % num_alignment_width;
         uint32_t index_off = index_w_index * num_elements_per_alignment;
         uint32_t index_start = index_off;
-        uint32_t index_end = std::min(index_off + num_elements_per_alignment, index_size_w);
+        uint32_t index_end = min(index_off + num_elements_per_alignment, index_size_w);
 
         uint32_t j = 0;
         for (uint32_t index_index = index_start; index_index < index_end; index_index++, j++) {
diff --git a/ttnn/cpp/ttnn/operations/moreh/moreh_getitem/device/moreh_getitem_tilized_kernels/writer_moreh_getitem_tilize_w.cpp b/ttnn/cpp/ttnn/operations/moreh/moreh_getitem/device/moreh_getitem_tilized_kernels/writer_moreh_getitem_tilize_w.cpp
index 4faacac81f..03dbd34f3b 100644
--- a/ttnn/cpp/ttnn/operations/moreh/moreh_getitem/device/moreh_getitem_tilized_kernels/writer_moreh_getitem_tilize_w.cpp
+++ b/ttnn/cpp/ttnn/operations/moreh/moreh_getitem/device/moreh_getitem_tilized_kernels/writer_moreh_getitem_tilize_w.cpp
@@ -2,7 +2,6 @@
 //
 // SPDX-License-Identifier: Apache-2.0
 
-#include <algorithm>
 #include "dataflow_api.h"
 #include "cpp/ttnn/operations/moreh/moreh_getitem/device/moreh_getitem_tilized_kernels/common.hpp"
 
@@ -50,7 +49,7 @@ void kernel_main() {
         uint32_t output_stick_w = i % num_alignment_width;
         uint32_t w_off = output_stick_w * num_elements_per_alignment;
         uint32_t w_start = w_off;
-        uint32_t w_end = std::min(w_off + num_elements_per_alignment, output_size_w_without_padding);
+        uint32_t w_end = min(w_off + num_elements_per_alignment, output_size_w_without_padding);
 
         uint32_t stick_y = (i / num_alignment_width);
         uint32_t stick_x = w_start / FACE_WIDTH;
diff --git a/ttnn/cpp/ttnn/operations/moreh/moreh_nll_loss/moreh_nll_loss_step2/device/kernels/reader_moreh_nll_loss_step2_2d.cpp b/ttnn/cpp/ttnn/operations/moreh/moreh_nll_loss/moreh_nll_loss_step2/device/kernels/reader_moreh_nll_loss_step2_2d.cpp
index 4cdd0e257a..09b8e9f08a 100644
--- a/ttnn/cpp/ttnn/operations/moreh/moreh_nll_loss/moreh_nll_loss_step2/device/kernels/reader_moreh_nll_loss_step2_2d.cpp
+++ b/ttnn/cpp/ttnn/operations/moreh/moreh_nll_loss/moreh_nll_loss_step2/device/kernels/reader_moreh_nll_loss_step2_2d.cpp
@@ -2,7 +2,6 @@
 //
 // SPDX-License-Identifier: Apache-2.0
 
-#include <algorithm>
 #include "cpp/ttnn/deprecated/tt_dnn/kernels/dataflow/moreh_common.hpp"
 
 void kernel_main() {
@@ -73,7 +72,7 @@ void kernel_main() {
     for (uint32_t i = start_id; i < end_id; ++i) {
         // loop from n_start to n_end
         uint32_t n_start = i * TILE_HEIGHT;
-        uint32_t n_end = std::min(i * TILE_HEIGHT + TILE_HEIGHT, N);
+        uint32_t n_end = min(i * TILE_HEIGHT + TILE_HEIGHT, N);
         uint32_t nt = i;
 
         // target: (1, N)
diff --git a/ttnn/cpp/ttnn/operations/moreh/moreh_nll_loss/moreh_nll_loss_step2/device/kernels/reader_moreh_nll_loss_step2_3d.cpp b/ttnn/cpp/ttnn/operations/moreh/moreh_nll_loss/moreh_nll_loss_step2/device/kernels/reader_moreh_nll_loss_step2_3d.cpp
index 3843ad68cf..7c54fafbea 100644
--- a/ttnn/cpp/ttnn/operations/moreh/moreh_nll_loss/moreh_nll_loss_step2/device/kernels/reader_moreh_nll_loss_step2_3d.cpp
+++ b/ttnn/cpp/ttnn/operations/moreh/moreh_nll_loss/moreh_nll_loss_step2/device/kernels/reader_moreh_nll_loss_step2_3d.cpp
@@ -2,7 +2,6 @@
 //
 // SPDX-License-Identifier: Apache-2.0
 
-#include <algorithm>
 #include "cpp/ttnn/deprecated/tt_dnn/kernels/dataflow/moreh_common.hpp"
 
 void kernel_main() {
@@ -106,7 +105,7 @@ void kernel_main() {
         auto tmp_input_l1_ptr = get_write_ptr<FP32_DEST_ACC_FTYPE>(cb_tmp_input);
         auto target_l1_ptr = get_read_ptr<int32_t>(cb_target);
 
-        uint32_t idx_max = std::min(w + FACE_WIDTH, W);
+        uint32_t idx_max = min(w + FACE_WIDTH, W);
         for (uint32_t idx = 0; idx < idx_max; idx++) {
             int32_t target_val = target_l1_ptr[idx];
 
diff --git a/ttnn/cpp/ttnn/operations/normalization/CMakeLists.txt b/ttnn/cpp/ttnn/operations/normalization/CMakeLists.txt
index 4d5df29389..fd05d612c0 100644
--- a/ttnn/cpp/ttnn/operations/normalization/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/normalization/CMakeLists.txt
@@ -1,14 +1,8 @@
 add_library(ttnn_op_normalization ${LIB_TYPE})
-add_library(TTNN::Ops::Normalization ALIAS ttnn_op_normalization)
+add_library(TT::NN::Ops::Normalization ALIAS ttnn_op_normalization)
 
 target_precompile_headers(ttnn_op_normalization REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_normalization)
-set_target_properties(
-    ttnn_op_normalization
-    PROPERTIES
-        VERIFY_INTERFACE_HEADER_SETS
-            FALSE
-)
 
 target_sources(
     ttnn_op_normalization
@@ -33,31 +27,12 @@ target_sources(
         rmsnorm/rmsnorm.cpp
 )
 
-file(GLOB_RECURSE kernels softmax/device/kernels/*)
-target_sources(
-    ttnn_op_normalization
-    PUBLIC
-        FILE_SET kernels
-        TYPE HEADERS
-        BASE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}
-        FILES ${kernels}
-)
-
 target_include_directories(ttnn_op_normalization PRIVATE ${FixmeOpIncDirs})
 target_link_libraries(
     ttnn_op_normalization
     PRIVATE
         TT::Metalium
-        TTNN::Core
-)
-
-install(
-    TARGETS
-        ttnn_op_normalization
-    FILE_SET
-    kernels
-        DESTINATION ${CMAKE_INSTALL_LIBEXECDIR}/tt-metalium/ttnn/cpp/ttnn/operations/normalization
-        COMPONENT ttnn-runtime
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_normalization LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/pool/CMakeLists.txt b/ttnn/cpp/ttnn/operations/pool/CMakeLists.txt
index c1a86f288d..22afd47f8a 100644
--- a/ttnn/cpp/ttnn/operations/pool/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/pool/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_pool ${LIB_TYPE})
-add_library(TTNN::Ops::Pool ALIAS ttnn_op_pool)
+add_library(TT::NN::Ops::Pool ALIAS ttnn_op_pool)
 
 target_precompile_headers(ttnn_op_pool REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_pool)
@@ -27,7 +27,7 @@ target_link_libraries(
     ttnn_op_pool
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_pool LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/compute/pool_2d_multi_core_large_kernel.cpp b/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/compute/pool_2d_multi_core_large_kernel.cpp
index 0315561e7c..bdaf0952b8 100644
--- a/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/compute/pool_2d_multi_core_large_kernel.cpp
+++ b/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/compute/pool_2d_multi_core_large_kernel.cpp
@@ -64,7 +64,6 @@ template <
     uint32_t num_output_tiles,
     bool is_partial_tile,
     uint32_t max_rows_for_reduction,
-    uint32_t unpA_face_r_dim,
     bool neginf_srca_maxpool,
     bool zero_srca_avgpool>
 inline void reduce_h_fused(const uint32_t interm_cb_id, const uint32_t in_scalar_cb_id, const uint32_t out_cb_id) {
@@ -81,7 +80,7 @@ inline void reduce_h_fused(const uint32_t interm_cb_id, const uint32_t in_scalar
         num_output_tiles,
         0 /*tile idx for Src b is 0 because only 1 tile of constants is loaded*/,
         num_faces_in_input_tile /* unpack 1 or 2 faces ) */,
-        unpA_face_r_dim);
+        max_rows_for_reduction);
     for (uint32_t c_i = 0; c_i < num_output_tiles; ++c_i) {
         reduce_tile_math(c_i, num_faces_in_input_tile /* reduce 1 or 2 faces */);
     }
@@ -120,8 +119,6 @@ void MAIN {
     constexpr uint32_t interm_cb_id = get_compile_time_arg_val(15);
     constexpr uint32_t in_one_cb_id = get_compile_time_arg_val(16);
     constexpr bool one_scalar_per_core = get_compile_time_arg_val(17);
-    constexpr uint32_t sync_cb_id1 = get_compile_time_arg_val(18);
-    constexpr uint32_t sync_cb_id2 = get_compile_time_arg_val(19);
 
     constexpr bool is_partial_tile = in_c < 32;
     static_assert((!is_partial_tile || (in_c == 16)), "Partial tile must have c_dim 16");
@@ -139,18 +136,19 @@ void MAIN {
     constexpr bool neginf_srca_maxpool = (REDUCE_OP == PoolType::MAX) ? true : false;
     constexpr bool zero_srca_avgpool = (REDUCE_OP == PoolType::SUM) ? true : false;
 
-    constexpr uint32_t face_r_dim = 16;
     tilizeA_B_reduce_init<neginf_srca_maxpool, zero_srca_avgpool>(
-        in_cb_id_0, in_scalar_cb_id_0, max_tiles_per_iter, interm_cb_id, num_faces_in_input_tile, face_r_dim);
+        in_cb_id_0,
+        in_scalar_cb_id_0,
+        max_tiles_per_iter,
+        interm_cb_id,
+        num_faces_in_input_tile,
+        max_rows_for_reduction);
 
     constexpr uint32_t remaining_elems = window_size_hw % max_rows_for_reduction;
     constexpr uint32_t interm_reduction_chunks =
         remaining_elems ? window_size_hw / max_rows_for_reduction + 1 : window_size_hw / max_rows_for_reduction;
-
-    // wait for initialization to complete
-    cb_wait_front(sync_cb_id1, 2);
-    if constexpr (split_reader) {
-        cb_wait_front(sync_cb_id2, 2);
+    if constexpr (one_scalar_per_core) {
+        cb_wait_front(in_scalar_cb_id_0, 1);
     }
 
     for (uint32_t i = 0; i < nsticks_per_core_by_nblocks; ++i) {
@@ -173,7 +171,7 @@ void MAIN {
                     is_partial_tile,
                     max_rows_for_reduction,
                     split_reader,
-                    face_r_dim,
+                    max_rows_for_reduction,
                     neginf_srca_maxpool,
                     zero_srca_avgpool>(in_cb_id_0, in_cb_id_1, curr_scalar_cb_id, i, h, interm_cb_id);
             }
@@ -186,7 +184,6 @@ void MAIN {
                 max_tiles_per_iter,
                 is_partial_tile,
                 max_rows_for_reduction,
-                face_r_dim,
                 neginf_srca_maxpool,
                 zero_srca_avgpool>(
                 interm_cb_id, REDUCE_OP == PoolType::MAX ? in_scalar_cb_id_0 : in_one_cb_id, out_cb_id);
@@ -203,7 +200,7 @@ void MAIN {
                 is_partial_tile,
                 max_rows_for_reduction,
                 split_reader,
-                face_r_dim,
+                max_rows_for_reduction,
                 neginf_srca_maxpool,
                 zero_srca_avgpool>(in_cb_id_0, in_cb_id_1, curr_scalar_cb_id, i, h, interm_cb_id);
         }
@@ -216,7 +213,6 @@ void MAIN {
             partial_iter_output_tiles,
             is_partial_tile,
             max_rows_for_reduction,
-            face_r_dim,
             neginf_srca_maxpool,
             zero_srca_avgpool>(interm_cb_id, REDUCE_OP == PoolType::MAX ? in_scalar_cb_id_0 : in_one_cb_id, out_cb_id);
         if constexpr (!one_scalar_per_core) {
diff --git a/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/dataflow/reader_pool2d_sharded_common.hpp b/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/dataflow/reader_pool2d_sharded_common.hpp
index 7013831aad..04607ce2b9 100644
--- a/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/dataflow/reader_pool2d_sharded_common.hpp
+++ b/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/dataflow/reader_pool2d_sharded_common.hpp
@@ -20,29 +20,3 @@ ALWI bool fill_with_val(uint32_t begin_addr, uint32_t n, uint16_t val, bool unco
 
     return true;
 }
-
-template <uint32_t cb_id, uint32_t clear_value_cb_id>
-FORCE_INLINE void clear_out_tiles() {
-    constexpr uint32_t tile_size = get_tile_size(cb_id);
-    const uint32_t num_pages = get_local_cb_interface(cb_id).fifo_num_pages;
-    const uint32_t num_tiles = get_local_cb_interface(cb_id).fifo_page_size / tile_size;
-    const uint64_t clear_value_addr = get_noc_addr(get_read_ptr(clear_value_cb_id));
-    uint64_t write_addr = get_noc_addr(get_write_ptr(cb_id));
-
-    for (uint32_t i = 0; i < num_tiles * num_pages; ++i) {
-        noc_async_read(clear_value_addr, write_addr, tile_size);
-        write_addr += tile_size;
-    }
-    noc_async_read_barrier();
-}
-
-template <uint32_t clear_value_cb_id, uint32_t num_tiles>
-FORCE_INLINE void clear_out_tiles(uint64_t write_addr, uint64_t clear_value_addr) {
-    constexpr uint32_t tile_size = get_tile_size(clear_value_cb_id);
-
-    for (uint32_t i = 0; i < num_tiles; ++i) {
-        noc_async_read(clear_value_addr, write_addr, tile_size);
-        write_addr += tile_size;
-    }
-    noc_async_read_barrier();
-}
diff --git a/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/dataflow/reader_pool_2d_multi_core_sharded.cpp b/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/dataflow/reader_pool_2d_multi_core_sharded.cpp
index 74f0330297..061353473d 100644
--- a/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/dataflow/reader_pool_2d_multi_core_sharded.cpp
+++ b/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/dataflow/reader_pool_2d_multi_core_sharded.cpp
@@ -13,6 +13,32 @@
 #include "debug/dprint_pages.h"
 #endif
 
+template <uint32_t cb_id, uint32_t clear_value_cb_id>
+FORCE_INLINE void clear_out_tiles() {
+    constexpr uint32_t tile_size = get_tile_size(cb_id);
+    const uint32_t num_pages = get_local_cb_interface(cb_id).fifo_num_pages;
+    const uint32_t num_tiles = get_local_cb_interface(cb_id).fifo_page_size / tile_size;
+    const uint64_t clear_value_addr = get_noc_addr(get_read_ptr(clear_value_cb_id));
+    uint64_t write_addr = get_noc_addr(get_write_ptr(cb_id));
+
+    for (uint32_t i = 0; i < num_tiles * num_pages; ++i) {
+        noc_async_read(clear_value_addr, write_addr, tile_size);
+        write_addr += tile_size;
+    }
+    noc_async_read_barrier();
+}
+
+template <uint32_t clear_value_cb_id, uint32_t num_tiles>
+FORCE_INLINE void clear_out_tiles(uint64_t write_addr, uint64_t clear_value_addr) {
+    constexpr uint32_t tile_size = get_tile_size(clear_value_cb_id);
+
+    for (uint32_t i = 0; i < num_tiles; ++i) {
+        noc_async_read(clear_value_addr, write_addr, tile_size);
+        write_addr += tile_size;
+    }
+    noc_async_write_barrier();
+}
+
 /**
  * Pool 2D (Max pool 2D and Avg pool 2D)
  */
diff --git a/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/dataflow/reader_pool_2d_multi_core_sharded_with_halo_large_kernel_v2.cpp b/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/dataflow/reader_pool_2d_multi_core_sharded_with_halo_large_kernel_v2.cpp
index e5e0fef745..8382b1e68b 100644
--- a/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/dataflow/reader_pool_2d_multi_core_sharded_with_halo_large_kernel_v2.cpp
+++ b/ttnn/cpp/ttnn/operations/pool/generic/device/kernels/dataflow/reader_pool_2d_multi_core_sharded_with_halo_large_kernel_v2.cpp
@@ -44,7 +44,6 @@ void kernel_main() {
     constexpr uint32_t max_rows_for_reduction = get_compile_time_arg_val(14);
     constexpr uint32_t ceil_pad_w = get_compile_time_arg_val(15);
 
-    constexpr uint32_t TILE_HEIGHT = 32;
     constexpr uint32_t TILE_WIDTH = 32;
     constexpr uint32_t MAX_ELE_PER_REDUCTION = 512;  // TILE_WIDTH * 8 * numbytes
 
@@ -55,14 +54,8 @@ void kernel_main() {
     constexpr uint32_t in_scalar_cb_id_1 = get_compile_time_arg_val(21);
     constexpr uint32_t interm_reduction_cb_id = get_compile_time_arg_val(22);
     constexpr uint32_t in_one_cb_id = get_compile_time_arg_val(23);
-    constexpr uint32_t clear_value_cb_id = get_compile_time_arg_val(24);
-    constexpr bool is_avg_pool = (bool)get_compile_time_arg_val(25);
     constexpr bool one_scalar_per_core = get_compile_time_arg_val(26);
     constexpr uint32_t config_cb_id = get_compile_time_arg_val(27);
-    constexpr uint32_t multi_buffering_factor = get_compile_time_arg_val(28);
-    constexpr uint32_t sync_cb_id1 = get_compile_time_arg_val(29);
-    constexpr uint32_t sync_cb_id2 = get_compile_time_arg_val(30);
-
     constexpr uint32_t in_scalar_cb_id =
         split_reader && reader_id == 1 && !one_scalar_per_core ? in_scalar_cb_id_1 : in_scalar_cb_id_0;
 
@@ -71,68 +64,21 @@ void kernel_main() {
     uint32_t scalar_end = 1;
     uint32_t scalar_value = 0;
 
-    constexpr uint32_t window_size_hw = window_h * window_w;
-    constexpr uint32_t remaining_elems = window_size_hw % max_rows_for_reduction;
-    constexpr uint32_t interm_reduction_chunks =
-        remaining_elems ? window_size_hw / max_rows_for_reduction + 1 : window_size_hw / max_rows_for_reduction;
-    // we only need to initialize the in_cb if we will not fill each multibuffering chunk with max_rows worth of data
-    constexpr bool need_to_initialize_in_cb = remaining_elems && interm_reduction_chunks <= multi_buffering_factor;
-    constexpr uint32_t in_cb_ntiles = in_cb_sz / (TILE_WIDTH * TILE_HEIGHT);  // only use the non-multi buffering size
-
-    // fill the clear cb
-    if constexpr (split_reader) {
-        constexpr uint32_t half_tile = TILE_HEIGHT * TILE_WIDTH / 2;
-        if constexpr (reader_id == 0) {
-            fill_with_val(get_write_ptr(clear_value_cb_id), half_tile, bf16_init_value);
-        } else {
-            fill_with_val(get_write_ptr(clear_value_cb_id) + 2 * half_tile, half_tile, bf16_init_value);  // 2 for bf16
-        }
-    } else {
-        if constexpr (reader_id == 0) {
-            fill_with_val(get_write_ptr(clear_value_cb_id), TILE_HEIGHT * TILE_WIDTH, bf16_init_value);
-        }
-    }
-
-    // ensure the clear CB is full before proceeding
-    if constexpr (reader_id == 0) {
-        cb_push_back(sync_cb_id1, 1);
-        if constexpr (split_reader) {
-            cb_wait_front(sync_cb_id2, 1);
-        }
-    } else {
-        cb_push_back(sync_cb_id2, 1);
-        cb_wait_front(sync_cb_id1, 1);
-    }
-
-    if constexpr (need_to_initialize_in_cb && !is_avg_pool) {  // for avg pool fill_with_val runs in loop, no need to
-                                                               // initialize
-        clear_out_tiles<in_cb_id, clear_value_cb_id>();
-    }
-
     if constexpr (reader_id == 0) {
         constexpr uint32_t bf16_one_u16 = bf16_one_u32 >> 16;
-        // initialize buffers
-        clear_out_tiles<interm_reduction_cb_id, clear_value_cb_id>();
+        // fill interm buffer with init_value
+        fill_with_val(get_write_ptr(interm_reduction_cb_id), in_cb_sz, bf16_init_value);
         if constexpr (one_scalar_per_core) {
+            cb_reserve_back(in_scalar_cb_id_0, 1);
             fill_with_val(get_write_ptr(in_scalar_cb_id_0), TILE_WIDTH, bf16_scalar >> 16);
+            cb_push_back(in_scalar_cb_id_0, 1);
         }
-        if constexpr (is_avg_pool) {
-            // for avgpool, we use a one's CB to avoid double division by kernel size for large kernel case.
+        if (bf16_scalar != bf16_one_u32 || !one_scalar_per_core) {
+            // Pool operation is not maxpool
             fill_with_val(get_write_ptr(in_one_cb_id), TILE_WIDTH, bf16_one_u16);
         }
     }
 
-    // ensure initialization is done before proceeding
-    if constexpr (reader_id == 0) {
-        cb_push_back(sync_cb_id1, 1);
-        if constexpr (split_reader) {
-            cb_wait_front(sync_cb_id2, 2);
-        }
-    } else {
-        cb_push_back(sync_cb_id2, 1);
-        cb_wait_front(sync_cb_id1, 2);
-    }
-
     const uint32_t in_l1_read_base_addr = get_read_ptr(in_shard_cb_id);
     uint32_t reader_indices_l1_addr = get_read_ptr(in_reader_indices_cb_id);
     volatile tt_l1_ptr uint16_t* reader_indices_ptr =
@@ -144,6 +90,7 @@ void kernel_main() {
 
     uint32_t counter = reader_id;
     constexpr uint32_t total_elems_to_reduce = window_h * window_w;
+    constexpr uint32_t remaining_elems = total_elems_to_reduce % max_rows_for_reduction;
     constexpr bool wide_reduction = in_nblocks_c > 1;
     constexpr uint32_t read_bytes =
         wide_reduction ? MAX_ELE_PER_REDUCTION : in_nbytes_c;  // in_cb is MAX_ELE_PER_REDUCTION for wide reductions
@@ -198,17 +145,9 @@ void kernel_main() {
                         cb_push_back(in_cb_id, 1);
                         cb_reserve_back(in_cb_id, 1);
                         out_l1_write_addr = get_write_ptr(in_cb_id);
-                        // If next is last chunk, fill whole buffer with the init_value. note for max pool we do
-                        // not need to fill the CB for the partial chunk since as long as we have N>1 chunks we
-                        // are guaranteed that the junk data remaining from chunk N-1 will fill the entire CB and
-                        // cannot contain values greater than the max value, and if we have N=1 chunks we already
-                        // initialized the entire CB with the init value, but for avg pool we need to fill the
-                        // entire CB with the init value since the junk data will contribute to the average.
-                        if constexpr (is_avg_pool) {
-                            if ((total_elems_to_reduce - processed_rows) < max_rows_for_reduction) {
-                                clear_out_tiles<clear_value_cb_id, in_cb_ntiles>(
-                                    get_noc_addr(out_l1_write_addr), get_noc_addr(get_read_ptr(clear_value_cb_id)));
-                            }
+                        // If next is last chunk, fill whole buffer with the init_value.
+                        if ((total_elems_to_reduce - processed_rows) < max_rows_for_reduction) {
+                            fill_with_val(out_l1_write_addr, in_cb_sz, bf16_init_value);
                         }
                     }
                 }
diff --git a/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp b/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp
index 48e1a40d7e..00e2ac582a 100644
--- a/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp
+++ b/ttnn/cpp/ttnn/operations/pool/generic/device/pool_multi_core_program_factory.cpp
@@ -296,14 +296,9 @@ Pool2D::MultiCore::cached_program_t pool2d_multi_core_sharded_with_halo_v2_impl_
     const bool is_large_kernel =
         is_partial_tile ? kernel_size_hw > tt::constants::TILE_HEIGHT / 2 : kernel_size_hw > tt::constants::TILE_HEIGHT;
 
-    // TODO: enable 32 sticks per tile for reduction for all cases, we can only support 16 row reductions for
-    // partial tiles, and there is currently a bug forcing us to use 16 row reductions for avg pool when there
-    // is 1 remainder C tile
+    // ToDo: enable 32 sticks per tile for reduction for all cases.
     const uint32_t max_rows_for_reduction =
-        !is_partial_tile && !(is_wide_reduction && pool_type == Pool2DType::AVG_POOL2D &&
-                              in_ntiles_c % MAX_TILES_PER_REDUCTION == 1)
-            ? tt::constants::TILE_HEIGHT
-            : tt::constants::TILE_HEIGHT / 2;
+        (!is_partial_tile && !is_large_kernel) ? tt::constants::TILE_HEIGHT : tt::constants::TILE_HEIGHT / 2;
     TT_FATAL(nblocks == 1, "Multiple blocks not yet supported");
 
     if (input_shape[3] < tt::constants::TILE_WIDTH) {
@@ -365,22 +360,14 @@ Pool2D::MultiCore::cached_program_t pool2d_multi_core_sharded_with_halo_v2_impl_
     }
 
     uint32_t clear_value_cb_id = 32;
-    if (max_rows_for_reduction == tt::constants::TILE_HEIGHT || is_large_kernel ||
-        (is_wide_reduction && in_ntiles_c % MAX_TILES_PER_REDUCTION != 0)) {
+    if (max_rows_for_reduction == tt::constants::TILE_HEIGHT) {
         // CB storing just "clear value" (-inf for maxpool, 0 for avgpool)
-        // is needed only if we use more then 16 sticks per tile for reduction
-        // or if we use large kernel size.
+        // is needed only if we use more then 16 sticks per tile for reduction.
         clear_value_cb_id = next_cb_index++;
         tt::tt_metal::create_cb(clear_value_cb_id, program, all_cores, tile_size(in_df), 1, in_df);
         log_debug(tt::LogOp, "CB {} :: PS = {}, NP = {}", clear_value_cb_id, tile_size(in_df), 1);
     }
 
-    // CBs for NC/BR synchornization
-    int32_t sync_cb_id1 = next_cb_index++;
-    auto sync_cb1 = tt::tt_metal::create_cb(sync_cb_id1, program, all_cores, 2, 2, tt::DataFormat::UInt16);
-    int32_t sync_cb_id2 = next_cb_index++;
-    auto sync_cb2 = tt::tt_metal::create_cb(sync_cb_id2, program, all_cores, 2, 2, tt::DataFormat::UInt16);
-
     // incoming data is the input cb instead of raw l1/dram addr
     // this input shard has halo and padding inserted.
     const uint32_t raw_in_cb_npages = input.shard_spec().value().shape[0];
@@ -454,7 +441,7 @@ Pool2D::MultiCore::cached_program_t pool2d_multi_core_sharded_with_halo_v2_impl_
     uint32_t max_pool_partials_cb_id = 32;
     if (is_large_kernel) {
         max_pool_partials_cb_id = next_cb_index++;  // max_pool partials
-        const uint32_t max_pool_partials_cb_pagesize = in_cb_pagesize;
+        const uint32_t max_pool_partials_cb_pagesize = out_cb_pagesize;
         const uint32_t max_pool_partials_cb_npages = nblocks;
 
         tt::tt_metal::create_cb(
@@ -553,10 +540,7 @@ Pool2D::MultiCore::cached_program_t pool2d_multi_core_sharded_with_halo_v2_impl_
         clear_value_cb_id,
         (uint32_t)pool_type,
         one_scalar_per_core,
-        config_cb_id,
-        multi_buffering_factor,
-        sync_cb_id1,
-        sync_cb_id2};
+        config_cb_id};
     std::vector<uint32_t> reader1_ct_args = reader0_ct_args;
     reader1_ct_args[8] = 1;  // split reader id for reader1
 
@@ -605,9 +589,7 @@ Pool2D::MultiCore::cached_program_t pool2d_multi_core_sharded_with_halo_v2_impl_
         out_cb_id,
         max_pool_partials_cb_id,
         in_one_cb_id,
-        one_scalar_per_core,
-        sync_cb_id1,
-        sync_cb_id2};
+        one_scalar_per_core};
 
     auto compute_config = tt::tt_metal::ComputeConfig{
         .math_fidelity = MathFidelity::HiFi4,
diff --git a/ttnn/cpp/ttnn/operations/pool/upsample/device/kernels/dataflow/reader_bilinear_multi_core_sharded.cpp b/ttnn/cpp/ttnn/operations/pool/upsample/device/kernels/dataflow/reader_bilinear_multi_core_sharded.cpp
index e483fcc9e8..e57a4f8d13 100644
--- a/ttnn/cpp/ttnn/operations/pool/upsample/device/kernels/dataflow/reader_bilinear_multi_core_sharded.cpp
+++ b/ttnn/cpp/ttnn/operations/pool/upsample/device/kernels/dataflow/reader_bilinear_multi_core_sharded.cpp
@@ -3,7 +3,6 @@
 // SPDX-License-Identifier: Apache-2.0
 
 #include "dataflow_api.h"
-#include <algorithm>
 #include "cpp/ttnn/deprecated/tt_dnn/kernels/dataflow/moreh_common.hpp"
 
 #define ALWI inline __attribute__((always_inline))
@@ -79,10 +78,10 @@ void kernel_main() {
 
                 uint32_t x1 = int(x);
                 uint32_t y1 = int(y);
-                uint32_t x2 = std::min(x1 + 1, in_w - 1);
+                uint32_t x2 = min(x1 + 1, in_w - 1);
                 uint32_t y2 = y1 + 1;
                 if (is_last_row) {
-                    y2 = std::min(y2, in_image_rows_per_core);  // if last row, y2 should be in_image_rows_per_core
+                    y2 = min(y2, in_image_rows_per_core);  // if last row, y2 should be in_image_rows_per_core
                 }
 
                 fill_four_val(
diff --git a/ttnn/cpp/ttnn/operations/prefetcher/CMakeLists.txt b/ttnn/cpp/ttnn/operations/prefetcher/CMakeLists.txt
index a3ca042c35..fc53f2a310 100644
--- a/ttnn/cpp/ttnn/operations/prefetcher/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/prefetcher/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_prefetcher ${LIB_TYPE})
-add_library(TTNN::Ops::Prefetcher ALIAS ttnn_op_prefetcher)
+add_library(TT::NN::Ops::Prefetcher ALIAS ttnn_op_prefetcher)
 
 target_precompile_headers(ttnn_op_prefetcher REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_prefetcher)
@@ -17,7 +17,7 @@ target_link_libraries(
     ttnn_op_prefetcher
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_prefetcher LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/reduction/CMakeLists.txt b/ttnn/cpp/ttnn/operations/reduction/CMakeLists.txt
index d42aab5c6a..bd710e2391 100644
--- a/ttnn/cpp/ttnn/operations/reduction/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/reduction/CMakeLists.txt
@@ -1,14 +1,8 @@
 add_library(ttnn_op_reduction ${LIB_TYPE})
-add_library(TTNN::Ops::Reduction ALIAS ttnn_op_reduction)
+add_library(TT::NN::Ops::Reduction ALIAS ttnn_op_reduction)
 
 target_precompile_headers(ttnn_op_reduction REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_reduction)
-set_target_properties(
-    ttnn_op_reduction
-    PROPERTIES
-        VERIFY_INTERFACE_HEADER_SETS
-            FALSE
-)
 
 target_sources(
     ttnn_op_reduction
@@ -38,31 +32,12 @@ target_sources(
         reduction_common/reduction_common.cpp
 )
 
-file(GLOB_RECURSE kernels argmax/device/kernels/*)
-target_sources(
-    ttnn_op_reduction
-    PUBLIC
-        FILE_SET kernels
-        TYPE HEADERS
-        BASE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}
-        FILES ${kernels}
-)
-
 target_include_directories(ttnn_op_reduction PRIVATE ${FixmeOpIncDirs})
 target_link_libraries(
     ttnn_op_reduction
     PRIVATE
         TT::Metalium
-        TTNN::Core
-)
-
-install(
-    TARGETS
-        ttnn_op_reduction
-    FILE_SET
-    kernels
-        DESTINATION ${CMAKE_INSTALL_LIBEXECDIR}/tt-metalium/ttnn/cpp/ttnn/operations/reduction
-        COMPONENT ttnn-runtime
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_reduction LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/reduction/argmax/device/kernels/reader_argmax_interleaved_multicore.cpp b/ttnn/cpp/ttnn/operations/reduction/argmax/device/kernels/reader_argmax_interleaved_multicore.cpp
index f82a0fe674..3dbe26ef3c 100644
--- a/ttnn/cpp/ttnn/operations/reduction/argmax/device/kernels/reader_argmax_interleaved_multicore.cpp
+++ b/ttnn/cpp/ttnn/operations/reduction/argmax/device/kernels/reader_argmax_interleaved_multicore.cpp
@@ -3,7 +3,6 @@
 // SPDX-License-Identifier: Apache-2.0
 
 #include <stdint.h>
-#include <algorithm>
 
 #include "dataflow_api.h"
 #include "utils/bfloat16.h"
@@ -65,7 +64,7 @@ inline void find_argmax_for_core(
                 max_val = val;
             } else if (val == max_val) {
                 auto full_idx = outer_idx * inner_dim_units * red_dim_units + j * red_dim_units + i;
-                max_idx = reduce_all ? std::min(max_idx, full_idx) : std::min(max_idx, i);
+                max_idx = reduce_all ? min(max_idx, full_idx) : min(max_idx, i);
             }
         }
 
diff --git a/ttnn/cpp/ttnn/operations/reduction/topk/device/kernels/compute/topk_local.cpp b/ttnn/cpp/ttnn/operations/reduction/topk/device/kernels/compute/topk_local.cpp
index cebdfc1000..59cd76653b 100644
--- a/ttnn/cpp/ttnn/operations/reduction/topk/device/kernels/compute/topk_local.cpp
+++ b/ttnn/cpp/ttnn/operations/reduction/topk/device/kernels/compute/topk_local.cpp
@@ -46,7 +46,6 @@ void MAIN {
 
     ckernel::topk_tile_init();
     transpose_wh_init(input_cb_index, input_transposed_cb_index);
-    transpose_wh_init(index_cb_index, index_transposed_cb_index);
 
     bool switch_dir = (K == 64);
     int seq_per_2tiles = std::max((2 * 32) / K, (uint32_t)2);
diff --git a/ttnn/cpp/ttnn/operations/reduction/topk/device/kernels/dataflow/writer_local_topk.cpp b/ttnn/cpp/ttnn/operations/reduction/topk/device/kernels/dataflow/writer_local_topk.cpp
index 5365d0f751..54935991d5 100644
--- a/ttnn/cpp/ttnn/operations/reduction/topk/device/kernels/dataflow/writer_local_topk.cpp
+++ b/ttnn/cpp/ttnn/operations/reduction/topk/device/kernels/dataflow/writer_local_topk.cpp
@@ -70,5 +70,4 @@ void kernel_main() {
         // set the receiver ready semaphore to invalid until the receiver is ready to receive data
         noc_semaphore_set(receiver_semaphore_addr, INVALID);
     }
-    noc_async_atomic_barrier();
 }
diff --git a/ttnn/cpp/ttnn/operations/reduction/topk/device/topk_op.cpp b/ttnn/cpp/ttnn/operations/reduction/topk/device/topk_op.cpp
index a6d9898640..f2394c34d3 100644
--- a/ttnn/cpp/ttnn/operations/reduction/topk/device/topk_op.cpp
+++ b/ttnn/cpp/ttnn/operations/reduction/topk/device/topk_op.cpp
@@ -13,9 +13,9 @@ static inline uint32_t largest_power_of_two(std::uint32_t x) { return x == 0 ? 0
 
 static inline bool verify_multi_core_cost(
     const std::vector<Tensor>& input_tensors,
-    uint32_t width,
-    uint32_t min_dim,
-    uint32_t max_dim,
+    uint16_t width,
+    uint16_t min_dim,
+    uint16_t max_dim,
     uint32_t k,
     const CoreRangeSet& core_range_set) {
     auto device = input_tensors.at(0).device();
@@ -27,10 +27,10 @@ static inline bool verify_multi_core_cost(
 
     const auto core_range = core_range_set.ranges().at(0);
     const auto max_cores = core_range.end_coord.y - core_range.start_coord.y - 1;
-    uint32_t start_split_size = width / largest_power_of_two(max_cores);
-    for (uint32_t split_size = start_split_size; split_size <= max_dim; split_size *= 2) {
-        uint32_t rem = width % split_size;
-        uint32_t num_cores = width / split_size + (rem > 0);
+    uint16_t start_split_size = width / largest_power_of_two(max_cores);
+    for (uint16_t split_size = start_split_size; split_size <= max_dim; split_size *= 2) {
+        uint16_t rem = width % split_size;
+        uint16_t num_cores = width / split_size + (rem > 0);
         uint32_t memory_cost_gather =
             2 * num_cores * (value_tile_size + index_tile_size);  // gathering one index and one value tile from each
                                                                   // local core, allocating two CBs for each
@@ -178,7 +178,17 @@ operation::ProgramWithCallbacks TopK::create_program(
 
     multicore_supported &= (this->k <= 64);  // old implementation cannot handle k>64
 
-    if (multicore_supported) {
+    if (!multicore_supported) {
+        return detail::topk_single_core_interleaved(
+            input_tensor,
+            this->k,
+            this->dim,
+            this->largest,
+            this->sorted,
+            this->sub_core_grids,
+            output_tensors.at(0),
+            output_tensors.at(1));
+    } else {
         return detail::topk_multicore_interleaved(
             input_tensor,
             this->k,
@@ -189,16 +199,6 @@ operation::ProgramWithCallbacks TopK::create_program(
             output_tensors.at(0),
             output_tensors.at(1));
     }
-
-    return detail::topk_single_core_interleaved(
-        input_tensor,
-        this->k,
-        this->dim,
-        this->largest,
-        this->sorted,
-        this->sub_core_grids,
-        output_tensors.at(0),
-        output_tensors.at(1));
 }
 
 }  // namespace ttnn::operations::reduction
diff --git a/ttnn/cpp/ttnn/operations/sliding_window/CMakeLists.txt b/ttnn/cpp/ttnn/operations/sliding_window/CMakeLists.txt
index 0c6b1fc7ee..451fa243c8 100644
--- a/ttnn/cpp/ttnn/operations/sliding_window/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/sliding_window/CMakeLists.txt
@@ -1,14 +1,8 @@
 add_library(ttnn_op_sliding_window ${LIB_TYPE})
-add_library(TTNN::Ops::SlidingWindow ALIAS ttnn_op_sliding_window)
+add_library(TT::NN::Ops::SlidingWindow ALIAS ttnn_op_sliding_window)
 
 target_precompile_headers(ttnn_op_sliding_window REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_sliding_window)
-set_target_properties(
-    ttnn_op_sliding_window
-    PROPERTIES
-        VERIFY_INTERFACE_HEADER_SETS
-            FALSE
-)
 
 target_sources(
     ttnn_op_sliding_window
@@ -19,30 +13,12 @@ target_sources(
         sliding_window.cpp
 )
 
-file(GLOB_RECURSE kernels halo/device/kernels/*)
-target_sources(
-    ttnn_op_sliding_window
-    PUBLIC
-        FILE_SET kernels
-        TYPE HEADERS
-        BASE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}
-        FILES ${kernels}
-)
-
 target_include_directories(ttnn_op_sliding_window PRIVATE ${FixmeOpIncDirs})
 target_link_libraries(
     ttnn_op_sliding_window
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
-install(
-    TARGETS
-        ttnn_op_sliding_window
-    FILE_SET
-    kernels
-        DESTINATION ${CMAKE_INSTALL_LIBEXECDIR}/tt-metalium/ttnn/cpp/ttnn/operations/sliding_window
-        COMPONENT ttnn-runtime
-)
 install(TARGETS ttnn_op_sliding_window LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/transformer/CMakeLists.txt b/ttnn/cpp/ttnn/operations/transformer/CMakeLists.txt
index 42f21395ca..57a94643a3 100644
--- a/ttnn/cpp/ttnn/operations/transformer/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/transformer/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_transformer ${LIB_TYPE})
-add_library(TTNN::Ops::Transformer ALIAS ttnn_op_transformer)
+add_library(TT::NN::Ops::Transformer ALIAS ttnn_op_transformer)
 
 target_precompile_headers(ttnn_op_transformer REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_transformer)
@@ -25,7 +25,7 @@ target_link_libraries(
     ttnn_op_transformer
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_transformer LIBRARY COMPONENT tar)
diff --git a/ttnn/cpp/ttnn/operations/uniform/CMakeLists.txt b/ttnn/cpp/ttnn/operations/uniform/CMakeLists.txt
index 0627fdb2b9..4dce0dc7ab 100644
--- a/ttnn/cpp/ttnn/operations/uniform/CMakeLists.txt
+++ b/ttnn/cpp/ttnn/operations/uniform/CMakeLists.txt
@@ -1,5 +1,5 @@
 add_library(ttnn_op_uniform ${LIB_TYPE})
-add_library(TTNN::Ops::Uniform ALIAS ttnn_op_uniform)
+add_library(TT::NN::Ops::Uniform ALIAS ttnn_op_uniform)
 
 target_precompile_headers(ttnn_op_uniform REUSE_FROM TT::CommonPCH)
 TT_ENABLE_UNITY_BUILD(ttnn_op_uniform)
@@ -17,7 +17,7 @@ target_link_libraries(
     ttnn_op_uniform
     PRIVATE
         TT::Metalium
-        TTNN::Core
+        TT::NN::Core
 )
 
 install(TARGETS ttnn_op_uniform LIBRARY COMPONENT tar)
diff --git a/ttnn/test/CMakeLists.txt b/ttnn/test/CMakeLists.txt
deleted file mode 100644
index 26b03167d5..0000000000
--- a/ttnn/test/CMakeLists.txt
+++ /dev/null
@@ -1,16 +0,0 @@
-# Spell out the RPATH so that shlibdeps can find deps across package components.
-set(CMAKE_INSTALL_RPATH "$ORIGIN/../lib")
-
-# Smoke
-add_executable(tt-nn-validation-smoke)
-
-target_link_libraries(tt-nn-validation-smoke PRIVATE TTNN::Test::Smoke)
-
-install(TARGETS tt-nn-validation-smoke RUNTIME COMPONENT ttnn-validation)
-
-# Basic
-add_executable(tt-nn-validation-basic)
-
-target_link_libraries(tt-nn-validation-basic PRIVATE TTNN::Test::Basic)
-
-install(TARGETS tt-nn-validation-basic RUNTIME COMPONENT ttnn-validation)
diff --git a/ttnn/ttnn/__init__.py b/ttnn/ttnn/__init__.py
index 42aa700499..cab0902920 100644
--- a/ttnn/ttnn/__init__.py
+++ b/ttnn/ttnn/__init__.py
@@ -131,7 +131,7 @@ from ttnn._ttnn.global_circular_buffer import (
     create_global_circular_buffer,
 )
 
-from ttnn._ttnn.fabric import FabricConfig, set_fabric_config
+from ttnn._ttnn.fabric import FabricConfig, initialize_fabric_config
 
 from ttnn._ttnn.global_semaphore import (
     create_global_semaphore,
